{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from processing_pdf import processing_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the input and output path.\n",
    "> &#x26a0;&#xfe0f; **The last line will delete all files in the output folder.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\D'\n",
      "C:\\Users\\Kotorin\\AppData\\Local\\Temp\\ipykernel_19884\\826408512.py:2: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  dataset_path = \"F:\\Datasets/\"\n"
     ]
    }
   ],
   "source": [
    "# path to the folder containing PDF files (1512, removed researchpaper1.pdf)\n",
    "dataset_path = \"F:\\Datasets/\"\n",
    "# output folder and file name\n",
    "output_path = \"dataset2\"\n",
    "processing_pdf.clear_processed_folder(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicated files, only keep the papers (941) with newest version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the base filename without the version suffix\n",
    "def get_base_filename(filename):\n",
    "    match = re.match(r'^(.*?)v\\d\\.pdf+$', filename)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return filename\n",
    "\n",
    "# Function to find the newest version of each file\n",
    "def find_newest_versions(folder):\n",
    "    files_by_base_name = defaultdict(list)\n",
    "\n",
    "    # Group files by base filename\n",
    "    for filename in os.listdir(folder):\n",
    "        base_name = get_base_filename(filename)\n",
    "        files_by_base_name[base_name].append(filename)\n",
    "\n",
    "    # Find the newest version of each file\n",
    "    newest_versions = []\n",
    "    for base_name, filenames in files_by_base_name.items():\n",
    "        newest_version = max(filenames, key=lambda x: int(re.search(r'v(\\d+)\\.pdf$', x).group(1)))\n",
    "        newest_versions.append(newest_version)\n",
    "\n",
    "    return newest_versions\n",
    "\n",
    "# Function to remove duplicated files\n",
    "def remove_duplicates(folder):\n",
    "    newest_versions = find_newest_versions(folder)\n",
    "\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename not in newest_versions:\n",
    "            file_path = os.path.join(folder, filename)\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "\n",
    "remove_duplicates(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\D'\n",
      "C:\\Users\\Kotorin\\AppData\\Local\\Temp\\ipykernel_19884\\3393441614.py:2: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  training_data_path = \"F:\\Datasets/training/\"\n",
      "C:\\Users\\Kotorin\\AppData\\Local\\Temp\\ipykernel_19884\\3393441614.py:3: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  evaluation_data_path = \"F:\\Datasets/evaluation/\"\n",
      "C:\\Users\\Kotorin\\AppData\\Local\\Temp\\ipykernel_19884\\3393441614.py:4: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  test_data_path = \"F:\\Datasets/test/\"\n"
     ]
    }
   ],
   "source": [
    "# Split PDF files into different folders to use as training, evaluation and test data\n",
    "training_data_path = \"F:\\Datasets/training/\"\n",
    "evaluation_data_path = \"F:\\Datasets/evaluation/\"\n",
    "test_data_path = \"F:\\Datasets/test/\"\n",
    "\n",
    "# Define the number of PDF files we want to use for each set of data\n",
    "training_data_limit = 98\n",
    "evaluation_data_limit = 21\n",
    "test_data_limit = 21\n",
    "\n",
    "# Define the output file name\n",
    "training_data_output_file = \"dataset\"\n",
    "evaluation_data_output_file = \"dataset_eval\"\n",
    "test_data_output_file = \"dataset_test\"\n",
    "\n",
    "# Assign the variables to the corresponding dataset.\n",
    "dataset_path = test_data_path\n",
    "dataset_limit = test_data_limit\n",
    "output_file = test_data_output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Breakdown PDF content by sections and subsections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one_pdf(file_path):\n",
    "    doc, total_text, _ = processing_pdf.open_file(file_path)\n",
    "    table_of_content = doc.get_toc()\n",
    "\n",
    "    if len(table_of_content) > 0:\n",
    "        print(\"Auto generated table of content:\")\n",
    "        display(table_of_content)\n",
    "        # separate content into sections\n",
    "        _, json_dict = processing_pdf.separate_content(total_text, table_of_content)\n",
    "        return json_dict\n",
    "    # some papers have not table of content\n",
    "    #if len(table_of_content) == 0:\n",
    "    #    print(\"The paper has not table of content. Need to use regular expression to map table of content.\")\n",
    "    #    table_of_content = processing_pdf.auto_find_toc(doc)\n",
    "    #    display(table_of_content)\n",
    "    \n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PDF file: 1901.00141v1.pdf\n",
      "F:\\Datasets/test/1901.00141v1.pdf\n",
      "Processing PDF file: 1901.00142v2.pdf\n",
      "F:\\Datasets/test/1901.00142v2.pdf\n",
      "Processing PDF file: 1901.00143v1.pdf\n",
      "F:\\Datasets/test/1901.00143v1.pdf\n",
      "Processing PDF file: 1901.00144v2.pdf\n",
      "F:\\Datasets/test/1901.00144v2.pdf\n",
      "Auto generated table of content:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, '1. Introduction', 1],\n",
       " [1, '2. Topology of the tempered dual', 3],\n",
       " [1,\n",
       "  '3. Topology of the motion group dual and remarks on the Mackey-Higson bijection',\n",
       "  6],\n",
       " [1, '4. Continuity of the Mackey-Higson bijection', 8],\n",
       " [1, 'References', 11]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting looking for all the sections according to the provided section title info...\n",
      "Done with PDF file: 1901.00144v2.pdf\n",
      "# of sections: 6\n",
      "Total # of sections: 6 Total # of files: 1\n",
      "============================================================\n",
      "Processing PDF file: 1901.00145v3.pdf\n",
      "F:\\Datasets/test/1901.00145v3.pdf\n",
      "Auto generated table of content:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, '1. Introduction', 1],\n",
       " [1, '2. Preliminaries', 8],\n",
       " [1, '3. The Thom Isomorphism', 14],\n",
       " [1, '4. Poincaré Duality', 16],\n",
       " [1, '5. Doubling', 22],\n",
       " [1, '6. Finite Coverings', 24],\n",
       " [1, '7. Fibrations', 25],\n",
       " [1, '8. Historical Remarks', 27],\n",
       " [1, 'Appendix A. Skew-Commutativity of Cup Products', 28],\n",
       " [1, 'Appendix B. The Künneth Theorems', 30],\n",
       " [1, 'Appendix C. Proof of Theorem 3.6', 35],\n",
       " [1, 'References', 38]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting looking for all the sections according to the provided section title info...\n",
      "Done with PDF file: 1901.00145v3.pdf\n",
      "# of sections: 13\n",
      "Total # of sections: 19 Total # of files: 2\n",
      "============================================================\n",
      "Processing PDF file: 1901.00146v1.pdf\n",
      "F:\\Datasets/test/1901.00146v1.pdf\n",
      "Auto generated table of content:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, '1 Introduction', 1],\n",
       " [1, '2 Ground-based gamma-ray detectors', 1],\n",
       " [2, '2.1 Detection principle of Cherenkov telescopes', 1],\n",
       " [2, '2.2 Current Cherenkov telescopes', 2],\n",
       " [2, '2.3 Cherenkov Telescope Array', 2],\n",
       " [2, '2.4 Timing arrays', 3],\n",
       " [1, '3 Neutrino telescopes', 4],\n",
       " [2, '3.1 Low-energy neutrino detectors', 4],\n",
       " [2, '3.2 Deep-ice and deep-water neutrino telescopes', 5],\n",
       " [2, '3.3 Other neutrino detectors', 7],\n",
       " [1, '4 Cosmic-ray and hybrid detectors', 7],\n",
       " [1, '5 Conclusion and Outlook', 8]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting looking for all the sections according to the provided section title info...\n",
      "Done with PDF file: 1901.00146v1.pdf\n",
      "# of sections: 6\n",
      "Total # of sections: 25 Total # of files: 3\n",
      "============================================================\n",
      "Processing PDF file: 1901.00147v1.pdf\n",
      "F:\\Datasets/test/1901.00147v1.pdf\n",
      "Processing PDF file: 1901.00148v4.pdf\n",
      "F:\\Datasets/test/1901.00148v4.pdf\n",
      "Processing PDF file: 1901.00149v1.pdf\n",
      "F:\\Datasets/test/1901.00149v1.pdf\n",
      "Processing PDF file: 1901.00150v3.pdf\n",
      "F:\\Datasets/test/1901.00150v3.pdf\n",
      "Auto generated table of content:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, '1 Introduction', 1],\n",
       " [1, '2 Problem formulation', 5],\n",
       " [1, '3 Convergence rates', 7],\n",
       " [2, '3.1 General convergence theorems', 7],\n",
       " [2, '3.2 Maximum likelihood estimation', 8],\n",
       " [2, '3.3 Maximum a posteriori probability estimation', 9],\n",
       " [2, '3.4 Tightness of the rate of convergence', 10],\n",
       " [2, '3.5 A simple illustrative numerical example', 10],\n",
       " [1, '4 Accelerated MAP inference', 11],\n",
       " [2, '4.1 General convergence theorems', 11],\n",
       " [2, '4.2 Convergence rate for the BT model', 12],\n",
       " [1, '5 Numerical results', 14],\n",
       " [2, '5.1 Datasets', 14],\n",
       " [2, '5.2 Experimental results', 15],\n",
       " [1, '6 Further discussion', 17],\n",
       " [1, '7 Proofs and Additional Results', 18],\n",
       " [2, '7.1 Proof of Theorem 3.1', 18],\n",
       " [2, '7.2 Proof of Theorem 3.2', 19],\n",
       " [2, '7.3 Comparison of Theorem 3.2 with Proposition 2.7 in M15', 19],\n",
       " [2, '7.4 Proof of Lemma 3.1', 20],\n",
       " [2, '7.5 Proof of Lemma 3.3', 20],\n",
       " [2,\n",
       "  '7.6 Surrogate function (2.7) for the Bradley-Terry model is a first-order surrogate function',\n",
       "  21],\n",
       " [2, '7.7 Proof of Lemma 3.4', 21],\n",
       " [2, '7.8 The asymptote in Section 3.4', 21],\n",
       " [2, '7.9 Proof of Theorem 4.1', 22],\n",
       " [2, '7.10 Proof of Lemma 4.1', 22],\n",
       " [2, '7.11 Proof of Lemma 4.2', 23],\n",
       " [2, '7.12 Proof of Lemma 7.1', 23],\n",
       " [2, '7.13 Proof of Lemma 7.5', 24],\n",
       " [2, '7.14 Derivation of the convergence time bound (6.1)', 24],\n",
       " [2, '7.15 Generalized Bradley-Terry models', 25],\n",
       " [3, '7.15.1 Model definitions', 25],\n",
       " [3, '7.15.2 Rao-Kupper model', 26],\n",
       " [3, '7.15.3 Luce choice model', 26],\n",
       " [3, '7.15.4 Plackett-Luce ranking model', 27]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting looking for all the sections according to the provided section title info...\n",
      "Done with PDF file: 1901.00150v3.pdf\n",
      "# of sections: 8\n",
      "Total # of sections: 33 Total # of files: 4\n",
      "============================================================\n",
      "Processing PDF file: 1901.00151v1.pdf\n",
      "F:\\Datasets/test/1901.00151v1.pdf\n",
      "Auto generated table of content:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, '1 Introduction', 1],\n",
       " [1, '2 Anomalies in neutrino data', 3],\n",
       " [2, '2.1 Appearance and disappearance data', 3],\n",
       " [2,\n",
       "  '2.2 Interpretation of anomalies within the hypothesis of neutrino oscillation',\n",
       "  3],\n",
       " [1, '3 Concept of Sterile Neutrino', 3],\n",
       " [2, '3.1 Masses of fermions in the SM ', 3],\n",
       " [2, '3.2 Four right-handed neutrinos and three left-handed doublets', 4],\n",
       " [2, '3.3 How sterile neutrino state can be observed', 7],\n",
       " [2, '3.4 Loss of coherence for sterile neutrino', 8],\n",
       " [2, '3.5 Confusions in terminology', 8],\n",
       " [2, '3.6 Current status and perspectives', 8],\n",
       " [1, '4 Summary', 9],\n",
       " [1, '5 Acknowledgments', 9]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting looking for all the sections according to the provided section title info...\n",
      "Done with PDF file: 1901.00151v1.pdf\n",
      "# of sections: 6\n",
      "Total # of sections: 39 Total # of files: 5\n",
      "============================================================\n",
      "Processing PDF file: 1901.00152v1.pdf\n",
      "F:\\Datasets/test/1901.00152v1.pdf\n",
      "Auto generated table of content:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, '1. Introduction', 1], [1, '2. Results', 2], [1, 'References', 6]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting looking for all the sections according to the provided section title info...\n",
      "Done with PDF file: 1901.00152v1.pdf\n",
      "# of sections: 4\n",
      "Total # of sections: 43 Total # of files: 6\n",
      "============================================================\n",
      "Processing PDF file: 1901.00153v2.pdf\n",
      "F:\\Datasets/test/1901.00153v2.pdf\n",
      "Auto generated table of content:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1,\n",
       "  'Occupation time statistics of a gas of interacting diffusing particles',\n",
       "  1],\n",
       " [2, 'Abstract', 1],\n",
       " [2, 'I Introduction', 1],\n",
       " [2, 'II Occupation fraction: from a single particle to many', 1],\n",
       " [2, 'III Macroscopic fluctuation theory of occupation statistics', 3],\n",
       " [2, 'IV Non-interacting Random Walkers', 5],\n",
       " [2, 'V Simple Symmetric Exclusion Process ', 6],\n",
       " [3, 'A General', 6],\n",
       " [3, 'B Survival, =1', 8],\n",
       " [3, 'C Close packing, n= 1', 8],\n",
       " [3, 'D Dilute limit, n1', 8],\n",
       " [2, 'VI Zero Range Process', 8],\n",
       " [2, 'VII Occupation statistics on a ring', 9],\n",
       " [3, 'A ZRP on a ring: a dynamical phase transition', 9],\n",
       " [2, 'VIII Summary and Discussion', 11],\n",
       " [2, ' ACKNOWLEDGMENTS', 12],\n",
       " [2, 'A Dilute limit of the SSEP', 12],\n",
       " [2, 'B RWs on a ring', 12],\n",
       " [3, '1 General', 12],\n",
       " [3, '2 Gaussian fluctuations, -', 13],\n",
       " [3, '3 Close to survival, 1', 13],\n",
       " [3, '4 Three asymptotics', 14],\n",
       " [2, ' References', 14]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting looking for all the sections according to the provided section title info...\n",
      "Done with PDF file: 1901.00153v2.pdf\n",
      "# of sections: 2\n",
      "Total # of sections: 45 Total # of files: 7\n",
      "============================================================\n",
      "Processing PDF file: 1901.00154v1.pdf\n",
      "F:\\Datasets/test/1901.00154v1.pdf\n",
      "Auto generated table of content:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1,\n",
       "  'Hexagonal MASnI3 exhibiting strong absorption of ultraviolet photons',\n",
       "  1],\n",
       " [2, 'Abstract', 1]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting looking for all the sections according to the provided section title info...\n",
      "Done with PDF file: 1901.00154v1.pdf\n",
      "# of sections: 2\n",
      "Total # of sections: 47 Total # of files: 8\n",
      "============================================================\n",
      "Processing PDF file: 1901.00155v1.pdf\n",
      "F:\\Datasets/test/1901.00155v1.pdf\n",
      "Auto generated table of content:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, '1 Введение.', 1],\n",
       " [1, '2 Постановка задачи.', 2],\n",
       " [2, '2.1 Формальные определения и нотация.', 2],\n",
       " [2, '2.2 Последовательный алгоритм.', 2],\n",
       " [1, '3 Параллельный алгоритм поиска диссонансов PhiDD', 3],\n",
       " [2, '3.1 Проектирование алгоритма', 3],\n",
       " [2, '3.2 Реализация алгоритма', 4],\n",
       " [1, '4 Вычислительные эксперименты.', 6],\n",
       " [2, '4.1 Цели, аппаратная платформа и наборы данных экспериментов', 6],\n",
       " [2, '4.2 Результаты экспериментов', 8],\n",
       " [1, '5 Заключение.', 10]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting looking for all the sections according to the provided section title info...\n",
      "Done with PDF file: 1901.00155v1.pdf\n",
      "# of sections: 6\n",
      "Total # of sections: 53 Total # of files: 9\n",
      "============================================================\n",
      "Processing PDF file: 1901.00156v2.pdf\n",
      "F:\\Datasets/test/1901.00156v2.pdf\n",
      "Auto generated table of content:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, 'Introduction', 1],\n",
       " [1, 'Preliminaries', 2],\n",
       " [1, 'NP-hardness', 2],\n",
       " [1, 'The Edit-Sequence Approach', 5],\n",
       " [1, 'Critical Cliques', 7],\n",
       " [1, 'A 6k-vertex kernel ', 8],\n",
       " [1, 'An FPT algorithm', 10],\n",
       " [1, 'Conclusion', 11]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting looking for all the sections according to the provided section title info...\n",
      "Done with PDF file: 1901.00156v2.pdf\n",
      "# of sections: 9\n",
      "Total # of sections: 62 Total # of files: 10\n",
      "============================================================\n",
      "Processing PDF file: 1901.00157v2.pdf\n",
      "F:\\Datasets/test/1901.00157v2.pdf\n",
      "Auto generated table of content:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, 'I Introduction', 2],\n",
       " [1, 'II The Higgs Signal and Physics Background', 4],\n",
       " [2, 'A The Higgs Signal in Top Decay', 4],\n",
       " [2, 'B The Physics Background', 6],\n",
       " [2, 'C Mass Reconstruction', 7],\n",
       " [1, 'III Realistic Acceptance Cuts', 9],\n",
       " [1, 'IV Discovery Potential at the LHC', 10],\n",
       " [1, 'V Conclusions', 12],\n",
       " [1, ' Acknowledgments', 13],\n",
       " [1, ' References', 14]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting looking for all the sections according to the provided section title info...\n",
      "Done with PDF file: 1901.00157v2.pdf\n",
      "# of sections: 8\n",
      "Total # of sections: 70 Total # of files: 11\n",
      "============================================================\n",
      "Processing PDF file: 1901.00158v2.pdf\n",
      "F:\\Datasets/test/1901.00158v2.pdf\n",
      "Processing PDF file: 1901.00159v1.pdf\n",
      "F:\\Datasets/test/1901.00159v1.pdf\n",
      "Auto generated table of content:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1,\n",
       "  'Hard X-ray spectroscopy of the itinerant magnets RFe4Sb12 (R=Na, K, Ca, Sr, Ba)',\n",
       "  1],\n",
       " [2, 'Abstract', 1],\n",
       " [2, 'I Introduction', 1],\n",
       " [2, 'II Methods', 2],\n",
       " [2, 'III Results and discussion', 2],\n",
       " [2, 'IV Summary and Outlook', 6],\n",
       " [2, ' Acknowledgments', 6],\n",
       " [2, ' References', 6]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting looking for all the sections according to the provided section title info...\n",
      "Done with PDF file: 1901.00159v1.pdf\n",
      "# of sections: 2\n",
      "Total # of sections: 72 Total # of files: 12\n",
      "============================================================\n",
      "Processing PDF file: 1901.00160v4.pdf\n",
      "F:\\Datasets/test/1901.00160v4.pdf\n",
      "Auto generated table of content:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, '1 Introduction', 1],\n",
       " [1, '2 Puzzle 1: an inverse anisotropy', 2],\n",
       " [1, '3 Puzzle 2: peculiarity of the amplitude and phase', 2],\n",
       " [1, '4 Puzzle 3: the nature of the Single Source', 6],\n",
       " [1, '5 Conclusion', 8]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting looking for all the sections according to the provided section title info...\n",
      "Done with PDF file: 1901.00160v4.pdf\n",
      "# of sections: 6\n",
      "Total # of sections: 78 Total # of files: 13\n",
      "============================================================\n",
      "Processing PDF file: 1901.00161v3.pdf\n",
      "F:\\Datasets/test/1901.00161v3.pdf\n",
      "Auto generated table of content:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, '0. Introduction', 1],\n",
       " [1, '1. Preliminaries', 1],\n",
       " [1, '2. The boundness of (W,S,L)', 4],\n",
       " [1, '3. The lowest two-side cell c0', 6],\n",
       " [1, '4. The based ring of c0', 10],\n",
       " [1, 'References', 13]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting looking for all the sections according to the provided section title info...\n",
      "Done with PDF file: 1901.00161v3.pdf\n",
      "# of sections: 7\n",
      "Total # of sections: 85 Total # of files: 14\n",
      "============================================================\n",
      "Processing PDF file: 1901.00162v4.pdf\n",
      "F:\\Datasets/test/1901.00162v4.pdf\n",
      "Processing PDF file: 1901.00163v1.pdf\n",
      "F:\\Datasets/test/1901.00163v1.pdf\n",
      "Processing PDF file: 1901.00164v1.pdf\n",
      "F:\\Datasets/test/1901.00164v1.pdf\n",
      "Auto generated table of content:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, 'I Introduction', 1],\n",
       " [2, 'I-A Contributions', 2],\n",
       " [1, 'II Properties of minrank of a side-information graph', 3],\n",
       " [2, 'II-A A heuristic method to reduce the minrank computation problem', 6],\n",
       " [1, 'III Code construction for groupcast index coding problems', 8],\n",
       " [2, 'III-A Converting groupcast ICP into single unicast ICP', 8],\n",
       " [2,\n",
       "  'III-B Steps to construct index code for groupcast index coding problems',\n",
       "  9],\n",
       " [1, 'IV conclusion and discussions', 9],\n",
       " [1, 'References', 9],\n",
       " [1, 'Appendix', 11],\n",
       " [2, 'A Least Difference Greedy Clique-Cover Algorithm', 11],\n",
       " [2, 'B Extended Least Difference Greedy Clique-Cover Algorithm', 11],\n",
       " [2, 'C Algorithm description', 12],\n",
       " [2, 'D Computational complexity of Algorithm 1', 12]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting looking for all the sections according to the provided section title info...\n",
      "Done with PDF file: 1901.00164v1.pdf\n",
      "# of sections: 7\n",
      "Total # of sections: 92 Total # of files: 15\n",
      "============================================================\n",
      "Processing PDF file: 1901.00165v1.pdf\n",
      "F:\\Datasets/test/1901.00165v1.pdf\n",
      "Auto generated table of content:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, '1 Introduction', 2],\n",
       " [1, '2 Non-smooth analysis for locally Lipschitz functional', 9],\n",
       " [1, '3 Proof of Theorem ??', 16],\n",
       " [1, '4 Proof of Theorem ??', 22]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting looking for all the sections according to the provided section title info...\n",
      "Done with PDF file: 1901.00165v1.pdf\n",
      "# of sections: 5\n",
      "Total # of sections: 97 Total # of files: 16\n",
      "============================================================\n",
      "Processing PDF file: 1901.00166v1.pdf\n",
      "F:\\Datasets/test/1901.00166v1.pdf\n",
      "Auto generated table of content:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, 'I Introduction', 1],\n",
       " [1, 'II CNN Refresher', 1],\n",
       " [1, 'III The Capsule Network', 2],\n",
       " [2, 'III-A Primary Capsules', 2],\n",
       " [2, 'III-B Digit Capsules', 2],\n",
       " [2, 'III-C Dynamic routing', 2],\n",
       " [2, 'III-D Loss Function', 3],\n",
       " [2, 'III-E Regularization', 3],\n",
       " [1, 'IV Experimentations and Results', 3],\n",
       " [2, 'IV-A Datasets', 4],\n",
       " [2, 'IV-B Architecture and Hyperparameters', 4],\n",
       " [2, 'IV-C Result and Analysis', 4],\n",
       " [1, 'V Conclusion', 5],\n",
       " [1, 'References', 5]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting looking for all the sections according to the provided section title info...\n",
      "Done with PDF file: 1901.00166v1.pdf\n",
      "# of sections: 7\n",
      "Total # of sections: 104 Total # of files: 17\n",
      "============================================================\n",
      "Processing PDF file: 1901.00167v3.pdf\n",
      "F:\\Datasets/test/1901.00167v3.pdf\n",
      "Auto generated table of content:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, ' Contents', 1],\n",
       " [1, '1 Introduction', 2],\n",
       " [1, '2 The original model', 4],\n",
       " [1, '3 Evolution from kination to matter-radiation equality', 8],\n",
       " [2, '3.1 Decay before the end of kination', 9],\n",
       " [2, '3.2 Decay after the end of kination', 12],\n",
       " [1, '4 Evolution from the matter-radiation equality', 13],\n",
       " [1, '5 Exponential quintessence potential', 19],\n",
       " [1, '6 Concluding remarks', 23],\n",
       " [1, ' References', 24]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting looking for all the sections according to the provided section title info...\n",
      "Done with PDF file: 1901.00167v3.pdf\n",
      "# of sections: 9\n",
      "Total # of sections: 113 Total # of files: 18\n",
      "============================================================\n",
      "Processing PDF file: 1901.00168v1.pdf\n",
      "F:\\Datasets/test/1901.00168v1.pdf\n",
      "Auto generated table of content:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, 'Optimal Object Placement using a Virtual Axis', 1]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting looking for all the sections according to the provided section title info...\n",
      "Done with PDF file: 1901.00168v1.pdf\n",
      "# of sections: 2\n",
      "Total # of sections: 115 Total # of files: 19\n",
      "============================================================\n",
      "Processing PDF file: 1901.00169v1.pdf\n",
      "F:\\Datasets/test/1901.00169v1.pdf\n",
      "Processing PDF file: 1901.00170v1.pdf\n",
      "F:\\Datasets/test/1901.00170v1.pdf\n",
      "Auto generated table of content:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, '1. Introduction and main result', 1],\n",
       " [1, '2. Technical lemmas', 2],\n",
       " [1, '3. Proof of Theorem ??', 3],\n",
       " [1, '4. Extension to integer points close to smooth curves', 5],\n",
       " [1, 'References', 5]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting looking for all the sections according to the provided section title info...\n",
      "Done with PDF file: 1901.00170v1.pdf\n",
      "# of sections: 6\n",
      "Total # of sections: 121 Total # of files: 20\n",
      "============================================================\n",
      "Processing PDF file: 1901.00171v2.pdf\n",
      "F:\\Datasets/test/1901.00171v2.pdf\n",
      "Auto generated table of content:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, '1 Introduction', 1],\n",
       " [1, '2 Related Work', 2],\n",
       " [1, '3 Measurement and Observation', 2],\n",
       " [1, '4 DCA: Disparity-preserved Deep Cross-platform Association', 3],\n",
       " [2, '4.1 Problem Formulation', 3],\n",
       " [2, '4.2 Disparity-preserved Deep Cross-platform Association', 3],\n",
       " [1, '5 Cross-platform Video Recommendation', 4],\n",
       " [2, '5.1 Problem Formulation', 4],\n",
       " [2, '5.2 DCA-based Cross-platform Video Recommendation', 4],\n",
       " [1, '6 Experiments', 5],\n",
       " [2, '6.1 Dataset', 5],\n",
       " [2, '6.2 Experimental Settings', 5],\n",
       " [3, 'Linear Regression-based Association (LR)', 5],\n",
       " [3, 'Latent Attribute-based Association (LA)', 5],\n",
       " [3, 'MLP-based Nonlinear Mapping (MLP)', 5],\n",
       " [2, '6.3 Evaluation of Cross-platform Association', 5],\n",
       " [2, '6.4 Evaluation of Cross-platform Video Recommendation', 6],\n",
       " [1, '7 Conclusion', 6]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting looking for all the sections according to the provided section title info...\n",
      "Done with PDF file: 1901.00171v2.pdf\n",
      "# of sections: 8\n",
      "Total # of sections: 129 Total # of files: 21\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "file_count = 0\n",
    "all_json_dicts = []\n",
    "# Loop through files in the folder\n",
    "for file_name in os.listdir(dataset_path):\n",
    "    file_path = os.path.join(dataset_path, file_name)\n",
    "    # Check if it's a file and if it has a \".pdf\" extension\n",
    "    if os.path.isfile(file_path) and file_name.endswith('.pdf'):\n",
    "        # Process the PDF file\n",
    "        print(\"Processing PDF file:\", file_name)\n",
    "        json_dict = process_one_pdf(file_path)\n",
    "        if len(json_dict) > 0:\n",
    "            all_json_dicts.extend(list(json_dict.values()))\n",
    "            file_count += 1\n",
    "            print(\"Done with PDF file:\", file_name)\n",
    "            print(\"# of sections:\", len(json_dict.values()))\n",
    "            print(\"Total # of sections:\", len(all_json_dicts), \"Total # of files:\", file_count)\n",
    "            print(60*\"=\")\n",
    "        \n",
    "        # Terminate when reaching dataset limit\n",
    "        if file_count >= dataset_limit:\n",
    "            break\n",
    "\n",
    "json_list = json.dumps(all_json_dicts)\n",
    "output_file_path = f\"{output_path}/{output_file}.json\"\n",
    "with open(output_file_path, \"w\") as jsonfile: \n",
    "    jsonfile.write(json_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1.2: Generated data validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sections: 129\n",
      "Number of sections with empty Text: 23\n",
      "Number of sections with sub sections: 114\n"
     ]
    }
   ],
   "source": [
    "with open(f\"{output_path}/{output_file}.json\",  encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "total_objects = len(data)\n",
    "empty_text_count = 0\n",
    "sub_sections_exist_count = 0\n",
    "\n",
    "# Iterate over each section\n",
    "for obj in data:\n",
    "    if obj['Text'] == '' and not 'reference' in obj['Section'].lower():\n",
    "        empty_text_count += 1\n",
    "    if len(obj['Subsections']) < 1:\n",
    "        sub_sections_exist_count += 1\n",
    "\n",
    "# Print the results\n",
    "print(\"Total number of sections:\", total_objects)\n",
    "print(\"Number of sections with empty Text:\", empty_text_count)\n",
    "print(\"Number of sections with sub sections:\", sub_sections_exist_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Generate GPT summary for each section and subsection as ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_summary import get_summaries\n",
    "\n",
    "# output_file = training_data_output_file\n",
    "with open(f\"{output_path}/{output_file}.json\", encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "get_summaries(data)\n",
    "\n",
    "output_file_path_ground_truth = f\"{output_path}/{output_file}_ground_truth.json\"\n",
    "with open(output_file_path_ground_truth, 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
