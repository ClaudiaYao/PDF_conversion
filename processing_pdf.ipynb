{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "# ensure that the imported .py file will get auto imported and updated whenever there is a change\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyMuPDF in c:\\users\\claud\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.24.0)\n",
            "Requirement already satisfied: PyMuPDFb==1.24.0 in c:\\users\\claud\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from PyMuPDF) (1.24.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install PyMuPDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbxlvBEhC-kO",
        "outputId": "faebd136-d3f6-4d9b-aa34-09c270578605"
      },
      "outputs": [],
      "source": [
        "\n",
        "import fitz\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "_V2cOEPzSe5U"
      },
      "outputs": [],
      "source": [
        "\n",
        "project_path = os.getcwd()\n",
        "project_data_path = project_path + \"/data\"\n",
        "project_processed_data_path = project_path + \"/processed\"\n",
        "if not os.path.exists(project_data_path):\n",
        "    os.makedirs(project_data_path)\n",
        "if not os.path.exists(project_processed_data_path):\n",
        "    os.makedirs(project_processed_data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9of1D8dKPfj",
        "outputId": "7540a334-3420-47bb-e006-4d1dcd168097"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Claud\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Claud\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Claud\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize #Used to extract words from documents\n",
        "from nltk.stem import WordNetLemmatizer #Used to lemmatize words\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import pandas as pd\n",
        "pd.options.display.max_colwidth = 1000\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import processing_pdf\n",
        "pd.options.display.max_colwidth = 2000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 238,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Auto generated table of content:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[[1, '1 Introduction', 1],\n",
              " [1, '2 Related Work', 2],\n",
              " [1, '3 Method', 3],\n",
              " [2, '3.1 Vision Transformer (ViT)', 3],\n",
              " [2, '3.2 Fine-tuning and Higher Resolution', 4],\n",
              " [1, '4 Experiments', 4],\n",
              " [2, '4.1 Setup', 4],\n",
              " [2, '4.2 Comparison to State of the Art', 5],\n",
              " [2, '4.3 Pre-training Data Requirements', 6],\n",
              " [2, '4.4 Scaling Study', 8],\n",
              " [2, '4.5 Inspecting Vision Transformer', 8],\n",
              " [2, '4.6 Self-supervision', 8],\n",
              " [1, '5 Conclusion', 9],\n",
              " [1, 'A Multihead Self-attention', 13],\n",
              " [1, 'B Experiment details', 13],\n",
              " [2, 'B.1 Training', 13],\n",
              " [3, 'B.1.1 Fine-tuning', 13],\n",
              " [3, 'B.1.2 Self-supervision', 14],\n",
              " [1, 'C Additional Results', 14],\n",
              " [1, 'D Additional Analyses', 15],\n",
              " [2, 'D.1 SGD vs. Adam for ResNets', 15],\n",
              " [2, 'D.2 Transformer shape', 16],\n",
              " [2, 'D.3 Head Type and class token', 16],\n",
              " [2, 'D.4 Positional Embedding', 17],\n",
              " [2, 'D.5 Empirical Computational Costs', 18],\n",
              " [2, 'D.6 Axial Attention', 19],\n",
              " [2, 'D.7 Attention Distance', 20],\n",
              " [2, 'D.8 Attention Maps', 20],\n",
              " [2, 'D.9 ObjectNet Results', 20],\n",
              " [2, 'D.10 VTAB Breakdown', 20]]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# pdf_file =  \"An Empirical Survey on Long Document Summarization.pdf\"\n",
        "# pdf_file = \"1901.00009v1.pdf\"\n",
        "# pdf_file = \"1901.00936v3.pdf\"\n",
        "pdf_file = \"an image is worth 16 by 16 words.pdf\"\n",
        "doc, total_text, total_pages = processing_pdf.open_file(project_data_path + \"/\" + pdf_file)\n",
        "table_of_content = doc.get_toc()\n",
        "print(\"Auto generated table of content:\")\n",
        "display(table_of_content)\n",
        "\n",
        "# some papers have not table of content\n",
        "if len(table_of_content) == 0:\n",
        "    print(\"The paper has not table of content. Need to use regular expression to map table of content.\")\n",
        "    res = processing_pdf.auto_find_toc(doc)\n",
        "    print(\"Not satified the generated result or want to adjust? Build your own table of content by using this template:\")\n",
        "    display(res)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 239,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "starting looking for all the sections according to the provided section title info...\n",
            "save the dataframe to d:\\Start_Dec_19_2023\\application\\NUS Course\\Semester_1\\Neural Network and Deep Learning\\PDF_Conversion/processed/an image is worth 16 by 16 words.csv\n",
            "save the dataframe to d:\\Start_Dec_19_2023\\application\\NUS Course\\Semester_1\\Neural Network and Deep Learning\\PDF_Conversion/processed/an image is worth 16 by 16 words.json\n",
            "\n",
            "Title: AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\n",
            "\n",
            "Authors:Alexey Dosovitskiy∗,†, Lucas Beyer∗, Alexander Kolesnikov∗, Dirk Weissenborn∗,Xiaohua Zhai∗, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby∗,†\n",
            "\n",
            "Other info: ∗equal technical contribution, †equal advisingGoogle Research, Brain Team{adosovitskiy, neilhoulsby}@google.com\n",
            "\n",
            "Abstract: While the Transformer architecture has become the de-facto standard for naturallanguage processing tasks, its applications to computer vision remain limited. Invision, attention is either applied in conjunction with convolutional networks, orused to replace certain components of convolutional networks while keeping theiroverall structure in place. We show that this reliance on CNNs is not necessaryand a pure transformer applied directly to sequences of image patches can performvery well on image classiﬁcation tasks. When pre-trained on large amounts ofdata and transferred to multiple mid-sized or small image recognition benchmarks(ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellentresults compared to state-of-the-art convolutional networks while requiring sub-stantially fewer computational resources to train.1\n"
          ]
        }
      ],
      "source": [
        "#uncomment this list to customize table-of-content\n",
        "# table_of_content = [[1, 'I. INTRODUCTION', 1],\n",
        "#  [1, 'II. SFC BASED ON IPV6 SEGMENT ROUTING', 2],\n",
        "#  [1, 'III. DESIGN OF THE SRV6 PROXY', 4],\n",
        "#  [2, 'A. General Concepts and State-of-the-art', 4],\n",
        "#  [2, 'B. SRNKv1', 5],\n",
        "#  [2, 'C. SRNKv2', 7],\n",
        "#  [2, 'D. Implementation of other SR proxy types', 8],\n",
        "#  [1, 'IV. TESTING ENVIRONMENT', 8],\n",
        "#  [1, 'V. PERFORMANCE ANALYSIS', 9],\n",
        "#  [1, 'VII. CONCLUSIONS', 11]]\n",
        "\n",
        "# separate content into sections\n",
        "processing_pdf.clear_processed_folder(project_processed_data_path)\n",
        "title, authors, other_info, abstract = processing_pdf.find_meta_data(doc, table_of_content)\n",
        "ds, json_dict = processing_pdf.separate_content(total_text, table_of_content)\n",
        "processing_pdf.save_dataframe(ds, json_dict, project_processed_data_path,  pdf_file.rsplit(\".\", 1)[0])\n",
        "# extract images\n",
        "processing_pdf.find_images(doc, table_of_content, total_pages, project_processed_data_path)\n",
        "\n",
        "# display(ds)\n",
        "print(f\"\\nTitle: {title}\")\n",
        "print(f\"\\nAuthors:{authors}\")\n",
        "print(f\"\\nOther info: {other_info}\")\n",
        "print(f\"\\nAbstract:{abstract}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Title: AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\n",
            "\n",
            "Authors:Alexey Dosovitskiy∗,†, Lucas Beyer∗, Alexander Kolesnikov∗, Dirk Weissenborn∗,Xiaohua Zhai∗, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby∗,†\n",
            "\n",
            "Other info: ∗equal technical contribution, †equal advisingGoogle Research, Brain Team{adosovitskiy, neilhoulsby}@google.com\n",
            "\n",
            "Abstract: While the Transformer architecture has become the de-facto standard for naturallanguage processing tasks, its applications to computer vision remain limited. Invision, attention is either applied in conjunction with convolutional networks, orused to replace certain components of convolutional networks while keeping theiroverall structure in place. We show that this reliance on CNNs is not necessaryand a pure transformer applied directly to sequences of image patches can performvery well on image classiﬁcation tasks. When pre-trained on large amounts ofdata and transferred to multiple mid-sized or small image recognition benchmarks(ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellentresults compared to state-of-the-art convolutional networks while requiring sub-stantially fewer computational resources to train.1\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# open json file\n",
        "with open(project_processed_data_path + \"/an image is worth 16 by 16 words.json\") as f:\n",
        "    data = json.load(f)\n",
        "    for item in data:\n",
        "        display(item)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
