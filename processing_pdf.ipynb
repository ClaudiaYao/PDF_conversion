{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ensure that the imported .py file will get auto imported and updated whenever there is a change\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install PyMuPDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbxlvBEhC-kO",
        "outputId": "faebd136-d3f6-4d9b-aa34-09c270578605"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Claud\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Claud\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Claud\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import nltk, os, re\n",
        "from nltk.tokenize import word_tokenize #Used to extract words from documents\n",
        "from nltk.stem import WordNetLemmatizer #Used to lemmatize words\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import processing_pdf\n",
        "pd.options.display.max_colwidth = 1000\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_V2cOEPzSe5U"
      },
      "outputs": [],
      "source": [
        "\n",
        "project_path = os.getcwd()\n",
        "project_data_path = project_path + \"/data\"\n",
        "project_processed_data_path = project_path + \"/processed\"\n",
        "if not os.path.exists(project_data_path):\n",
        "    os.makedirs(project_data_path)\n",
        "if not os.path.exists(project_processed_data_path):\n",
        "    os.makedirs(project_processed_data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Auto generated table of content:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[[1, 'I Introduction', 1],\n",
              " [1, 'II  Modeling', 2],\n",
              " [2, 'A Assume f(t) proportional to (e+pe)', 3],\n",
              " [2, 'B Assume f(t) proportional to e', 5],\n",
              " [1, 'III Comparison with the early universe', 5],\n",
              " [1, 'IV Summary', 6],\n",
              " [1, ' References', 6]]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# pdf_file =  \"An Empirical Survey on Long Document Summarization.pdf\"\n",
        "# pdf_file = \"1901.00009v1.pdf\"\n",
        "pdf_file = \"1901.00002v1.pdf\"\n",
        "# pdf_file = \"1901.00936v3.pdf\"\n",
        "# pdf_file = \"an image is worth 16 by 16 words.pdf\"\n",
        "doc, total_text, total_pages = processing_pdf.open_file(project_data_path + \"/\" + pdf_file)\n",
        "\n",
        "# Use processing_pdf.auto_find_toc(), which will clean up the original toc\n",
        "# table_of_content = doc.get_toc()\n",
        "table_of_content = processing_pdf.auto_find_toc(doc)\n",
        "display(table_of_content)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "starting looking for all the sections according to the provided section title info...\n",
            "save the dataframe to d:\\Start_Dec_19_2023\\application\\NUS Course\\Semester_1\\Neural Network and Deep Learning\\PDF_Conversion/processed/1901.00002v1.csv\n",
            "save the dataframe to d:\\Start_Dec_19_2023\\application\\NUS Course\\Semester_1\\Neural Network and Deep Learning\\PDF_Conversion/processed/1901.00002v1_meta.csv\n",
            "save the dataframe to d:\\Start_Dec_19_2023\\application\\NUS Course\\Semester_1\\Neural Network and Deep Learning\\PDF_Conversion/processed/1901.00002v1.json\n",
            "\n",
            "Title: Black holes in the turbulence phase of viscous rip cosmology\n",
            "\n",
            "Authors:Iver Brevik1 and Mubasher Jamil2\n",
            "\n",
            "Other info: \n",
            "\n",
            "Abstract:1Department of Energy and Process Engineering, Norwegian University of Science and Technology, N-7491 Trondheim, Norway2Department of Mathematics, School of Natural Sciences (SNS),National University of Sciences and Technology (NUST), H-12, Islamabad, Pakistan(Dated: January 3, 2019) We study the phantom ﬂuid in the late universe, thus assuming the equation of state parameter wto be less than −1. The ﬂuid is assumed to consist of two components, one laminar component ρ andone turbulent component ρT , the latter set proportional to ρ as well as to the Hubble parameter, ρT =3τHρ with τ a positive constant associated with the turbulence. The effective energy density is takento be ρe = ρ + ρT , and the corresponding effective pressure is pe = wρe, with w constant. These basicassumptions lead to a Big Rip universe; the physical quantities diverging during a ﬁnite rip time ts.We then consider the mass accretion of a black hole in such a universe. The most natural assumptionof setting the rate dM/dt proportional to M 2 times the sum ρe+pe, leads to a negative mass accretion,where M(t) goes to zero linearly in (ts −t) near the singularity. The Hubble parameter diverges as(ts −t)−1, whereas ρe and pe diverge as (ts −t)−2. We also discuss other options and include, for thesake of comparison, some essential properties of mass accretion in the early (inﬂationary) universe. PACS numbers: 04.40.-b, 95.30.Sf, 98.62.Sb; Mathematics Subject Classiﬁcation 2010: 83F05 CosmologyKeywords: viscous universe, late universe, turbulent universe\n"
          ]
        }
      ],
      "source": [
        "#uncomment this list to customize table-of-content\n",
        "# table_of_content = [[1, 'I. INTRODUCTION', 1],\n",
        "#  [1, 'II. SFC BASED ON IPV6 SEGMENT ROUTING', 2],\n",
        "#  [1, 'III. DESIGN OF THE SRV6 PROXY', 4],\n",
        "#  [2, 'A. General Concepts and State-of-the-art', 4],\n",
        "#  [2, 'B. SRNKv1', 5],\n",
        "#  [2, 'C. SRNKv2', 7],\n",
        "#  [2, 'D. Implementation of other SR proxy types', 8],\n",
        "#  [1, 'IV. TESTING ENVIRONMENT', 8],\n",
        "#  [1, 'V. PERFORMANCE ANALYSIS', 9],\n",
        "#  [1, 'VII. CONCLUSIONS', 11]]\n",
        "\n",
        "# separate content into sections\n",
        "processing_pdf.clear_processed_folder(project_processed_data_path)\n",
        "title, authors, other_info, abstract = processing_pdf.find_meta_data(doc, table_of_content)\n",
        "df_meta = pd.DataFrame([title, abstract]).T\n",
        "df_meta.columns = [\"Title\", \"Abstract\"]\n",
        "ds, json_dict = processing_pdf.separate_content(total_text, table_of_content)\n",
        "processing_pdf.save_dataframe(ds, df_meta, json_dict, project_processed_data_path,  pdf_file.rsplit(\".\", 1)[0])\n",
        "# extract images\n",
        "processing_pdf.find_images(doc, table_of_content, total_pages, project_processed_data_path)\n",
        "\n",
        "# display(ds)\n",
        "print(f\"\\nTitle: {title}\")\n",
        "print(f\"\\nAuthors:{authors}\")\n",
        "print(f\"\\nOther info: {other_info}\")\n",
        "print(f\"\\nAbstract:{abstract}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# open json file\n",
        "with open(project_processed_data_path + \"/1901.00039v2.json\") as f:\n",
        "    data = json.load(f)\n",
        "    for item in data:\n",
        "        display(item)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
