{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "# ensure that the imported .py file will get auto imported and updated whenever there is a change\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install PyMuPDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbxlvBEhC-kO",
        "outputId": "faebd136-d3f6-4d9b-aa34-09c270578605"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Claud\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Claud\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Claud\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import nltk, os, re\n",
        "from nltk.tokenize import word_tokenize #Used to extract words from documents\n",
        "from nltk.stem import WordNetLemmatizer #Used to lemmatize words\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import processing_pdf\n",
        "pd.options.display.max_colwidth = 1000\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "_V2cOEPzSe5U"
      },
      "outputs": [],
      "source": [
        "\n",
        "project_path = os.getcwd()\n",
        "project_data_path = project_path + \"/data\"\n",
        "project_processed_data_path = project_path + \"/processed\"\n",
        "if not os.path.exists(project_data_path):\n",
        "    os.makedirs(project_data_path)\n",
        "if not os.path.exists(project_processed_data_path):\n",
        "    os.makedirs(project_processed_data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Auto generated table of content:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[[1, '1 Introduction', 1],\n",
              " [1, '2 Observations and Data reduction', 2],\n",
              " [1, '3 Variability Analysis', 3],\n",
              " [2, '3.1 Cross-matches to external catalogs', 4],\n",
              " [2, '3.2 Variability cutoffs', 4],\n",
              " [2, '3.3 Variability Classification', 4],\n",
              " [2, '3.4 Blending Corrections', 6],\n",
              " [1, '4 Results', 6],\n",
              " [1, '5 Conclusions', 8]]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# pdf_file =  \"An Empirical Survey on Long Document Summarization.pdf\"\n",
        "pdf_file = \"1901.00009v1.pdf\"\n",
        "# pdf_file = \"1901.00936v3.pdf\"\n",
        "# pdf_file = \"an image is worth 16 by 16 words.pdf\"\n",
        "doc, total_text, total_pages = processing_pdf.open_file(project_data_path + \"/\" + pdf_file)\n",
        "table_of_content = doc.get_toc()\n",
        "print(\"Auto generated table of content:\")\n",
        "display(table_of_content)\n",
        "\n",
        "# some papers have not table of content\n",
        "if len(table_of_content) == 0:\n",
        "    print(\"The paper has not table of content. Need to use regular expression to map table of content.\")\n",
        "    res = processing_pdf.auto_find_toc(doc)\n",
        "    print(\"Not satified the generated result or want to adjust? Build your own table of content by using this template:\")\n",
        "    display(res)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "starting looking for all the sections according to the provided section title info...\n",
            "save the dataframe to d:\\Start_Dec_19_2023\\application\\NUS Course\\Semester_1\\Neural Network and Deep Learning\\PDF_Conversion/processed/1901.00009v1.csv\n",
            "save the dataframe to d:\\Start_Dec_19_2023\\application\\NUS Course\\Semester_1\\Neural Network and Deep Learning\\PDF_Conversion/processed/1901.00009v1.json\n",
            "\n",
            "Title: The ASAS-SN Catalog of Variable Stars III: Variables inthe Southern TESS Continuous Viewing Zone\n",
            "\n",
            "Authors:T. Jayasinghe1,2⋆, K. Z. Stanek1,2, C. S. Kochanek1,2, B. J. Shappee3,T. W. -S. Holoien4, Todd A. Thompson1,2,5, J. L. Prieto6,7, Subo Dong8, M. Pawlak9,O. Pejcha9, J. V. Shields1, G. Pojmanski10, S. Otero11, N. Hurst12, C. A. Britt12,D. Will1,12\n",
            "\n",
            "Other info: 1Department of Astronomy, The Ohio State University, 140 West 18th Avenue, Columbus, OH 43210, USA2Center for Cosmology and Astroparticle Physics, The Ohio State University, 191 W. WoodruﬀAvenue, Columbus, OH 43210, USA3Institute for Astronomy, University of Hawaii, 2680 Woodlawn Drive, Honolulu, HI 96822,USA4Carnegie Observatories, 813 Santa Barbara Street, Pasadena, CA 91101, USA5Institute for Advanced Study, Princeton, NJ, 085406N´ucleo de Astronom´ıa de la Facultad de Ingenier´ıa y Ciencias, Universidad Diego Portales, Av. Ej´ercito 441, Santiago, Chile7Millennium Institute of Astrophysics, Santiago, Chile8Kavli Institute for Astronomy and Astrophysics, Peking University, Yi He Yuan Road 5, Hai Dian District, China9Institute of Theoretical Physics, Faculty of Mathematics and Physics, Charles University in Prague, Czech Republic10Warsaw University Observatory, Al Ujazdowskie 4, 00-478 Warsaw, Poland11The American Association of Variable Star Observers, 49 Bay State Road, Cambridge, MA 02138, USA12ASC Technology Services, 433 Mendenhall Laboratory 125 South Oval Mall Columbus OH, 43210, USA Accepted XXX. Received YYY; in original form ZZZ\n",
            "\n",
            "Abstract:The All-Sky Automated Survey for Supernovae (ASAS-SN) provides long baseline (∼4yrs) light curves for sources brighter than V≲17 mag across the whole sky. The Tran-siting Exoplanet Survey Satellite (TESS) has started to produce high-quality lightcurves with a baseline of at least 27 days, eventually for most of the sky. The combi-nation of ASAS-SN and TESS light curves probes both long and short term variabilityin great detail, especially towards the TESS continuous viewing zones (CVZ) at theecliptic poles. We have produced ∼1.3 million V-band light curves covering a total of∼1000 deg2 towards the southern TESS CVZ and have systematically searched thesesources for variability. We have identiﬁed ∼11, 700 variables, including ∼7, 000 new dis-coveries. The light curves and characteristics of the variables are all available throughthe ASAS-SN variable stars database (https://asas-sn.osu.edu/variables). Wealso introduce an online resource to obtain pre-computed ASAS-SN V-band lightcurves (https://asas-sn.osu.edu/photometry) starting with the light curves of the∼1.3 million sources studied in this work. This eﬀort will be extended to provide ASAS-SN light curves for ∼50 million sources over the entire sky. Key words: stars:variables – stars:binaries:eclipsing – catalogues –surveys\n"
          ]
        }
      ],
      "source": [
        "#uncomment this list to customize table-of-content\n",
        "# table_of_content = [[1, 'I. INTRODUCTION', 1],\n",
        "#  [1, 'II. SFC BASED ON IPV6 SEGMENT ROUTING', 2],\n",
        "#  [1, 'III. DESIGN OF THE SRV6 PROXY', 4],\n",
        "#  [2, 'A. General Concepts and State-of-the-art', 4],\n",
        "#  [2, 'B. SRNKv1', 5],\n",
        "#  [2, 'C. SRNKv2', 7],\n",
        "#  [2, 'D. Implementation of other SR proxy types', 8],\n",
        "#  [1, 'IV. TESTING ENVIRONMENT', 8],\n",
        "#  [1, 'V. PERFORMANCE ANALYSIS', 9],\n",
        "#  [1, 'VII. CONCLUSIONS', 11]]\n",
        "\n",
        "# separate content into sections\n",
        "processing_pdf.clear_processed_folder(project_processed_data_path)\n",
        "title, authors, other_info, abstract = processing_pdf.find_meta_data(doc, table_of_content)\n",
        "ds, json_dict = processing_pdf.separate_content(total_text, table_of_content)\n",
        "processing_pdf.save_dataframe(ds, json_dict, project_processed_data_path,  pdf_file.rsplit(\".\", 1)[0])\n",
        "# extract images\n",
        "processing_pdf.find_images(doc, table_of_content, total_pages, project_processed_data_path)\n",
        "\n",
        "# display(ds)\n",
        "print(f\"\\nTitle: {title}\")\n",
        "print(f\"\\nAuthors:{authors}\")\n",
        "print(f\"\\nOther info: {other_info}\")\n",
        "print(f\"\\nAbstract:{abstract}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# open json file\n",
        "with open(project_processed_data_path + \"/an image is worth 16 by 16 words.json\") as f:\n",
        "    data = json.load(f)\n",
        "    for item in data:\n",
        "        display(item)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
