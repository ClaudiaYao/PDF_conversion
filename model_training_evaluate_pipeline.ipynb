{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgMFPhHk_zbr"
      },
      "source": [
        "# 1. Preparation\n",
        "a. Install or import relevant libraries <br>\n",
        "b. Runtime -> change runtime type -> choose any GPU type available for you <br>\n",
        "c. Set variable `RUN_LOCALLY`: <br>\n",
        "    - set it to True: you will run this notebook locally. <br>\n",
        "    - set it to False: you will run this notebook on Colab. <br>\n",
        "\n",
        "d. If you are running the notebook in Colab, you need to: <br>\n",
        "    - specify the project path by setting the value of `project_path` <br>\n",
        "\n",
        "e. No matter you are running in Colab or locally, ensure:\n",
        "    - There is a subfolder called \"dataset\". There are at least two files: train.csv and val.csv inside it.<br>\n",
        "    - In Colab project path, there is a subfolder called \"processed\". There is a CSV file represents the segmented PDF file. e.g. `1901.00936v3.csv`<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnOGyYcaJIMm",
        "outputId": "1502f64d-658d-448a-9711-f22ce8d67e85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.19.0-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Collecting huggingface-hub>=0.21.2 (from datasets)\n",
            "  Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "Successfully installed datasets-2.19.0 dill-0.3.8 huggingface-hub-0.22.2 multiprocess-0.70.16 xxhash-3.4.1\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.25.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.2)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=79a8f83158e7adf8554bd08404b86953339faa948085dd2f2eb921f7e0c95d0e\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (0.22.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (3.13.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install rouge_score\n",
        "!pip install huggingface-hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Su__gj9U26xb",
        "outputId": "2ad4d661-5c67-4765-c61c-1d106235d136"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Apr 27 11:46:45 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
            "| N/A   32C    P8              11W /  72W |      1MiB / 23034MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgElBEsLtMUr"
      },
      "source": [
        "Set variable `RUN_LOCALLY`: <br>\n",
        "    - set it to True: you will run this notebook locally. <br>\n",
        "    - set it to False: you will run this notebook on Colab. <br>\n",
        "\n",
        "If you are running the notebook in Colab, specify the project path by setting the value of `project_path` <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vbxlvBEhC-kO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cf629fa-bed3-4e38-c841-bc1687f35469"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os, json, logging\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import re\n",
        "\n",
        "transformers_logger = logging.getLogger(\"transformers\")\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "from transformers import LEDForConditionalGeneration, LEDTokenizer\n",
        "from datasets import Dataset, load_dataset, load_metric, load_from_disk, DatasetDict\n",
        "from rouge_score import rouge_scorer\n",
        "pd.options.display.max_colwidth = 1000\n",
        "torch.device('cpu')\n",
        "RUN_LOCALLY = False\n",
        "######### set RUN_LOCALLY to True/False. Set project_path if the notebook is running in Colab ########\n",
        "if RUN_LOCALLY:\n",
        "    project_path = os.getcwd()\n",
        "else:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\")\n",
        "    project_path = \"/content/drive/MyDrive/Colab Notebooks/Course-NUS-cs5242-Neural Network and Deep Learning/final_project\"\n",
        "\n",
        "project_dataset_path = project_path + \"/dataset\"\n",
        "project_processed_data_path = project_path + \"/processed\"\n",
        "if not os.path.exists(project_dataset_path):\n",
        "    raise Exception(\"The `dataset` sub folder does not exist.\")\n",
        "if not os.path.exists(project_processed_data_path):\n",
        "    raise Exception(\"The `processed` sub folder does not exist.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vb3sduyTBAfr"
      },
      "source": [
        "# 2. Define functions\n",
        "This class `Model_operation` wraps all the functions related to the model's operation. At the end of the cell, we create an object of this class. This object will be used in the following trainning and testing steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "T0NcbDWxJIMp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b3232192745b4a22a39bf8cd6b58cb23",
            "4e48f42681844a558a603c84cefc90e5",
            "196df4e4cc354454a0cd3da3fafb35b5",
            "576d7598256b4900a3fc6b4945fd7ab6",
            "ea06137e82af45efadc7b09d895c98ea",
            "1691d7c03b664526ae81f7cf9a3bff03",
            "4e5fd2bca32e44709937dc2db74e24f9",
            "c1a77102140d41dea360a32755f648a3",
            "3a208393058f4685bf8ce4eaefac7b32",
            "8f9f11b388b34886a8e355d397ba86c7",
            "944b2223f38942e2a4db54f962c4bdc8",
            "d367c7e3beab4ac680e7a3de0f14bc92",
            "b993db9f5ed6449781e344cf0e0f31c1",
            "6ebeb44d1bb342a78334f88bfdb8b281",
            "40e5e35daa4b420cb64bdaee0375c022",
            "042e7a4f7ca944b784e956ed63da1179",
            "06ee76276f9e447487f5320898909145",
            "560b4f97f14a4384bb85a185c40e540c",
            "66e68534c8024dfb9e51bfbd7cdf26af",
            "209f9d1dc43e425f9005068a67349eb3",
            "22c063550b0c4ff09d30b76a0ee1fc02",
            "32a1be30fcaf428f9707b9ed369c4a18",
            "933987ad51354a6b8a40b345cf581dce",
            "f6c06e1d57e14638ba6c7d41f4632e4c",
            "fb525afdda86418b80467affa08770a1",
            "b5eaaf3d8e7246978607f67da372c883",
            "2b3e9682f8054cf2ad5e7510cd3f5174",
            "9b3578ca65354481859885943cd13708",
            "0ae3a4c35d0b425eb8635c025576ba33",
            "cbce0985d9aa43219ab41461b8bbc844",
            "63ddaaa094a749d89ac7cf04d0eda114",
            "8fe2702722f246baa24969e44bb2d11b",
            "00a7e173f3c247eab6fa7fc63cb490f7",
            "c13258637b9948ef9cc5c1d639810394",
            "942abe83b2514eb3a7a3bf885cfb30f8",
            "3121fc55216b43ee89151df4365a2ecc",
            "960348b804784fda826814cb042bb153",
            "e8c72e736ed6420489d754590136da20",
            "5292ea8848db4efda5fe3367bfe7427d",
            "e65e15326a7240aba05442413713841e",
            "dd5fe25858b141d4b7917f89261886be",
            "c13bbf2323714fc1984d73b94087490b",
            "d63099c863d84a34830881ced21f2095",
            "0846e34881984e928a1b3d609323d83d",
            "439aa6ee1f4f41eeb48b5f8da5518190",
            "258ac97917454162b7eb1b75816fe0e9",
            "4fd6b44a325a4e228741a0aebd51f9c4",
            "8d7497d5a6fb45d9a83add7c3a57c93c",
            "8dd6509310304fcb88ebabddafe53863",
            "596baf0ae5c04bd8b84e4b54beb02927",
            "86c9dd27deb641d0bd5e4cf7da1c0b0f",
            "a241c33e4dc34f28aafe417879af16c7",
            "8472bf0f31934bc991ef743f3984ee2c",
            "f806340d284f489395ea3c919ed037b3",
            "0c56c702228d4bfbb77e47281ec212b1",
            "7e17bd75744d4de5bb962d0596ff4eb7",
            "59978dc0538543cb8535faa4ddda42b7",
            "02a8ba7ad363409f8a2f44be3b7ff296",
            "06cca6f4f31d4065a8d31389e659d4a4",
            "7f34543d4bfc47eb8087a27ef1680b28",
            "99b0a63ab9f441c083d846d0a204869c",
            "eb672d0aebf5499e84761ee544cdaa4c",
            "1a26c872b9334e2692b015c48b1d245c",
            "89ca36852a3045238cedb941e7787ce2",
            "73193ebfa45d48a19c0f50e90b6bf8f3",
            "0601868ed3fc4f6f9b6ec722d585d346",
            "1255d1255b604487bd0c14ec820c81c1",
            "0384b82cc94b46bb9ce3f14f9a35bce3",
            "66fc8c237ce74476947e4350d240dbdf",
            "39dead09521a463da81e0041796e4b8e",
            "f85a9f98597c4cbd96a9759d1677d59c",
            "280236c46a2748abbe24b66f51635d30",
            "9ed7388a793a427ba2e21e2144781cd0",
            "870748ae632a4c01aab379c8ef56cf90",
            "e38041035f2f4dbf86cc5a9d00569759",
            "5492ab187df24351aa4fed914626af27",
            "649092607c38470797d4be05f3643ca1",
            "b1aa9c419a074b0496c9ecec5b99e227",
            "1efc9afbc9034cf3ad280196a8a373a3",
            "79a416fb7e474eef8c06aaa737476d22",
            "643838433a59418cb2ca0a3eb733b194",
            "c8453d0433004aaeae2a2cfbb80787c1",
            "315a5146fdaa45fe955a1d1985c427a8",
            "07457f5e74e14984a486e41efa946ff7",
            "071450748bf241c3be4ba97d715a320e",
            "dd15c7f451454d378d110b58e6910c75",
            "4f416453d132489a8be070ee92964c81",
            "11f7c8e107124051a2ee7ca49e82366c"
          ]
        },
        "outputId": "7ebdd9f7-a48e-4adb-b95b-a138289198ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/27.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b3232192745b4a22a39bf8cd6b58cb23"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d367c7e3beab4ac680e7a3de0f14bc92"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "933987ad51354a6b8a40b345cf581dce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c13258637b9948ef9cc5c1d639810394"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "439aa6ee1f4f41eeb48b5f8da5518190"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.84G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e17bd75744d4de5bb962d0596ff4eb7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/207 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1255d1255b604487bd0c14ec820c81c1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-4d2f79688e1b>:12: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
            "  self.rouge = load_metric(\"rouge\")\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:759: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/rouge/rouge.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/2.17k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1aa9c419a074b0496c9ecec5b99e227"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-trained Model Config:\n",
            " LEDConfig {\n",
            "  \"_name_or_path\": \"allenai/led-large-16384-arxiv\",\n",
            "  \"_num_labels\": 3,\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"LEDForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"attention_window\": [\n",
            "    1024,\n",
            "    1024,\n",
            "    1024,\n",
            "    1024,\n",
            "    1024,\n",
            "    1024,\n",
            "    1024,\n",
            "    1024,\n",
            "    1024,\n",
            "    1024,\n",
            "    1024,\n",
            "    1024\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.0,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_decoder_position_embeddings\": 1024,\n",
            "  \"max_encoder_position_embeddings\": 16384,\n",
            "  \"max_length\": 512,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"led\",\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": false,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": \" \",\n",
            "  \"transformers_version\": \"4.40.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "Pre-trained Model state-dict:\n",
            "\n",
            "final_logits_bias  -  torch.Size([1, 50265])\n",
            "led.shared.weight  -  torch.Size([50265, 1024])\n",
            "led.encoder.embed_tokens.weight  -  torch.Size([50265, 1024])\n",
            "led.encoder.embed_positions.weight  -  torch.Size([16384, 1024])\n",
            "led.encoder.layers.0.self_attn.longformer_self_attn.query.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.0.self_attn.longformer_self_attn.query.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.0.self_attn.longformer_self_attn.key.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.0.self_attn.longformer_self_attn.key.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.0.self_attn.longformer_self_attn.value.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.0.self_attn.longformer_self_attn.value.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.0.self_attn.longformer_self_attn.query_global.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.0.self_attn.longformer_self_attn.query_global.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.0.self_attn.longformer_self_attn.key_global.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.0.self_attn.longformer_self_attn.key_global.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.0.self_attn.longformer_self_attn.value_global.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.0.self_attn.longformer_self_attn.value_global.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.0.self_attn.output.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.0.self_attn.output.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.0.self_attn_layer_norm.weight  -  torch.Size([1024])\n",
            "led.encoder.layers.0.self_attn_layer_norm.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.0.fc1.weight  -  torch.Size([4096, 1024])\n",
            "led.encoder.layers.0.fc1.bias  -  torch.Size([4096])\n",
            "led.encoder.layers.0.fc2.weight  -  torch.Size([1024, 4096])\n",
            "led.encoder.layers.0.fc2.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.0.final_layer_norm.weight  -  torch.Size([1024])\n",
            "led.encoder.layers.0.final_layer_norm.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.1.self_attn.longformer_self_attn.query.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.1.self_attn.longformer_self_attn.query.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.1.self_attn.longformer_self_attn.key.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.1.self_attn.longformer_self_attn.key.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.1.self_attn.longformer_self_attn.value.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.1.self_attn.longformer_self_attn.value.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.1.self_attn.longformer_self_attn.query_global.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.1.self_attn.longformer_self_attn.query_global.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.1.self_attn.longformer_self_attn.key_global.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.1.self_attn.longformer_self_attn.key_global.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.1.self_attn.longformer_self_attn.value_global.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.1.self_attn.longformer_self_attn.value_global.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.1.self_attn.output.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.1.self_attn.output.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.1.self_attn_layer_norm.weight  -  torch.Size([1024])\n",
            "led.encoder.layers.1.self_attn_layer_norm.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.1.fc1.weight  -  torch.Size([4096, 1024])\n",
            "led.encoder.layers.1.fc1.bias  -  torch.Size([4096])\n",
            "led.encoder.layers.1.fc2.weight  -  torch.Size([1024, 4096])\n",
            "led.encoder.layers.1.fc2.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.1.final_layer_norm.weight  -  torch.Size([1024])\n",
            "led.encoder.layers.1.final_layer_norm.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.2.self_attn.longformer_self_attn.query.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.2.self_attn.longformer_self_attn.query.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.2.self_attn.longformer_self_attn.key.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.2.self_attn.longformer_self_attn.key.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.2.self_attn.longformer_self_attn.value.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.2.self_attn.longformer_self_attn.value.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.2.self_attn.longformer_self_attn.query_global.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.2.self_attn.longformer_self_attn.query_global.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.2.self_attn.longformer_self_attn.key_global.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.2.self_attn.longformer_self_attn.key_global.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.2.self_attn.longformer_self_attn.value_global.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.2.self_attn.longformer_self_attn.value_global.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.2.self_attn.output.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.2.self_attn.output.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.2.self_attn_layer_norm.weight  -  torch.Size([1024])\n",
            "led.encoder.layers.2.self_attn_layer_norm.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.2.fc1.weight  -  torch.Size([4096, 1024])\n",
            "led.encoder.layers.2.fc1.bias  -  torch.Size([4096])\n",
            "led.encoder.layers.2.fc2.weight  -  torch.Size([1024, 4096])\n",
            "led.encoder.layers.2.fc2.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.2.final_layer_norm.weight  -  torch.Size([1024])\n",
            "led.encoder.layers.2.final_layer_norm.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.3.self_attn.longformer_self_attn.query.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.3.self_attn.longformer_self_attn.query.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.3.self_attn.longformer_self_attn.key.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.3.self_attn.longformer_self_attn.key.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.3.self_attn.longformer_self_attn.value.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.3.self_attn.longformer_self_attn.value.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.3.self_attn.longformer_self_attn.query_global.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.3.self_attn.longformer_self_attn.query_global.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.3.self_attn.longformer_self_attn.key_global.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.3.self_attn.longformer_self_attn.key_global.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.3.self_attn.longformer_self_attn.value_global.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.3.self_attn.longformer_self_attn.value_global.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.3.self_attn.output.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.3.self_attn.output.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.3.self_attn_layer_norm.weight  -  torch.Size([1024])\n",
            "led.encoder.layers.3.self_attn_layer_norm.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.3.fc1.weight  -  torch.Size([4096, 1024])\n",
            "led.encoder.layers.3.fc1.bias  -  torch.Size([4096])\n",
            "led.encoder.layers.3.fc2.weight  -  torch.Size([1024, 4096])\n",
            "led.encoder.layers.3.fc2.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.3.final_layer_norm.weight  -  torch.Size([1024])\n",
            "led.encoder.layers.3.final_layer_norm.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.4.self_attn.longformer_self_attn.query.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.4.self_attn.longformer_self_attn.query.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.4.self_attn.longformer_self_attn.key.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.4.self_attn.longformer_self_attn.key.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.4.self_attn.longformer_self_attn.value.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.4.self_attn.longformer_self_attn.value.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.4.self_attn.longformer_self_attn.query_global.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.4.self_attn.longformer_self_attn.query_global.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.4.self_attn.longformer_self_attn.key_global.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.4.self_attn.longformer_self_attn.key_global.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.4.self_attn.longformer_self_attn.value_global.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.4.self_attn.longformer_self_attn.value_global.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.4.self_attn.output.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.4.self_attn.output.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.4.self_attn_layer_norm.weight  -  torch.Size([1024])\n",
            "led.encoder.layers.4.self_attn_layer_norm.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.4.fc1.weight  -  torch.Size([4096, 1024])\n",
            "led.encoder.layers.4.fc1.bias  -  torch.Size([4096])\n",
            "led.encoder.layers.4.fc2.weight  -  torch.Size([1024, 4096])\n",
            "led.encoder.layers.4.fc2.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.4.final_layer_norm.weight  -  torch.Size([1024])\n",
            "led.encoder.layers.4.final_layer_norm.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.5.self_attn.longformer_self_attn.query.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.5.self_attn.longformer_self_attn.query.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.5.self_attn.longformer_self_attn.key.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.5.self_attn.longformer_self_attn.key.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.5.self_attn.longformer_self_attn.value.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.5.self_attn.longformer_self_attn.value.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.5.self_attn.longformer_self_attn.query_global.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.5.self_attn.longformer_self_attn.query_global.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.5.self_attn.longformer_self_attn.key_global.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.5.self_attn.longformer_self_attn.key_global.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.5.self_attn.longformer_self_attn.value_global.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.5.self_attn.longformer_self_attn.value_global.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.5.self_attn.output.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.5.self_attn.output.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.5.self_attn_layer_norm.weight  -  torch.Size([1024])\n",
            "led.encoder.layers.5.self_attn_layer_norm.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.5.fc1.weight  -  torch.Size([4096, 1024])\n",
            "led.encoder.layers.5.fc1.bias  -  torch.Size([4096])\n",
            "led.encoder.layers.5.fc2.weight  -  torch.Size([1024, 4096])\n",
            "led.encoder.layers.5.fc2.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.5.final_layer_norm.weight  -  torch.Size([1024])\n",
            "led.encoder.layers.5.final_layer_norm.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.6.self_attn.longformer_self_attn.query.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.6.self_attn.longformer_self_attn.query.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.6.self_attn.longformer_self_attn.key.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.6.self_attn.longformer_self_attn.key.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.6.self_attn.longformer_self_attn.value.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.6.self_attn.longformer_self_attn.value.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.6.self_attn.longformer_self_attn.query_global.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.6.self_attn.longformer_self_attn.query_global.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.6.self_attn.longformer_self_attn.key_global.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.6.self_attn.longformer_self_attn.key_global.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.6.self_attn.longformer_self_attn.value_global.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.6.self_attn.longformer_self_attn.value_global.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.6.self_attn.output.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.6.self_attn.output.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.6.self_attn_layer_norm.weight  -  torch.Size([1024])\n",
            "led.encoder.layers.6.self_attn_layer_norm.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.6.fc1.weight  -  torch.Size([4096, 1024])\n",
            "led.encoder.layers.6.fc1.bias  -  torch.Size([4096])\n",
            "led.encoder.layers.6.fc2.weight  -  torch.Size([1024, 4096])\n",
            "led.encoder.layers.6.fc2.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.6.final_layer_norm.weight  -  torch.Size([1024])\n",
            "led.encoder.layers.6.final_layer_norm.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.7.self_attn.longformer_self_attn.query.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.7.self_attn.longformer_self_attn.query.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.7.self_attn.longformer_self_attn.key.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.7.self_attn.longformer_self_attn.key.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.7.self_attn.longformer_self_attn.value.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.7.self_attn.longformer_self_attn.value.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.7.self_attn.longformer_self_attn.query_global.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.7.self_attn.longformer_self_attn.query_global.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.7.self_attn.longformer_self_attn.key_global.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.7.self_attn.longformer_self_attn.key_global.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.7.self_attn.longformer_self_attn.value_global.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.7.self_attn.longformer_self_attn.value_global.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.7.self_attn.output.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.7.self_attn.output.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.7.self_attn_layer_norm.weight  -  torch.Size([1024])\n",
            "led.encoder.layers.7.self_attn_layer_norm.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.7.fc1.weight  -  torch.Size([4096, 1024])\n",
            "led.encoder.layers.7.fc1.bias  -  torch.Size([4096])\n",
            "led.encoder.layers.7.fc2.weight  -  torch.Size([1024, 4096])\n",
            "led.encoder.layers.7.fc2.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.7.final_layer_norm.weight  -  torch.Size([1024])\n",
            "led.encoder.layers.7.final_layer_norm.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.8.self_attn.longformer_self_attn.query.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.8.self_attn.longformer_self_attn.query.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.8.self_attn.longformer_self_attn.key.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.8.self_attn.longformer_self_attn.key.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.8.self_attn.longformer_self_attn.value.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.8.self_attn.longformer_self_attn.value.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.8.self_attn.longformer_self_attn.query_global.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.8.self_attn.longformer_self_attn.query_global.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.8.self_attn.longformer_self_attn.key_global.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.8.self_attn.longformer_self_attn.key_global.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.8.self_attn.longformer_self_attn.value_global.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.8.self_attn.longformer_self_attn.value_global.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.8.self_attn.output.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.8.self_attn.output.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.8.self_attn_layer_norm.weight  -  torch.Size([1024])\n",
            "led.encoder.layers.8.self_attn_layer_norm.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.8.fc1.weight  -  torch.Size([4096, 1024])\n",
            "led.encoder.layers.8.fc1.bias  -  torch.Size([4096])\n",
            "led.encoder.layers.8.fc2.weight  -  torch.Size([1024, 4096])\n",
            "led.encoder.layers.8.fc2.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.8.final_layer_norm.weight  -  torch.Size([1024])\n",
            "led.encoder.layers.8.final_layer_norm.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.9.self_attn.longformer_self_attn.query.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.9.self_attn.longformer_self_attn.query.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.9.self_attn.longformer_self_attn.key.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.9.self_attn.longformer_self_attn.key.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.9.self_attn.longformer_self_attn.value.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.9.self_attn.longformer_self_attn.value.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.9.self_attn.longformer_self_attn.query_global.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.9.self_attn.longformer_self_attn.query_global.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.9.self_attn.longformer_self_attn.key_global.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.9.self_attn.longformer_self_attn.key_global.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.9.self_attn.longformer_self_attn.value_global.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.9.self_attn.longformer_self_attn.value_global.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.9.self_attn.output.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.9.self_attn.output.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.9.self_attn_layer_norm.weight  -  torch.Size([1024])\n",
            "led.encoder.layers.9.self_attn_layer_norm.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.9.fc1.weight  -  torch.Size([4096, 1024])\n",
            "led.encoder.layers.9.fc1.bias  -  torch.Size([4096])\n",
            "led.encoder.layers.9.fc2.weight  -  torch.Size([1024, 4096])\n",
            "led.encoder.layers.9.fc2.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.9.final_layer_norm.weight  -  torch.Size([1024])\n",
            "led.encoder.layers.9.final_layer_norm.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.10.self_attn.longformer_self_attn.query.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.10.self_attn.longformer_self_attn.query.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.10.self_attn.longformer_self_attn.key.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.10.self_attn.longformer_self_attn.key.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.10.self_attn.longformer_self_attn.value.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.10.self_attn.longformer_self_attn.value.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.10.self_attn.longformer_self_attn.query_global.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.10.self_attn.longformer_self_attn.query_global.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.10.self_attn.longformer_self_attn.key_global.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.10.self_attn.longformer_self_attn.key_global.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.10.self_attn.longformer_self_attn.value_global.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.10.self_attn.longformer_self_attn.value_global.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.10.self_attn.output.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.10.self_attn.output.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.10.self_attn_layer_norm.weight  -  torch.Size([1024])\n",
            "led.encoder.layers.10.self_attn_layer_norm.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.10.fc1.weight  -  torch.Size([4096, 1024])\n",
            "led.encoder.layers.10.fc1.bias  -  torch.Size([4096])\n",
            "led.encoder.layers.10.fc2.weight  -  torch.Size([1024, 4096])\n",
            "led.encoder.layers.10.fc2.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.10.final_layer_norm.weight  -  torch.Size([1024])\n",
            "led.encoder.layers.10.final_layer_norm.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.11.self_attn.longformer_self_attn.query.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.11.self_attn.longformer_self_attn.query.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.11.self_attn.longformer_self_attn.key.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.11.self_attn.longformer_self_attn.key.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.11.self_attn.longformer_self_attn.value.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.11.self_attn.longformer_self_attn.value.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.11.self_attn.longformer_self_attn.query_global.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.11.self_attn.longformer_self_attn.query_global.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.11.self_attn.longformer_self_attn.key_global.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.11.self_attn.longformer_self_attn.key_global.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.11.self_attn.longformer_self_attn.value_global.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.11.self_attn.longformer_self_attn.value_global.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.11.self_attn.output.weight  -  torch.Size([1024, 1024])\n",
            "led.encoder.layers.11.self_attn.output.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.11.self_attn_layer_norm.weight  -  torch.Size([1024])\n",
            "led.encoder.layers.11.self_attn_layer_norm.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.11.fc1.weight  -  torch.Size([4096, 1024])\n",
            "led.encoder.layers.11.fc1.bias  -  torch.Size([4096])\n",
            "led.encoder.layers.11.fc2.weight  -  torch.Size([1024, 4096])\n",
            "led.encoder.layers.11.fc2.bias  -  torch.Size([1024])\n",
            "led.encoder.layers.11.final_layer_norm.weight  -  torch.Size([1024])\n",
            "led.encoder.layers.11.final_layer_norm.bias  -  torch.Size([1024])\n",
            "led.encoder.layernorm_embedding.weight  -  torch.Size([1024])\n",
            "led.encoder.layernorm_embedding.bias  -  torch.Size([1024])\n",
            "led.decoder.embed_tokens.weight  -  torch.Size([50265, 1024])\n",
            "led.decoder.embed_positions.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.0.self_attn.k_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.0.self_attn.k_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.0.self_attn.v_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.0.self_attn.v_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.0.self_attn.q_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.0.self_attn.q_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.0.self_attn.out_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.0.self_attn.out_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.0.self_attn_layer_norm.weight  -  torch.Size([1024])\n",
            "led.decoder.layers.0.self_attn_layer_norm.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.0.encoder_attn.k_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.0.encoder_attn.k_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.0.encoder_attn.v_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.0.encoder_attn.v_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.0.encoder_attn.q_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.0.encoder_attn.q_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.0.encoder_attn.out_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.0.encoder_attn.out_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.0.encoder_attn_layer_norm.weight  -  torch.Size([1024])\n",
            "led.decoder.layers.0.encoder_attn_layer_norm.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.0.fc1.weight  -  torch.Size([4096, 1024])\n",
            "led.decoder.layers.0.fc1.bias  -  torch.Size([4096])\n",
            "led.decoder.layers.0.fc2.weight  -  torch.Size([1024, 4096])\n",
            "led.decoder.layers.0.fc2.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.0.final_layer_norm.weight  -  torch.Size([1024])\n",
            "led.decoder.layers.0.final_layer_norm.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.1.self_attn.k_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.1.self_attn.k_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.1.self_attn.v_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.1.self_attn.v_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.1.self_attn.q_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.1.self_attn.q_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.1.self_attn.out_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.1.self_attn.out_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.1.self_attn_layer_norm.weight  -  torch.Size([1024])\n",
            "led.decoder.layers.1.self_attn_layer_norm.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.1.encoder_attn.k_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.1.encoder_attn.k_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.1.encoder_attn.v_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.1.encoder_attn.v_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.1.encoder_attn.q_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.1.encoder_attn.q_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.1.encoder_attn.out_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.1.encoder_attn.out_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.1.encoder_attn_layer_norm.weight  -  torch.Size([1024])\n",
            "led.decoder.layers.1.encoder_attn_layer_norm.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.1.fc1.weight  -  torch.Size([4096, 1024])\n",
            "led.decoder.layers.1.fc1.bias  -  torch.Size([4096])\n",
            "led.decoder.layers.1.fc2.weight  -  torch.Size([1024, 4096])\n",
            "led.decoder.layers.1.fc2.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.1.final_layer_norm.weight  -  torch.Size([1024])\n",
            "led.decoder.layers.1.final_layer_norm.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.2.self_attn.k_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.2.self_attn.k_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.2.self_attn.v_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.2.self_attn.v_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.2.self_attn.q_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.2.self_attn.q_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.2.self_attn.out_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.2.self_attn.out_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.2.self_attn_layer_norm.weight  -  torch.Size([1024])\n",
            "led.decoder.layers.2.self_attn_layer_norm.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.2.encoder_attn.k_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.2.encoder_attn.k_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.2.encoder_attn.v_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.2.encoder_attn.v_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.2.encoder_attn.q_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.2.encoder_attn.q_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.2.encoder_attn.out_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.2.encoder_attn.out_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.2.encoder_attn_layer_norm.weight  -  torch.Size([1024])\n",
            "led.decoder.layers.2.encoder_attn_layer_norm.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.2.fc1.weight  -  torch.Size([4096, 1024])\n",
            "led.decoder.layers.2.fc1.bias  -  torch.Size([4096])\n",
            "led.decoder.layers.2.fc2.weight  -  torch.Size([1024, 4096])\n",
            "led.decoder.layers.2.fc2.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.2.final_layer_norm.weight  -  torch.Size([1024])\n",
            "led.decoder.layers.2.final_layer_norm.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.3.self_attn.k_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.3.self_attn.k_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.3.self_attn.v_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.3.self_attn.v_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.3.self_attn.q_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.3.self_attn.q_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.3.self_attn.out_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.3.self_attn.out_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.3.self_attn_layer_norm.weight  -  torch.Size([1024])\n",
            "led.decoder.layers.3.self_attn_layer_norm.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.3.encoder_attn.k_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.3.encoder_attn.k_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.3.encoder_attn.v_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.3.encoder_attn.v_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.3.encoder_attn.q_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.3.encoder_attn.q_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.3.encoder_attn.out_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.3.encoder_attn.out_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.3.encoder_attn_layer_norm.weight  -  torch.Size([1024])\n",
            "led.decoder.layers.3.encoder_attn_layer_norm.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.3.fc1.weight  -  torch.Size([4096, 1024])\n",
            "led.decoder.layers.3.fc1.bias  -  torch.Size([4096])\n",
            "led.decoder.layers.3.fc2.weight  -  torch.Size([1024, 4096])\n",
            "led.decoder.layers.3.fc2.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.3.final_layer_norm.weight  -  torch.Size([1024])\n",
            "led.decoder.layers.3.final_layer_norm.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.4.self_attn.k_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.4.self_attn.k_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.4.self_attn.v_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.4.self_attn.v_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.4.self_attn.q_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.4.self_attn.q_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.4.self_attn.out_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.4.self_attn.out_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.4.self_attn_layer_norm.weight  -  torch.Size([1024])\n",
            "led.decoder.layers.4.self_attn_layer_norm.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.4.encoder_attn.k_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.4.encoder_attn.k_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.4.encoder_attn.v_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.4.encoder_attn.v_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.4.encoder_attn.q_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.4.encoder_attn.q_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.4.encoder_attn.out_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.4.encoder_attn.out_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.4.encoder_attn_layer_norm.weight  -  torch.Size([1024])\n",
            "led.decoder.layers.4.encoder_attn_layer_norm.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.4.fc1.weight  -  torch.Size([4096, 1024])\n",
            "led.decoder.layers.4.fc1.bias  -  torch.Size([4096])\n",
            "led.decoder.layers.4.fc2.weight  -  torch.Size([1024, 4096])\n",
            "led.decoder.layers.4.fc2.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.4.final_layer_norm.weight  -  torch.Size([1024])\n",
            "led.decoder.layers.4.final_layer_norm.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.5.self_attn.k_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.5.self_attn.k_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.5.self_attn.v_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.5.self_attn.v_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.5.self_attn.q_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.5.self_attn.q_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.5.self_attn.out_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.5.self_attn.out_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.5.self_attn_layer_norm.weight  -  torch.Size([1024])\n",
            "led.decoder.layers.5.self_attn_layer_norm.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.5.encoder_attn.k_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.5.encoder_attn.k_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.5.encoder_attn.v_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.5.encoder_attn.v_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.5.encoder_attn.q_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.5.encoder_attn.q_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.5.encoder_attn.out_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.5.encoder_attn.out_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.5.encoder_attn_layer_norm.weight  -  torch.Size([1024])\n",
            "led.decoder.layers.5.encoder_attn_layer_norm.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.5.fc1.weight  -  torch.Size([4096, 1024])\n",
            "led.decoder.layers.5.fc1.bias  -  torch.Size([4096])\n",
            "led.decoder.layers.5.fc2.weight  -  torch.Size([1024, 4096])\n",
            "led.decoder.layers.5.fc2.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.5.final_layer_norm.weight  -  torch.Size([1024])\n",
            "led.decoder.layers.5.final_layer_norm.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.6.self_attn.k_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.6.self_attn.k_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.6.self_attn.v_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.6.self_attn.v_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.6.self_attn.q_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.6.self_attn.q_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.6.self_attn.out_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.6.self_attn.out_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.6.self_attn_layer_norm.weight  -  torch.Size([1024])\n",
            "led.decoder.layers.6.self_attn_layer_norm.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.6.encoder_attn.k_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.6.encoder_attn.k_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.6.encoder_attn.v_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.6.encoder_attn.v_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.6.encoder_attn.q_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.6.encoder_attn.q_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.6.encoder_attn.out_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.6.encoder_attn.out_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.6.encoder_attn_layer_norm.weight  -  torch.Size([1024])\n",
            "led.decoder.layers.6.encoder_attn_layer_norm.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.6.fc1.weight  -  torch.Size([4096, 1024])\n",
            "led.decoder.layers.6.fc1.bias  -  torch.Size([4096])\n",
            "led.decoder.layers.6.fc2.weight  -  torch.Size([1024, 4096])\n",
            "led.decoder.layers.6.fc2.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.6.final_layer_norm.weight  -  torch.Size([1024])\n",
            "led.decoder.layers.6.final_layer_norm.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.7.self_attn.k_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.7.self_attn.k_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.7.self_attn.v_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.7.self_attn.v_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.7.self_attn.q_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.7.self_attn.q_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.7.self_attn.out_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.7.self_attn.out_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.7.self_attn_layer_norm.weight  -  torch.Size([1024])\n",
            "led.decoder.layers.7.self_attn_layer_norm.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.7.encoder_attn.k_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.7.encoder_attn.k_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.7.encoder_attn.v_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.7.encoder_attn.v_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.7.encoder_attn.q_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.7.encoder_attn.q_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.7.encoder_attn.out_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.7.encoder_attn.out_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.7.encoder_attn_layer_norm.weight  -  torch.Size([1024])\n",
            "led.decoder.layers.7.encoder_attn_layer_norm.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.7.fc1.weight  -  torch.Size([4096, 1024])\n",
            "led.decoder.layers.7.fc1.bias  -  torch.Size([4096])\n",
            "led.decoder.layers.7.fc2.weight  -  torch.Size([1024, 4096])\n",
            "led.decoder.layers.7.fc2.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.7.final_layer_norm.weight  -  torch.Size([1024])\n",
            "led.decoder.layers.7.final_layer_norm.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.8.self_attn.k_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.8.self_attn.k_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.8.self_attn.v_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.8.self_attn.v_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.8.self_attn.q_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.8.self_attn.q_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.8.self_attn.out_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.8.self_attn.out_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.8.self_attn_layer_norm.weight  -  torch.Size([1024])\n",
            "led.decoder.layers.8.self_attn_layer_norm.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.8.encoder_attn.k_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.8.encoder_attn.k_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.8.encoder_attn.v_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.8.encoder_attn.v_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.8.encoder_attn.q_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.8.encoder_attn.q_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.8.encoder_attn.out_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.8.encoder_attn.out_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.8.encoder_attn_layer_norm.weight  -  torch.Size([1024])\n",
            "led.decoder.layers.8.encoder_attn_layer_norm.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.8.fc1.weight  -  torch.Size([4096, 1024])\n",
            "led.decoder.layers.8.fc1.bias  -  torch.Size([4096])\n",
            "led.decoder.layers.8.fc2.weight  -  torch.Size([1024, 4096])\n",
            "led.decoder.layers.8.fc2.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.8.final_layer_norm.weight  -  torch.Size([1024])\n",
            "led.decoder.layers.8.final_layer_norm.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.9.self_attn.k_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.9.self_attn.k_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.9.self_attn.v_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.9.self_attn.v_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.9.self_attn.q_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.9.self_attn.q_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.9.self_attn.out_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.9.self_attn.out_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.9.self_attn_layer_norm.weight  -  torch.Size([1024])\n",
            "led.decoder.layers.9.self_attn_layer_norm.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.9.encoder_attn.k_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.9.encoder_attn.k_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.9.encoder_attn.v_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.9.encoder_attn.v_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.9.encoder_attn.q_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.9.encoder_attn.q_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.9.encoder_attn.out_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.9.encoder_attn.out_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.9.encoder_attn_layer_norm.weight  -  torch.Size([1024])\n",
            "led.decoder.layers.9.encoder_attn_layer_norm.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.9.fc1.weight  -  torch.Size([4096, 1024])\n",
            "led.decoder.layers.9.fc1.bias  -  torch.Size([4096])\n",
            "led.decoder.layers.9.fc2.weight  -  torch.Size([1024, 4096])\n",
            "led.decoder.layers.9.fc2.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.9.final_layer_norm.weight  -  torch.Size([1024])\n",
            "led.decoder.layers.9.final_layer_norm.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.10.self_attn.k_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.10.self_attn.k_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.10.self_attn.v_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.10.self_attn.v_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.10.self_attn.q_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.10.self_attn.q_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.10.self_attn.out_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.10.self_attn.out_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.10.self_attn_layer_norm.weight  -  torch.Size([1024])\n",
            "led.decoder.layers.10.self_attn_layer_norm.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.10.encoder_attn.k_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.10.encoder_attn.k_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.10.encoder_attn.v_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.10.encoder_attn.v_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.10.encoder_attn.q_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.10.encoder_attn.q_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.10.encoder_attn.out_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.10.encoder_attn.out_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.10.encoder_attn_layer_norm.weight  -  torch.Size([1024])\n",
            "led.decoder.layers.10.encoder_attn_layer_norm.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.10.fc1.weight  -  torch.Size([4096, 1024])\n",
            "led.decoder.layers.10.fc1.bias  -  torch.Size([4096])\n",
            "led.decoder.layers.10.fc2.weight  -  torch.Size([1024, 4096])\n",
            "led.decoder.layers.10.fc2.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.10.final_layer_norm.weight  -  torch.Size([1024])\n",
            "led.decoder.layers.10.final_layer_norm.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.11.self_attn.k_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.11.self_attn.k_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.11.self_attn.v_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.11.self_attn.v_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.11.self_attn.q_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.11.self_attn.q_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.11.self_attn.out_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.11.self_attn.out_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.11.self_attn_layer_norm.weight  -  torch.Size([1024])\n",
            "led.decoder.layers.11.self_attn_layer_norm.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.11.encoder_attn.k_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.11.encoder_attn.k_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.11.encoder_attn.v_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.11.encoder_attn.v_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.11.encoder_attn.q_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.11.encoder_attn.q_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.11.encoder_attn.out_proj.weight  -  torch.Size([1024, 1024])\n",
            "led.decoder.layers.11.encoder_attn.out_proj.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.11.encoder_attn_layer_norm.weight  -  torch.Size([1024])\n",
            "led.decoder.layers.11.encoder_attn_layer_norm.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.11.fc1.weight  -  torch.Size([4096, 1024])\n",
            "led.decoder.layers.11.fc1.bias  -  torch.Size([4096])\n",
            "led.decoder.layers.11.fc2.weight  -  torch.Size([1024, 4096])\n",
            "led.decoder.layers.11.fc2.bias  -  torch.Size([1024])\n",
            "led.decoder.layers.11.final_layer_norm.weight  -  torch.Size([1024])\n",
            "led.decoder.layers.11.final_layer_norm.bias  -  torch.Size([1024])\n",
            "led.decoder.layernorm_embedding.weight  -  torch.Size([1024])\n",
            "led.decoder.layernorm_embedding.bias  -  torch.Size([1024])\n",
            "lm_head.weight  -  torch.Size([50265, 1024])\n"
          ]
        }
      ],
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#Define the Model's operation in one class\n",
        "class Model_operation:\n",
        "    def __init__(self, model_name):\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = LEDTokenizer.from_pretrained(model_name, torch_dtype = torch.float16)\n",
        "        self.model = LEDForConditionalGeneration.from_pretrained(model_name).to(DEVICE)\n",
        "        self.config = LEDForConditionalGeneration.from_pretrained(model_name).config\n",
        "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=2e-5, weight_decay=0.01)\n",
        "\n",
        "        self.rouge = load_metric(\"rouge\")\n",
        "        self.batchsize = 4\n",
        "        self.max_input_length = 1024\n",
        "        self.max_output_length = 128\n",
        "\n",
        "    # tokenize the data\n",
        "    def process_data_to_model_inputs(self, batch):\n",
        "        inputs = self.tokenizer(\n",
        "            batch['Text'],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_input_length,\n",
        "        )\n",
        "\n",
        "        batch[\"input_ids\"] = inputs.input_ids\n",
        "        batch[\"attention_mask\"] = inputs.attention_mask\n",
        "\n",
        "        # put global attention on <s> token\n",
        "        # according to https://github.com/huggingface/transformers/issues/18190, As you are running summarization, it is LEDForConditionalGeneration. For this model, we should put 1 for the global_attention_mask on the first token <s> in the encoder input sequence.\n",
        "        batch[\"global_attention_mask\"] = len(batch[\"input_ids\"]) * [\n",
        "            [1 if index == 0 else 0 for index in range(len(batch[\"input_ids\"][0]))]\n",
        "        ]\n",
        "\n",
        "        outputs = self.tokenizer(\n",
        "            batch['Groundtruth'],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_output_length,\n",
        "        )\n",
        "        batch[\"labels\"] = outputs.input_ids\n",
        "        # We have to make sure that the PAD token is ignored by setting it to -100\n",
        "        batch[\"labels\"] = [\n",
        "            [-100 if token == self.tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]\n",
        "        return batch\n",
        "\n",
        "    # Function to calculate ROUGE scores for generated summary and ground truth\n",
        "    def calculate_rouge_scores(self, generated_summary, ground_truth_summary):\n",
        "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "        rouge1_f1, rouge2_f1, rougeL_f1 = [], [], []\n",
        "        for k in range(len(generated_summary)):\n",
        "            scores = scorer.score(generated_summary[k], ground_truth_summary[k])\n",
        "            rouge1_f1.append(scores['rouge1'].fmeasure)\n",
        "            rouge2_f1.append(scores['rouge2'].fmeasure)\n",
        "            rougeL_f1.append(scores['rougeL'].fmeasure)\n",
        "        return np.mean(rouge1_f1), np.mean(rouge2_f1), np.mean(rougeL_f1)\n",
        "\n",
        "    # convert the logits to real text\n",
        "    def convert_logits_to_text(self, pred_logits):\n",
        "        texts = []\n",
        "        probs = torch.softmax(pred_logits, dim=-1)\n",
        "        generated_ids = torch.argmax(probs, dim=-1)\n",
        "\n",
        "        for i in range(len(generated_ids)):\n",
        "            pred_str = self.tokenizer.decode(generated_ids[i], skip_special_tokens=True)\n",
        "            texts.append(pred_str)\n",
        "        # pred_str = self.tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "        # labels_ids[labels_ids == -100] = self.tokenizer.pad_token_id\n",
        "        # label_str = self.tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
        "        return texts\n",
        "\n",
        "    # convert tokens to real text. The difference from convert_logits_to_text is that this function does not need to use softmax\n",
        "    def convert_tokens_to_text(self, tokenized_sequences):\n",
        "        texts = []\n",
        "        for i in range(len(tokenized_sequences)):\n",
        "          tokens_list = tokenized_sequences[i].tolist()\n",
        "          if -100 in tokens_list:\n",
        "              end_index = tokens_list.index(-100)\n",
        "          else:\n",
        "              end_index = len(tokens_list)\n",
        "          pred_str = self.tokenizer.decode(tokenized_sequences[i][:end_index], skip_special_tokens=True)\n",
        "          texts.append(pred_str)\n",
        "        return texts\n",
        "\n",
        "    def log_metrics(self,epoch, train_loss, val_loss, train_rouge_scores, val_rouge_scores):\n",
        "        log_file = project_path + \"/metrics_log.txt\"\n",
        "        with open(log_file, \"a\") as f:\n",
        "            timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "            log_str = f\"{timestamp}, Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train ROUGE: {train_rouge_scores}, Val ROUGE: {val_rouge_scores}\\n\"\n",
        "            f.write(log_str)\n",
        "\n",
        "    def log_generated_summary(self,epoch, expected_summary, generated_summary, other_info=\"\"):\n",
        "        log_file = project_path + \"/generated_summary_log.txt\"\n",
        "        with open(log_file, \"a\") as f:\n",
        "            timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "            log_str = f\"{timestamp}, Epoch {epoch + 1}, \\nExpected: {expected_summary}, \\nGenerated: {generated_summary}, \\nOther Info: {other_info}\\n\"\n",
        "            f.write(log_str)\n",
        "\n",
        "    # tokenize the test data\n",
        "    def process_test_data_to_model_inputs(self, batch):\n",
        "        inputs = self.tokenizer(\n",
        "            list(batch['text']),\n",
        "            padding=\"max_length\",    #  'do_not_pad'\n",
        "            truncation=True,\n",
        "            max_length=self.max_input_length,\n",
        "        )\n",
        "\n",
        "        batch[\"input_ids\"] = inputs.input_ids\n",
        "        batch[\"attention_mask\"] = inputs.attention_mask\n",
        "\n",
        "        # create 0 global_attention_mask lists\n",
        "        batch[\"global_attention_mask\"] = len(batch[\"input_ids\"]) * [\n",
        "            [1 if index ==0 else 0 for index in range(len(batch[\"input_ids\"][0]))]\n",
        "        ]\n",
        "        return batch\n",
        "\n",
        "    # test the model by passing it a pdf file dataframe. Return a dataframe with generated summary\n",
        "    def generate_summary_for_user_pdf(self, pdf_df):\n",
        "        data_df = pdf_df.copy()\n",
        "        self.process_test_data_to_model_inputs(data_df)\n",
        "\n",
        "        input_ids = torch.tensor(data_df[\"input_ids\"]).to(DEVICE)\n",
        "        test_am = torch.tensor(data_df[\"attention_mask\"]).to(DEVICE)\n",
        "        test_gam = torch.tensor(data_df[\"global_attention_mask\"]).to(DEVICE)\n",
        "\n",
        "        # when not passing labels, the outputs's loss will be None\n",
        "        predicted_ids = self.model.generate(input_ids=input_ids,\n",
        "                attention_mask = test_am, global_attention_mask = test_gam)\n",
        "\n",
        "        generated = self.convert_tokens_to_text(predicted_ids)\n",
        "\n",
        "        for idx, text in enumerate(generated):\n",
        "            generated[idx] = re.sub(r\"\\\\n\", \"\", text).strip()\n",
        "\n",
        "        pdf_df['generated'] = generated\n",
        "        return pdf_df\n",
        "\n",
        "    def show_model_state_dict(self):\n",
        "        for key, value in self.model.state_dict().items():\n",
        "            print(key, \" - \", value.size())\n",
        "        # for name, param in model_action.model.named_parameters():\n",
        "        #     print(name, \" - \", param.shape)\n",
        "\n",
        "    def save_model_checkpoint(self, checkpoint_model_name):\n",
        "        if not os.path.exists(project_path + \"/Checkpoints\"):\n",
        "            os.mkdir(project_path + \"/Checkpoints\")\n",
        "        checkpoint_path = project_path + \"/Checkpoints/\" +checkpoint_model_name\n",
        "        torch.save(self.model.state_dict(), checkpoint_path)\n",
        "\n",
        "\n",
        "model_action = Model_operation(model_name = \"allenai/led-large-16384-arxiv\")\n",
        "# optional: set hyperparameters. Some parameters inherit from https://huggingface.co/docs/transformers/v4.40.1/en/main_classes/configuration#transformers.PretrainedConfig\n",
        "model_action.config.num_beams = 2\n",
        "model_action.config.max_length = model_action.max_output_length\n",
        "model_action.config.min_length = 100\n",
        "model_action.config.length_penalty = 2.0\n",
        "model_action.config.early_stopping = True\n",
        "model_action.config.no_repeat_ngram_size = 3\n",
        "print(\"Pre-trained Model Config:\\n\", model_action.model.config)\n",
        "print(\"Pre-trained Model state-dict:\\n\")\n",
        "model_action.show_model_state_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJ4An9HeDUru"
      },
      "source": [
        "# 3. Prepare for DataSet and DataLoader\n",
        "All the training data has been put into CSV file. We load CSV file and convert it to Huggingface Dataset. We then use Dataset's map method to tokenize the data. We format the Dataset and then load the data into PyTorch DataLoader to achieve the best performance.\n",
        "\n",
        "The variable `CREATE_NEW_TOKENIZED_DATASET` decides if we use the existing tokenized dataset or re-create new ones. Using existing tokenized dataset saves time if the training data and verification data do not change from the previous training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8Ll9mvD-JIMq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672,
          "referenced_widgets": [
            "ce45359114b34afebf4e46f0115dde8f",
            "f9ee516d27f640c5a6bf98eb0f7f5e2b",
            "d6099042f12a42989d90f411e7085e9e",
            "078ab9ae2f534e61a565b56f80700371",
            "97a448a2152245d484d1b8454f1ad1c0",
            "4293247c42bd4a8dac167a2de9122eca",
            "841eb259cae1414fb4c5d40f67f3e613",
            "c708c17b97cd4b849680bc89df9e4147",
            "6bc9cdda3ac74fa0a21f87ba13f0f1e1",
            "d90b623c405d4640bf21b24bf3531722",
            "c81f6c4e4f9247939157ffcb7c05f7ba",
            "c6110a63dbfe4ce8a995ff7ae56c3c51",
            "4a0ca1c965ed456e908c6df0963f3df0",
            "83f69f9f65f9443abb2c96cee2b7c6a8",
            "0399253c060a46a1abf59014c5814647",
            "55ebc8492d7d453f8f27d1d5898aea5b",
            "e367cf6db65d4b99af750c1001ad5f0e",
            "e4534c0ca08541fda845afbfa774e181",
            "876099633c43465ba92598a18ed0e68f",
            "feeb350195f44d87b1bfc81d2c059121",
            "e73e1a77ccbe4611994f71aa25a6c438",
            "b4bb0d6b9f7940db8a1046a5d8b309fa",
            "ab7c61a564364a519266b006bfcfb6c0",
            "837a81a4eed943bd9667790b1927e135",
            "94ad5ec3c28647d4afaccbb945294b7e",
            "b86dd08692fe4e20b8380204a9adbc58",
            "caac8fb62a2d4ed2a11a01364a3a2a3a",
            "251ad3c1e3e24ac1876126ececedc333",
            "3d5fba239e0b41dd942fa0e619c59b93",
            "584b896dba314e8da1771c4c3867e31e",
            "a9964c1451754971b6c4175b5bd6a0cb",
            "cc90f475afb445ecb98797ebc46d40d5",
            "5022fd03f3734aed9d5dcf6f96f26788",
            "63a516c6b8214f6caf02797841f2d206",
            "12807c9cf33e4be48115e5403ca8db03",
            "38c8b8c0b7e44990833f1a23ebac16ff",
            "4b6ae9a86892497db5e657e4725fb557",
            "e978f6011d32469e8aa84030a0bfdd17",
            "18d063ed7b27436497bc7b61f7eef93a",
            "324ef4c515ff45acbf11f4628c8a8229",
            "00c4cf0d0d78454381f1b9be5f1cccca",
            "a044f22a9ca14e71a28dcd76802971c6",
            "ea3c725a4e234670a90646eed642b073",
            "36dfbfdcd3df4c9ea35d6dbaa5b73b25"
          ]
        },
        "outputId": "160fa02a-a8dc-4b54-e79e-a8f35a095dc4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataset records: (556, 3)\n",
            "val_dataset records: (140, 3)\n",
            "selected train_dataset records: (100, 3)\n",
            "selected val_dataset records: (40, 3)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce45359114b34afebf4e46f0115dde8f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c6110a63dbfe4ce8a995ff7ae56c3c51"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab7c61a564364a519266b006bfcfb6c0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/40 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "63a516c6b8214f6caf02797841f2d206"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "CREATE_NEW_TOKENIZED_DATASET = True\n",
        "\n",
        "train_df = pd.read_csv(project_path + \"/dataset/training.csv\")\n",
        "train_df.dropna(subset=['Text', 'Groundtruth'], inplace=True, axis=0)\n",
        "val_df = pd.read_csv(project_path + \"/dataset/eval.csv\")\n",
        "val_df.dropna(subset=['Text', 'Groundtruth'], inplace=True, axis=0)\n",
        "\n",
        "text_len = train_df['Text'].str.len()\n",
        "plt.hist(text_len, bins = 20)\n",
        "plt.title(\"Token Number Distribution\")\n",
        "plt.xlabel(\"Total Tokens\")\n",
        "plt.ylabel(\"Sample Number\")\n",
        "plt.show()\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "\n",
        "print(\"train_dataset records:\", train_dataset.shape)\n",
        "print(\"val_dataset records:\", val_dataset.shape)\n",
        "train_dataset = train_dataset.select(range(100))\n",
        "val_dataset = val_dataset.select(range(40))\n",
        "print(\"selected train_dataset records:\", train_dataset.shape)\n",
        "print(\"selected val_dataset records:\", val_dataset.shape)\n",
        "\n",
        "if CREATE_NEW_TOKENIZED_DATASET:\n",
        "    # tokenize the data te prepare for the training\n",
        "    train_dataset = train_dataset.map(\n",
        "        model_action.process_data_to_model_inputs,\n",
        "        batched=True,\n",
        "        batch_size=model_action.batchsize)\n",
        "\n",
        "    val_dataset = val_dataset.map(\n",
        "        model_action.process_data_to_model_inputs,\n",
        "        batched=True,\n",
        "        batch_size=model_action.batchsize)\n",
        "\n",
        "    train_dataset.set_format(\n",
        "        type=\"torch\",\n",
        "        columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\n",
        "    )\n",
        "    val_dataset.set_format(\n",
        "        type=\"torch\",\n",
        "        columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\n",
        "    )\n",
        "    # save to disk: https://huggingface.co/docs/datasets/v1.5.0/processing.html#:~:text=You%20can%20save%20your%20dataset,objects%2C%20you%20can%20use%20datasets.\n",
        "    train_dataset.save_to_disk(project_dataset_path + \"/train_dataset/\")\n",
        "    val_dataset.save_to_disk(project_dataset_path + \"/val_dataset/\")\n",
        "\n",
        "else:\n",
        "    # load from the existing formmated tokenized dataset\n",
        "    train_dataset = load_from_disk(project_dataset_path + \"/train_dataset/\")\n",
        "    val_dataset = load_from_disk(project_dataset_path + \"/val_dataset\")\n",
        "\n",
        "train_data_loader = DataLoader(train_dataset, batch_size = model_action.batchsize, shuffle=True,drop_last=True)\n",
        "val_data_loader = DataLoader(val_dataset, batch_size = model_action.batchsize, shuffle=True,drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfEOwmQLakt0"
      },
      "source": [
        "# 3. Fine-tune the Model and Save Check-point\n",
        "\n",
        "The fine tune only occurs in the last layer: lm_head and final_logits_bias. All of the other layers' parameters are frozen.\n",
        "\n",
        "There is early-exit mechanism. When the val_loss continues going up, stop the training process. The latest checkpoint model is saved to sub folder `Checkpoints`. All the training and verification loss, and rouge metrics are saved to folder `Runing_result`too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "o4oOckh1JIMq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "44900722-c9af-4855-cfcd-6d73789a82d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/8 [00:00<?, ?it/s]\n",
            "0it [00:00, ?it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, batch 0, training loss: 3.61143159866333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "1it [00:05,  5.12s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, batch 1, training loss: 3.643406867980957\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "2it [00:08,  4.06s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, batch 2, training loss: 4.1581196784973145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "3it [00:11,  3.78s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, batch 3, training loss: 3.6040360927581787\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "4it [00:15,  3.65s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, batch 4, training loss: 3.8111531734466553\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "5it [00:18,  3.59s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, batch 5, training loss: 4.106921672821045\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "6it [00:22,  3.54s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, batch 6, training loss: 3.6163923740386963\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "7it [00:25,  3.52s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, batch 7, training loss: 3.6028873920440674\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "8it [00:29,  3.50s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, batch 8, training loss: 3.682450771331787\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "9it [00:32,  3.49s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, batch 9, training loss: 3.732048273086548\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "10it [00:36,  3.49s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, batch 10, training loss: 3.6100430488586426\n",
            "epoch 0, prediction loss: 3.4481613636016846\n",
            "\n",
            "prediction text:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[' time timeadic time series motifs are determined by the relative magnitude and ordinal order of three data points that chosen from the time series. when times are different by the types types of on the data on full bythe study is from the horizontals of horizontal visibility graphs ( well considers not visibility and magn of the points as the to the between the series motifs explore the than structures of to the visibility graphs (s. x,, bbbbbbbbbbbbabababababbybyby)=()=()=(bbbbbbpbpbpbpbpbpbpbpb',\n",
              " ' we we sampling introduces a new probabilistically safe local steering primitive for sampling-based motion planning in complex high-dimensional con spaces, our proposed is based on a ax probabilistically safe corridors around tangent hyperplanes of cond intervalsipso- of gaussian mixture models learned from prior collision history. we using a random motion planning graph towards a sample goal using its onto prob prob, we proposed e exploitsages the geometry of guide proper steering direction and adapt steering stepsize. we proposed local method is evaluated to generate effective steering around d regions of narrow passages and minimizing collision likelihood.  of simulation planning scenarios on both simulation and on',\n",
              " ' we weirillov-reshetikhin modules k )modules constitute an families of the quantum anine algebra uq(b g )in to the complex simple lie algebra.. the how a k module orposes into anreducible uq-modules is an fundamental problem in in the fermionic formula by killov and reshetikhin gives a explicit to it is not used to prove it, in kino removal rule is a explicit and for decom type g, but the polyhedral formula with for exceptional type g. but it with multiplicityities greater largely conjectural. in paper presents a method to']"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "11it [00:40,  3.81s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, batch 11, training loss: 3.7802534103393555\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "12it [00:44,  3.66s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, batch 12, training loss: 3.8396079540252686\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "13it [00:47,  3.60s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, batch 13, training loss: 3.9204154014587402\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "14it [00:50,  3.56s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, batch 14, training loss: 4.204233169555664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "15it [00:54,  3.53s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, batch 15, training loss: 3.7488744258880615\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "16it [00:57,  3.52s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, batch 16, training loss: 3.564725160598755\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "17it [01:01,  3.51s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, batch 17, training loss: 3.961836576461792\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "18it [01:04,  3.50s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, batch 18, training loss: 3.413154363632202\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "19it [01:08,  3.49s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, batch 19, training loss: 3.945551633834839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "20it [01:11,  3.49s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, batch 20, training loss: 4.006403923034668\n",
            "epoch 0, prediction loss: 3.9842629432678223\n",
            "\n",
            "prediction text:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[' the the bre of a improved betation of the bremsstrahlung cross section of electron electron in the atomic electric field using the ordered ordered perturbative theory. the results are compared with the bethe-heitinler formula. which a strong screening parameter- dependent cross section, predicted for by previous bre.  results of the importance of the the-- and and a soft version for the the cross.  words : bremsstrahlung process timeed, time screening e. x                        ',\n",
              " ' we we rkn-type method is order p is the simpl for the regular problems,the simplifying assumptions are introduced for thekn methods crkn methods. and the assumption being legendre polynomials..the methodence of the r for the theorem of a crkn method is the p{p, +, +,is proved.den using the rodratic theorem to aation the of we method methodrkn method is obtained. the legendic rlynomial method.the method of the methodkn method is of in on the legend of the legendlynomial coefficients the legendrature of. simpl',\n",
              " ' we we time of a new to tri series class based on triadic time series motifs and we types of tris are proposed and investigated in and are extracted from log types maps and based analyzing the occurrence of we of of different time series are be estimated and and to classification classification of based proposed is applied on the ucr time series class archive and provides that classification than the dynamic time wrappingping method some data.  words : : series motif, time, motif analysis, similarity dynamic time wrappingping. k..bbbbddddddddddddddddddddddddddbddabdd']"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "21it [01:16,  3.82s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, batch 21, training loss: 3.705491304397583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "22it [01:19,  3.68s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, batch 22, training loss: 4.082788944244385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "23it [01:23,  3.62s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, batch 23, training loss: 3.4851491451263428\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "24it [01:26,  3.58s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, batch 24, training loss: 3.840693473815918\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "25it [01:30,  3.61s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch {}'s average training loss: {} 3.7871227931976317\n",
            "epoch {}'s average verification loss: {} 3.7162121534347534\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|‚ñà‚ñé        | 1/8 [01:53<13:11, 113.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The checkpoint model is saved after finishing epoch {epochi}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, batch 0, training loss: 4.1153669357299805\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "1it [00:04,  4.48s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, batch 1, training loss: 4.287501811981201\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "2it [00:07,  3.90s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, batch 2, training loss: 4.011754989624023\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "3it [00:11,  3.72s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, batch 3, training loss: 3.6333727836608887\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "4it [00:14,  3.63s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, batch 4, training loss: 3.4667091369628906\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "5it [00:18,  3.58s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, batch 5, training loss: 3.9790239334106445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "6it [00:21,  3.55s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, batch 6, training loss: 4.087514877319336\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "7it [00:25,  3.53s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, batch 7, training loss: 3.8476529121398926\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "8it [00:28,  3.51s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, batch 8, training loss: 3.5969176292419434\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "9it [00:32,  3.51s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, batch 9, training loss: 3.7919886112213135\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "10it [00:35,  3.51s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, batch 10, training loss: 4.059605598449707\n",
            "epoch 1, prediction loss: 3.3021440505981445\n",
            "\n",
            "prediction text:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[' the therom in bre ph in to bre of oons or nuclei in bre as bremsstrahlung. which second which in nearly branches of physics. recentlyhe and heitler ( a quantum-mechanical formula of themsstrahlung emission which to the bethe-heititler formula. which used in astroph electrom and astrophysics. recently, a puzzled puzzled of the-ray energy spectra of the validity. validity. in new bremsstrahlung formula is the potential is derived, where the ordered ordered perturbative theory to separate the and radiation processes atthe formula formula predicts',\n",
              " ' we we aim is a method for proving ahedral formulas for which on the case of the functions inoted by p and q, when is the importance necessary to prove a identity and which the the of and and and weightsetries of in method also discusses some to computing the and coefficients the equality of co co co. given cases of in is with discussing some practical complexity involved by proving proof of some a to furtherifying. , the text is the method approach to proving ahedral formulas for discusses at some issues in need arise in the proof.  -@     b   ab     ',\n",
              " ' in in prove the attractivity of we system invariant set for to be determined for the system. to system consists {,, in, r,, rn, im)t|ii + ri, im, i =,, n} is an to be aant underthe as is shown as :z dd = g) with the cases of z of on the positiveplanes structure are the systemant set are discussed respectivelythe is shown that the system e the system is globally attractive on the. to asymptotic behavior of solutions of also. the two functions functions as and)| f)  as e']"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "11it [00:40,  3.84s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, batch 11, training loss: 3.492631435394287\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "12it [00:43,  3.69s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, batch 12, training loss: 3.59307599067688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "13it [00:47,  3.64s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, batch 13, training loss: 3.158764123916626\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "14it [00:50,  3.60s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, batch 14, training loss: 3.6748383045196533\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "15it [00:54,  3.57s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, batch 15, training loss: 3.590085983276367\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "16it [00:57,  3.55s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, batch 16, training loss: 3.410536766052246\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "17it [01:01,  3.54s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, batch 17, training loss: 3.9265129566192627\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "18it [01:04,  3.53s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, batch 18, training loss: 3.8663792610168457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "19it [01:08,  3.53s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, batch 19, training loss: 3.675159454345703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "20it [01:11,  3.52s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, batch 20, training loss: 3.2179858684539795\n",
            "epoch 1, prediction loss: 3.978102684020996\n",
            "\n",
            "prediction text:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[' adaptive adaptive performance of the guided and adaptive - the - - r- )rt )plan adaptive dynamic ( both local and a real robot. in of evaluated by comparing the-rrt with several rrt planners andinussian mixture models ( learned online for collision generated the rrt planning and and and degrees sizes for for on the number data space andin gams learning is around for clusters collision,in-rrt is shown on a real humanoid robot and and that comp performance over a non performance to the r.  addition- performance (, the-rrt is that comp time and higher collision checks than to the rrt planners',\n",
              " ' we we aim on a concepts and lemmas for to the stability and hur thezler and hurwitz andand metagonally stable matrices,the is that between the mat properties and the ammas for the and of awitz and andzler matrices andthe, it is the description a smooth dynamical system and and as the of susceptible that susceptible to immun, and recovered, and well as the of to the recovery with the susceptibility.the results also discusses on the question of di di invariant system of the context of the smoothical system. by a smooth set.the -..    ',\n",
              " ' we we continuous of a continuous of symmetric integrators by continuous-stage runge-kutta-nyystr omm- for the crucial based there polynomial expansion and with symmetric conditions order conditions is as families solutions are the numericalurbed pendul equation and derived by the families.  of symmetric integrators are derived in theussian- type and lobatto-type quadrature formulas.  familiesrature formulas can also be considered for devising symmetric integrators.  newgments  references : provided.  the by to theian boundary, ham methods of. the energy. - ']"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "21it [01:16,  3.85s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, batch 21, training loss: 3.7950847148895264\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "22it [01:19,  3.71s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, batch 22, training loss: 3.661559820175171\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "23it [01:23,  3.65s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, batch 23, training loss: 3.4449994564056396\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "24it [01:26,  3.60s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, batch 24, training loss: 3.5344223976135254\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "25it [01:30,  3.62s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch {}'s average training loss: {} 3.716777830123901\n",
            "epoch {}'s average verification loss: {} 3.6401233673095703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|‚ñà‚ñà‚ñå       | 2/8 [03:28<10:16, 102.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The checkpoint model is saved after finishing epoch {epochi}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2, batch 0, training loss: 3.8122239112854004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "1it [00:04,  4.09s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2, batch 1, training loss: 3.6750879287719727\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "2it [00:07,  3.74s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2, batch 2, training loss: 3.6464834213256836\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "3it [00:11,  3.64s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2, batch 3, training loss: 3.6549956798553467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "4it [00:14,  3.59s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2, batch 4, training loss: 3.8293402194976807\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "5it [00:18,  3.57s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2, batch 5, training loss: 3.362358331680298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "6it [00:21,  3.56s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2, batch 6, training loss: 3.9340903759002686\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "7it [00:25,  3.54s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2, batch 7, training loss: 3.331737518310547\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "8it [00:28,  3.54s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2, batch 8, training loss: 3.3808741569519043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "9it [00:32,  3.53s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2, batch 9, training loss: 3.1351912021636963\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "10it [00:35,  3.53s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2, batch 10, training loss: 3.5884928703308105\n",
            "epoch 2, prediction loss: 3.815767526626587\n",
            "\n",
            "prediction text:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[' adaptive adaptive performance of the guided and adaptive - the walks - r- )rt )plan adaptive dynamic ( both local and a real robot. in measures evaluated by comparing the-rrt with several rrt planners andinussian mixture models ( learned online for collision generated the rrt planning and and and degrees sizes for for on the number data space andin gams learning is around for clusters collision,in-rrt is shown on a real humanoid robot and and that comp performance over a non performance to the r.  addition- performance (, the-rrt is that comp time and higher collision checks than to the rrt planners',\n",
              " ' a a sir proposes a node-based susceptibleiss epidemic model with complex networks with where the infective propagation and theoretical propagation is a asymptot stability of the endemic equilibrium and theoretical -ical and of the results that and that strong correlation between network degree and infected rate,the show that the network average average infected percentage are with the increase rate of media epidemicive media and the stable by the infected and infectedivity and the- connected networks n-world networks, and the exp with the appropriate of the e e rate of  impact show that the control rate can be the rates at by         ',\n",
              " ' the the bethe-heititler formula describes bremsstrahlung of high energy energy electrons in a pure couomb potential, which leads to an inn total cross section since to the long-range interaction of theomb scattering. however get the problem we natural potential is be used to which the complex interference e scatter and radiation sub-processes makesicates the an analytical solution for for using-deriving the formula for the to- framework, we is proved that the cor of a at high energy maypose the bremsstrahlung cross section to two sub-processes at which to a r bre for contains the']"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "11it [00:40,  3.87s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2, batch 11, training loss: 3.084702968597412\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "12it [00:43,  3.72s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2, batch 12, training loss: 3.201852560043335\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "13it [00:47,  3.66s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2, batch 13, training loss: 4.3889360427856445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "14it [00:50,  3.62s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2, batch 14, training loss: 3.582180976867676\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "15it [00:54,  3.59s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2, batch 15, training loss: 3.692124128341675\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "16it [00:57,  3.56s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2, batch 16, training loss: 3.830979585647583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "17it [01:01,  3.55s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2, batch 17, training loss: 3.981184959411621\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "18it [01:04,  3.54s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2, batch 18, training loss: 3.7965855598449707\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "19it [01:08,  3.54s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2, batch 19, training loss: 3.60446834564209\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "20it [01:11,  3.53s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2, batch 20, training loss: 3.5316596031188965\n",
            "epoch 2, prediction loss: 3.4562184810638428\n",
            "\n",
            "prediction text:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[' in in prove the attractivity of we system invariant set is to be determined for the system. to system consists {,, in, r,, rn, im)t|ii + ri, im, i =,, n} is an to be aant underthe as is shown as :z dd = g) with the cases of z of on the positiveplanes structure are the systemant set are discussed respectively to is shown that the system e the system is globally attractive on the. to asymptotic behavior of solutions of explored. d two functions functions as and)| f)  as e',\n",
              " ' in in concept is the advantages of numerical methods methods which preserve at features of aical systems with it with as symmlectic methods symmetric and and-preserving, invari invari integrators are presented. and of to a dynam with. itmmetric integrators are which,ge-kutta-, partition shown as reversible ability to preserve theibility and and to a long-time numerical behavior. -stage methods, also as theizations of traditional numerical -stage methods with and a of terms and numerical of it paper is includes the development of new numericaletric integrators for reversible- and systems differential equations and and the advantages of',\n",
              " ' let let f is a concepthedral formula for the decomposition of kr modules, the form of the integral weights.the is the f of a integers j dominant j the n set of and a formula for the decomposition of the - andnomial.the formulaermionic formula is used to decompose the k of the zx to a in can the form decom.the f also a f for a separate argument of to prove the poly. the m values of m z in -,.      iiiiiiiii         by   dbbyabpb']"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "21it [01:16,  3.85s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2, batch 21, training loss: 4.209298133850098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "22it [01:19,  3.70s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2, batch 22, training loss: 3.3951148986816406\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "23it [01:23,  3.64s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2, batch 23, training loss: 3.9101579189300537\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "24it [01:26,  3.60s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2, batch 24, training loss: 3.4429385662078857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "25it [01:30,  3.62s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch {}'s average training loss: {} 3.6401223945617676\n",
            "epoch {}'s average verification loss: {} 3.635993003845215\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [05:03<08:16, 99.25s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The checkpoint model is saved after finishing epoch {epochi}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3, batch 0, training loss: 3.294358015060425\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "1it [00:04,  4.61s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3, batch 1, training loss: 3.7523341178894043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "2it [00:08,  3.96s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3, batch 2, training loss: 3.5546653270721436\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "3it [00:11,  3.76s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3, batch 3, training loss: 3.2083590030670166\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "4it [00:15,  3.66s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3, batch 4, training loss: 3.666790723800659\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "5it [00:18,  3.61s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3, batch 5, training loss: 3.7569353580474854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "6it [00:22,  3.58s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3, batch 6, training loss: 3.340928792953491\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "7it [00:25,  3.56s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3, batch 7, training loss: 3.559065103530884\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "8it [00:29,  3.55s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3, batch 8, training loss: 4.864933013916016\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "9it [00:32,  3.54s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3, batch 9, training loss: 3.079272747039795\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "10it [00:36,  3.53s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3, batch 10, training loss: 3.6991848945617676\n",
            "epoch 3, prediction loss: 3.2877097129821777\n",
            "\n",
            "prediction text:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[' let letirillov-reshetikhin modules k )modules are an families of the quantum anine algebra uq(b g )in with the complex simple lie algebra.. the how a k module orposes into anreducible uq-modules is an fundamental problem in in the fermionic formula by killov and reshetikhin gives a explicit to it is not used to prove it, in kino removal rule is a explicit and for decom type g. but the polyhedral formula with for exceptional type g. but it with multiplicityities greater largely conjectural. in paper presents a method to',\n",
              " ' in in poly is thatorem that  showing that polyhedral formula for res = lie algebra of type f when a = or, a functions in t. the is a newhedral formula for a = for a the table of q m, of show the formula.the proof is a table of a and a froma the w. dis-invariant wthe the section, we weyl orbitator is = sub sub are explicitly explicitlythe, the is made to the accompanying to computing theyl orbits of cos minimal notebook for some computer calculations. -.. ggg         ',\n",
              " ' in in sampling introduces a new probabilistically safe local steering primitive for sampling-based motion planning in complex high-dimensional con spaces, our proposed is based on a ax probabilistically safe corridors around tangent hyperplanes of cond intervalsipso- of gaussian mixture models learned from prior collision history. we using a random motion planning graph towards a sample goal using its onto prob prob, we proposed e exploitsages the geometry of guide proper steering direction and adapt steering stepsize. we proposed local method is evaluated to generate effective steering around d regions of narrow passages and minimizing collision likelihood.  of simulation planning scenarios on both simulation and on']"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "11it [00:40,  3.87s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3, batch 11, training loss: 3.8182547092437744\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "12it [00:44,  3.72s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3, batch 12, training loss: 3.353400230407715\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "13it [00:47,  3.66s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3, batch 13, training loss: 3.5185773372650146\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "14it [00:51,  3.61s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3, batch 14, training loss: 4.127179145812988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "15it [00:54,  3.59s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3, batch 15, training loss: 3.7785282135009766\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "16it [00:58,  3.56s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3, batch 16, training loss: 3.7211902141571045\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "17it [01:01,  3.55s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3, batch 17, training loss: 3.738403081893921\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "18it [01:05,  3.54s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3, batch 18, training loss: 3.7025277614593506\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "19it [01:08,  3.53s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3, batch 19, training loss: 3.622274875640869\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "20it [01:12,  3.52s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3, batch 20, training loss: 3.1189095973968506\n",
            "epoch 3, prediction loss: 3.657827138900757\n",
            "\n",
            "prediction text:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[' the therom in bre ph in to bre of proons or nuclei in bre as bremsstrahlung. which second which in nearly branches of physics. recentlyhe and heitler ( a quantum-mechanical formula of themsstrahlung emission which to the bethe-heititler formula. which used in astroph electrom and astrophysics. recently, a puzzled die of the-ray energy spectra of the validity. validity. in new bremsstrahlung formula is the potential is derived using where the ordered ordered perturbative theory to separate the and radiation processes atthe formula formula predicts',\n",
              " ' we we m is a computation complexity involved in the the poly of the rational identities for coted by c and d, we main for obtained explicitly the computer polyhedral formula for the the the functions inwe main d case arises when in the k sub, to case are carried on a computer and a a the weyl group w disets w check the denomin for then their the is d. we results is illustrated and and the computation of the numericalets and the computation consuming to the computer calculation. be the +- thegggghab                ab ',\n",
              " ' symm symmmmetric integ are numerical numerical of numericalrs - are are the certain conditions for a. in numerical one-step method is called symmetric if it sat the conditions. are that numerical method and the adjoint method give identical numerical. in conditions for a crkn method to be symmetric are the the conditions conditions and in introducing these conditions, we crkn method is be uniquely to be symmetric. in conditions of symm are such the of on legendre polynomials, are be used to construct symmetric methodsrators of arbitrarily high order. in conditions of introducingating constructinguting into the conditions one obtain the']"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "21it [01:16,  3.85s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3, batch 21, training loss: 3.5471670627593994\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "22it [01:20,  3.70s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3, batch 22, training loss: 3.091209888458252\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "23it [01:23,  3.64s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3, batch 23, training loss: 3.8287150859832764\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "24it [01:27,  3.60s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3, batch 24, training loss: 3.2946300506591797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "25it [01:30,  3.64s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch {}'s average training loss: {} 3.6015117740631104\n",
            "epoch {}'s average verification loss: {} 3.4727684259414673\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [06:39<06:31, 97.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The checkpoint model is saved after finishing epoch {epochi}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4, batch 0, training loss: 4.0974440574646\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "1it [00:03,  3.82s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4, batch 1, training loss: 4.021117687225342\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "2it [00:07,  3.64s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4, batch 2, training loss: 3.7283549308776855\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "3it [00:10,  3.57s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4, batch 3, training loss: 3.110077381134033\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "4it [00:14,  3.55s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4, batch 4, training loss: 3.0690648555755615\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "5it [00:17,  3.54s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4, batch 5, training loss: 3.471203565597534\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "6it [00:21,  3.53s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4, batch 6, training loss: 3.646890878677368\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "7it [00:24,  3.52s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4, batch 7, training loss: 3.277514696121216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "8it [00:28,  3.52s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4, batch 8, training loss: 3.2799999713897705\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "9it [00:31,  3.52s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4, batch 9, training loss: 3.362175464630127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "10it [00:35,  3.52s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4, batch 10, training loss: 3.5261919498443604\n",
            "epoch 4, prediction loss: 3.485187530517578\n",
            "\n",
            "prediction text:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[' the the continuous-stage runkn method with a numerical method for for solve the value problems with by second second-order system with as is a extension over the r based it allows less storage and the the work compared compared as paper is based as a of is butcher tableau. which it with the numerical, solving the- and systems,the, the formal of is the continuousrkn method is proposed den and a counterpart of the classical rkn method.the... bbb         dddd  ddab   ddddadababababababab',\n",
              " ' symm symm construction of symm construction of symmetric integrators for continuous-stage runge-kutta-nyystr omm ( for solving systems on in constructionrators are based on solving second-order ordinary die equations on and there expansion technique in with symmetric conditions and simplifying assumptions for order conditions. new construction also the families of symmetric integrators as illust illust experiments to illustrate the numerical behaviors.  words : continuous-stage runge-kutta-nystr√∂m methods, symm systems, symmetric integrators, legendifying assumptions, legend legendre polynomials. w..',\n",
              " ' in in stability of a stability of a spreading models in complex networks with and on the of as mathematical b, physics, social social science,the epidemic spreading with been into assume the and features of theive media and however present of on the node-based siriss epidemic model with anive media on and to understand the in of in of the characteristics on in equilibrium presents a features and and a equilibrium with its of and global global analysis with and provides a simulations to three network topologies tothe global of some parameters on the per of the between network degree are studied theoreticallythe global on to understand the understanding of the propagation on on complex']"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "11it [00:40,  3.86s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4, batch 11, training loss: 3.5988736152648926\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "12it [00:43,  3.71s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4, batch 12, training loss: 3.681891441345215\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "13it [00:46,  3.65s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4, batch 13, training loss: 3.5206124782562256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "14it [00:50,  3.61s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4, batch 14, training loss: 3.587183952331543\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "15it [00:53,  3.58s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4, batch 15, training loss: 3.39093017578125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "16it [00:57,  3.56s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4, batch 16, training loss: 3.444295883178711\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "17it [01:01,  3.55s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4, batch 17, training loss: 3.3275718688964844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "18it [01:04,  3.54s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4, batch 18, training loss: 3.2906172275543213\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "19it [01:08,  3.53s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4, batch 19, training loss: 3.215571641921997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "20it [01:11,  3.52s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4, batch 20, training loss: 3.892331123352051\n",
            "epoch 4, prediction loss: 3.7815775871276855\n",
            "\n",
            "prediction text:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[' in in this paper, we results are performed to compare the numerical of the symm for for symmetric symmkn methods are presented for the to the perturbed pendulum equation with for show that the theetric r are be nearly preserve the over long times of theylectic-stural methods are more for long -term numerical simulation of general nonian systems. formmetric methods are more for general reversible non -hamian systems. in - this,,,,,,bbbbbddbbabddddabababddpbbbbpbpbpbpbbpbpbpbpbpbpbpbpbpb',\n",
              " ' a a spread proposes a node-based susceptibleiss epidemic model with complex networks with where the infective propagation and theoretical propagation is a asymptot stability of the endemic equilibrium and theoretical.ical and of the results that and that strong correlation between network degree and infected rate,the show that the network average average infected percentage are with the increase rate of media epidemicive media and the stable by the spread and infectedivity and the- connected networks small-world networks, and decrease exp with the appropriate of the e recovered rate of  impact also that the control rates can be the rates at by .      ',\n",
              " ' symm symmmmetric integ are numerical numerical of numericalrsk are are the certain conditions for a of in numerical one-step method is called symmetric if it sat the conditions. are that symmetry method and the adjoint method give identical numerical. in conditions for a crkn method to be symmetric are the the conditions conditions and in introducing these conditions, we crkn method is be uniquely to be symmetric. in conditions of symm are such the of on legendre polynomials, are be used to construct symmetric methodsrators of arbitrarily high order. in conditions of introducingating constructinguting into the conditions one obtain the']"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "21it [01:16,  3.85s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4, batch 21, training loss: 3.7182252407073975\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "22it [01:19,  3.70s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4, batch 22, training loss: 3.2881827354431152\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "23it [01:23,  3.65s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4, batch 23, training loss: 3.5217623710632324\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "24it [01:26,  3.60s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4, batch 24, training loss: 3.7460360527038574\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "25it [01:30,  3.60s/it]\n",
            " 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [08:09<04:45, 95.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch {}'s average training loss: {} 3.512564849853516\n",
            "epoch {}'s average verification loss: {} 3.633382558822632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5, batch 0, training loss: 3.428086519241333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "1it [00:03,  3.49s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5, batch 1, training loss: 3.170328140258789\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "2it [00:07,  3.51s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5, batch 2, training loss: 3.1978976726531982\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "3it [00:10,  3.51s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5, batch 3, training loss: 3.6219520568847656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "4it [00:14,  3.51s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5, batch 4, training loss: 3.721456289291382\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "5it [00:17,  3.51s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5, batch 5, training loss: 3.4011173248291016\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "6it [00:21,  3.51s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5, batch 6, training loss: 3.9012489318847656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "7it [00:24,  3.51s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5, batch 7, training loss: 3.4518237113952637\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "8it [00:28,  3.51s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5, batch 8, training loss: 3.7041542530059814\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "9it [00:31,  3.51s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5, batch 9, training loss: 3.654205083847046\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "10it [00:35,  3.51s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5, batch 10, training loss: 3.4775867462158203\n",
            "epoch 5, prediction loss: 3.2346649169921875\n",
            "\n",
            "prediction text:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[' in in study introduces a new probabilistically safe local steering primitive for sampling-based motion planning in complex high-dimensional con spaces based our proposed is based on a ax probabilistically safe corridors around tangent hyperplanes of cond intervalsipso- of gaussian mixture models learned from prior collision history. we using a random motion planning graph towards a sample goal using its onto prob prob, we proposed e exploitsages the geometry of guide proper steering direction and adapt steering stepsize. we proposed local method is evaluated to be effective steering around d regions of narrow passages and minimizing collision likelihood.  of simulation planning scenarios with both simulation and on',\n",
              " ' symm symmmmetric integ are numerical numerical of numericalrsk are are the certain conditions for a of in numerical one-step method is called symmetric if it sat the conditions. are that symmetry method and the adjoint method give identical numerical. in conditions for a crkn method to be symmetric are the the conditions conditions and in introducing these conditions, we crkn method is be uniquely to be symmetric. in conditions of symm are such the of on legendre polynomials, are be used to construct symmetric methodsrators of arbitrarily high order. in conditions of introducingating constructinguting into the conditions one obtain the',\n",
              " ' the the bethe-heititler formula describes bremsstrahlung of high energy energy electrons in a pure couomb potential, which leads to an inn total cross section since to the long-range interaction of couomb scattering. however get the problem we natural potential is be used to which the complex interference e scatter and radiation sub-processes makesicates the an analytical solution for for using-deriving the formula for the to- framework, we is proved that the cor of a at high energy maypose the bremsstrahlung cross section to two sub-processes at which to a r bre for contains the']"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "11it [00:39,  3.85s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5, batch 11, training loss: 3.509800910949707\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "12it [00:43,  3.70s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5, batch 12, training loss: 3.43264102935791\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "13it [00:46,  3.64s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5, batch 13, training loss: 3.82358717918396\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "14it [00:50,  3.61s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5, batch 14, training loss: 3.140645742416382\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "15it [00:53,  3.57s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5, batch 15, training loss: 3.141505718231201\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "16it [00:57,  3.55s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5, batch 16, training loss: 3.6260035037994385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "17it [01:00,  3.54s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5, batch 17, training loss: 3.344977378845215\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "18it [01:04,  3.53s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5, batch 18, training loss: 3.232758045196533\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "19it [01:07,  3.52s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5, batch 19, training loss: 3.4259095191955566\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "20it [01:11,  3.52s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5, batch 20, training loss: 3.3704686164855957\n",
            "epoch 5, prediction loss: 3.4786529541015625\n",
            "\n",
            "prediction text:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[' in in continuous on the-stage runge-kutta-nyystr omm methods and their applications parameter inin is the the notion of the some results which are in constructing such of arbitrarily order ininch --@@@,gngngngnibibgnadadgnibgdgdgdgdgdgdgdgdgdgdbbivalentgdgdpbivalentivalentddgdddababababadjadjababababadenababababababababababababababababababababababababababababababababab',\n",
              " ' time time proposed of a new to tri series class based on triadic time series motifs and we types of tris are proposed and investigated in and are extracted from log types maps and based analyzing the occurrence with we of are different time series are be estimated and and to classification classification of based proposed is applied on the ucr time series class archive and provides evidence classification than the dynamic time wrappingping method some data.  words : : series motif, time, motif analysis, dynamic dynamic time wrappingping. k..bbbbddddddddddddddddddddddddddddabdd',\n",
              " ' we we modules form areducible n ndimensional representations of the quantum anine algebra uq(b g )where q c a a root of unity. in i module is to a n-dimensional uq-module resoted by res w m. which of the spectral parameter.. we q of k modules satisfy the die equation that the q-system fromnajima and her proved the the q-characters of satisfy the t-system fromn q is the of the q recurrence relation with in the appendix m ) m the relation function q := nÔøΩ( m tm. in is a q properties']"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "21it [01:15,  3.85s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5, batch 21, training loss: 3.6731929779052734\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "22it [01:19,  3.70s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5, batch 22, training loss: 3.3984782695770264\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "23it [01:22,  3.65s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5, batch 23, training loss: 3.68205189704895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "24it [01:26,  3.60s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5, batch 24, training loss: 3.1801438331604004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "25it [01:29,  3.59s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch {}'s average training loss: {} 3.4684808540344236\n",
            "epoch {}'s average verification loss: {} 3.356658935546875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [09:44<03:09, 94.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The checkpoint model is saved after finishing epoch {epochi}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6, batch 0, training loss: 3.3535265922546387\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "1it [00:04,  4.61s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6, batch 1, training loss: 3.188161849975586\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "2it [00:08,  3.97s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6, batch 2, training loss: 3.3052430152893066\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "3it [00:11,  3.76s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6, batch 3, training loss: 3.557513952255249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "4it [00:15,  3.66s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6, batch 4, training loss: 3.5338077545166016\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "5it [00:18,  3.60s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6, batch 5, training loss: 3.2852227687835693\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "6it [00:22,  3.58s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6, batch 6, training loss: 3.4955482482910156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "7it [00:25,  3.55s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6, batch 7, training loss: 3.8289804458618164\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "8it [00:29,  3.54s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6, batch 8, training loss: 3.3129844665527344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "9it [00:32,  3.53s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6, batch 9, training loss: 3.5324923992156982\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "10it [00:36,  3.53s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6, batch 10, training loss: 3.4316327571868896\n",
            "epoch 6, prediction loss: 3.4140002727508545\n",
            "\n",
            "prediction text:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[' in in continuous on the-stage runge-kutta-nyystr omm methods and their applications parameter inin is the the notion of the some results which are in constructing such of arbitrarily order ininch --@@@,gngngngnibibgnadadgnibgdgdgdgdgdgdgdgdgdgdbbivalentgdgdpbivalentivalentddgdddababababadjadjababababadenababababababababababababababababababababababababababababibabababab',\n",
              " ' in in prove the attractivity of we system invariant set is to be determined for a system. to system consists {,, in, r,, rn, im)t|ii + ri, im, i =,, n} is an to be aant underto as is shown as :z dd = g) with the cases of z of on the positiveplanes structure are the systemant set are discussed respectively to is shown that the system e the system is globally attractive on {}. to asymptotic behavior of solutions of explored by d two functions functions as and)| f) both as e',\n",
              " ' in in concept is the advantages of numerical methods methods which preserve at features of aical systems with it with as symmlectic methods symmetric and and-preserving, invari energy-rators are presented. and of to a dynam with. itmmetric integrators are which,ge-kutta-, are shown as reversible ability to preserve theibility and and to a long-time numerical behavior. in-stage methods are also as theizations of traditional numerical-stage methods with and a of terms and numerical of it paper is includes the development of new numericaletric integrators for reversible- and systems differential equations and and their advantages of']"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "11it [00:40,  3.86s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6, batch 11, training loss: 3.7438859939575195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "12it [00:44,  3.71s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6, batch 12, training loss: 3.0432071685791016\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "13it [00:47,  3.65s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6, batch 13, training loss: 3.4355738162994385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "14it [00:51,  3.61s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6, batch 14, training loss: 2.8275694847106934\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "15it [00:54,  3.58s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6, batch 15, training loss: 3.5297579765319824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "16it [00:58,  3.56s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6, batch 16, training loss: 3.5498807430267334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "17it [01:01,  3.54s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6, batch 17, training loss: 3.525481700897217\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "18it [01:05,  3.54s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6, batch 18, training loss: 2.670114040374756\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "19it [01:08,  3.53s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6, batch 19, training loss: 4.020702838897705\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "20it [01:12,  3.52s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6, batch 20, training loss: 3.7514455318450928\n",
            "epoch 6, prediction loss: 3.0494790077209473\n",
            "\n",
            "prediction text:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[' let let poly is a concepthedral formula for the decomposition of kr modules, the form of the integral weights forlet is the f of a integers j dominant j the n set of and a formula for the decomposition of the- andnomial.in formulaermionic formula is used to decompose the k of the zx to a in can the form decom.the f also a use for a proof argument of to prove the poly. the m values of m. in.,....   iiiiiiiii  }\\\\     d  abdbabab',\n",
              " ' we we poly of a method to prove a polyhedral branching formula for killov- reshetikhin modules over a quantum anine algebra. when method is which conjectural, the type lie algebras, is proved into an identity between two functions of a single variable with by identity isifies the proof of by a theities of the at known locations locations. by using this strategy, we computer provides a computer-assisted proof of a conjectural polyhedral formula in type f. -...,                      ',\n",
              " ' in in design of a prob steering approach for sampling- based motion planning using probabiistically safe corridors of con gaussian mixture models of con spaces. we approach is a corridors using con using con a prob steering primitive that extend a planning towards a goals using which collision likelihood. in proposed is the performance in challenging regions, es as es passages, by adjusting steering direction and steps size. inulating and experiments with a real robot manipulator demonstrate that improvement improvement over the straight for  work using to extend this work to using online gaussian mixture learning learning for uncertainty-aware adaptive planning. -....g    ']"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "21it [01:16,  3.85s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6, batch 21, training loss: 3.3844313621520996\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "22it [01:20,  3.70s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6, batch 22, training loss: 3.0963919162750244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "23it [01:23,  3.64s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6, batch 23, training loss: 3.749728202819824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "24it [01:27,  3.60s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6, batch 24, training loss: 3.379984140396118\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "25it [01:30,  3.63s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch {}'s average training loss: {} 3.4213307666778565\n",
            "epoch {}'s average verification loss: {} 3.231739640235901\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [11:19<01:35, 95.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The checkpoint model is saved after finishing epoch {epochi}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 7, batch 0, training loss: 3.06221866607666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "1it [00:03,  3.63s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 7, batch 1, training loss: 3.524139881134033\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "2it [00:07,  3.55s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 7, batch 2, training loss: 3.275552749633789\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "3it [00:10,  3.54s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 7, batch 3, training loss: 3.813321590423584\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "4it [00:14,  3.52s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 7, batch 4, training loss: 3.0654077529907227\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "5it [00:17,  3.52s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 7, batch 5, training loss: 3.338245391845703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "6it [00:21,  3.52s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 7, batch 6, training loss: 2.941864252090454\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "7it [00:24,  3.52s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 7, batch 7, training loss: 3.0206871032714844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "8it [00:28,  3.53s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 7, batch 8, training loss: 3.462301731109619\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "9it [00:31,  3.52s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 7, batch 9, training loss: 3.4967775344848633\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "10it [00:35,  3.52s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 7, batch 10, training loss: 3.2207982540130615\n",
            "epoch 7, prediction loss: 3.671844720840454\n",
            "\n",
            "prediction text:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[' time time series motif is theifying the similarity of two series and which is a for explor time in the time to as theuclidean distance and correlation time warping ( widely to measure similarity between but the time warping being being a performance than the a optimization to in series motifs are which the subsequences in time series data are useful to exploratory data mining and often of. inadic time series motifs are which by network motifs in horizontal visibility graphs, are useful used in discover time time data and in have used timeadic time series motifs to discover the rates of heart market dynamics, and the time series generated com',\n",
              " ' a a spread proposes a node-based susceptibleiss epidemic model with complex networks with where the infective propagation and theoretical propagation is a asymptot stability of the endemic equilibrium and theoretical.ical simulations of the results that and that strong correlation between network degree and infected rate,the show that the network average average infected percentage are with the increase rate of media epidemicive media and the stable by the spread and infectedivity and the- connected networks small-world networks and and decrease exp with the appropriate of the e recovered rate of  impact also that the control rates can be the rates at by .      ',\n",
              " ' in in poly is thatorem that  showing that polyhedral formula for res = lie algebra of type f with a = or, a functions in t. the is a newhedral formula for a = for a the table of q m, of show the formula.the proof is a table of a = a froma the w and dis-invariant wthe the section, we weyl denominator is = sub sub are explicitly explicitlythe, the is made to the accompanying to computing theyl orbits of cos minimal notebook for some computer calculations. -.. ggg         ']"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "11it [00:39,  3.86s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 7, batch 11, training loss: 3.766258716583252\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "12it [00:43,  3.72s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 7, batch 12, training loss: 3.0015134811401367\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "13it [00:46,  3.66s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 7, batch 13, training loss: 3.1536941528320312\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "14it [00:50,  3.62s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 7, batch 14, training loss: 3.9096648693084717\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "15it [00:53,  3.59s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 7, batch 15, training loss: 3.5581886768341064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "16it [00:57,  3.57s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 7, batch 16, training loss: 3.1830387115478516\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "17it [01:00,  3.55s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 7, batch 17, training loss: 3.2470529079437256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "18it [01:04,  3.54s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 7, batch 18, training loss: 3.2748711109161377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "19it [01:07,  3.53s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 7, batch 19, training loss: 3.3959693908691406\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "20it [01:11,  3.52s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 7, batch 20, training loss: 3.1838085651397705\n",
            "epoch 7, prediction loss: 4.152740955352783\n",
            "\n",
            "prediction text:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[' adaptive adaptive rapid of the guided and adaptive iter the access - r- )rt )plan adaptive dynamic ( both local and a real robot. in measures evaluated by comparison the-rrt with several rrt planners andinussian mixture models ( learned online for collision generated the rrt planning and and and degrees sizes for for on the number data space andin gams learning is around for clusters collision andin-rrt is shown on a real humanoid robot and and that comp performance with a non performance to the r.g addition- performance (, g-rrt is a comp time and higher collision checks compared to the rrt planners',\n",
              " ' in in concept is the advantages of numerical methods methods which preserve at features of aical systems with it with as symmlectic methods symmetric and volume-preserving, invari energy-rators are presented. and of to a dynam with. itmmetric integrators are which,ge-kutta- and are shown as reversible ability to preserve theibility and and to a long-time numerical behavior. in-stage methods are also as theizations of traditional numerical-stage methods with and a of terms and numerical of it paper is focuses the development of new numericaletric integrators for reversible-order systems differential equations and and their advantages of',\n",
              " ' in in continuous on the-stage runge-kutta-nyystr omm methods and their applications parameter inin is the the notion of the some results which are in constructing such of arbitrarily order ininch --@@@,gngngngnibibgnadadibibgdgdgdgdgdgdgdgdgdgdbbivalentgdgdivalentivalentivalentddgdddababababadjadjababababadenababababababababababababababababababababababababababababibabababab']"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "21it [01:16,  3.85s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 7, batch 21, training loss: 3.159334421157837\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "22it [01:19,  3.71s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 7, batch 22, training loss: 3.6742279529571533\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "23it [01:22,  3.65s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 7, batch 23, training loss: 3.97924542427063\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "24it [01:26,  3.60s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 7, batch 24, training loss: 3.7098803520202637\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "25it [01:29,  3.60s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [12:49<00:00, 96.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch {}'s average training loss: {} 3.3767225456237795\n",
            "epoch {}'s average verification loss: {} 3.9122928380966187\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 8\n",
        "total_train_loss, total_val_loss, total_train_rouge, total_val_rouge = [], [], [], []\n",
        "torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True\n",
        "best_val_loss = float('inf')\n",
        "#Declare variable for storing the checkpoint\n",
        "checkpoint_filename = \"LED_model_checkpoint.pt\"\n",
        "patience = 3\n",
        "\n",
        "for epochi in tqdm(range(num_epochs)):\n",
        "    val_batch_data_iter = iter(val_data_loader)\n",
        "    train_loss, train_rouge = [], []\n",
        "    val_loss, val_rouge = [], []\n",
        "    for idx, data in tqdm(enumerate(train_data_loader)):\n",
        "        ids = data['input_ids'].to(DEVICE)\n",
        "        am = data['attention_mask'].to(DEVICE)\n",
        "        gam = data['global_attention_mask'].to(DEVICE)\n",
        "        labels = data['labels'].to(DEVICE)\n",
        "\n",
        "        # freeze all the layers except the last layer lm_head\n",
        "        model_action.model.train()\n",
        "        for parameter in model_action.model.parameters():\n",
        "            parameter.requires_grad = False\n",
        "        for parameter in model_action.model.lm_head.parameters():\n",
        "            parameter.requires_grad = True\n",
        "        model_action.model.final_logits_bias.requires_grad = True\n",
        "\n",
        "        # output = model_action.model(input_ids = ids, attention_mask = am, labels = labels, global_attention_mask = gam, use_cache = False)\n",
        "        output = model_action.model(input_ids = ids, attention_mask = am, global_attention_mask = gam, labels = labels, use_cache = False)\n",
        "\n",
        "        torch.device('cpu')\n",
        "        loss = output.loss\n",
        "        logits = output.logits\n",
        "\n",
        "        print(\"epoch {}, batch {}, training loss: {}\".format(epochi, idx, loss.item()))\n",
        "        p_text = model_action.convert_logits_to_text(output.logits)[1:]\n",
        "        e_text = model_action.convert_tokens_to_text(labels)\n",
        "        model_action.log_generated_summary(epochi, e_text, p_text, f\"training batch:{idx}\")\n",
        "\n",
        "        rouge1, rouge2, rougeL = model_action.calculate_rouge_scores(p_text, e_text)\n",
        "        train_rouge.append((rouge1, rouge2, rougeL))\n",
        "\n",
        "        # Backward and optimize\n",
        "        model_action.optimizer.zero_grad()\n",
        "        train_loss.append(loss.item())\n",
        "        loss.backward()\n",
        "        model_action.optimizer.step()\n",
        "\n",
        "        # evaluate once every 20 mini-batches\n",
        "        if idx > 0 and idx % 10 == 0:\n",
        "            val_batch_data = next(iter(val_data_loader))\n",
        "\n",
        "            # model. eval() will notify all your layers that you are in eval mode, that way, batchnorm or dropout layers will work in eval mode instead of training mode. torch. no_grad() impacts the autograd engine and deactivate it.\n",
        "            model_action.model.eval()\n",
        "            with torch.no_grad():\n",
        "                val_data = next(val_batch_data_iter)\n",
        "                val_ids = val_data['input_ids'].to(DEVICE)\n",
        "                val_am = val_data['attention_mask'].to(DEVICE)\n",
        "                val_gam = val_data['global_attention_mask'].to(DEVICE)\n",
        "                val_labels = val_data['labels'].to(DEVICE)\n",
        "                val_output = model_action.model(input_ids = val_ids, attention_mask = val_am, labels = val_labels, global_attention_mask = val_gam, use_cache = False)\n",
        "\n",
        "            torch.device('cpu')\n",
        "            vloss = val_output.loss.item()\n",
        "            val_loss.append(vloss)\n",
        "\n",
        "            print(\"epoch {}, prediction loss: {}\".format(epochi, vloss))\n",
        "            # The first word is always a repeat, so delete it.\n",
        "            pred_text = model_action.convert_logits_to_text(val_output.logits)[1:]\n",
        "            exp_text = model_action.convert_tokens_to_text(val_labels)\n",
        "            model_action.log_generated_summary(epochi, e_text, p_text, f\"verification batch:{idx}\")\n",
        "            rouge1, rouge2, rougeL = model_action.calculate_rouge_scores(pred_text, exp_text)\n",
        "            val_rouge.append((rouge1, rouge2, rougeL))\n",
        "            print(\"\\nprediction text:\")\n",
        "            display(pred_text)\n",
        "\n",
        "    total_train_loss.append(train_loss)\n",
        "    total_val_loss.append(val_loss)\n",
        "    avg_train_loss = np.mean(train_loss)\n",
        "    avg_val_loss = np.mean(val_loss)\n",
        "    total_train_rouge.append(train_rouge)\n",
        "    total_val_rouge.append(val_rouge)\n",
        "    model_action.log_metrics(epochi, avg_train_loss, avg_val_loss, train_rouge, val_rouge)\n",
        "    print(\"epoch {}'s average training loss: {}\", np.mean(train_loss))\n",
        "    print(\"epoch {}'s average verification loss: {}\", np.mean(val_loss))\n",
        "\n",
        "    # The following uses Sudha's code to stop training and save checkpoint when the val loss improves\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        epochs_no_improve = 0\n",
        "        model_action.save_model_checkpoint(checkpoint_filename)\n",
        "        print(\"The checkpoint model is saved after finishing epoch {epochi}\")\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        if epochs_no_improve == patience:\n",
        "            print(f\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "# save running result for evaluation\n",
        "running_res_folder = project_path + \"/Running_result\"\n",
        "if not os.path.exists(running_res_folder):\n",
        "    os.mkdir(running_res_folder)\n",
        "\n",
        "tt_loss = pd.DataFrame(total_train_loss)\n",
        "tt_loss.to_csv(running_res_folder + \"/total_train_loss.csv\")\n",
        "tv_loss = pd.DataFrame(total_val_loss)\n",
        "tt_loss.to_csv(running_res_folder + \"/total_val_loss.csv\")\n",
        "\n",
        "tt_train_rouge = pd.DataFrame(total_train_rouge)\n",
        "tt_train_rouge.to_csv(running_res_folder + \"/total_train_rouge.csv\")\n",
        "tt_val_rouge = pd.DataFrame(total_val_rouge)\n",
        "tt_val_rouge.to_csv(running_res_folder + \"/total_val_rouge.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rjrm3EwaCxXw"
      },
      "source": [
        "# 4. Inference: Test PDF Summarization\n",
        "Load a CSV file from `processed` folder. This CSV file is created after running `processing_pdf.jpynb`. The text of sections, subsections, subsubsections are put in different rows and marked with section number and section titles.\n",
        "\n",
        "Use your own CSV file name to replace the sample `1901.00936v3.csv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Qd6oIgod7KnZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "758067fc-5ad7-4786-95af-6ca55131da79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start generating summary...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
              "0  an efficient linux kernel implementation of service function chaining for legacy vnfs based on ipv6 segment routing andrea mayer*, stefano salsano*, pier luigi ventre*, ahmed abdelsalam {double dagger} {section sign}, luca chiaraviglio*, clarence filsfils {section sign} *university of rome tor vergata, cnit, {double dagger}gran sasso science institute, {section sign}cisco systems extended version of the paper accepted to ieee netsoft 2019 - v04 - july 2019 abstract {em dash}we consider the ipv6 segment routing technology for service function chaining of virtual network functions . most of the vnfs are legacy vnfs and expect to process traditional ip packets. an sr proxy is needed to support them. we have extended the implementation of srv6 in the linux kernel, realizing an open source sr-proxy, referred to as srnk . the performance of the proposed solution has been evaluated, identifying a poor scalability with respect to the number of vnfs to be supported in a node. therefore we p...   \n",
              "1  network operators are facing difficult challenges to keep up with the increasing demand for capacity, the need to support fast service creation and at the same time the goal of reducing the costs. network function virtualization and software defined networking represent an answer to these challenges and are changing the way ip networks are designed and operated. leveraging cloud computing principles, nfv moves the traditional data-plane network functions from expensive, closed and proprietary hardware to the so-called virtual network functions running over a distributed, cloud-like infrastructure referred to as nfvi . the sdn architecture splits the data and control planes and moves the intelligence to the sdn controller. sdn aims at simplifying the introduction of new services and fostering flexibility thanks to the centralized network state view. the concept of services chaining is directly associated to nfv. actually, the idea of creating a processing path across services pre-da...   \n",
              "2  the segment routing architecture is based on the source routing approach : it is possible to include a list of instructions in the packet headers. a comprehensive survey on segment routing can be found in this work considers the use of srv6 for sfc, leveraging its scalability properties.thanks to the source routing approach, srv6 is able to simplify network operations. generally speaking, the advantage of approaches based on source routing lies in the possibility to add state information in the packet headers, thus avoiding or minimizing the information that needs to be configured and maintained by the internal nodes. the possibility to interact only with the edge nodes to setup complex services is extremely appealing from the point of view of simplicity and efficiency. this greatly improves the scalability of services based on sr and allows simpler and faster service setup and re-configuration. in the scaling capability of segment routing has been demonstrated considering an use c...   \n",
              "3  1) network programming model: the srv6 network programming model extends the ipv6 segment routing concept from the simple steering of packets across nodes to a general network programming approach. quoting from each segment represents a function to be called at a specific location in the network, a function can span from a simple action like forwarding or a complex processing defined by the user. going into the details, each srv6 capable node maintains the so-called my local sid table , each entry of this table maps a segment into a local function. as a consequence, when a packet enters in an srv6 enabled node with an active segment matching an entry of the table, the associated function is applied to the packet. leveraging the fact the segments are represented as regular ipv6 addresses, the node can advertise them using any routing protocol. combining these network instructions it is possible to literally program the network and realize very complex network behaviors. the associat...   \n",
              "4  in this section we present the design of our first kernel implementation of the dynamic proxy , referred to as srnkv1. most of the following design choices apply also to the static proxy , which can be seen as a by-product of the the dynamic proxy implementation. in order to simplify the discussion we just mention the dynamic proxy in the paragraphs and in the images. srnkv1 design relies on two distinct lwts which manage respectively the inbound and fromvnf traffic. for each lwt, state information is maintained in order to correctly perform the proxy operations. in particular, the inbound processing needs an entry on the my local sid table and uses a pernetwork namespace hashtable to store the headers that have to be restored during the fromvnf processing. as regards the traffic coming from the legacy vnf, a policy routing entry for each vnf is necessary to classify the packets, a routing table with a default route pointing to the lwt is used for the vnf and finally the per-netns ...   \n",
              "\n",
              "  section_num:                                section  \\\n",
              "0     Abstract                               Abstract   \n",
              "1            I                        I. INTRODUCTION   \n",
              "2           II  II. SFC BASED ON IPV6 SEGMENT ROUTING   \n",
              "3            A          III. DESIGN OF THE SRV6 PROXY   \n",
              "4            B          III. DESIGN OF THE SRV6 PROXY   \n",
              "\n",
              "                                 subsection  \\\n",
              "0                                       NaN   \n",
              "1                                       NaN   \n",
              "2                                       NaN   \n",
              "3  A. General Concepts and State-of-the-art   \n",
              "4                                 B. SRNKv1   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 generated  \n",
              "0                                                                                                      we consider the ipv6 segment routing technology for service function chaining of virtual network functions ( vnfs ). \\n most of the vnfs are legacy vnfs and expect to process traditional ip packets. \\n an sr proxy is needed to support them. we have extended the implementation of srv6 in the linux kernel, realizing an open source sr-proxy, referred to as srnk. \\n the performance of the proposed solution has been evaluated, identifying a poor scalability with respect to the number of vnfs to be supported in a node. therefore we provided a second design, enhancing the linux policy routing framework. \\n the performance of srnkv2 is independent from the number of supported vnfs in a node. we compared the performance of srnkv2 with a reference scenario not performing the encapsulation and decapsulation operation and demonstrated that the overhead of srnkv2 is very small, on the order of 3.5%.  \n",
              "1  the ietf sfc working group has investigated the scenarios and issues related to dynamic service chaining and proposed a reference architecture. \\n the sfc framework proposed in does not pose any restriction on the function that can be chained: they can be both virtualized or physical functions. for the sake of simplicity, \\n we will only refer to the virtualized case and will simply use the term vnf instead of service function. the ietf sfc wg is considering the network service header as a specific solution for the realization of the sfc architecture. the nsh header defines the service-level data-plane encapsulation for realizing the vnfs chaining. moreover, it defines the packet meta-data that arxiv:1901.00936v3 23 jul 2019 can be inserted into the header to exchange state between the nodes of the sfc architecture. in this work we are advocating the use of ipv6 segment routing to implement service function chaining,. segment routing, is a form of source routing, which allows to ad...  \n",
              "2  this work considers the use of srv6 for sfc, leveraging its scalability properties. in the scaling capability of segment routing \\n has been demonstrated considering an use case of 600,000 nodes and 300 millions of endpoints.the segment routing architecture is based on the source routing approach : it is possible to include a list of instructions in the packet headers. by exploiting the srv6 approach the vnfs can be mapped in ipv6 addresses in the segments list and we can represent the vnf chain using this list carried in the segment routing header. \\n the sr information can be pushed into the packets using two different approaches, denoted as insert and encap modes, respectively.according to the srv6 network programming document, when a node uses the insert mode the srh is pushed as next header in the original ipv6 packet, immediately after the ipv6 header and before the transport header. \\n the original ipv6 packet is transported as the inner packet of an ipv6in-ipv6 encapsulated...  \n",
              "3  the seg6local lwt is the specific type of lightweight tunnel that supports the srv6 network programming features in the linux kernel. \\n the purpose of this work is to extend the implementation of the srv6 network programming model currently available in the linux kernel to support the dynamic proxy.    \\n the purpose of this work is to extend the implementation of the srv6 network programming model currently available in the linux kernel to support the dynamic proxy. \\n the seg6local lwt is the specific type of lightweight tunnel that supports the srv6 network programming features in the linux kernel. \\n starting from linux kernel 4.14 a subset of the behaviors described in have been implemented, while the sr proxy behaviors are not supported yet.1) network programming model : the srv6 network programming model extends the ipv6 segment routing concept from the simple steering of packets across nodes to a general network programming approach. going into the details, each srv6 capab...  \n",
              "4  we present the design of our first kernel implementation of the dynamic proxy, referred to as srnkv1. in order to simplify the discussion we just mention the dynamic proxy in the paragraphs and in the images. srnkv1 design relies on two distinct lwt which manage respectively the inbound and fromvnf traffic. for each lwt, state information is maintained in order to correctly perform the proxy operations. as regards the traffic coming from the legacy vnf, a policy routing entry for each vnf is necessary to classify the packets, a routing table with a default route pointing to the lwt is used for the vnf and finally the per-netns hashtable is used to read the headers stored previously by the inbound processing. in particular, the inbound processing needs an entry on the my local sid table and uses a per-netns hashtable to store the headers that have to be restored during the fromvnf processing. removed headers at step are indexed in the per-netns hashtable by using the identifier of t...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-899de809-9775-4d58-bd98-922026cd8351\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>section_num:</th>\n",
              "      <th>section</th>\n",
              "      <th>subsection</th>\n",
              "      <th>generated</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>an efficient linux kernel implementation of service function chaining for legacy vnfs based on ipv6 segment routing andrea mayer*, stefano salsano*, pier luigi ventre*, ahmed abdelsalam {double dagger} {section sign}, luca chiaraviglio*, clarence filsfils {section sign} *university of rome tor vergata, cnit, {double dagger}gran sasso science institute, {section sign}cisco systems extended version of the paper accepted to ieee netsoft 2019 - v04 - july 2019 abstract {em dash}we consider the ipv6 segment routing technology for service function chaining of virtual network functions . most of the vnfs are legacy vnfs and expect to process traditional ip packets. an sr proxy is needed to support them. we have extended the implementation of srv6 in the linux kernel, realizing an open source sr-proxy, referred to as srnk . the performance of the proposed solution has been evaluated, identifying a poor scalability with respect to the number of vnfs to be supported in a node. therefore we p...</td>\n",
              "      <td>Abstract</td>\n",
              "      <td>Abstract</td>\n",
              "      <td>NaN</td>\n",
              "      <td>we consider the ipv6 segment routing technology for service function chaining of virtual network functions ( vnfs ). \\n most of the vnfs are legacy vnfs and expect to process traditional ip packets. \\n an sr proxy is needed to support them. we have extended the implementation of srv6 in the linux kernel, realizing an open source sr-proxy, referred to as srnk. \\n the performance of the proposed solution has been evaluated, identifying a poor scalability with respect to the number of vnfs to be supported in a node. therefore we provided a second design, enhancing the linux policy routing framework. \\n the performance of srnkv2 is independent from the number of supported vnfs in a node. we compared the performance of srnkv2 with a reference scenario not performing the encapsulation and decapsulation operation and demonstrated that the overhead of srnkv2 is very small, on the order of 3.5%.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>network operators are facing difficult challenges to keep up with the increasing demand for capacity, the need to support fast service creation and at the same time the goal of reducing the costs. network function virtualization and software defined networking represent an answer to these challenges and are changing the way ip networks are designed and operated. leveraging cloud computing principles, nfv moves the traditional data-plane network functions from expensive, closed and proprietary hardware to the so-called virtual network functions running over a distributed, cloud-like infrastructure referred to as nfvi . the sdn architecture splits the data and control planes and moves the intelligence to the sdn controller. sdn aims at simplifying the introduction of new services and fostering flexibility thanks to the centralized network state view. the concept of services chaining is directly associated to nfv. actually, the idea of creating a processing path across services pre-da...</td>\n",
              "      <td>I</td>\n",
              "      <td>I. INTRODUCTION</td>\n",
              "      <td>NaN</td>\n",
              "      <td>the ietf sfc working group has investigated the scenarios and issues related to dynamic service chaining and proposed a reference architecture. \\n the sfc framework proposed in does not pose any restriction on the function that can be chained: they can be both virtualized or physical functions. for the sake of simplicity, \\n we will only refer to the virtualized case and will simply use the term vnf instead of service function. the ietf sfc wg is considering the network service header as a specific solution for the realization of the sfc architecture. the nsh header defines the service-level data-plane encapsulation for realizing the vnfs chaining. moreover, it defines the packet meta-data that arxiv:1901.00936v3 23 jul 2019 can be inserted into the header to exchange state between the nodes of the sfc architecture. in this work we are advocating the use of ipv6 segment routing to implement service function chaining,. segment routing, is a form of source routing, which allows to ad...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>the segment routing architecture is based on the source routing approach : it is possible to include a list of instructions in the packet headers. a comprehensive survey on segment routing can be found in this work considers the use of srv6 for sfc, leveraging its scalability properties.thanks to the source routing approach, srv6 is able to simplify network operations. generally speaking, the advantage of approaches based on source routing lies in the possibility to add state information in the packet headers, thus avoiding or minimizing the information that needs to be configured and maintained by the internal nodes. the possibility to interact only with the edge nodes to setup complex services is extremely appealing from the point of view of simplicity and efficiency. this greatly improves the scalability of services based on sr and allows simpler and faster service setup and re-configuration. in the scaling capability of segment routing has been demonstrated considering an use c...</td>\n",
              "      <td>II</td>\n",
              "      <td>II. SFC BASED ON IPV6 SEGMENT ROUTING</td>\n",
              "      <td>NaN</td>\n",
              "      <td>this work considers the use of srv6 for sfc, leveraging its scalability properties. in the scaling capability of segment routing \\n has been demonstrated considering an use case of 600,000 nodes and 300 millions of endpoints.the segment routing architecture is based on the source routing approach : it is possible to include a list of instructions in the packet headers. by exploiting the srv6 approach the vnfs can be mapped in ipv6 addresses in the segments list and we can represent the vnf chain using this list carried in the segment routing header. \\n the sr information can be pushed into the packets using two different approaches, denoted as insert and encap modes, respectively.according to the srv6 network programming document, when a node uses the insert mode the srh is pushed as next header in the original ipv6 packet, immediately after the ipv6 header and before the transport header. \\n the original ipv6 packet is transported as the inner packet of an ipv6in-ipv6 encapsulated...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1) network programming model: the srv6 network programming model extends the ipv6 segment routing concept from the simple steering of packets across nodes to a general network programming approach. quoting from each segment represents a function to be called at a specific location in the network, a function can span from a simple action like forwarding or a complex processing defined by the user. going into the details, each srv6 capable node maintains the so-called my local sid table , each entry of this table maps a segment into a local function. as a consequence, when a packet enters in an srv6 enabled node with an active segment matching an entry of the table, the associated function is applied to the packet. leveraging the fact the segments are represented as regular ipv6 addresses, the node can advertise them using any routing protocol. combining these network instructions it is possible to literally program the network and realize very complex network behaviors. the associat...</td>\n",
              "      <td>A</td>\n",
              "      <td>III. DESIGN OF THE SRV6 PROXY</td>\n",
              "      <td>A. General Concepts and State-of-the-art</td>\n",
              "      <td>the seg6local lwt is the specific type of lightweight tunnel that supports the srv6 network programming features in the linux kernel. \\n the purpose of this work is to extend the implementation of the srv6 network programming model currently available in the linux kernel to support the dynamic proxy.    \\n the purpose of this work is to extend the implementation of the srv6 network programming model currently available in the linux kernel to support the dynamic proxy. \\n the seg6local lwt is the specific type of lightweight tunnel that supports the srv6 network programming features in the linux kernel. \\n starting from linux kernel 4.14 a subset of the behaviors described in have been implemented, while the sr proxy behaviors are not supported yet.1) network programming model : the srv6 network programming model extends the ipv6 segment routing concept from the simple steering of packets across nodes to a general network programming approach. going into the details, each srv6 capab...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>in this section we present the design of our first kernel implementation of the dynamic proxy , referred to as srnkv1. most of the following design choices apply also to the static proxy , which can be seen as a by-product of the the dynamic proxy implementation. in order to simplify the discussion we just mention the dynamic proxy in the paragraphs and in the images. srnkv1 design relies on two distinct lwts which manage respectively the inbound and fromvnf traffic. for each lwt, state information is maintained in order to correctly perform the proxy operations. in particular, the inbound processing needs an entry on the my local sid table and uses a pernetwork namespace hashtable to store the headers that have to be restored during the fromvnf processing. as regards the traffic coming from the legacy vnf, a policy routing entry for each vnf is necessary to classify the packets, a routing table with a default route pointing to the lwt is used for the vnf and finally the per-netns ...</td>\n",
              "      <td>B</td>\n",
              "      <td>III. DESIGN OF THE SRV6 PROXY</td>\n",
              "      <td>B. SRNKv1</td>\n",
              "      <td>we present the design of our first kernel implementation of the dynamic proxy, referred to as srnkv1. in order to simplify the discussion we just mention the dynamic proxy in the paragraphs and in the images. srnkv1 design relies on two distinct lwt which manage respectively the inbound and fromvnf traffic. for each lwt, state information is maintained in order to correctly perform the proxy operations. as regards the traffic coming from the legacy vnf, a policy routing entry for each vnf is necessary to classify the packets, a routing table with a default route pointing to the lwt is used for the vnf and finally the per-netns hashtable is used to read the headers stored previously by the inbound processing. in particular, the inbound processing needs an entry on the my local sid table and uses a per-netns hashtable to store the headers that have to be restored during the fromvnf processing. removed headers at step are indexed in the per-netns hashtable by using the identifier of t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-899de809-9775-4d58-bd98-922026cd8351')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-899de809-9775-4d58-bd98-922026cd8351 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-899de809-9775-4d58-bd98-922026cd8351');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4abc0055-5c41-4989-863e-d8ee7971b651\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4abc0055-5c41-4989-863e-d8ee7971b651')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4abc0055-5c41-4989-863e-d8ee7971b651 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(generated_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"network operators are facing difficult challenges to keep up with the increasing demand for capacity, the need to support fast service creation and at the same time the goal of reducing the costs. network function virtualization and software defined networking represent an answer to these challenges and are changing the way ip networks are designed and operated. leveraging cloud computing principles, nfv moves the traditional data-plane network functions from expensive, closed and proprietary hardware to the so-called virtual network functions running over a distributed, cloud-like infrastructure referred to as nfvi . the sdn architecture splits the data and control planes and moves the intelligence to the sdn controller. sdn aims at simplifying the introduction of new services and fostering flexibility thanks to the centralized network state view. the concept of services chaining is directly associated to nfv. actually, the idea of creating a processing path across services pre-dates the nfv concept as stated in and . in fact, service chaining has been traditionally realized in a static way by putting hardware functions as middle-points of the processing paths and in some cases by diverting the forwarding paths with manual configuration of vlans stitching or policy routing. however, these static approaches comes with several drawbacks which are detailed in . in particular, they are intrinsically difficult to scale and hard to reconfigure. on the other hand, the current view of sfc applied to nfv is that it has to be highly dynamic and scalable.the ietf sfc working group has investigated the scenarios and issues related to dynamic service chaining and proposed a reference architecture . the main logical elements of this architecture are i) classifiers; ii) service functions forwarders , iii) the service functions, iv) sfc proxies. the classifiers match the traffic against a set of policies in order to associate the proper service function chain. the sffs forward the traffic towards the service functions or towards other sffs and handle the traffic coming back from the service functions. the sfc framework proposed in does not pose any restriction on the function that can be chained: they can be both virtualized or physical functions . for the sake of simplicity, hereafter in the paper we will only refer to the virtualized case and will simply use the term vnf instead of service function. in this scenario, the forwarding of traffic along a service chain needs to be supported by specific protocols and mechanisms that allow the architectural elements to exchange context information. the vnfs can participate to these chaining mechanisms and in this case they are called sfc aware. on the other hand, the legacy vnfs that do not interact with the sfc protocols and mechanisms are called sfc unaware. the sfc proxy elements are needed for the latter type of vnfs. an sfc proxy hides the sfc mechanisms to the sfc unaware vnfs, that will receive and send plain ip traffic. the ietf sfc wg is considering the network service header as a specific solution for the realization of the sfc architecture. the nsh header defines the service-level data-plane encapsulation for realizing the vnfs chaining. the nsh header identifies a service chain which is associated to the packets. moreover, it defines the packet meta-data that arxiv:1901.00936v3 23 jul 2019 can be inserted into the header to exchange state between the nodes of the sfc architecture. in this work we are advocating the use of ipv6 segment routing to implement service function chaining , . segment routing , is a form of source routing, which allows to add a sequence of segments in the packet headers to influence the packet forwarding and processing within the network. segment routing has been designed and implemented for the mpls and ipv6 data planes, we only focus here on the ipv6 version, denoted as srv6. in the srv6 architecture, the segments are expressed as ipv6 addresses. the srv6 network programming model , leveraging the huge ipv6 addressing space, extends the srv6 architecture from a simple forwarding mechanism for steering packets to a more general network programming abstraction. a segment can represent an instruction or behavior and not only a network location. our proposed approach is fully aligned with the network programming model described in . the srv6 architecture is not limited to service function chaining, which represents only a possible use case. indeed, srv6 can support several applications in a network provider backbone like traffic engineering, network resilience , virtual private networks , multicast, content delivery networks . with respect to the mpls based data plane, srv6 it has the advantage that it can be better integrated in host networking stack. for this reason data center applications could also benefit from srv6. a relevant subset of the srv6 and network programming model specifications have been implemented and integrated in the mainline linux kernel . in this paper, we rely on this existing work and extend it to focus on the service function chaining of legacy vnfs, which are not able to process the srv6 headers. the support of legacy vnfs is important for internet service providers for different reasons: i) it guarantees a feasible migration strategy saving past investments; ii) it facilitates the interoperability and the multi-vendor scenarios, i.e deployments composed by vnfs of different vendors; iii) the development of srv6 aware vnfs requires a new implementation cycle which can be more expensive in the short period. as introduced above, a proxy element needs to be inserted in the processing chain as relay mechanism in order to support srv6 unaware vnfs . the latest linux kernel still lacks of the functionality to implement such srv6 proxy element. in a prior work , we have provided this functionality as an external module not integrated with the most recent srv6 developments in the linux kernel. considering the importance of the support of legacy sr-unaware applications in nfv deployments, the main contribution this paper is the design and implementation of an sr-proxy integrated in the linux kernel networking components. we refer to this work as srnk . we designed a first version of srnk and evaluated its performance, identifying a poor scalability with respect to the number of vnfs to be supported. the issue is actually related to the implementation of policy routing framework in linux. therefore we provided a second design, enhancing the fig. 1: srv6 nfv node with sr-proxy for sr-unaware vnf linux policy routing framework, whose performance does not depend on the number of supported vnfs in a node. the content of the paper is as follows. section ii introduces sfc based on srv6 considering both srv6 aware and unaware vnfs. the proposed design and implementation of srv6 proxy to support legacy vnfs in the linux kernel is described in section iii. our testing environment and methodologies for performance analysis are reported in section iv. sections v details the performance evaluation of the implemented solutions. finally, in section vii we draw some conclusions and discuss future work. this work has been performed in the context of the rose research project which focuses on the development of an open source srv6 ecosystem. the source code of all components of srnk including the patches to the user space utilities are freely available at .\",\n          \"in this section we present the design of our first kernel implementation of the dynamic proxy , referred to as srnkv1. most of the following design choices apply also to the static proxy , which can be seen as a by-product of the the dynamic proxy implementation. in order to simplify the discussion we just mention the dynamic proxy in the paragraphs and in the images. srnkv1 design relies on two distinct lwts which manage respectively the inbound and fromvnf traffic. for each lwt, state information is maintained in order to correctly perform the proxy operations. in particular, the inbound processing needs an entry on the my local sid table and uses a pernetwork namespace hashtable to store the headers that have to be restored during the fromvnf processing. as regards the traffic coming from the legacy vnf, a policy routing entry for each vnf is necessary to classify the packets, a routing table with a default route pointing to the lwt is used for the vnf and finally the per-netns hashtable is used to read the headers stored previously by the inbound processing. figures 4 show an high-level view of the processing inside a srv6 enabled node and how ipv6 routing network subsystem interacts with the srv6 dynamic proxy implementation. 1) inbound processing: the inbound processing is depicted in figure 4a. as soon as an ipv6 packet arrives at interface eth0 of the nfv node, it enters into the linux networking stack. after passing the pre-routing stage, the kernel tries to look up the route with the longest prefix that matches the active segment of the packet. due to policy-routing settings, the linux kernel looks first at my local sid table and if no matching route has been found, it considers the other tables and possibly moves on the next stages of the processing . figure 4a shows this process in details, the packet destination address matches with prefix sid1 and the correspondent route is used. therefore, the linux kernel executes the processing function associated with the route: the inbound end.ad operation. the inbound end.ad operates in three different stages: i) it pops the outer ipv6 and srv6 headers from the incoming packet; ii) it updates the sid pointer of the srv6 header to select the next one; iii) it stores such retrieved headers into a per-netns hashtable data structure; iv) it sends out the decapsulated ipv6 plain packet to its designated legacy vnf. removed headers at step are indexed in the per-netns hashtable by using the identifier of the packet outgoing interface , the one used to communicate with the legacy vnf . due to the necessity of sharing ipv6 and srv6 headers between inbound and fromvnf processing, the choice of storing them within a external shared data structure turned out to be the right solution. this design simplifies the access pattern to the stored data, as well as it increases performance. indeed, the hashtable is well suitable to support fast data retrieving with a very low computational cost and, ideally it is independent with regard to the number inbound processing fromvnf processing fig. 4: srnkv1 design of stored entries. from a configuration point of view, the inbound processing just relies on the plain ipv6 routing through my local sid table: the new route is added with the ip -6 route add command of the iproute2 suite, by also specifying the behavior to be activated in the parameters of the command. appendix a provides further details on the configuration of the inbound processing. 2) auto-learning process: the auto-learning process consists in learning the information related of the vnfs chain from the inbound packets, without the need of a static configuration. the learned information is saved in a per-netns hashtable. we have introduced an age parameter to control the rate at which the per-netns hashtable can be updated. this parameter can be set during the setup of the lwt routing entry in my local sid table. when different from 0, the age parameter represents the minimum interval between two write operations in the per-netns hashtable for the same vnf. setting the age to 1 second corresponds to a maximum reconfiguration delay of 1 second for a nfv node when the vnf chain is changed by an ingress node and this is the default we used in our experiments. if age equals 0, the per-netns hashtable is updated for every inbound packets, providing the fastest possible reconfiguration time for a vnf chain. in the performance evaluation section, we have analyzed the performance cost for the continuous updating of the per-netns hashtable with respect to the default minimum reconfiguration delay of 1 second. the age parameter registers the last time the headers have been updated and it is used also to determine, when a packet is received, if it is the time to replace stale data with new fresh one. the auto-learning operation is performed only during the inbound processing. the learned information is retrieved during the fromvnf processing using the incoming interface2 of the packet to rebuild the whole srv6 packet ready for being forwarded into the network. setting properly the age parameter has an important impact on the performance of the system and a proper trade-off is necessary according to the use case to be supported. in a shared-memory producer-consumer context, we can identify the inbound processing as the content producer, and the fromvnf one as the consumer. indeed, the former is in charge of keeping the per-netns hashtable up-to-date, while the latter accesses the structure for retrieving the headers. considering this model, the aging parameter can be seen as the upper-bound of data production/refresh rate. by setting it to the maximum limit, it is possible to prevent overloading of the srv6 nfv node caused by high-rate writing in the shared memory. this problem is particularly noticeable in all of those systems based on multi-core architectures: the linux networking stack allows to assign the received packets to all available computing units in order to process them in parallel and to support high data rates. however, this means that several end.ad processing operations may occur at once and, potentially, they may involve updating the same ipv6 and srv6 headers. very frequent and simultaneous shared memory updates by multiple cpus can lead to conflicts that can negatively affect the overall performance of the system. for all these reasons, small values of the age parameter make the system more responsive to chain changes, but on the other side they can push heavy and unnecessarily load to the srv6 nfv node due to high data refresh rate. 3) end.as design: the end.ad differs from the end.as just on the way the stored headers are managed. the end.as behavior is a simplification of the end.ad because it does 2the current implementation of the dynamic proxy assumes that the same interface is used to interact with vnf in the two directions not need to deal with the auto-learning process. indeed, it uses chain information which has been saved once during the behavior configuration. the list of segments does not change during the entire life of the end.as instance unless it is first deleted and then saved with new headers values. 4) fromvnf processing: the fromvnf lwt tunnel is meant to work in tandem with its inbound counterpart. fromvnf packets do not carry any sid as it happens for the inbound ones. as result, in order to select the correct lwt tunnel and processing each packet accordingly, we can rely only on the incoming interface between the vnf and the nfv node through which packets come back. hence, we add an entry in the ipv6 routing policy db for each vnf to be supported. every rpdb entry is also known as ipv6 rule, as the command used to configure it is ip -6 rule. the rule points to a different routing table for each vnf, in which there is only a default route, pointing to the lwt tunnel associated to the vnf. this means that for n vnfs, we will have n rules and n routing tables. figure 4b provides a representation of the described fromvnf processing. let us analyze with more details the motivation for this design. the fromvnf lwt tunnel can not be tied to any route with a specific prefix because the ipv6 packets sent by vnf can use any destination address and do not have any relationship with the sids. moreover, each end.ad fromvnf tunnel expects to receive traffic by its own layer-2 interface , with no regards about the ipv6 destination address of the packets. this means that, in order to apply the fromvnf processing function to an incoming packet, the srv6 nfv node has to retrieve the route that points to the right lwt tunnel using only the identifier of the interface where such as packet has been received. as a consequence of this, the fromvnf end.ad design has to deal with: i) the problem of designating an ipv6 prefix to be used for creating a route pointing to a custom processing function , and ii) the issue of steering incoming traffic received on a specific interface through such as route. the first issue can be easily solved by using as route prefix the any address which is often indicated by ::. generally, the default route is selected by the routing algorithm when the ipv6 destination address can not be managed by any other. however, this usage gives rise to a new problem. indeed, creating a lwt on a default route has the side effect that no more than one vnf can be handled by the srv6 node using a single table. moreover, control traffic that transits through the srv6 node and for which there are no given explicit routes may be wrongly handled by the lwt installed on the default route. thankfully, this problem can be easily solved by installing every default route into a different ipv6 routing table and creating, for each of these, a rule in the ipv6 routing policy db. such rule is meant to instruct the ipv6 network system to perform route look-up on a specific table based on a specified match. the usage of an ipv6 policy route solves also the issue ii) as at this point we can use the fromvnf interface as match and a goto-table n as action predicate. in this way we can relate an interface to a specific default route that has attached to a lwt. figure 4b shows an high-level overview of proposed solution with the fromvnf lwt tunnel integrated in the ipv6 routing network subsystem. whenever a plain ipv6 packets, sent by vnf, arrives at srv6 nfv node, it is handled by the linux networking stack. after passing the pre-routing stage, the kernel tries to determine the right processing of the packet. it invokes the route look-up operation, but this time the routing algorithm finds first an entry in the rpdb of the node and does not consider ipv6 destination address at first. indeed, thanks to custom ipv6 rules the routing algorithm is capable to retrieve the ipv6 table tied to the incoming interface of the packet. at this point, the routing algorithm makes use of this table to find out the route that matches with the received packet. in this specific case, the routing algorithm selects and returns the only route available, the default one, that is attached to a specific end.ad tunnel. once the plain ipv6 packet has been received by the fromvnf processing function, it leverages the identifier of the incoming interface of the packet to search for the popped ipv6 and srv6 headers within the per-netns hashtable. if a result is found, the processing function forges a new packet and sets the headers of such as packet with those that have just been retrieved. the plain ipv6 packet is encapsulated into the newly created one and then the whole packet is delivered towards its destination. this concludes the job of the fromvnf lwt tunnel processing operation.\",\n          \"the segment routing architecture is based on the source routing approach : it is possible to include a list of instructions in the packet headers. a comprehensive survey on segment routing can be found in this work considers the use of srv6 for sfc, leveraging its scalability properties.thanks to the source routing approach, srv6 is able to simplify network operations. generally speaking, the advantage of approaches based on source routing lies in the possibility to add state information in the packet headers, thus avoiding or minimizing the information that needs to be configured and maintained by the internal nodes. the possibility to interact only with the edge nodes to setup complex services is extremely appealing from the point of view of simplicity and efficiency. this greatly improves the scalability of services based on sr and allows simpler and faster service setup and re-configuration. in the scaling capability of segment routing has been demonstrated considering an use case of 600,000 nodes and 300 millions of endpoints. by exploiting the srv6 approach the vnfs can be mapped in ipv6 addresses in the segments list and we can represent the vnf chain using this list carried in the segment routing header . the sr information can be pushed into the packets using two different approaches, denoted as insert and encap modes, respectively.according to the srv6 network programming document , when a node uses the insert mode the srh is pushed as next header in the original ipv6 packet, immediately after the ipv6 header and before the transport header. the original ipv6 header is changed, in particular the next header is modified according to the value of srh, the ipv6 destination address is replaced with the ipv6 address of the first sid in the segment list, while the original ipv6 destination address is carried in the srh header as the last segment of the list. in this work we only consider the encap mode: the original ipv6 packet is transported as the inner packet of an ipv6in-ipv6 encapsulated packet and travels unmodified in the network. the outer ipv6 packet carries the srh header with the segments list.1 an sr-aware vnf can process the srh of the incoming packets and can use it to influence the processing/forwarding of the packets. such vnfs interact with the node operating system or with sr modules in order to read and/or set the information contained in the srh. on the other side, the srunaware vnfs are not capable to process the srv6 sfc encapsulation. in this scenario an sr proxy is necessary to remove the srv6 header and deliver a clean ip packet to the vnf. figure 1 provides the reference architecture for a srv6 nfv node that includes an sr-unaware vnf . we refer to packets incoming to the srv6 nfv node that should be forwarded to the vnf by the srproxy as inbound packets. the sr-proxy needs to intercept the packets coming out from the vnf and re-apply the srv6 sfc encapsulation. we refer to these packets as fromvnf packets. in , a set of sr-proxy behaviors have been defined, among them we mention: i) static proxy ; ii) dynamic proxy ; iii) masquerading proxy . the first two cases support ipv6 sr packets in encap mode. the encapsulated packets can be ipv6, ipv4 or l2 packets. the sr proxy intercepts sr packets before being handed to the sr-unaware vnf, hence it can remove the sr encapsulation from packets. for packets coming back from sr-unaware vnf, the sr proxy can restore the srv6 encapsulation updating the srh properly. the difference between the static and the dynamic proxies is that the sr information that needs to be pushed back in the packets is statically configured in the first case and it is learned from the incoming packets in the dynamic case.instead, the masquerading proxy supports sr packets travelling in insert mode. it masquerades the sr packets before they are sent to the legacy vnf by replacing the ipv6 destination address with the original ipv6 destination . it is assumed that a vnf compatible with this operating mode is processing ipv6 packets and does not alter the srh, it just ignores it. in this way, when packets are received back, the sr proxy can restore the correct information in the ipv6 header in a stateless way, just using the information contained in the srh. 1as any tunneling method, srv6 introduces overhead the packets. the insert mode introduces an overhead of 8 + n *16 where n is the number of segments, while in the encap mode the overhead is 40 + 8 + n *16. let us discuss the operational model and the state information that need to be configured and maintained in the srv6 nfv nodes. figure 2 illustrates a srv6 based nfv domain, in which the vnfs are hosted in different nfv nodes. the packets to be associated to vnf chains are classified in ingress nodes, where the sr encapsulation is added. a network operator willing to use srv6 sfc chaining for srunaware vnfs, will first need to associate vnfs to segment ids in the hosting srv6 nfv nodes. we recall that a sid is represented by an ipv6 address. each srv6 nfv node has a pool of ipv6 addresses that are available to be used as sids for its vnfs. these prefixes are distributed using regular routing protocols, so that the reachability of all vnfs is assured. the association of the ipv6 address sid to a vnf is a configuration operation to be performed in the srv6 nfv node and it binds the sid to the virtual interface that connects the sr-proxy to the vnf. this operation is performed when a legacy vnf is created in a nfv node. the corresponding state information is used in the inbound direction, when packets directed to the vnf are processed by the sr-proxy. the second step is to configure a vnf chain across the vnfs that are running over the srv6 nfv nodes. the vnf chain will be applied to a packet by inserting a sid list in the ipv6 sr header in the ingress node. therefore, the classification of packets and the association with the sid list has to be configured in the ingress node. each nfv node which runs a legacy vnf needs the proper information to process the packets in the fromvnf direction. this is done differently for the respective types of proxy. in the static proxy case , the state information needed to process the packets coming from the vnf is done by statically configuring the sr-proxy with the sid list to be re-inserted in the packet. both the dynamic proxy and the masquerading one have the good property that they do not need to be configured when a new chain is added. the dynamic proxy learns the sid list from the packets in the inbound direction . the masquerading proxy does not even need to save the state information as the sid list is carried along with the packet through the legacy vnf . table i compares the different sr proxy behaviors. in this table i: comparison of sr proxy behaviours end.ad end.as end.am generate traffic yes yes no modify packets yes yes no stateless no no yes state-config auto manual n/a traffic supported ipv4/ipv6/l2 ipv4/ipv6/l2 ipv6 work, we focus on the design and in-kernel implementation of the sr dynamic proxy as it represents the most versatile solution and it offers a simple operational model. fig. 2: sfc scenario\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"section_num:\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"I\",\n          \"B\",\n          \"II\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"section\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"I. INTRODUCTION\",\n          \"III. DESIGN OF THE SRV6 PROXY\",\n          \"Abstract\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"subsection\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"B. SRNKv1\",\n          \"A. General Concepts and State-of-the-art\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"generated\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"the ietf sfc working group has investigated the scenarios and issues related to dynamic service chaining and proposed a reference architecture. \\n the sfc framework proposed in does not pose any restriction on the function that can be chained: they can be both virtualized or physical functions. for the sake of simplicity, \\n we will only refer to the virtualized case and will simply use the term vnf instead of service function. the ietf sfc wg is considering the network service header as a specific solution for the realization of the sfc architecture. the nsh header defines the service-level data-plane encapsulation for realizing the vnfs chaining. moreover, it defines the packet meta-data that arxiv:1901.00936v3 23 jul 2019 can be inserted into the header to exchange state between the nodes of the sfc architecture. in this work we are advocating the use of ipv6 segment routing to implement service function chaining,. segment routing, is a form of source routing, which allows to add a sequence of segments in the packet headers to influence the packet forwarding and processing within the network. in the srv6 architecture, the segments are expressed as ipv6 addresses. the\",\n          \"we present the design of our first kernel implementation of the dynamic proxy, referred to as srnkv1. in order to simplify the discussion we just mention the dynamic proxy in the paragraphs and in the images. srnkv1 design relies on two distinct lwt which manage respectively the inbound and fromvnf traffic. for each lwt, state information is maintained in order to correctly perform the proxy operations. as regards the traffic coming from the legacy vnf, a policy routing entry for each vnf is necessary to classify the packets, a routing table with a default route pointing to the lwt is used for the vnf and finally the per-netns hashtable is used to read the headers stored previously by the inbound processing. in particular, the inbound processing needs an entry on the my local sid table and uses a per-netns hashtable to store the headers that have to be restored during the fromvnf processing. removed headers at step are indexed in the per-netns hashtable by using the identifier of the outgoing interface, the one used to communicate with the legacy vnf. due to the necessity of sharing ipv6 and srv6 headers between inbound and fromvnf processing, the choice of storing them within a external shared data structure turned out to be the right solution. indeed, the hashtable is well suitable to support fast data retrieving with a very low computational cost and, ideally it is independent with regard to the number inbound processing\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "pdf_df = pd.read_csv(project_processed_data_path + \"/1901.00936v3.csv\")\n",
        "\n",
        "print(\"Start generating summary...\")\n",
        "generated_df = model_action.generate_summary_for_user_pdf(pdf_df)\n",
        "display(generated_df.head())\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b3232192745b4a22a39bf8cd6b58cb23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4e48f42681844a558a603c84cefc90e5",
              "IPY_MODEL_196df4e4cc354454a0cd3da3fafb35b5",
              "IPY_MODEL_576d7598256b4900a3fc6b4945fd7ab6"
            ],
            "layout": "IPY_MODEL_ea06137e82af45efadc7b09d895c98ea"
          }
        },
        "4e48f42681844a558a603c84cefc90e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1691d7c03b664526ae81f7cf9a3bff03",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4e5fd2bca32e44709937dc2db74e24f9",
            "value": "tokenizer_config.json:‚Äá100%"
          }
        },
        "196df4e4cc354454a0cd3da3fafb35b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1a77102140d41dea360a32755f648a3",
            "max": 27,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3a208393058f4685bf8ce4eaefac7b32",
            "value": 27
          }
        },
        "576d7598256b4900a3fc6b4945fd7ab6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f9f11b388b34886a8e355d397ba86c7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_944b2223f38942e2a4db54f962c4bdc8",
            "value": "‚Äá27.0/27.0‚Äá[00:00&lt;00:00,‚Äá2.36kB/s]"
          }
        },
        "ea06137e82af45efadc7b09d895c98ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1691d7c03b664526ae81f7cf9a3bff03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e5fd2bca32e44709937dc2db74e24f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1a77102140d41dea360a32755f648a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a208393058f4685bf8ce4eaefac7b32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8f9f11b388b34886a8e355d397ba86c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "944b2223f38942e2a4db54f962c4bdc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d367c7e3beab4ac680e7a3de0f14bc92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b993db9f5ed6449781e344cf0e0f31c1",
              "IPY_MODEL_6ebeb44d1bb342a78334f88bfdb8b281",
              "IPY_MODEL_40e5e35daa4b420cb64bdaee0375c022"
            ],
            "layout": "IPY_MODEL_042e7a4f7ca944b784e956ed63da1179"
          }
        },
        "b993db9f5ed6449781e344cf0e0f31c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06ee76276f9e447487f5320898909145",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_560b4f97f14a4384bb85a185c40e540c",
            "value": "vocab.json:‚Äá100%"
          }
        },
        "6ebeb44d1bb342a78334f88bfdb8b281": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66e68534c8024dfb9e51bfbd7cdf26af",
            "max": 898822,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_209f9d1dc43e425f9005068a67349eb3",
            "value": 898822
          }
        },
        "40e5e35daa4b420cb64bdaee0375c022": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22c063550b0c4ff09d30b76a0ee1fc02",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_32a1be30fcaf428f9707b9ed369c4a18",
            "value": "‚Äá899k/899k‚Äá[00:00&lt;00:00,‚Äá6.47MB/s]"
          }
        },
        "042e7a4f7ca944b784e956ed63da1179": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06ee76276f9e447487f5320898909145": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "560b4f97f14a4384bb85a185c40e540c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66e68534c8024dfb9e51bfbd7cdf26af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "209f9d1dc43e425f9005068a67349eb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "22c063550b0c4ff09d30b76a0ee1fc02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32a1be30fcaf428f9707b9ed369c4a18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "933987ad51354a6b8a40b345cf581dce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f6c06e1d57e14638ba6c7d41f4632e4c",
              "IPY_MODEL_fb525afdda86418b80467affa08770a1",
              "IPY_MODEL_b5eaaf3d8e7246978607f67da372c883"
            ],
            "layout": "IPY_MODEL_2b3e9682f8054cf2ad5e7510cd3f5174"
          }
        },
        "f6c06e1d57e14638ba6c7d41f4632e4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b3578ca65354481859885943cd13708",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0ae3a4c35d0b425eb8635c025576ba33",
            "value": "merges.txt:‚Äá100%"
          }
        },
        "fb525afdda86418b80467affa08770a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbce0985d9aa43219ab41461b8bbc844",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_63ddaaa094a749d89ac7cf04d0eda114",
            "value": 456318
          }
        },
        "b5eaaf3d8e7246978607f67da372c883": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fe2702722f246baa24969e44bb2d11b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_00a7e173f3c247eab6fa7fc63cb490f7",
            "value": "‚Äá456k/456k‚Äá[00:00&lt;00:00,‚Äá3.53MB/s]"
          }
        },
        "2b3e9682f8054cf2ad5e7510cd3f5174": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b3578ca65354481859885943cd13708": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ae3a4c35d0b425eb8635c025576ba33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cbce0985d9aa43219ab41461b8bbc844": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63ddaaa094a749d89ac7cf04d0eda114": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8fe2702722f246baa24969e44bb2d11b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00a7e173f3c247eab6fa7fc63cb490f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c13258637b9948ef9cc5c1d639810394": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_942abe83b2514eb3a7a3bf885cfb30f8",
              "IPY_MODEL_3121fc55216b43ee89151df4365a2ecc",
              "IPY_MODEL_960348b804784fda826814cb042bb153"
            ],
            "layout": "IPY_MODEL_e8c72e736ed6420489d754590136da20"
          }
        },
        "942abe83b2514eb3a7a3bf885cfb30f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5292ea8848db4efda5fe3367bfe7427d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e65e15326a7240aba05442413713841e",
            "value": "special_tokens_map.json:‚Äá100%"
          }
        },
        "3121fc55216b43ee89151df4365a2ecc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd5fe25858b141d4b7917f89261886be",
            "max": 772,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c13bbf2323714fc1984d73b94087490b",
            "value": 772
          }
        },
        "960348b804784fda826814cb042bb153": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d63099c863d84a34830881ced21f2095",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0846e34881984e928a1b3d609323d83d",
            "value": "‚Äá772/772‚Äá[00:00&lt;00:00,‚Äá71.9kB/s]"
          }
        },
        "e8c72e736ed6420489d754590136da20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5292ea8848db4efda5fe3367bfe7427d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e65e15326a7240aba05442413713841e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd5fe25858b141d4b7917f89261886be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c13bbf2323714fc1984d73b94087490b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d63099c863d84a34830881ced21f2095": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0846e34881984e928a1b3d609323d83d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "439aa6ee1f4f41eeb48b5f8da5518190": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_258ac97917454162b7eb1b75816fe0e9",
              "IPY_MODEL_4fd6b44a325a4e228741a0aebd51f9c4",
              "IPY_MODEL_8d7497d5a6fb45d9a83add7c3a57c93c"
            ],
            "layout": "IPY_MODEL_8dd6509310304fcb88ebabddafe53863"
          }
        },
        "258ac97917454162b7eb1b75816fe0e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_596baf0ae5c04bd8b84e4b54beb02927",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_86c9dd27deb641d0bd5e4cf7da1c0b0f",
            "value": "config.json:‚Äá100%"
          }
        },
        "4fd6b44a325a4e228741a0aebd51f9c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a241c33e4dc34f28aafe417879af16c7",
            "max": 1291,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8472bf0f31934bc991ef743f3984ee2c",
            "value": 1291
          }
        },
        "8d7497d5a6fb45d9a83add7c3a57c93c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f806340d284f489395ea3c919ed037b3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0c56c702228d4bfbb77e47281ec212b1",
            "value": "‚Äá1.29k/1.29k‚Äá[00:00&lt;00:00,‚Äá126kB/s]"
          }
        },
        "8dd6509310304fcb88ebabddafe53863": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "596baf0ae5c04bd8b84e4b54beb02927": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86c9dd27deb641d0bd5e4cf7da1c0b0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a241c33e4dc34f28aafe417879af16c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8472bf0f31934bc991ef743f3984ee2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f806340d284f489395ea3c919ed037b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c56c702228d4bfbb77e47281ec212b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e17bd75744d4de5bb962d0596ff4eb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_59978dc0538543cb8535faa4ddda42b7",
              "IPY_MODEL_02a8ba7ad363409f8a2f44be3b7ff296",
              "IPY_MODEL_06cca6f4f31d4065a8d31389e659d4a4"
            ],
            "layout": "IPY_MODEL_7f34543d4bfc47eb8087a27ef1680b28"
          }
        },
        "59978dc0538543cb8535faa4ddda42b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99b0a63ab9f441c083d846d0a204869c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_eb672d0aebf5499e84761ee544cdaa4c",
            "value": "pytorch_model.bin:‚Äá100%"
          }
        },
        "02a8ba7ad363409f8a2f44be3b7ff296": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a26c872b9334e2692b015c48b1d245c",
            "max": 1839633783,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_89ca36852a3045238cedb941e7787ce2",
            "value": 1839633783
          }
        },
        "06cca6f4f31d4065a8d31389e659d4a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73193ebfa45d48a19c0f50e90b6bf8f3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0601868ed3fc4f6f9b6ec722d585d346",
            "value": "‚Äá1.84G/1.84G‚Äá[00:06&lt;00:00,‚Äá304MB/s]"
          }
        },
        "7f34543d4bfc47eb8087a27ef1680b28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99b0a63ab9f441c083d846d0a204869c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb672d0aebf5499e84761ee544cdaa4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a26c872b9334e2692b015c48b1d245c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89ca36852a3045238cedb941e7787ce2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "73193ebfa45d48a19c0f50e90b6bf8f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0601868ed3fc4f6f9b6ec722d585d346": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1255d1255b604487bd0c14ec820c81c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0384b82cc94b46bb9ce3f14f9a35bce3",
              "IPY_MODEL_66fc8c237ce74476947e4350d240dbdf",
              "IPY_MODEL_39dead09521a463da81e0041796e4b8e"
            ],
            "layout": "IPY_MODEL_f85a9f98597c4cbd96a9759d1677d59c"
          }
        },
        "0384b82cc94b46bb9ce3f14f9a35bce3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_280236c46a2748abbe24b66f51635d30",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9ed7388a793a427ba2e21e2144781cd0",
            "value": "generation_config.json:‚Äá100%"
          }
        },
        "66fc8c237ce74476947e4350d240dbdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_870748ae632a4c01aab379c8ef56cf90",
            "max": 207,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e38041035f2f4dbf86cc5a9d00569759",
            "value": 207
          }
        },
        "39dead09521a463da81e0041796e4b8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5492ab187df24351aa4fed914626af27",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_649092607c38470797d4be05f3643ca1",
            "value": "‚Äá207/207‚Äá[00:00&lt;00:00,‚Äá15.4kB/s]"
          }
        },
        "f85a9f98597c4cbd96a9759d1677d59c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "280236c46a2748abbe24b66f51635d30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ed7388a793a427ba2e21e2144781cd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "870748ae632a4c01aab379c8ef56cf90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e38041035f2f4dbf86cc5a9d00569759": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5492ab187df24351aa4fed914626af27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "649092607c38470797d4be05f3643ca1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1aa9c419a074b0496c9ecec5b99e227": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1efc9afbc9034cf3ad280196a8a373a3",
              "IPY_MODEL_79a416fb7e474eef8c06aaa737476d22",
              "IPY_MODEL_643838433a59418cb2ca0a3eb733b194"
            ],
            "layout": "IPY_MODEL_c8453d0433004aaeae2a2cfbb80787c1"
          }
        },
        "1efc9afbc9034cf3ad280196a8a373a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_315a5146fdaa45fe955a1d1985c427a8",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_07457f5e74e14984a486e41efa946ff7",
            "value": "Downloading‚Äábuilder‚Äáscript:‚Äá"
          }
        },
        "79a416fb7e474eef8c06aaa737476d22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_071450748bf241c3be4ba97d715a320e",
            "max": 2169,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dd15c7f451454d378d110b58e6910c75",
            "value": 2169
          }
        },
        "643838433a59418cb2ca0a3eb733b194": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f416453d132489a8be070ee92964c81",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_11f7c8e107124051a2ee7ca49e82366c",
            "value": "‚Äá5.65k/?‚Äá[00:00&lt;00:00,‚Äá455kB/s]"
          }
        },
        "c8453d0433004aaeae2a2cfbb80787c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "315a5146fdaa45fe955a1d1985c427a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07457f5e74e14984a486e41efa946ff7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "071450748bf241c3be4ba97d715a320e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd15c7f451454d378d110b58e6910c75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4f416453d132489a8be070ee92964c81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11f7c8e107124051a2ee7ca49e82366c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce45359114b34afebf4e46f0115dde8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f9ee516d27f640c5a6bf98eb0f7f5e2b",
              "IPY_MODEL_d6099042f12a42989d90f411e7085e9e",
              "IPY_MODEL_078ab9ae2f534e61a565b56f80700371"
            ],
            "layout": "IPY_MODEL_97a448a2152245d484d1b8454f1ad1c0"
          }
        },
        "f9ee516d27f640c5a6bf98eb0f7f5e2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4293247c42bd4a8dac167a2de9122eca",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_841eb259cae1414fb4c5d40f67f3e613",
            "value": "Map:‚Äá100%"
          }
        },
        "d6099042f12a42989d90f411e7085e9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c708c17b97cd4b849680bc89df9e4147",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6bc9cdda3ac74fa0a21f87ba13f0f1e1",
            "value": 100
          }
        },
        "078ab9ae2f534e61a565b56f80700371": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d90b623c405d4640bf21b24bf3531722",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c81f6c4e4f9247939157ffcb7c05f7ba",
            "value": "‚Äá100/100‚Äá[00:01&lt;00:00,‚Äá109.81‚Äáexamples/s]"
          }
        },
        "97a448a2152245d484d1b8454f1ad1c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4293247c42bd4a8dac167a2de9122eca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "841eb259cae1414fb4c5d40f67f3e613": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c708c17b97cd4b849680bc89df9e4147": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bc9cdda3ac74fa0a21f87ba13f0f1e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d90b623c405d4640bf21b24bf3531722": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c81f6c4e4f9247939157ffcb7c05f7ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6110a63dbfe4ce8a995ff7ae56c3c51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4a0ca1c965ed456e908c6df0963f3df0",
              "IPY_MODEL_83f69f9f65f9443abb2c96cee2b7c6a8",
              "IPY_MODEL_0399253c060a46a1abf59014c5814647"
            ],
            "layout": "IPY_MODEL_55ebc8492d7d453f8f27d1d5898aea5b"
          }
        },
        "4a0ca1c965ed456e908c6df0963f3df0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e367cf6db65d4b99af750c1001ad5f0e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e4534c0ca08541fda845afbfa774e181",
            "value": "Map:‚Äá100%"
          }
        },
        "83f69f9f65f9443abb2c96cee2b7c6a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_876099633c43465ba92598a18ed0e68f",
            "max": 40,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_feeb350195f44d87b1bfc81d2c059121",
            "value": 40
          }
        },
        "0399253c060a46a1abf59014c5814647": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e73e1a77ccbe4611994f71aa25a6c438",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b4bb0d6b9f7940db8a1046a5d8b309fa",
            "value": "‚Äá40/40‚Äá[00:00&lt;00:00,‚Äá124.93‚Äáexamples/s]"
          }
        },
        "55ebc8492d7d453f8f27d1d5898aea5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e367cf6db65d4b99af750c1001ad5f0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4534c0ca08541fda845afbfa774e181": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "876099633c43465ba92598a18ed0e68f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "feeb350195f44d87b1bfc81d2c059121": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e73e1a77ccbe4611994f71aa25a6c438": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4bb0d6b9f7940db8a1046a5d8b309fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab7c61a564364a519266b006bfcfb6c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_837a81a4eed943bd9667790b1927e135",
              "IPY_MODEL_94ad5ec3c28647d4afaccbb945294b7e",
              "IPY_MODEL_b86dd08692fe4e20b8380204a9adbc58"
            ],
            "layout": "IPY_MODEL_caac8fb62a2d4ed2a11a01364a3a2a3a"
          }
        },
        "837a81a4eed943bd9667790b1927e135": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_251ad3c1e3e24ac1876126ececedc333",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3d5fba239e0b41dd942fa0e619c59b93",
            "value": "Saving‚Äáthe‚Äádataset‚Äá(1/1‚Äáshards):‚Äá100%"
          }
        },
        "94ad5ec3c28647d4afaccbb945294b7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_584b896dba314e8da1771c4c3867e31e",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a9964c1451754971b6c4175b5bd6a0cb",
            "value": 100
          }
        },
        "b86dd08692fe4e20b8380204a9adbc58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc90f475afb445ecb98797ebc46d40d5",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5022fd03f3734aed9d5dcf6f96f26788",
            "value": "‚Äá100/100‚Äá[00:00&lt;00:00,‚Äá257.53‚Äáexamples/s]"
          }
        },
        "caac8fb62a2d4ed2a11a01364a3a2a3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "251ad3c1e3e24ac1876126ececedc333": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d5fba239e0b41dd942fa0e619c59b93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "584b896dba314e8da1771c4c3867e31e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9964c1451754971b6c4175b5bd6a0cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cc90f475afb445ecb98797ebc46d40d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5022fd03f3734aed9d5dcf6f96f26788": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "63a516c6b8214f6caf02797841f2d206": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_12807c9cf33e4be48115e5403ca8db03",
              "IPY_MODEL_38c8b8c0b7e44990833f1a23ebac16ff",
              "IPY_MODEL_4b6ae9a86892497db5e657e4725fb557"
            ],
            "layout": "IPY_MODEL_e978f6011d32469e8aa84030a0bfdd17"
          }
        },
        "12807c9cf33e4be48115e5403ca8db03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18d063ed7b27436497bc7b61f7eef93a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_324ef4c515ff45acbf11f4628c8a8229",
            "value": "Saving‚Äáthe‚Äádataset‚Äá(1/1‚Äáshards):‚Äá100%"
          }
        },
        "38c8b8c0b7e44990833f1a23ebac16ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00c4cf0d0d78454381f1b9be5f1cccca",
            "max": 40,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a044f22a9ca14e71a28dcd76802971c6",
            "value": 40
          }
        },
        "4b6ae9a86892497db5e657e4725fb557": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea3c725a4e234670a90646eed642b073",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_36dfbfdcd3df4c9ea35d6dbaa5b73b25",
            "value": "‚Äá40/40‚Äá[00:00&lt;00:00,‚Äá117.35‚Äáexamples/s]"
          }
        },
        "e978f6011d32469e8aa84030a0bfdd17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18d063ed7b27436497bc7b61f7eef93a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "324ef4c515ff45acbf11f4628c8a8229": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "00c4cf0d0d78454381f1b9be5f1cccca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a044f22a9ca14e71a28dcd76802971c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ea3c725a4e234670a90646eed642b073": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36dfbfdcd3df4c9ea35d6dbaa5b73b25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}