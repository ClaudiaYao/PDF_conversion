{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "https://learnlearn.uk/ai/2023/05/08/automatically-creating-powerpoints-using-chatgpt-and-python/\n",
    "\n",
    "https://python.langchain.com/docs/modules/model_io/output_parsers/types/json\n",
    "https://python.langchain.com/docs/modules/model_io/output_parsers/types/pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from generate_ppt import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_name = \"1901.00039v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"gpt_summaries/{article_name}.json\", encoding='utf-8') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wang Zihan\\.conda\\envs\\dba5102\\lib\\site-packages\\transformers\\generation\\utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "section_texts = []\n",
    "section_names = []\n",
    "section_image_paths = []\n",
    "for section in data:\n",
    "    section_names.append(section[\"Section\"])\n",
    "    section_texts.append(get_section_groundtruth(section)) # Use GPT generated summaries\n",
    "    section_texts.append(get_section_summary(section, 'allenai')) # Use pretrained model generated summaries\n",
    "    section_image_paths.append(get_section_image_paths(section))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I Introduction\n",
      "Crowd counting is a significant topic in the multimedia and computer vision community, aiming to count objects, such as people, in images or video frames. The traditional approach utilizes density maps generated from dot-level annotations due to challenges in detailed annotations. This paper proposes a novel method that separates foreground/background mask prediction from density map estimation for improved accuracy. By studying five different incorporation variants, the most effective method is identified, which involves feeding estimated object posteriors into convolutional layers along with input image information. Extensive experiments on five public datasets demonstrate the superior performance of the proposed approach, achieving state-of-the-art crowd counting results.\n",
      "\n",
      "II Related work\n",
      "Various approaches have been developed to address crowd counting issues, with early methods utilizing hand-crafted features and classifiers for pedestrian detection. More recent advancements involve CNN-based detectors like Faster R-CNN and YOLO for improved performance and speed. Regression-based approaches have also been proposed, mapping local patch features to object counts to address heavily crowded scenarios. Density map regression frameworks have become a mainstream solution, offering robust crowd counting in challenging scenarios. Spatial relationships and cumulative attribute-based representations have been explored to enhance regression models. Recent works predominantly focus on convolutional neural networks, with innovations such as end-to-end CNN regression models, multi-column networks, and contextual pyramid CNNs demonstrating state-of-the-art performance. A novel mask-aware network approach has been proposed for crowd counting, integrating background/foreground mask data for accurate density regression, showing promising results in experiments.\n",
      "\n",
      "III Our proposed method\n",
      "The proposed method in the text focuses on density map estimation using a network with three main modules: the backbone, mask prediction branch, and mask-aware density regression branch. The network uses point-wise annotations to create ground-truth density maps and employs Gaussian kernels for density estimation. The density regression problem is framed as a mean square loss regression. The backbone sub-network includes multi-layer CNN and Inception network blocks for feature extraction. The mask prediction branch uses focal loss for training, and the mask-aware density regressor incorporates mask information in various ways, such as element-wise multiplication and convolutional layers. Training details include using PyTorch, Gaussian kernel for ground truth generation, and different optimizers based on the dataset size.\n",
      "\n",
      "IV Experiment\n",
      "The IV Experiment section details experiments conducted on challenging public datasets to demonstrate the effectiveness of the proposed mask-aware strategies. The experiments aim to verify improvements over baselines, identify the most effective density estimation solution, and compare the proposed approach against state-of-the-art methods. Evaluation metrics include mean absolute error and mean square error. The datasets used are the shanghaitech dataset, ucf cc dataset, and worldexpo dataset. Different strategies and architectures are evaluated, leading to significant performance improvements. Additionally, ablation studies and comparisons with state-of-the-art methods highlight the effectiveness and efficiency of the proposed method across various crowd counting datasets.\n",
      "\n",
      "V Conclusion\n",
      "The paper presents a deep neural network approach to address the crowd counting problem. The authors introduce a method that uses a dedicated network branch for predicting foreground/background masks and incorporates mask prediction into density map estimation. They evaluate the performance of their method on various datasets, showing that it achieves state-of-the-art crowd counting results. The study identifies the best performing design for the mask-aware density estimator through experimentation. The research has been supported by various research foundations and programs.\n",
      "\n",
      "References\n",
      "The text covers a wide range of references related to crowd understanding and object detection in various research papers published in prestigious journals and conferences such as IEEE Transactions, ACM International Conference on Multimedia, European Conference on Computer Vision, and more. Topics include deep learning methods for crowd counting, crowd behavior analysis, real-time object detection, and pedestrian detection in dense crowds, among others. The references also delve into techniques such as CNN-based single image crowd counting, histograms of oriented gradients for human detection, and the use of convolutional neural networks for crowd behavior analysis.\n",
      "\n",
      "No_title\n",
      "Mask-aware networks address the crowd counting problem by introducing a dedicated network branch to predict object/non-object masks, which are then combined with the input image to generate a density map. This approach differentiates between the presence and absence of objects, reducing the difficulty of density estimation. By incorporating mask prediction into density map estimation, the network achieves superior performance compared to traditional methods. Experimental validation on public datasets demonstrates the proposed approach's state-of-the-art performance in crowd counting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_summary = \"\\n\".join(section_texts)\n",
    "print(full_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_slide_data = get_title_slide_data(article_name)\n",
    "toc_slide_data = get_toc_slide_data(article_name, section_names)\n",
    "content_slide_datas = get_content_slide_datas(section_names, section_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "theme = \"Parcel\"\n",
    "prs = generate_section_level_ppt(theme, title_slide_data, toc_slide_data, content_slide_datas, section_image_paths)\n",
    "prs.save(f\"powerpoints/{article_name}_section_level.pptx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_summary_data = get_full_summary_data(full_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Title 1\n",
      "1 Subtitle 2\n",
      "0 Title 1\n",
      "1 Content Placeholder 2\n",
      "0 Title 1\n",
      "1 Content Placeholder 2\n",
      "0 Title 1\n",
      "1 Content Placeholder 2\n",
      "0 Title 1\n",
      "1 Content Placeholder 2\n",
      "0 Title 1\n",
      "1 Content Placeholder 2\n",
      "0 Title 1\n",
      "1 Content Placeholder 2\n"
     ]
    }
   ],
   "source": [
    "prs = generate_full_summary_ppt(theme, full_summary_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prs.save(f\"powerpoints/{article_name}_full_summary.pptx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title Slide\n",
      "0 Title 1\n",
      "1 Subtitle 2\n",
      "Title and Content\n",
      "0 Title 1\n",
      "1 Content Placeholder 2\n",
      "Section Header\n",
      "0 Title 1\n",
      "1 Text Placeholder 2\n",
      "Two Content\n",
      "0 Title 1\n",
      "1 Content Placeholder 2\n",
      "2 Content Placeholder 3\n",
      "Comparison\n",
      "0 Title 5\n",
      "1 Text Placeholder 1\n",
      "2 Content Placeholder 2\n",
      "4 Content Placeholder 3\n",
      "13 Text Placeholder 4\n",
      "Title Only\n",
      "0 Title 1\n",
      "Blank\n",
      "Content with Caption\n",
      "0 Title 1\n",
      "1 Content Placeholder 2\n",
      "2 Text Placeholder 3\n",
      "Picture with Caption\n",
      "0 Title 1\n",
      "1 Picture Placeholder 2\n",
      "2 Text Placeholder 3\n",
      "Title and Vertical Text\n",
      "0 Title 1\n",
      "1 Vertical Text Placeholder 2\n"
     ]
    }
   ],
   "source": [
    "# See the format of each layout\n",
    "for i in range(10):\n",
    "    slide_layout = prs.slide_layouts[i]\n",
    "    print(slide_layout.name)\n",
    "    new_slide = prs.slides.add_slide(slide_layout)\n",
    "    for shape in new_slide.placeholders:\n",
    "        print('%d %s' % (shape.placeholder_format.idx, shape.name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
