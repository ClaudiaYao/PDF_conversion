[
    {
        "Section_Num": "4",
        "Section": "4. Experiments",
        "Text": "the term spatial common sense is broad and concerns the ability to perceive and understand properties and regu- larities regarding spatial arrangements and motion that are shared by (common to) nearly all people. such common sense includes the fact that objects have d shape as op- posed to being oating d surfaces, the fact that scenes are comprised of objects, the d non intersection principle, the fact that objects do not spontaneously disappear, and many others . the model we propose in this work targets un- derstanding of static scenes, that is, scenes that do not con- tain any independently moving objects, and that are viewed under a potentially moving observer. thus, we restrict the term spatial common sense to refer to rules and regularities that can be perceived in static worlds. our experiments aim to answer the following questions: do grnns learn spatial common sense? are geometric structural biases necessary for spatial common sense to emerge? how well do grnns perform on egomotion estima- tion and d object detection? . view prediction we consider the following simulation datasets: i) shapenet arrangement from that contains scenes with synthetic d object models from shapenet arranged on a table surface. the objects in this dataset belong to four object categories, namely, cups, bowls, helmets and cam- eras. we follow the same train/test split of shapenet so that object instances which appear in the training scenes do not appear in the test scenes. each scene contains two objects, and each image is rendered from a viewing sphere which has possible views with camera elevations (, , ) and azimuths (, , . . . , ). there are different scenes in the training set and scenes with novel objects in the test set. ii) shepard metzler shapes dataset from that contains scenes with seven colored cubes stuck together in random arrangements. we use the train and test split of . iii) rooms ring camera dataset from that contains rooms with random oor and wall colors, in which there are variable numbers of objects with different shapes and colors. we compare grnns against the recent tower archi- tecture of eslami et al. , a d network trained under a similar view prediction loss. at each time step, the tower architecture takes as input a d rgb image and performs a series of convolutions on it. the camera pose from which the image was taken is tiled along the width and height axes and then concatenated with the feature map after the third convolution. finally, the feature maps from all views are combined via average pooling. both our model and the baseline use the same autoregressive decoder network. for fairness of comparison, we use groundtruth egomotion rather than estimated egomotion in all view prediction ex- periments, and only rgb input (no depth input of depth es- timation) for both our model and the tower baseline. in both the baseline and our model, we did not use any stochastic units for simplicity and speed of training. adding stochastic units in both is part of our future work. test results from our model and baseline on test images of shapenet arrangements and shepard metzler datasets are shown in figure reconstruction test error for the shapenet arrangement test set is shown in table ",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "1",
        "Section": "1. GRNNs",
        "Text": "",
        "Subsections": [
            {
                "Section_Num": "NA",
                "Section": "NA",
                "Text": "have a much lower reconstruction test error than the tower baseline. in figure , in the rst four rows, the distribu- tion of the test scenes matches the training scene distribu- tion. our model outperforms the baseline in visual delity. in figure , in the last four rows, the test scene distribu- tion does not match the training one: we test our model and baseline on scenes with four objects, while both models are trained on scenes with exactly two objects. in this case, our model shows strong generalization and outper- forms by a margin the geometry unaware baseline of , the latter refuses to see more than two objects present. we input view v, v, v query gt (a, b) query gt geom rnn d lstm input view v, v, v same statistics train/test strong generalization to scenes with more objects grnns (ours) tower (baseline) (a, b) - query gt geom rnn d lstm input view v, v, v same statistics train/test strong generalization to scenes with more objects strong generalization same statistics train/test figure view prediction results for the proposed grnns and the tower model of eslami et al. . columns from left to right show the three input views, the groundtruth image from the query viewpoint, the view predictions for grnns and for the tower base- line. the rst two rows are from the shapenet arrangement test set of , the next two rows are from the shepard metzler test set of , and the following two rows are from the rooms ring camera dataset also from . the last four rows show generalization to scenes with four objects from the shapenet arrangement dataset, while both models were trained only on scenes with two objects. grnns outperform the baseline by a large margin and strongly generalize under a varying number of objects. argue the ability to spatially reason should not be affected by the number of objects present in the scene. our re- sults suggest that geometry unaware models may be merely memorizing views with small interpolation capabilities, as opposed to learning to spatially reason. scene arithmetics the learnt representations of grnns are capable of scene arithmetics, as we show in figure the ability to add and subtract individual objects from d scenes just by adding and subtracting their corresponding latent representations demonstrates that our model disentan- gles what from where. in other words, our model learns to store object specic information in the regions of the mem- ory which correspond to the spatial location of the corre- sponding object in the scene. implementation details and more qualitative view prediction results are included in the supplementary le. (a) (b) (c) query gt (a b+c) grnns (ours) tower (baseline) figure scene arithmetic with grnns and the model of eslami et al. (tower). each row is a separate equation. we start with the representation of the scene in the leftmost column, then subtract (the representation of) the scene in the second column, and add the (representation of the) scene in the third column. we decode the resulting representation into an image. the groundtruth image is shown in the forth column. it is much more visually similar to the prediction of grnns than to the tower baseline. tower grnns (baseline) (ours) shapenet shepard metzler table view prediction loss and the standard deviation for the shapenet arrangement test set for two object test scenes. our model and baseline were trained on scenes that also contain two objects with different object instances. . egomotion estimation ",
                "Subsections": [],
                "Groundtruth": ""
            },
            {
                "Section_Num": "4_2",
                "Section": "4.2. Egomotion estimation",
                "Text": "in this section, we quantify the error of our egomotion estimation component. we train our egomotion estimation # views one two three avg. grnns / / / / table egomotion estimation error of grnns in elevation and azimuth angles for the shapenet arrangement test set using different number of views. the error decreases with more views integrated in the memory. module using groundtruth egomotion from a simulator, us- ing the shapenet arrangement dataset. in table , we show egomotion estimation error in elevation and azimuth an- gles. our model improves its egomotion estimates with more views, since then a more complete feature memory is compared against each input unprojected tensor. . d object detection and segmentation we use the shapenet arrangement dataset, and the train/test scene split of . we use mean average precision (map) to score the performance of our model and baselines for d object detection and d segmentation. mean aver- age precision measures the area under the precision recall curve. we vary the cutoff threshold of intersection over union (iou) to be , and between our predic- tions and the groundtruth d boxes and masks. we consider four ablations for our model: predicted egomotion (pego) versus groundtruth egomotion (gtego) used, and predicted depth (pd) versus groundtruth depth (gtd) used as input. we use sufxes to indicate the model we use. we compare against the following d baseline model, which we call d rnn: we remove the unprojection, ego- motion estimation and stabilization and projection opera- tions from our model. the baseline takes as input an image and the corresponding depth map, feeds it to a d encoder- decoder network with skip connections to obtain a d fea- ture tensor. the camera parameters for this view are con- catenated as additional channels to the d feature tensor and altogether they are fed to another d encoder decoder network to obtain the d feature tensor for a d gru mem- ory update. we then feed the d memory feature tensor to an additional d encoder decoder network and reshape the channel dimension of its output into d feature vector of length (one value for the anchor box prediction, six values for the d bounding boxes adjustments) to form a d tensor of size w h d as prediction. we show mean average precision for d object detection and d segmentation for our model and the baseline in ta- ble , and visualize predicted d bounding boxes and seg- mentations from grnns (grnn gtego gtd) in figure grnns signicantly outperform the d rnn. groundtruth depth input signicantly helps d segmentation. this sug- gests that inferring depth using a cost volume as in would potentially help depth inference as opposed to rely- ing on a per frame depthnet that does not have access to multiple views to improve its predictions. implementa- t input views predicted d boxes (top view) t = ",
                "Subsections": [],
                "Groundtruth": ""
            }
        ],
        "Groundtruth": ""
    },
    {
        "Section_Num": "3",
        "Section": "3\nInput  ",
        "Text": " views predicted d boxes (top view) t view view view predicted d boxes and segmentation gt gt t = view view view predicted d boxes and segmentation gt gt figure d object detection and segmentation with grnns. in the rst and second row on the left we show the input images over time, and their corresponding object detection results for a top view, respectively. blue voxels denote groundtruth objects and the predicted bounding boxes are shown in red and green . on the right, we show segmentation results for the third time step, visualizing the results from two views. predicted d boxes and their corresponding predicted masks are show in red and green, and we show in blue the corresponding groundtruth. best seen in color. detection drnn- gtego- gtd grnn- gtego pd grnn- gtego- gtd grnn- pego gtd segmentation drnn- gtego- gtd grnn- gtego pd grnn- gtego- gtd grnn- pego gtd mapd mapm mapd mapm mapd mapm table mean average precision (map) for d object detection and d segmentation for three different thresholds of intersection over union (iou) (,,) on shapenet arrangement test set of . tion details and more qualitative results are included in the supplementary le. ",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "5",
        "Section": "5. Conclusion",
        "Text": "we presented grnns, recurrent neural networks equipped with differentiable geometric operations to esti- mate egomotion and build d deep feature maps for visual scene understanding on mobile visual agents. grnns add a new dimension to the latent space of previous recurrent models and ensure a geometrically consistent mapping be- tween the latent state and the d world scene. we showed spatial common sense emerges in grnns when trained in a self supervised manner for novel view prediction. they can predict object arrangements, visibility and occlusion re- lationships in scenes with novel number, appearance and conguration of objects. we also showed that view predic- tion as a loss does not sufce for spatial common sense to emerge, since -dimensional models of previous works fail to strongly generalize. thus far, grnns has been trained and tested on simu- lated scenes. deploying our model on more realistic envi- ronmentsis a clear avenue for future work. we expect pre- training in simulated environments to help performance in the real world. besides, one limitation of the current model is that it operates on static scenes. extending the proposed architectures to dynamic scenes, scenes with independently moving objects in addition to camera motion, is another very useful direction of future work. finally, exploiting the sparsity of our d tensors to save gpu memory is an im- portant direction for scaling up our model to large scenes. grnns pave the way for embodied agents that learn vi- sual representations and mental models by observing and moving in the world: these agents learn autonomously and develop the reasoning capabilities of young toddlers as op- posed to merely mapping pixels to labels using human su- pervision. acknowledgement we would like to thank xian zhou for his help on train- ing and testing the d maskrcnn. this work is partly funded by a google faculty award. references p. agrawal, j. carreira, and j. malik. learning to see by moving. corr, abs/, a. x. chang, t. funkhouser, l. guibas, p. hanrahan, q. huang, z. li, s. savarese, m. savva, s. song, h. su, j. xiao, l. yi, and f. yu. shapenet: an information rich d model repository. technical report arxiv: , stanford university princeton university toyota technological institute at chicago, r. cheng, z. wang, and k. fragkiadaki. geometry aware recurrent neural networks for active visual recognition. in nips, k. cho, b. van merrienboer, c . g ulc ehre, f. bougares, h. schwenk, and y. bengio. learning phrase representations using rnn encoder decoder for statistical machine transla- tion. corr, abs/, d. eigen, c. puhrsch, and r. fergus. depth map predic- tion from a single image using a multi scale deep network. in z. ghahramani, m. welling, c. cortes, n. d. lawrence, and k. q. weinberger, editors, advances in neural informa- tion processing systems , pages curran asso- ciates, inc., s. m. a. eslami, d. jimenez rezende, f. besse, f. vi- ola, a. s. morcos, m. garnelo, a. ruderman, a. a. rusu, i. danihelka, k. gregor, d. p. reichert, l. buesing, t. we- ber, o. vinyals, d. rosenbaum, n. rabinowitz, h. king, c. hillier, m. botvinick, d. wierstra, k. kavukcuoglu, and d. hassabis. neural scene representation and rendering. sci- ence, ():, c. e. freer, d. m. roy, and j. b. tenenbaum. towards common sense reasoning via conditional simulation: lega- cies of turing in articial intelligence. corr, abs/, c. godard, o. mac aodha, and g. j. brostow. unsuper- vised monocular depth estimation with left right consistency. corr, abs/, d. gordon, a. kembhavi, m. rastegari, j. redmon, d. fox, and a. farhadi. iqa: visual question answering in interac- tive environments. corr, abs/, s. gupta, j. davidson, s. levine, r. sukthankar, and j. ma- lik. cognitive mapping and planning for visual navigation. in proceedings of the ieee conference on computer vision and pattern recognition, pages , k. he, g. gkioxari, p. doll ar, and r. b. girshick. mask r cnn. corr, abs/, j. f. henriques and a. vedaldi. mapnet: an allocentric spatial memory for mapping environments. in proceedings of the ieee conference on computer vision and pattern recognition, d. jayaraman and k. grauman. learning image represen- tations equivariant to ego motion. corr, abs/, m. b. a. j. d. john mccormac, ronald clark and s. leutenegger. fusion++: volumetric object level slam. in arxiv:. a. kar, c. h ane, and j. malik. learning a multi view stereo machine. corr, abs/, c. kerl, j. sturm, and d. cremers. dense visual slam for rgb d cameras. in iros, m. liang, b. yang, s. wang, and r. urtasun. deep con- tinuous fusion for multi sensor d object detection. in the european conference on computer vision (eccv), septem- ber d. novotn y, d. larlus, and a. vedaldi. learning d object categories by looking around them. corr, abs/, e. parisotto and r. salakhutdinov. neural map: struc- tured memory for deep reinforcement learning. corr, abs/, l. regolin and g. vallortigara. perception of partly occluded objects by young chicks. perception & psychophysics, ():, s. ren, k. he, r. b. girshick, and j. sun. faster r cnn: towards real time object detection with region proposal net- works. corr, abs/, o. ronneberger, p. fischer, and t. brox. u net: convolu- tional networks for biomedical image segmentation. corr, abs/, t. sch ops, j. engel, and d. cremers. semi dense visual odometry for ar on a smartphone. in ismar, m. tatarchenko, a. dosovitskiy, and t. brox. single view to multi view: reconstructing unseen views with a convolu- tional network. corr, abs/, s. tulsiani, t. zhou, a. a. efros, and j. malik. multi view supervision for single view reconstruction via differentiable ray consistency. corr, abs/, h. f. tung, a. harley, w. seto, and k. fragkiadaki. adver- sarial inverse graphics networks: learning d to d lifting and image to image translation with unpaired supervision. iccv, s. vijayanarasimhan, s. ricco, c. schmid, r. sukthankar, and k. fragkiadaki. sfm net: learning of structure and mo- tion from video. arxiv preprint arxiv:, j. wu, y. wang, t. xue, x. sun, w. t. freeman, and j. b. tenenbaum. marrnet: d shape reconstruction via d sketches. corr, abs/, j. wu, t. xue, j. j. lim, y. tian, j. b. tenenbaum, a. tor- ralba, and w. t. freeman. d interpreter networks for viewer- centered wireframe modeling. international journal of com- puter vision (ijcv), y. xiang and d. fox. da rnn: semantic mapping with data associated recurrent neural networks. in robotics: science and systems (rss). b. yang, m. liang, and r. urtasun. hdnet: exploiting hd maps for d object detection. in a. billard, a. dragan, j. peters, and j. morimoto, editors, proceedings of the nd conference on robot learning, volume of proceedings of machine learning research, pages pmlr, oct t. zhou, m. brown, n. snavely, and d. g. lowe. unsu- pervised learning of depth and ego motion from video. in cvpr, y. zhou and o. tuzel. voxelnet: end to end learning for point cloud based d object detection. ieee conference on computer vision and patternrecognition (cvpr), supplementary material a. grnns implementation details the input images, output images, and predictions (for view prediction) all have size our pre- unprojection d encoder decoder network has encoder lay- ers with , , , and channels, respectively. the decoder layers are symmetric to the encoder layers. the sizes of these feature maps are , , and respectively, since each convolution has stride ",
        "Subsections": [],
        "Groundtruth": ""
    }
]