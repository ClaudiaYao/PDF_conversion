[
    {
        "Section_Num": "Abstract",
        "Section": "Abstract",
        "Text": "arxiv:v dec continuity of the mackey-higson bijection by alexandre afgoustidis & anne-marie aubert abstract. when g is a real reductive group and g is its cartan motion group, the mackey-higson bijection is a natural one-to-one correspondence between all irreducible tempered representations of g and all irreducible unitary representations of g in this short note, we collect some known facts about the topology of the tempered dual e g and that of the unitary dual c g, then verify that the mackey-higson bijection e g c g is continuous.",
        "Subsections": [],
        "Groundtruth": "The text discusses the continuity of the Mackey-Higson bijection between irreducible tempered representations of a real reductive group and irreducible unitary representations of its Cartan motion group. Known facts about the topology of the tempered dual and unitary dual are collected, and it is verified that the Mackey-Higson bijection is continuous."
    },
    {
        "Section_Num": "1",
        "Section": "1. Introduction",
        "Text": "let g be the group of real points of a connected reductive algebraic group dened over r. given a maximal compact subgroup k, form the cartan motion group g attached to g and k: if g = k p is the cartan decomposition of the lie algebra of g, then g is the semidirect product k p. write e g for the tempered dual of g and c g for the unitary dual of g recent years have brought proof that the parameters necessary to describe e g and c g are identical: there is a natural, but non-trivial, one-to-one correspondence between e g and c g . we will refer to this correspondence as the mackey-higson bijection. when equipped with the fell topology, neither e g nor c g is usually hausdor. but it has been known for a while that the existence of a topologically well-behaved bijection between e g and c g could be of particular signicance for certain aspects of representation theory, especially in relation with the study of group c-algebras. alain connes and nigel higson pointed out in the late s long before the actual construction of the mackey-higson bijection that the baum-connes-kasparov isomorphism for the k-theory of the reduced c-algebra of g can be viewed as a statement that e g and c g, although not homeomorphic, always have in a precise sense the same k-theory . the mackey-higson bijection does have pleasant topological properties: building on c-algebraic methods due to nigel higson in the complex case, one of us showed in that the bijection is a piecewise homeomorphism. for real groups, the homeomorphic pieces are dened through david vogans theory of lowest k-types. the pieces are stitched together dierently in both duals; but taking k-theory somehow blurs out that fact: the connes-kasparov isomorphism can actually be obtained in a rather elementary way from the topological properties of the mackey-higson bijection (this is the main result of ). in this short paper, we complete this topological information by proving that the mackey-higson bijection maps e g continuously onto c g, although it is never a homeomorphism . together with the results of , we obtain the following picture for its topological properties: continuity of the mackey-higson bijection theorem . the mackey-higson bijection m : e g c g is continuous, but m is not continuous unless g and g are isomorphic. let c be a nite subset of the unitary dual b k of k, and let e g (resp. c g) be the subset of e g (resp. c g) consisting of those representations whose set of lowest k-types is exactly c. the bijection m induces a homeomorphism between e g and c g let us further comment on property . the construction of the mackey-higson bijection is easier to write down in the c g e g direction; yet it is the reverse map, from e g to c g, which turns out to be continuous. this is only natural given the link with noncommutative geometry, particularly the recent work of nigel higson and angel romn on reduced c-algebras for complex groups . they proved the existence of a nontrivial continuous embedding c r c r , and characterized the mackey bijection in terms of this embedding in the particular case of complex groups, the existence of the embedding can be viewed as morally dual to our continuity result on the reduced duals. we should mention that most of the available information on the mackey bijection has been obtained through the existence of a continuous family t of groups interpolating between g and g: see . by contrast, our method here is direct: we use the ne details of the algebraic construction in , together with various known results on the topology of representation spaces. the properties we shall need are essentially algebraic, often boiling down to a careful analysis of lowest k-types, and do not rely on deformation theory. our proof, in , will be obtained by focusing on the restriction of the mackey-higson bijection to each connected component of e g. the fell topology on e g has been known quite precisely since the s , and the topology of c g has been described in ; we will use these descriptions and recall the necessary details in and we close this introduction with a few remarks on the possibility that analogues of properties and above may hold true for the admissible dual. (we thank the referee for suggesting and clarifying this.) consider the admissible duals adm and adm. the mackey-higson bijection extends to a bijection madm : adm adm: see . furthermore, both admissible duals are stratied according to lowest k-type theory. given a class b k, we can consider the subsets adm and adm consisting of those representations that admit as a lowest k-type. these strata do not dene a partition of the admissible duals, in contrast to the subsets e g of property : strata attached to dierent representations may intersect nontrivially. but each stratum is an ane algebraic variety , and the intersection of each stratum adm with the tempered dual e g is a nite disjoint union of subsets of the form e g. now, when g is a complex semisimple group, nigel higson proved in that for each b k, the map madm : adm adm induces an isomorphism of algebraic varieties between and . eyal subag conjectured in that the same property holds for arbitrary g. he veried this in for g = sl using the algebraic framework of . in view of these results, it it tempting to wonder about the topological properties of the extension madm : adm adm, and to speculate that it may be continuous (in the fell topologies described e.g. in ), but not a homeomorphism except in trivial cases. furthermore, it seems that subags conjecture may be the nal word on the algebraic analogue of property above. however, the fell topology on adm is more subtle than that of the tempered dual . because non-associated principal series representations can have a common subquotient, the connected components of adm cannot be described by simple results parallel to those of . therefore, extending our method to cover the case of the admissible duals would necessitate results on the fell continuity of the mackey-higson bijection topology of adm which we have been unable to locate in the literature, or to prove ourselves. as a result, we shall remain entirely conned within the tempered duals here. acknowledgments. we thank nigel higson for dissipating an early misconception about the topol- ogy of c g, and maarten solleveld for correcting a mistake in the rst version of proposition .",
        "Subsections": [],
        "Groundtruth": "The text discusses the concept of the Mackey-Higson bijection between the tempered dual and unitary dual of a connected reductive algebraic group defined over real numbers, denoted as e g and c g respectively. Recent research has shown that the parameters describing e g and c g are identical, leading to the construction of the bijection. The bijection is not a homeomorphism but is continuous, providing insights into representation theory and group C-algebras. The construction involves topological properties and is related to noncommutative geometry. The bijection also extends to the admissible duals of the group, with potential implications for algebraic varieties. The study remains focused on the tempered duals due to the complexity of the fell topology on the admissible duals."
    },
    {
        "Section_Num": "2",
        "Section": "2. Topology of the tempered dual",
        "Text": ". harish-chandra decomposition. suppose g is the group of real points of a connected reductive algebraic group dened over r. fix a haar measure on g, form the reduced c-algebra c r and the category mt = category of continuous nondegenerate c r -modules of tempered representations of g. let us rst recall how the general shape of harish-chandras plancherel formula yields an innite direct product decomposition of mt. the presentation is modeled on the p-adic case, more precisely, on schneider and zinks tempered version of the bernstein decomposition of the category of admissible representations . the results of this paragraph are true exactly as stated if f is any local eld and g is the group of f-points of a connected reductive algebraic group dened over f. let us call discrete pair any pair in which l is a levi subgroup of g e l is the equivalence class for a tempered irreducible representation of l that is square- integrable-modulo-center. the group g acts on the set of discrete pairs: if is a discrete pair and is a tempered irreducible representation of l with class , then for g g, the discrete pair g is the pair (glg, class of ). we write t for the space of orbits. for every levi subgroup l of g, let us call unramied unitary character of l any unitary character of l that is trivial on every compact subgroup of l; we will write xu for the set of unramied unitary characters of l. let us now x a discrete pair . the map l, : xu t orbit of sends xu to a subset l, = l, ) of t. inequivalent pairs map to disjoint subsets of t. we write bt = {l,, t} for the set of blocks. suppose is an irreducible tempered representation of g. then there exists a discrete pair and a parabolic subgroup p = ln of g with the property that is equivalent with one of the irreducible factors of indg ln. the pair determines an element l, of bt which depends only on , not on the choice of : we may and will call it the discrete support of . for all this, original sources include ; a convenient reference is . for every block bt, we write mt for the category of continuous nondegenerate c r - modules whose irreducible factors all have discrete support . harish-chandras work induces a direct sum decomposition c r = x bt c r , continuity of the mackey-higson bijection where the spectrum of a given component c r is e g = n e g whose discrete support is o ; see . this yields a partition e g = g bt e g of the tempered dual into disjoint subsets, and a direct product decomposition mt = y bt mt of the category of tempered representations. we refer to for the parallel with the non-archimedean case. . connected components of the tempered dual e g. the following remark is important for what follows. although it is well-known , we will sketch a proof. proposition . the connected components of e g are the e g, bt. proof. we note rst that for every , the subset e g of e g is closed: the decomposition in identi- es e g with the set of irreducible representations of c r that vanish on the ideal j = p = c r . that is indeed a closed subset . we now check that each e g is connected. to that end, we will x an element of e g and prove that every element in e g necessary lies in the same connected component of e g as . fix a component bt. among the discrete pairs with equivalence class , there is one that has the additional property that if l = ma is the langlands decomposition of l (where m is the subgroup of l generated by all compact subgroups of l, and a is the split component), then the restriction |a is trivial. we will assume that our pair has that property. writing a for the lie algebra of a and afor its vector space dual, recall that an element of ais called a-regular when its scalar product with every positive root of is nonzero. fix a parabolic subgroup p = ln with levi factor l; this determines an ordering of a. fix then an element in awhose scalar product with every positive root of is positive , then dene = indg p . since is a-regular, that representation is irreducible , and it lies in e g. for every representation in e g, there exists an element in awhich has the property that the scalar product of with every positive root of is nonnegative and that is equivalent with one of the irreducible constituents of indg p . for every t in , dene t = t + and t = indg p . for t > , the element t is a-regular because it has a positive scalar product with every positive root of ; thus t is irreducible for t > using the compact picture for induced representations, together with the routine criteria for the continuity of parameter integrals, we can exhibit every matrix element of as a limit (in the sense of fix a hilbert space v for ; then for each t , the representation t can be realized on the hilbert space h = \u001a f : k l v : u , f = f \u001b . suppose and are elements of h; then the matrix element c, : g , tcan be expressed using the iwasawa decomposition g = kman. if an element of g reads g = eh, where , , h, ) k ) a n, then we have c, = r k eit, hfdu, where f = , )v and is the usual half-sum of positive roots. see for instance . the claim easily follows. continuity of the mackey-higson bijection uniform convergence on compact subset of g) of a family of matrix elements of t> this proves that is in the closure of the family t>, and thus = and = must lie in the same connected component of e g. . vogans lowest-k-type picture for the decomposition. we now x a maximal compact subgroup k in g and recall vogans description of the blocks e g in terms of associate classes of lowest k-types . fix in e g. let be a discrete pair with equivalence class and the additional property that the restriction |a to the split component a of l is trivial . fix a parabolic subgroup p of g with levi factor l, then write c for the set of lowest k-types of the representation indg p . the notation is coherent, because all choices of with equivalence class , provided is trivial on the split component, lead to the same set of lowest k-types. proposition . let be an irreducible tempered representation of g. then lies in the component e g if and only if at least one of its lowest k-types lies in c. when that is the case, all the lowest k-types of lie in c. see for connected g, and for a class of groups that includes the current one. . closure of a subset of e g. we now formulate a version of the available results which yield a complete description of the topology of e g; let us mention contributions of delorme and milii . notations . fix a component e g in the tempered dual; assume e g is not a single point. consider : a discrete pair corresponding to ; l = ma : the langlands decomposition of l. we may and will assume that the restriction |a is trivial and will identify with its restriction to m . p = man : a parabolic subgroup with levi factor l + : the positive root system for that corresponds to n ; a+ = \b a, +, , . it is a closed cone in a. a/w = the quotient of aby the weyl group w = w. every class in a/w has a nite number of representatives in a+. for every irreducible tempered representation in the component e g, there is a unique = in a/w with the property that occurs in indg p . proposition . suppose b is a subset of e g. form the set v = {, b} of continuous parameters for representations in b, and its closure v in a/w. the closure of b in e g consists of those representations that belong to e g and satisfy v. proof. combine or with the information on connected compo- nents from . continuity of the mackey-higson bijection to obtain the closure of b in e g, we can thus form the set of representations indg p for in the closure of the set of continuous parameters for members of b, then consider all irreducible factors in these. example . suppose g = sl, man is the upper triangular subgroup (recall that m = {i} and a is the subgroup of diagonal matrices with positive entries and determinant one), and is the one non-trivial representation of m. the discrete pair determines a component in e g. we can identify the unitary unramied characters of ma with the unitary characters of a. now, consider the set b of representations of the form indg man, where is a non-trivial unitary character of a. all representations in b are irreducible and tempered. to obtain the closure of b in e g, we need to add to b the two limits of discrete series whose direct sum is the representation indg man. we shall need a consequence of the above results, taken from , that says what happens when one considers the continuous parameters for a convergent sequence of irreducible tempered representations. corollary . suppose is an irreducible tempered representation of g in the component e g, and nn is a sequence of irreducible tempered representations that admits as a limit point. there exists a subsequence jn of representations that all lie in e g and have the property that as j goes to innity, goes to in a/w.",
        "Subsections": [],
        "Groundtruth": "The text discusses the topology of the tempered dual, focusing on the Harish-Chandra decomposition and the category of continuous nondegenerate tempered representations of a reductive algebraic group. It explains the infinite direct product decomposition of this category and the formation of blocks based on discrete pairs. Harish-Chandra's work leads to a partition of the tempered dual into disjoint subsets and a direct product decomposition of tempered representations. The connected components of the tempered dual are discussed, along with Vogans' lowest k-type picture for the decomposition and the closure of subsets in the tempered dual. The text also presents propositions and examples related to irreducible tempered representations and their associated components."
    },
    {
        "Section_Num": "3",
        "Section": "3. Topology of the motion group dual and remarks on the Mackey-Higson bijection",
        "Text": ". mackey parameters for c g and e g. suppose is a unitary irreducible representation of the motion group g = k p of given a pair in which is an element of pand is an irreducible representation of the stabilizer k of in k, we shall say that is a mackey parameter for when is equivalent, as a representation of g, with indg kp. every unitary irreducible representation of g admits at least one mackey parameter, and two dierent mackey parameters for a given must be conjugate under k. it is perhaps a bit less natural to write down the denition of the mackey-higson bijection m : e g c g rather than that of its inverse, as in ; yet the information we shall need can be phrased as follows. if is an irreducible tempered representation of the reductive group g, we will say that is a mackey parameter for when it is a mackey parameter for the representation m of g crucial to the next statement are the notion of tempered representation with real innitesimal character , and, when a is the split component of a levi subgroup of g, the notion of a-regular element in a. lemma . let be an irreducible tempered representation of g. the pair is a mackey parameter for if and only there is an equivalence indg p \u0000 ei\u0001 , where the data satisfy the following conditions: p = ln = man is a parabolic subgroup of g, is an a-regular element of a, is a tempered representation of l that has real innitesimal character and whose restriction to k l = k admits as its lowest -type. continuity of the mackey-higson bijection the above statement depends on the fact k l = k whenever is a-regular, and on a theorem of vogan that says that given l and \\ k l, there exists a e l that satises the condition in the theorem. the fact that the statement is correct is a reformulation of parts of the construction of the correspondence in . . discontinuity of m : c g e g. before we consider the topology of c g in more detail, let us remark that we already know enough about e g to verify that the mackey bijection cannot be a homeomorphism. proposition . the bijection m : c g e g is never continuous, unless g and g are isomor- phic. an isomorphism between g and g exists if and only if g is a direct product of a compact group and an abelian vector group, in other words when the derived group of g is compact. when g and g are not isomorphic, we shall see, for instance, that the trivial representation of g is always a discontinuity point of m proof. suppose g and g are not isomorphic. choose a regular element in p(see , for the existence of such an element in our case). consider the induced representation = indg mp, where m = zk is the centralizer of in p. for every > , let us consider the representation = indg mp( ei ), and observe what happens as goes to innity. if b km is the set of those b k that occur in l, and if we identify the elements of b km with representations of g in which p acts trivially, then we will check that b = {, [, +[} b km is a connected subset of c g recall that we can realize as a representation that acts on l, in which k acts through the left regular representation l on l, and v p acts through f h u ei,advf i . suppose , are vectors in l that both lie in the isotypical subspace of l for a class b k. then the matrix element of attached to and a complex-valued function on k p reads r k ei ,v)du, where we viewed and as right-m-invariant functions on k. when goes to innity, that map converges to the map , ll, which is a matrix element for the representation of g that extends b km. this proves that b is connected. now, the image of b under m c ge g is not connected: the image of {, [, +[} \b triv b k is contained in the spherical principal series, which is by itself a connected component of e g. by contrast, the nontrivial elements of b km map to irreducible representations that do not contain the trivial k-type , and are thus contained in other connected components of e g. therefore the bijection m : c g e g sends the connected set b to a disconnected subset of e g, and we can conclude that m is not continuous. . baggetts description of the topology of c g theorem . let nn be a sequence of unitary irreducible representations of g, and be a unitary irreducible representation of g for every n in n {}, x a mackey parameter for n. it is the connected component associated with a discrete pair of the form where lmin is the levi factor for a borel subgroup of g and is the trivial representation of lmin. every representation in the spherical principal series contains the trivial k-type. continuity of the mackey-higson bijection the representation is a limit point of nn in c g if and only if nn admits a subsequence jn that satises the following conditions : as j goes to innity, nj goes to in p; the sequence \u0010 \u0011 jn is actually a constant in which klim is a subgroup of k; the induced representation indk klim contains . remark on the statement. the result is due to baggett , who works with the more general setting of a semidirect product h n where h is a compact group and n a locally compact abelian group. his main result does not look exactly like the above: there is the anecdotal point that he needs to use nets rather than sequences, and the more serious fact that he calls in a topology on the set a of pairs , where j is a closed subgroup of h and is an irreducible representation of j. to see how this translates into theorem in our case, let us rst give a statement closer in spirit to . for the cartan motion group g = k p, using the notations of theorem , baggett proves that is a limit point of nn if and only if there is a subsequence jn such that: as j goes to innity, nj goes to ; goes, in the fell space a, to a pair where klim is a subgroup of k, the induced representation indk klim contains . if we x a minimal parabolic subgroup pmin = mminaminnmin, we can choose the mackey parameters in such a way that n always belongs to the closed weyl chamber a+ min that comes with pmin. but then, since there is only a nite number of subgroups of k that can arise as the stabilizer of some in a+ min, there is only a nite number of subgroups that can arise as kn for some n. in baggetts statement above, we can thus replace with the sequence \u0010 \u0011 jn is eventually constant and knj is actually a subgroup of k. this leads to the statement in theorem .",
        "Subsections": [],
        "Groundtruth": "The Mackey-Higson bijection relates unitary irreducible representations of a motion group, stating that every representation admits at least one Mackey parameter. The bijection m : Cg → Eg is not continuous except when the groups are isomorphic; it maps connected sets to disconnected subsets. Baggett's description of the topology of Cg shows that a sequence of representations is a limit point if it satisfies specific conditions related to subgroups and induced representations. QMap Online Facilitation is a service that helps businesses improve their virtual communication and collaboration through interactive sessions and personalized support."
    },
    {
        "Section_Num": "4",
        "Section": "4. Continuity of the Mackey-Higson bijection",
        "Text": "theorem . the bijection m : e g c g is continuous. what we will actually check is that if is an element of e g and if nn is a sequence of irreducible tempered representations of g that admits as a limit point, then there exists a subsequence of nn whose image under m admits m as a limit point. . preliminaries on the reductive side. write e g for the connected component of e g that contains . there is a subsequence of nn whose terms all lie in e g; since we will have a handful of successive extractions to do hereafter, we will replace nn by such a subsequence without changing the notation. we now take up the setting of notations and consider a parabolic subgroup ln, a discrete series representation of l, and parameters kn{} in a+, in such a way that occurs in indg ln and for each n, n occurs in indg ln. by corollary , after passing to a subsequence if necessary, we may assume that goes to in b a/w. since the weyl group w is nite and n is a representative of (resp. continuity of the mackey-higson bijection ) in a+, we may assume, perhaps after another passage to subsequences, that n in fact goes to in a+. . strata in the weyl chamber. for each n in n {}, let us consider sn = \b + : , n> . that subset of + keeps track of the a-regularity of n: when sn = we have n = , whereas sn = + if and only if n is a-regular. observation . suppose is a subset of +. if {n n : sn = } is innite, then contains s. this is because when + lies in s, we have , > , so that eventually , n> and sn. let us now x a subset + such that {n n : sn = } is innite. after passing again to a subsequence if necessary, we may assume that sn = for all n. we now observe that the centralizer ln = zg of n in g does not depend on n. see for instance the details on the construction of the centralizer given in : if lmin = mminamin is a minimal levi subgroup of g contained in l, then ln is generated by mmin and the root subgroups for roots that lie in sn = . these do not depend on n. we will henceforth write lseq for the common centralizer of all ns, n n. given observation , if we write lfor the centralizer of in g, then l lseq l and if aseq and adenote the split components of lseq and l, then aaseq a. we remark, following the proof of , that l is in fact a levi subgroup in the reductive group lseq, and that lseq is itself a levi subgroup of l. with apologies for the clumsy notation, let us write e n for a subgroup of lseq such that l e n is a parabolic subgroup of lseq with levi factor l, and nseq for a subgroup of lsuch that lseqnseq is a parabolic subgroup of lwith levi factor lseq. we will also need the maximal compact subgroups kseq = k lseq and k= k l. . mackey parameters. the above observations make it easy to pin down the mackey parameters for each of the ns that remain at this stage . recall from vogans work that for every irreducible representation of kseq, there exists a unique irreducible tempered representation of lseq that has real innitesimal character and lowest kseq-type . write vlseq for that representation. the kseq-type is the unique lowest kseq-type of vlseq. lemma . for every n in n, there exists n in d kseq such that n is equivalent with the irreducible representation indg pseq \u0000vlseq ein\u0001 . furthermore, the representation n is uniquely determined by the set of lowest k-types of n. proof. recall that n occurs in indg ln and remark, as in the proof of , that if lseqn is a parabolic subgroup of g with levi factor lseq, then indg ln indg lseq \u0010 indlseq l e n ein\u0011 . for the coadjoint action, where we view n aas an element of g. continuity of the mackey-higson bijection the representation = indlseq l e n is tempered, has real innitesimal character and a nite number of irreducible components. since n is aseq-regular, for every irreducible component of , the repre- sentation indg lseqn is in fact irreducible (see for instance and the discussion in ). furthermore, its lowest k-types are entirely determined by . thus, the lowest k-types of n make it possible to pin down the one irreducible constituent n for which n indg lseqn. writing n for the unique lowest kseq-type of n, we obtain n vlseq, as desired. in the above statement, n may depend on n; but it determines the set of lowest k-types of n, which is a subset of the class c described in , and dierent values for n lead to dierent sets of lowest k-types . so there are only a nite number of possibilities for the element n in d kseq. after a new extraction, we obtain the following result: lemma . there exists a subsequence jn of nn with the property that nj does not depend on j, so that there exists seq d kseq such that: j n, nj indg pseq \u0000vlseq ein\u0001 . if we replace nn by the above subsequence and take up the notations of , we now know from lemma that for each n, a mackey parameter for n is . . remark on the lowest k-types of . at this stage, since each n is aseq-regular, we know that the set of lowest k-types of n does not depend on n: it is determined by the pair . example shows that does not necessarily have the exact same set of lowest k-types as the ns: if g = sl and is a limit of discrete series, then has a unique lowest so-type, but is a limit point of the nonspherical principal series, which consists of representations having two distinct lowest so-types . lemma . suppose cseq b k is the set of lowest k-types common to all n, n n. then the set of lowest k-types of is contained in cseq. proof. we rst recall that if is a class in b k and : k c is its character, there is a simple criterion for ascertaining that an irreducible tempered representation of g does not contain upon restriction to k: one needs only check that for every matrix element c : g c of , the convolution product \u0000c|k \u0001 vanishes. we also recall that the partial ordering on b k used to dene the notion of lowest k-type can be built from a positive-valued function b k on b k. all classes in cseq have the same norm b k ; write cseqb k for the common value. consider then a k-type such that b k cseqb k, but which does not belong to cseq, and a matrix element c of ; let us inspect the convolution \u0000c|k \u0001 . by denition of the fell topology, there exists a sequence nn of complex-valued functions on g, in which each map cn is a matrix element of n, and which as n goes to innity goes to c uniformly on compact subsets of g. since the k-type appears in none of the n, we have \u0000|k \u0001 = for each n. but |k)nn goes to c|k uniformly on k; so \u0000|k \u0001 does pointwise converge to the \u0000c|k \u0001 . thus the latter function must be zero. we conclude that a k-type whose norm does not exceed cseqb k can appear in only if it belongs to cseq. besides, lies in e g, so we already know that its lowest k-types all have norm cseqb k; this proves the lemma. this is the convolution product over k: the function k r k cdu from k to c. continuity of the mackey-higson bijection . verication of baggetts criterion. we can now prove that m is a limit point of )nn in the unitary dual c g in ., we showed that a mackey parameter for n is recall that in the present context, the centralizer of n in k is equal to kseq . by the argument already used for lemma , we also know that there exists a parabolic subgroup pwith levi factor l, and an irreducible representation d k, such that is equivalent with the irreducible representation indg p ei); the representation then admits as a mackey parameter. given baggetts criterion , to complete the proof of theorem , we need only verify the following fact: lemma . the representation of kis contained in indk kseq. proof. suppose the lemma is false. let us induce to k and compare the k-modules indk k and indk k \u0010 indk kseq \u0011 indk kseq; more precisely, let us inspect their lowest k-types. if is an irreducible representation of kthat appears in indk kseq, then under our assumption that the lemma is false, we have = ; but in that situation, we know that the representations indk k and indk k( ) have no lowest k-type in common. the second k-module in is a direct sum of k-modules that each read indk k( ) for some = ; so the two k-modules to be compared in actually have no lowest k-type in common. to see that this is impossible, we only have to point out that the two k-modules in are the restrictions to k of and n . lemma shows that each lowest k-type in is also a lowest k-type in n, so the set of lowest k-types of the rst module of is contained in the set of lowest k-types of the second. the lemma follows. references a. afgoustidis on the analogy between real reductive groups and cartan motion groups. i: the mackey- higson bijection, arxiv:v a. afgoustidis on the analogy between real reductive groups and cartan motion groups. iii: a proof of the connes-kasparov isomorphism, j. funct. anal. , no. , p. , on the analogy between real reductive groups and cartan motion groups. ii: contraction of irre- ducible tempered representations, duke math. j. , no. , p. l. baggett a description of the topology on the dual spaces of certain locally compact groups, trans. amer. math. soc. , p. e. p. van den ban induced representations and the langlands classication, in representation theory and automorphic forms , proc. sympos. pure math., vol. , amer. math. soc., providence, ri, , p. p. baum, a. connes & n. higson classifying space for proper actions and k-theory of group c- algebras, in c-algebras: , contemp. math., vol. , amer. math. soc., providence, ri, , p. j. bernstein, n. higson & e. subag algebraic families of harish-chandra pairs, int. math. res. not. imrn , no. , p. , contractions of representations and algebraic families of harish-chandra modules, int. math. res. not. imrn , no. , p. p. clare, t. crisp & n. higson parabolic induction and restriction via c-algebras and hilbert c-modules, compos. math. , no. , p. p. delorme formules limites et formules asymptotiques pour les multiplicits dans l, duke math. j. , no. , p. continuity of the mackey-higson bijection j. dixmier les c-algbres et leurs reprsentations, les grands classiques gauthier-villars. [gauthier- villars great classics], ditions jacques gabay, paris, , reprint of the second edition. j. m. g. fell weak containment and induced representations of groups. ii, trans. amer. math. soc. , p. , non-unitary dual spaces of groups, acta math. , p. n. higson the mackey analogy and k-theory, in group representations, ergodic theory, and mathe- matical physics: a tribute to george w. mackey, contemp. math., vol. , amer. math. soc., providence, ri, , p. , on the analogy between complex semisimple groups and their cartan motion groups, in non- commutative geometry and global analysis, contemp. math., vol. , amer. math. soc., providence, ri, , p. n. higson & a. romn the mackey bijection for complex reductive groups and continuous elds of reduced group c*-algebras, represent. theory , p. a. w. knapp & g. zuckerman classication of irreducible tempered representations of semi-simple lie groups, proc. nat. acad. sci. u.s.a. , no. , p. a. w. knapp representation theory of semisimple groups, princeton mathematical series, vol. , princeton university press, princeton, nj, , an overview based on examples. , lie groups beyond an introduction, second ed., progress in mathematics, vol. , birkhuser boston, inc., boston, ma, g. w. mackey on the analogy between semisimple lie groups and certain related semi-direct product groups, in lie groups and their representations (proc. summer school, bolyai jnos math. soc., budapest, ), halsted, new york, , p. p. pandi topology of nonunitary dual space of sl and sl, glas. mat. ser. iii , no. , p. c. rader spherical functions on cartan motion groups, trans. amer. math. soc. , no. , p. p. schneider & e.-w. zink k-types for the tempered components of a p-adic general linear group, j. reine angew. math. , p. , with an appendix by schneider and u. stuhler. b. speh & d. a. vogan, jr. reducibility of generalized principal series representations, acta math. , no. -, p. e. subag the algebraic mackey-higson bijections, j. lie theory , no. , p. p. c. trombi the tempered spectrum of a real semisimple lie group, amer. j. math. , no. , p. d. a. vogan, jr. the algebraic structure of the representation of semisimple lie groups. i, ann. of math. , no. , p. , classifying representations by lowest k-types, in applications of group theory in physics and mathematical physics , lectures in appl. math., vol. , amer. math. soc., providence, ri, , p. , a langlands classication for unitary representations, in analysis on homogeneous spaces and representation theory of lie groups, okayamakyoto , adv. stud. pure math., vol. , math. soc. japan, tokyo, , p. , branching to a maximal compact subgroup, in harmonic analysis, group representations, auto- morphic forms and invariant theory, lect. notes ser. inst. math. sci. natl. univ. singap., vol. , world sci. publ., hackensack, nj, , p. , isolated unitary representations, in automorphic forms and applications, ias/park city math. ser., vol. , amer. math. soc., providence, ri, , p. j.-l. waldspurger la formule de plancherel pour les groupes p-adiques , j. inst. math. jussieu , no. , p. continuity of the mackey-higson bijection alexandre afgoustidis, ceremade, universit paris-dauphine, psl, paris, france current aliation: cnrs & institut lie cartan de lorraine, umr e-mail : alexandre.afgoustidis@math.cnrs.fr anne-marie aubert, institut de mathmatiques de jussieu paris rive gauche, cnrs, sorbonne universit, universit de paris, paris, france e-mail : anne-marie.aubert@imj-prg.fr",
        "Subsections": [],
        "Groundtruth": "The theorem states that the Mackey-Higson bijection m: eg → cg is continuous. It is shown that if an element is a limit point of a sequence of irreducible tempered representations of g, then there exists a subsequence whose image under m also has m as a limit point. The text covers preliminaries on the reductive side, the setting of notations, strata in the Weyl chamber, Mackey parameters, and a verification of Baggett's criterion for the continuity of the Mackey-Higson bijection. Several lemmas and references are presented to support the arguments and conclusions made throughout the text. The development of the Mackey-Higson bijection is important for understanding the analogy between real reductive groups and Cartan motion groups."
    },
    {
        "Section_Num": "References",
        "Section": "References",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "Abstract",
        "Section": "Abstract",
        "Text": "arxiv:v jan on the various notions of poincar e duality pair john r. klein, lizhen qin, and yang su abstract. we establish a number of foundational results on poincar e spaces which result in several applications. one application settles an old conjecture of c.t.c. wall in the armative. another result shows that for any natural number n, there exists a nite cw pair satisfying relative poincar e duality in dimension n with the property that y fails to satisfy poincar e duality. we also prove a relative version of a result of gottlieb about poincar e duality and brations. contents",
        "Subsections": [],
        "Groundtruth": "The section discusses various notions of Poincaré duality in spaces and presents foundational results leading to several applications. These include settling an old conjecture by C.T.C. Wall and showing the existence of a finite CW pair satisfying relative Poincaré duality in a given dimension while failing to satisfy Poincaré duality. Additionally, a relative version of a result by Gottlieb concerning Poincaré duality and fibrations is proved."
    },
    {
        "Section_Num": "1",
        "Section": "1. Introduction",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "2",
        "Section": "2. Preliminaries",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "3",
        "Section": "3. The Thom Isomorphism",
        "Text": "poincar e duality",
        "Subsections": [],
        "Groundtruth": "The Thom Isomorphism in topology establishes a connection between the homology of a space and the cohomology of its compactification, known as Poincaré duality."
    },
    {
        "Section_Num": "4",
        "Section": "4. Poincaré Duality",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "5",
        "Section": "5. Doubling",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "6",
        "Section": "6. Finite Coverings",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "7",
        "Section": "7. Fibrations",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "8",
        "Section": "8. Historical Remarks",
        "Text": "appendix a. skew-commutativity of cup products appendix b. the k unneth theorems appendix c. proof of theorem references introduction appearing rst in the s, the concept of a poincar e duality space proved to be a crucial tool in the surgery classication of high dimensional mani- folds. it was bill browder who rst introduced the notion of a space satisfying mathematics subject classication. primary: p secondary: n, n key words and phrases. poincar e space, poincar e duality. john r. klein, lizhen qin, and yang su poincar e duality with integer coecients , . browders denition is well- suited for doing simply connected surgery but it is inadequate in the general case. to take into account some of the subtleties of the fundamental group, spivak later introduced another denition . although spivaks denition is adequate in the absolute setting, it is not sucient for doing surgery in the case of poincar e pairs. slightly thereafter, wall introduced a def- inition of poincar e pair which controls both the interior and the boundary suciently. we remark that all of the above denitions are equivalent in the simply connected case. in all the denitions of poincar e space, one sacrices the local homogeneity property that manifolds possess and one instead focuses on the overall global structure. this has both advantages and disadvantages. a principal advantage is that many of the standard tools of algebraic topology become available. a disadvantage is that one sometimes desires to have information about local structure. the current work had its genesis in a program of the authors on manifolds. in our investigations we needed a relative version of the bration theorem that says, roughly, in the presence of suitable niteness assumptions, the total space of a bration satises poincar e duality if and only if its base and ber do. the bration theorem was only stated in the absolute case, and the relative case remained an open problem. we originally thought that the relative case would follow mutatis mutandis by performing a reduction to the absolute case. however, such a reduction turned out to be more dicult and subtle than the authors had originally anticipated. the solution to this in the relative case appears as theorem g below. in proving the relative bration theorem, we had to apply some foundational results on poincar e pairs. however, after carefully checking the literature, we found that some foundational works were not clear and some old problems still remained unsolved. for example, to the best of our knowledge, walls con- jecture concerning the relationship between poincar e pairs and their bound- aries had not been solved . the ambiguity in these works partially arose because the denition of poincar e pairs is not histori- cally uniform . specically, walls denition is dierent from spivaks . in order to deal with this matter, we had to formulate and prove theorem e below though related results had been in the literature. a goal of this paper is to resolve the ambiguity and prove some new results. this can be seen from our theorem a below and section the result was announced without proof by quinn in , and proved by gottlieb in using manifold techniques. a homotopy theoretic proof was given by the rst author in . poincar e pairs walls denition of poincar e pair has been mostly used in surgery theory. our denition follows walls. in what follows we say is a space pair if y x is a subspace, where x is usually nonempty and y may be empty. when y is empty, the statements for y are considered vacuously true. we say is a -connected pair if x is a -connected space. by an orientation system on x, we mean an local system on x of innite cyclic groups. denition . a pair of topological spaces is a poincar e pair of formal dimension n with n if there exist an orientation system o and a singular homology class hn such that the homomorphisms : hm = hnm and : hm = hnm given by cap product with are isomorphisms, where b is a local system on x; the homomorphisms : hm = hnm are isomorphisms, where g is a local system on y and is the image of under the boundary homomorphism : hn hn here, we are taking singular homology with twisted coecients. if the above conditions hold, we call o the dualizing system of and a fundamental class. remark . it is not dicult to prove that for a poincar e pair , the pair is determined up to unique isomorphism. the fundamental class is not unique: for a given local system o, if equips with the structure of a poincar e pair, then so does . if x is path-connected, these are the only possibilities. however, the following uniqueness statement does hold for a poincar e pair : any pair satisfying the denition is dened up to unique isomorphism: if is another such pair for , then there exists precisely one isomorphism : o o such that such that = . remark . if the space y in denition is empty, then the above is vacuously true, and the isomorphisms and reduce to the single isomorphism : hm = hnm . john r. klein, lizhen qin, and yang su in this instance we say that x is a poincar e space. remark . in denition , we do not assume that x is connected. ap- plying with b = z, we obtain an isomorphism : hn = h since is nitely supported, the last isomorphism implies that x has only nitely many path components. we typically require that x and y have the homotopy type of cw complexes. remark . in the nitely dominated case, it is enough to check the duality isomorphisms for a single local system b dened by the fundamental group. precisely, when x is path connected we denote by the local system on x associated with z. if is a nitely dominated pair (cf. denition and remark ), wall proved that is equivalent to the isomorphisms: : hm = hnm , and is equivalent to the isomorphisms: : hm = hnm . one family of our main results are on the relations among , and . as mentioned in , there are some redundancies in denition . its trivial to prove + and + . however, we shall construct the following example which shows that + does not imply . theorem a + ). for n , let a be the punctured poincar e homology -sphere and let c denote the cone on asn then := , a sn) is a nite cw pair which satises and , but y is not a poincar e space of any formal dimension. in the nitely dominated case we shall also prove the following equivalence. theorem b . let be a pair of nitely dominated spaces. suppose we are given in which o is an orientation system on x and hn is a homology class. then with respect to these data is equivalent to . as an application, we will see that theorem b implies the following conjec- ture of c.t.c. wall: corollary c . with respect to the assumptions of theo- rem b, assume further or . suppose y is a poincar e space of formal dimension n then is a poincar e pair with fundamental class . poincar e pairs remark . here is an explanation of the context of corollary c. in , wall denes poincar e pairs as space pairs satisfying + . in the second paragraph appearing of page , wall writes: we conjecture that the two requirements enclosed in square brackets are redundant; however, we will have to impose them. here, wall is referring to the requirements the restriction o|y is the dualizing system of y , and the class is a fundamental class for y . note too that wall is working in the nitely dominated context. theorem b has an additional application. corollary d. under the assumptions of theorem b, assume further or . suppose, for each component z of y , every local system on z is the restriction of one on x. then is a poincar e pair with fundamental class . our other results rely heavily on the following theorem e which describes poincar e pairs homotopy theoretically. we recall the concept of a poincar e triad . we say is a space triad if y and y are subspaces of x. we say it is -connected if x is. denition . we say a space triad is a poincar e triad of formal dimension n if it satises the following conditions: is a poincar e pair of formal dimension n; {y, y} is an excisive couple ; if y is nonempty, then ) is a poincar e pair of formal dimension n , and its fundamental classes are the restrictions of those of y y in the above, y or y may be empty. if both are empty, one obtains a poincar e space. if one of them is empty, one obtains a poincar e pair. in the following theorem e, a poincar e cw triad is a cw triad which is also a poincar e triad. recall that a pair is k-connected if for j k, every map f : is homotopic rel sj to a map of dj into y . when writing f we mean the homotopy ber taken at any choice of basepoint, i.e., the bers over all points of x should have the homotopy type -spheres. if y = , the assertions for f will be considered vacuously true. when y is non-empty then we assume that the basepoints used in dening f and f coincide. note also that y may be disconnected. theorem e. suppose is a poincar e cw triad of formal dimension n + k such that x is connected and y = . set y := y y suppose john r. klein, lizhen qin, and yang su and are -connected. denote by f a homotopy ber of y x . then the following statements hold. if f is a homotopy -sphere, then both and hold for the pair for some orientation system o. conversely, if or holds, then f is a homotopy -sphere. is a poincar e pair of formal dimension n if and only if both f and f are homotopy -spheres. if is a poincar e pair of formal dimension n and y = , then the natural inclusion f f is a homotopy equivalence. remark . spivak formulated a statement similar to part of theorem e . however, there is a major dierence between spivaks result and ours in that spivak ignored the homotopy ber f . that is, spivak asserted that is a poincar e pair if and only if f itself is a homotopy sphere. the distinction originates from the fact that spivaks notion of poincar e pair is dierent from ours . the issue will be explained in section we wish to emphasize here that by theorem a, one has to take f into account if one uses our denition. otherwise, the conclusion of theorem e will be no longer true . theorem e has several applications which we now develop. it is well-known that if and are poincar e pairs of formal dimension n, then the amalgamation x y x is a poincar e space of formal dimension n (see , and lemma in this paper). one may ask about the extent to which the following partial converse is true: suppose that x y x is a poincar e space of formal dimension n and y is a poincar e space of formal dimension n then are and poincar e pairs of formal dimension n? in general the answer is no. here is an easy counterexample: let x = s, x = and x y x = x x, where y is the wedge point. however, a special case of the following theorem f implies that, roughly speaking, the answer is armative if the amalgamation is a double. theorem f . suppose is a nitely dominated triad (cf. denition ) with y = and y = y y suppose further is a poincar e pair of formal dimension n then is a poincar e pair of formal dimension n if and only if is poincar e triad of formal dimension n. poincar e pairs the merit of theorem f lies in the reduction of a poincar e triad to a poincar e pair . it will be used to prove the following theorem g which was the original motivation for this paper. theorem g . suppose and are nitely dominated pairs such that b is connected and based. let b be a pair of brations over b, where e e is a closed cobration. let e = e|b denote the pullback of e b induced by b b. assume further is a nitely dominated triad. then the following statements hold: the triad is a poincar e triad if and only if and are poincar e pairs. if is a poincar e triad, then its formal dimension is the sum of those of and ; a special case of theorem g is when b = and f = . we call this case the absolute version of the theorem. the absolute version was announced by quinn and proved in and by dierent methods. another special case of theorem g is the product of space pairs. thus it also generalizes theorem in . in , wall proved that if and are nitely dominated poincar e pairs, then so is the pair . however, wall neither stated nor proved the converse. the proof of theorem g will be obtained by a reduction to the absolute version by virtue of theorem f. lastly, we also prove the following result on nite coverings. theorem h . suppose ( x, y ) is a nite covering space of . then the following statements hold: if is a poincar e pair of formal dimension n, then so is ( x, y ) and a fundamental class for ( x, y ) is given by the transfer of a fundamental class for . assume that is a nitely dominated pair. if ( x, y ) is a poincar e pair of formal dimension n, then so is . some remarks about part of theorem h are in order. clearly, the result is a special case of theorem g. however, our proof of theorem g uses theorem h. when y = , part of theorem h is well-known. however, the relative version appears to be new. in fact, our proof requires theorem e. note that the nite domination assumption in part is needed: the double cover sof rp is contractible. consequently, sis a poincar e space of dimension , but rp is not a poincar e space. john r. klein, lizhen qin, and yang su outline. section recalls some facts frequently used in the paper. in section , we prove and recall some foundational results on the thom isomorphism. theorems a, b and e, and corollaries c and d are proved in section theorems f, h and g are proved in sections , and respectively. lastly, in section , we discuss the various denitions of poincar e pairs. acknowledgements. the authors are indebted to bill browder for explain- ing to us the genesis of poincar e duality spaces. the rst author is especially grateful to bill for his inspiration and support. we thank an anonymous referee who meticulously read the paper and made many helpful suggestions which led to an improved presentation. the rst author was partially supported by simons foundation collaboration grant the second author was par- tially supported by nsfc the third author was partially supported by nsfc preliminaries the purpose of this section is to recall some basic results that are frequently used in the paper. in the sequel, r denotes an associative ring with unit. lemma . assume c is a chain complex of right projective r-modules. assume further that c is connective, i.e., cp = for all p < then following statements are equivalent: c is contractible. c is acyclic, i.e., the homology groups hp vanish for all p. the homology groups hp = vanish for all p and all left (resp. right) r-modules m. the cohomology groups hp vanish for all p and all right (resp. left) r-modules m. proof. we only prove the case that c is a chain complex of right r-modules. the proof in the case of a left r-complex is similar. clearly, implies all of the others. trivially, . since c is degree- wise projective, it is well-known that . to nish the proof, it suces to prove . assuming , we prove using the universal coecient spectral sequence , whose e terms are ep,q = extq r, m). the spectral sequence converges to h. clearly, ep,q = for all q < since cp = for all p < , we see that ep,q = for all p < then homr, m) = e, = e, = poincar e pairs for all m. taking m = h, we infer h = then we know e,q = for all q and m. we further infer homr, m) = e, = e, = , which implies h = by induction on p, we see that hp = for all p, which completes the proof. corollary . for a homomorphism of f : c d of connective chain complexes of projective right r-modules, the following statements are equivalent: f is a chain homotopy equivalence. f is a quasi-isomorphism, i.e., the homomorphism f: hp hp is an isomorphism for all p. the homomorphism f: hp hp is an isomorphism for all p and all left r-modules m. the homomorphism f : hp hp is an isomorphism for all p and all right r-modules m. proof. apply lemma to the mapping cone. . local systems. for a space x we let x be its fundamental groupoid. this is the category whose objects are the points x of x where a morphism x y is a path homotopy class starting at x and terminating at y. a local system of abelian groups on x is a functor b: x ab , where ab denotes the category of abelian groups. if x is path-connected, then a local system b is determined up to canonical isomorphism by its value bx at a xed base point x x together with an action of on bx in particular, for a pair of spaces , one has the associated twisted singular chain and cochain complexes c and c . we recall how these are dened. in what follows, we may assume x is path- connected by working over each path component. let x x be the universal cover associated with a xed choice of basepoint x x. denote by the local system on x associated with the group ring := z. note that is a bimodule over itself. denote by y = y x x, i.e., the pullback of x x along y x. then c := c( x, y ; z) is a left -complex, where the -action is given by deck transforma- tions. the involution g g of induces one on by extend- ing linearly. denote this involution by . then the involution equips john r. klein, lizhen qin, and yang su c with the structure of a right -complex in which := . hence, c is both a left and a right -complex. suppose b is a local system on x. let m = bx then m is a right -module, and we have c := mc and c := hom, m). note that c is a free -complex. therefore, we can apply corol- lary to c and c. remark . let be a space pair. then for local systems g, h, one has cup products cp cq cp+q , as well as cap products cp cq cpq , and cp cq cpq . here, g h is the local system dened by the composition of functors x ab ab ab , where in the display, denotes the tensor product operation on abelian groups. remark . suppose is a simplicial pair. we use and to denote the corresponding simplicial chain and cochain complexes. clearly, is -free, = m and = hom, m). each of these is chain equivalent to its corresponding total singular chain complex, and the above results hold for these complexes as well. if we further assume is nite, then is also -free and = m . . reduction to the nite case. we introduce the following lemmas and which allow us to reduce a nitely dominated pair to a nite cw pair . a triple is said to be a space triad if a is a topological space and bi are subspaces of a, where bi may be empty . when a is a cw complex and bi are subcomplexes of a, we say is a cw triad. a map of space triads f : , poincar e pairs is a map of spaces a c such that f di for i = , if i = is the unit interval, then we may form the triad . if f, f: are maps of triads, then a homotopy from f to f is a map of triads h : such that h = f and h = f, where hi denotes the restriction of h to ia for i = , we write f f in this case. denition . a map of triads f : is called a homotopy equivalence of triads if there exists a map of triads g : such that f g id and g f id. denition . suppose is a space triad and is a nite cw triad. if there exist maps of triads such that the composition is homotopic to the identity, then is said to be a nitely dominated triad. remark . in particular, if the above b and d are empty, then those space triads are reduced to space pairs and . we also denote them by and . denitions and are reduced to concepts associated with space pairs. remark . there exist nitely dominated poincar e spaces which are not homotopy equivalent to any nite cw complex . remark . by , the fundamental group of a poincar e space is nitely generated, and a poincar e cw complex is nitely dominated if and only if its fundamental group is nitely presented. similar results also hold for poincar e pairs. there do exist poincar e spaces whose fundamental groups are not nitely presented, see . lemma . suppose is a nitely dominated triad and w is a nite cw complex with trivial euler characteristic. then there exist a nite cw triad and a homotopy equivalence of triads f : . proof. since b b is nitely dominated, w is nite and = , by gerstens product formula , walls niteness obstruction of (b b) w is hence, there exist a nite cw complex z and a homotopy equivalence j : z w. similarly, there are homotopy equivalences g : p a w, h : q b w and h : q b w, where p, q and q are nite cw complexes. john r. klein, lizhen qin, and yang su let hi be a homotopy inverse of hi. the map hij is homotopic to a cellular map i : z qi. let li be the mapping cylinder of i. obviously, there is a map h i : li bi w such that h i|qi = hi and h i|z = j. here z is a subspace of both l and l clearly, h i are homotopy equivalences. gluing l and l along r, we obtain a space l l and a map h : w; b w, b w) such that h|li = h i and l l = r. let g be a homotopy inverse of g. the map gh is homotopic to a cellular map : l l p. let k be the mapping cylinder of . there is a map f : such that f|ll = h and f|p = g. clearly, is a nite cw triad. its also easy to see f is a singular homotopy equivalence in the sense of [, p. ], i.e. a weak homotopy equivalence . furthermore, is a nitely dominated triad. now the conclusion follows from . assume x is path-connected. denote by the local system associated with z on x. let hn be a homology class, where o is an orientation system on x. suppose h is a fundamental class of s let p : x s x denote the projection. in the following, p o is the pullback of o via p . lemma . suppose is a nitely dominated pair. then the following statements - are equivalent. the following are isomorphisms for all : : h = hn. the following are isomorphisms for all and all local systems g on x: : h = hn. the following are isomorphisms for all : : h = hn+(x s, y s; p o ). the following are isomorphisms for all and all local systems g on x s: : h = hn+(x s, y s; p o g). furthermore, the following - are equivalent. the following are isomorphisms for all : : h = hn. poincar e pairs the following are isomorphisms for all and all local systems g on x: : h = hn. the following are isomorphisms for all : : h = hn+(x s; p o ). the following are isomorphisms for all and all local systems g on x s: : h = hn+(x s; p o g). remark . the equivalences and in lemma are not new, see e.g. . nevertheless, for the convenience of readers, we shall give a slightly dierent proof. remark . in the above, rather than taking product with s, we could have instead taken the product with s at the expense of changing the funda- mental group. proof of lemma . we shall only prove the equivalence from to . the argument for - is similar. firstly, we prove the equivalences and . by theorem b., the following square commutes up to sign. l q+r= hq hr \u000f = / h(x s; p g) \u000f l q+r= hnq hr = / hn+(x s, y s; p o p g) . since h and h are free, by the k unneth theorems b. and b., the two horizontal maps in this square are isomorphisms. thus the right vertical map is an isomorphism if and only if the left one is. again by the freeness of h and h, the left vertical isomorphism is equivalent to the isomorphism : hq = hnq. since p: is an isomorphism, we infer and . trivially, we have . to nish the proof, it suces to prove . by lemma , there exists a nite simplicial pair such that . john r. klein, lizhen qin, and yang su assume the isomorphisms : h = hn for some orientation system o on k. as mentioned in remark , we may work with the simplicial chain complexes and cochain complexes . choose a cycle n representing . then the following chain map : n induces an isomorphism between homology groups. since is nite, we know that = and n = when is small or large enough. thus, by remark and corollary , we obtain the isomorphisms : h = hn for all integers and all local systems b. this provides the implication . remark . instead of proving in lemma directly, we reduced the conclusion for the nitely dominated pair to that for the nite simplicial pair . this reduction will be frequently employed in the paper. the thom isomorphism we dene below a version of the thom isomorphism for space pairs and investigate its properties. denition . let be a space pair. suppose o is an orientation system on x, and u hk with k if u: h = hk+ is an isomorphism for all integers and local systems b on x, then we say satises the thom isomorphism of formal dimension k. we call u a thom class of . remark . suppose satises the thom isomorphism of formal di- mension k. we claim that k > if and only if is surjective. assuming is surjective, we infer h = if k = , we would have h = which is impossible since x is non-empty. con- versely, assuming k > , we get h = hk = obviously, is surjective. remark . its easy to see that satises the thom isomorphism of formal dimension if and only if y is empty. poincar e pairs theorem . suppose o and o are two orientation systems on a path-connected space x. suppose both u hk and u hk are thom classes. then k = k and there is a unique isomorphism : o = o covering identity map of x such that = u proof. since u is a thom class of formal dimension k, by denition , we see that h = for all local systems b and all < k furthermore, hk = h = z = in other words, there exists a local system b such that hk = applying the same argument to u, we see that k = k denote by k = k = k by denition , we see that h(x; o o) u = / hk h = z u = o , where o := hom is the linear dual of o, and we use the fact that hk(x, y ; o o o) = hk. we claim that o = o for if not, then o o is not constant. since x is path-connected, we infer that h(x; o o) = , which leads to a contradiction. since o and o are systems of innite cyclic groups, by the path connectiv- ity of x again, there are exactly two bundle isomorphisms and between o and o such that = we then obtain = since each homomorphism uiyields an isomorphism z = h = hk, we infer that ui is a generator of hk = z. consequently, we may suitably choose the desired as or in what follows suppose x is a path-connected space which has a universal cover, o is an orientation system on x, and u hk is a cohomology class. let be the local system associated with := z on x. theorem . with respect to the above assumptions the following statements are equivalent. the homomorphism u: hk+ = h is an isomorphism in all degrees. for all local systems b on x, the homomorphism u: hk+ = h. is an isomorphism in all degrees. john r. klein, lizhen qin, and yang su for all local systems b on x, the homomorphism u: h = hk+ is an isomorphism in all degrees. proof. choose a cocycle u ck representing u and dene a chain map t : ck+ c u . then c and c are free -complexes, c = c, and t is a -homomorphism. furthermore t induces chain maps u: ck+ c and u: c ck+, where uequals the adjoint map of t. the statements to be veried are then a direct consequence of and corollary . in the following, assume is a -connected space pair homotopy equiv- alent to a cw pair, and x is connected. let k be an integer. theorem . the pair satises the thom isomorphism in formal dimension k if and only if the homotopy ber of y x is sk remark . theorem is not really new. when x is -connected, the theorem amounts to and (each proved by dierent methods). when x is nitely dominated but not necessarily -connected, a proof is sketched in . for the convenience of the reader, we shall present in appendix c a detailed proof in the general case. poincar e duality in this section, we prove theorems a, b and e, and corollaries c and d. for all but the proof of theorem a, the key tool will be the following result which translates between poincar e duality and the thom isomorphism. proposition . let be a poincar e triad with dualizing system o and fundamental class hn+k. suppose o is an orientation system on m and u hk. let o = o o and = u hn. then the following statements are equivalent. poincar e pairs the following are isomorphisms for all and all local systems b: u: h = hk+. the following are isomorphisms for all and all local systems b: : h = hn. the following are isomorphisms for all and all local systems b: u : hk+ = h. the following are isomorphisms for all and all local systems b: : h = hn. proof. by theorem , we already have the equivalence . it suces to prove and . the following diagram commutes: h ) u/ hk+ \u000f hn . here we have used the fact that = a = a. since is a poincar e triad, we know that is an isomorphism, which implies . it remains to prove . consider the following commutative diagram. h t * / hn+k u \u000f hn , in which t := u. by the skew-commutativity of cup products , we infer u = = = a = a, where au hk+ and ua hk+. we have identied b o with o b. consequently, t is an isomorphism if and only if : h hn john r. klein, lizhen qin, and yang su is an isomorphism. since is an isomorphism, we infer that is an isomor- phism if and only if u: hn+k hn is an isomorphism. for a local system a, there is a canonical isomorphism a = o (o a). thus, when b runs over all local systems, so does o b. hence, . . digression on thickenings. the proof of theorem b will make use of thickenings in the sense of wall . let hj denote the upper half space in rj. this is a manifold with boundary rj by an m-thickening of a nite cw pair , we mean a manifold triad and a homotopy equivalence of pairs , where, m is a compact m-manifold with boundary m; m = n n a codimension one splitting, i.e., n and n are codi- mension zero submanifolds of m with common boundary n = = n let hm := [, +) rm be the upper half space in rm. assume that m is suciently large with respect to the dimension of x. then there exists an m-thickening of in which m hm and m rm = n, where the latter intersection is transversal (cf. in the absolute case y = ; the relative case similarly follows by rst constructing a thickening y inside rm and then thickening inside rm the cells in x that are attached to y ). proof of theorem b. we know that is a nitely dominated pair. taking the product with s if necessary, we may assume is a nite cw pair . as above, there exists an -thickening of with m hn+k, provided that k is suciently large. in particular, we have a homotopy equivalence of pairs . the orientation system o on x corresponds to an orientation system o on m, and hn corresponds to a homology class hn. clearly, is a poincar e triad. we can therefore construct o, o, and u as those in proposition . so the conclusion follows from that proposition. remark . in contrast with theorem , the proof we gave of theorem b is not purely algebraic: we relied on the existence of thickenings of nite cw pairs, a technique arising in manifold theory. note that poincar e duality for a poincar e pairs manifold triad plays a key role in the proof though we just refer to it in one sentence. walls conjecture and corollary d will easily follow from theorem b and the following result. lemma . suppose o is an orientation system on x and hn is a homology class. suppose and hold. then the following homo- morphisms are isomorphisms for all m and all local systems b on x: : hm = hnm. proof. consider the homology and cohomology long exact sequences of with coecients b and o b. then the conclusion follows from the five lemma. proof of corollary d. this follows directly from theorem b and lemma . lemma . suppose is a path-connected poincar e pair (in which b may be empty) having formal dimension d and dualizing system o. suppose o is another orientation system on a such that hd = then there is an isomorphism o = o. proof. the proof is similar to that of theorem . by the duality, we have h = hd = hd = if o o, then h = , which leads to a contradiction. lemma . let be a space pair such that y is a poincar e space of formal dimension n suppose o is an orientation system on x and hn is a homology class. suppose and hold. then is a poincar e pair with dualizing system o and fundamental class . proof. by lemma , we obtain the following isomorphism : h = hn therefore, for each path-component z of y , we have hn = by lemma , we infer that o|y is the dualizing system of y . then is a fundamental class of y . proof of corollary c. this follows from theorem b and lemma . we now turn to the proof of theorem e. suppose is a cw pair satisfying the thom isomorphism in formal dimension k. similar to the proof of theorem , convert to a pair of brations x, john r. klein, lizhen qin, and yang su in which . the ber over x x is identied with the pair , where cfx is contractible. suppose u hk is a thom class. lemma . for each x x, the restriction of u u|cfx hk = z is a generator of hk. proof. we outline two proofs. in the rst instance, by the proof of lemma c., one can construct a thom class u such that u|cfx is a generator. then the conclusion follows from the uniqueness of the thom class . an alternative proof can be derived from the proof lemma c. an argu- ment using the serre spectral sequence implies the conclusion. for details, see . proof of theorem e. . suppose hn+k is a fundamental class of . if f sk, by theorem , we know satises thom isomorphism with thom class u hk. dene o = o o and = u hn. by proposition , we obtain and . conversely, if or holds, then by proposition again, we see that satises thom iso- morphism of formal dimension k. by theorem again, f sk . by theorem , we know that f and f are homotopy -spheres if and only if and satisfy the thom isomorphism in formal dimension k. by proposition and lemma , this is in turn if and only if is a poincar e pair of formal dimension n. . we know that f and f are homotopy -spheres, and cf and cf are contractible. it suces to show that the inclusion induces an isomorphism hk = hk. let u hk be a thom class of . by lemma , it suces to show that u|y is a thom class for . (note that, in lemma , o|cfx is actually constant.) as in , let be a fundamental class for . then u is a fundamental class of . then = u|yy = |y u|y is a fundamental class of the -dimensional poincar e space y (here, |y is dened via excision). since |y is a fundamental class of the poincar e pairs -dimensional poincar e pair , by proposition again, u|y is a thom class of . before giving the proof of theorem a, we consider a related problem. if and hn satisfy , and with integer coef- cients, then we say satises poincar e duality with integer coecients, where the formal dimension is n. when y = , browder exhibited a space x satisfying poincar e duality with integer coecients but which is not a poincar e space. browder constructed x by taking the wedge of innitely many acyclic spaces whose fundamental groups are nontrivial (where acyclic in this case means the re- duced homology with integer coecients vanishes). the resulting space clearly satises poincar e duality with integer coecients in formal dimension how- ever, its fundamental group is not nitely generated. thus, by , x cannot be a poincar e space. note that browders counterexample is not homotopy nite, nor even nitely dominated. we present an alternative class of examples which are actually nite complexes. proposition . for each n , there are nite cw complexes x which satisfy poincar e duality with integer coecients having formal dimension n but are not poincar e spaces of any formal dimension. proof. let m be the poincar e homology -sphere . then g = is nontrivial, nite and perfect (recall that the latter means g equals its commutator subgroup). moreover, m is a compact smooth - manifold, so it admits the structure of a nite three dimensional cw complex that arises from a triangulation. let a be the eect of removing any -cell from m. then a is acyclic and satises poincar e duality with integer coecients, where the formal dimension is we shall prove that a is not a poincar e space of any formal dimension. since = g is perfect, a homomorphism from to z is trivial. consequently, an orientation system on a must be trivial. if a is a poincar e space of formal dimension n, then hn is nontrivial, and we infer n = by walls classication , this is impossible. for the readers convenience, we give some of the details here. let a a be the universal cover. since is nite, it follows from theorem h that a is a poincar e space of formal dimension hence, a is contractible and a is an eilenberg- mac lane space of type k. however, since g is nite and nontrivial, a cannot have the homotopy type of a nite complex , which is a contradiction. finally, for any nite complex p satisfying poincar e duality with integer coecients having formal dimension n, dene x = ap. then x is nite and satises poincar e duality with integer coecients and has formal dimension n. john r. klein, lizhen qin, and yang su however, x is not a poincar e space. this is easily deduced from theorem g, but it also follows from the simpler result . proof of theorem a. the space a in this theorem is the one in the proof of proposition . furthermore, y = a sn, and x = cy is the cone of y . then h = h and x is contractible. hence, satises and with integer coecients. since x is -connected, by the niteness of and lemma , we infer satises and . however, proposition shows that y is not a poincar e space. remark . suppose is a nite cw pair satisfying the rst two types of isomorphisms in denition . by taking a thickening of , we obtain a manifold triad such that . by assertion of theorem e, the homotopy ber f of y x has the homotopy type of sk however, theorem a suggests that poincar e duality might fail for y this motivates the consideration of f in assertion of theorem e. doubling the goal of this section is to prove theorem f. the implication of theorem f can be easily deduced from the following gluing lemma which was proved by wall in and reproved in . lemma . suppose and are poincar e cw triads of formal dimension n. assume y y = y y = y then is a poincar e triad of formal dimension n, where the fundamental classes of and are the restrictions of those of (x+y x, yy y). proof of theorem f. it remains to prove . if necessary, we take the product with s and use lemma to obtain a homotopy equivalence of triads f : , where is a nite cw triad. dene l = l l we see that is homotopy equivalent to . by lemma , to prove the conclusion, we may assume is a nite cw triad. we may also assume x is connected. we rst prove the special case y = . call the latter the absolute case. choose a manifold thickening of in some , where hm = [, +) rm, m+ = n n n , poincar e pairs n = m+rm, n is the closure of m+ , and n = nn now glue two copies of m+ together along n to produce a manifold m with boundary m = n n n clearly, m is a manifold thickening of x y x. if we choose m suciently large, the pairs and and will be -connected. since x y x and y are poincar e spaces, by in theorem e, we know that the homotopy bers of m m and n n have the homotopy type of sk for some k let f denote the homotopy ber of the map m m taken at a point of n and let f+ be the homotopy ber of n m+ taken at the same point. by theorem e again, it suces to show that f+ is homotopy equivalent to sk. clearly, since is the double of , one has an evident re- traction r: to the inclusion. the map r induces in turn a retraction of homotopy bers f f+. but f has the homotopy type of sk. this will imply that the space f+ is either contractible or it has the homotopy type of sk. if f+ were contractible, then the cohomology groups h would be trivial in all degrees and all local systems b on m+. by poincar e duality, it follows that the homology groups h are also trivial. but , so the groups h are trivial for all local systems b on x, which implies the isomorphism h = h . since y has formal dimension n, the mayer-vietoris exact sequence shows that hn = for any local system a on x y x. this contradicts the assumption that x y x has formal dimension n. consequently, f+ is not contractible, so it must be homotopy equivalent to sk, as was to be shown. we now turn to the case when y = . the idea will be to reduce to the absolute case. by assumption, y y y and y are poincar e spaces. by the absolute case, we infer that is a poincar e pair. since is also a poincar e pair, by lemma , y := y y y is a poincar e space (of formal dimension n ). we also know by assumption that is a poincar e pair. consider the poincar e pair , where z is y with y collapsed to y by means of the projection, and z = y y y then by lemma again, we can glue z to x y x along z to obtain the poincar e space z z . clearly, the latter is homotopy equivalent to x y x. consequently, x y x is also a poincar e space. by the absolute case again, we infer that is a poincar e pair. this completes the argument. it is natural to ask what happens in theorem f if one removes the assump- tion that is a poincar e pair of formal dimension n in this case, one can show that the conclusion of the theorem is no longer true: john r. klein, lizhen qin, and yang su example . let be the pair constructed in theorem a with n > then x y x is the unreduced suspension of y . since y is homologically sn, we infer that x y x sn, which is a poincar e space. however, is not a poincar e pair. in another direction, following the proof of theorem f, one obtains the following result, which uses assertion in theorem e. proposition . suppose is a nitely dominated pair with y = . suppose x y x is a poincar e space of formal dimension n. suppose further hn = for all orientation systems o on x y x. then and hold. note that example satises the assumptions of proposition of . finite coverings the purpose of this section is to prove theorem h. suppose p: x x is covering space, then a local system a on x pulls back to a local system pa on x. the operation a pa denes a functor pfrom the category of local systems on x to the category of local systems on x. the functor phas a right adjoint pdened as follows: if b is a local system on x, then pb is dened by x := y yp by , x x . the fact that this denes a functor uses the path lifting property. if we use the direct sum instead of the direct product in the above display, we obtain another local system that will be denoted p!b. the assignment b p!b denes a functor p! which is left adjoint to p. there is also an evident natural transformation p! p which is an isomorphism whenever p is a nite cover. the functors p! and p correspond respectively to induction and coinduction if interpret local systems as modules over the fundamental groupoid. there are natural isomorphisms of chain and cochain complexes c( x; b) = c and c( x; b) = c . similarly, if is a pair, and y y is the restriction of p to y , then we obtain a covering space of pairs ( x, y ) inducing isomorphisms of relative chain complexes c( x, y ; b) = c and c( x, y ; b) = c . poincar e pairs proof of theorem h. . let b be an arbitrary local system on x. since p is a nite cover, we have p!b = pb. suppose cn is a cycle representing a fundamental class of . let cn( x, y ; po) be the transfer of . then is also a cycle. using the isomorphisms and it is straightforward to check that the square c( x; b) \u000f = / c \u000f cn( x, y ; po b) = / cn commutes. since is a poincar e pair, the right vertical map of the square is a quasi-isomorphism. hence the left map is also a quasi-isomorphism. similar reasoning shows that the map : c( x, y ; b) cn( x; po b) is a quasi-isomorphism as well as the map : c( y ; g) cn( y ; po g) for all local systems g on y . consequently ( x, y ) is a poincar e pair with a fundamental class [ ]. . we know that is a nitely dominated pair. by lemmas and , we may assume that and ( x, y ) are nite cw pairs. by choosing a manifold thickening of , we obtain a compact manifold triad such that . furthermore, we can assume that and are -connected, where n = n n corresponding to the covering space pair ( x, y ) , we obtain a covering space -ad ( m; n, n; n) , in which ( x, y ) ( m; n). by part of theorem e, or ( m; n) is a poincar e pair if and only if certain homotopy bers are spheres. since the inclusions n m (resp. n n) and n m share the same homotopy ber, the conclusion holds. fibrations in this section, we shall prove theorem g. the strategy of the proof is to reduce to the absolute version using the doubling theorem f. proof of theorem g. if necessary, we can take product with s and use lem- mas and , to assume , and are homo- topy nite. john r. klein, lizhen qin, and yang su by assumption, b is connected. in particular, the bers over all points of b have the same homotopy type. we may also assume that e is connected since we can argue component-wise. note that even when e is connected, the ber f can be disconnected. however, by the niteness assumption it only has nitely many components. by choosing a suitable nite covering space b b, we obtain a commutative diagram f / e / b \u000f b where the top row is also a bration sequence whose ber f is a component of f. using theorem h, we are reduced to considering f e b. this reduces us to considering the case when f is connected and b = b. in the absolute case b = and f = , the conclusion was proved in and . (note that and also assumed that f is connected. furthermore, part is not stated in either or , but this statement is not hard to deduce from their proofs.) now suppose b = and f = . we apply theorem f to the double e e e b b b . we rst show that is a bration. indeed, let r: b b b b be the evident retract. then e e e b b b is identied with the pullback of the bration e b via r. consequently, is a bration. suppose is a poincar e pair of formal dimension n and f is a poincar e space of formal dimension k. since f e b is a bration, by the absolute version of the result, e is a poincar e space of formal dimension n+k (note that b and hence e may be disconnected. if that is the case, we argue component-wise. note also that all bers of e b share the same homotopy type since they are induced from e b.) by theorem f, we see that bb b is a poincar e space of formal dimension n. by the absolute version again, e e e is a poincar e space of formal dimension n + k. by theorem f again, is a poincar e pair of formal dimension n + k. conversely, suppose is a poincar e pair. applying the absolute version to e, we infer that b and f are poincar e spaces of formal dimension n and k respectively for some n and k. by theorem f, eee is a poincar e space of formal dimension n + k. by the absolute version again, b b b is a poincar e space of formal dimension n. by theorem f again, is a poincar e pair of formal dimension n. this establishes the result when f = . poincar e pairs it remains to prove the result when f = . consider the map e e e b. since b is a bration pair and e e is a closed cobration, by , the map is a bration whose ber over the basepoint is f f f. the conclusion now follows by applying the result in the previous case using theorem f applied to the triad . remark . besides theorem g, the absolute bration theorem has some other extensions. for example, a similar argument shows that under suit- able niteness assumptions is a poincar e triad if and only if is a poincar e triad and f is a poincar e space. remark . for a bration f e b, if we do not assume that f is nitely dominated, then the conclusion of theorem g will no longer be true. for example, consider the path bration b pb b. then pb is contractible and hence a poincar e space of formal dimension however, b is usually not a poincar e space even if b is a nite poincar e complex. the example of the double cover srp shows that the niteness assumption on b is also needed for the result to hold. historical remarks here we describe various inequivalent denitions of poincar e pairs appearing in the literature. the dierence among them is captured in the statement of theorem e. historically, there were at least four dierent denitions, all of which are equivalent in the case of nite cw pairs in which x and y are -connected. the earliest and simplest denition was provided by browder , , which only assumes , and with integer coecients, where the dualizing system is necessarily trivial. browders denition suces for doing surgery theory in the simply connected case . however, as seen in proposition , it is inadequate more generally. wall gave two denitions. the denition in is equivalent to the one used in this paper. in , wall dened the notion of a simple poincar e pair. this denition takes simple homotopy type into account, and is well-adapted for use in surgery theory in the non-simply connected case. yet another denition is due to spivak . suppose is a nite cw pair and p : x x is a universal cover. denote by y = p spivak considered homology with locally nite chains, denoted by hlf (which coincides with borel-moore homology). his denition respectively exchanges , and with isomorphisms [ x]: hm( x; z) = hlf nm( x, y ; z), john r. klein, lizhen qin, and yang su [ x]: hm( x, y ; z) = hlf nm( x; z), and [ x]: hm( y ; z) = hlf nm( y ; z). there are obvious dierences between spivaks denition and walls . in particular, if x is -connected, then spivaks denition amounts to browders denition. however, if y isnt -connected, then may fail to satisfy walls denition. a case in point is the in theorem a. note that, with merely integer coecients, and imply . consequently, satises spivaks denition but does not satisfy walls. lastly, we relate part of theorem e to the above, which is similar to spivaks . in the latter, spivak ignored the homotopy ber f : he claimed that is a poincar e pair if and only if f itself is a homotopy sphere. in fact, if we suppose f is a homotopy sphere, it is not hard to show that is indeed a poincar e pair with spivaks denition. however, if y is not -connected, remark shows that may not satisfy walls denition.",
        "Subsections": [],
        "Groundtruth": "The section discusses different definitions of Poincaré pairs appearing in the literature, including those by Browder, Wall, and Spivak. The differences in these definitions are highlighted, with an emphasis on their equivalence in the case of finite CW pairs where x and y are -connected. Browder's definition is simple and well-suited for simply connected surgery theory. Wall provided two definitions, with one matching the definition used in the paper. Spivak introduced a different definition involving homology with locally finite chains, which differs from Wall's definition. The section further explores how these definitions relate to each other, especially in the context of homotopy types and homotopy spheres. Moreover, the proof of theorem e demonstrates an application of these different definitions in the context of Poincaré pairs."
    },
    {
        "Section_Num": "Appendix",
        "Section": "Appendix A. Skew-Commutativity of Cup Products",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "Appendix",
        "Section": "Appendix B. The Künneth Theorems",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "Appendix",
        "Section": "Appendix C. Proof of Theorem 3.6",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "References",
        "Section": "References",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "Abstract",
        "Section": "Abstract",
        "Text": "cherenkov light imaging in astroparticle physics u. f. katz erlangen centre for astroparticle physics, friedrich-alexander university erlangen-n urnberg, erwin-rommel-str. , erlangen, germany abstract cherenkov light induced by fast charged particles in transparent dielectric media such as air or water is exploited by a variety of experimental techniques to detect and measure extraterrestrial particles impinging on earth. a selection of detection principles is discussed and corresponding experiments are presented together with breakthrough-results they achieved. some future develop- ments are highlighted. keywords: astroparticle physics, cherenkov detectors, neutrino telescopes, gamma-ray telescopes, cosmic-ray detectors",
        "Subsections": [],
        "Groundtruth": "The text highlights the use of Cherenkov light imaging in astroparticle physics for detecting and measuring extraterrestrial particles. It discusses the generation of Cherenkov light by fast charged particles in transparent media like air and water. Various experimental techniques utilizing Cherenkov light for detecting extraterrestrial particles are presented, along with notable breakthrough results. The text also mentions future developments in the field of astroparticle physics, emphasizing keywords such as Cherenkov detectors, neutrino telescopes, gamma-ray telescopes, and cosmic-ray detectors."
    },
    {
        "Section_Num": "1",
        "Section": "1 Introduction",
        "Text": "in , we commemorate the th anniversary of the award of the nobel prize to pavel alexeyewich cherenkov, ilya mikhailovich frank and igor yevgenyevich tamm for the dis- covery and the interpretation of the cherenkov eect . the impact of this discovery on astroparticle physics is enor- mous and persistent. cherenkov detection techniques were ins- tumental for the dicovery of neutrino oscillations; the detection of high-energy cosmic neutrinos; the establishment of ground- based gamma-ray astronomy; and important for the progress in cosmic-ray physics. the characteristics of cherenkov radiation are governed by its emission angle with respect to the particles direction of ight, cos c = / (n and = v/c being the refractive in- dex and the particle velocity, respectively) and by its intensity, given by the frank-tamm formula dn dx = n ! min max ! = /m in water/ice /m in air for min = nm max = nm . here, is the emitted wavelength and the range indicated roughly corresponds to the sensitivity range of typical light sensors. the geometry of cherenkov emission allows for re- constructing the particle trajectory, provided suciently many cherenkov photons are measured with good spatial and time resolution, and they can be separated from background light. the recorded cherenkov intensity furthermore can serve as a proxy for the particle energy. usually, the detectors need to be shielded from ambient light and employ photo-sensors that are sensitive to single photons with nanosecond time resolution. email address: katz@physik.uni-erlangen.de photomultiplier tubes and, more recently, silicon pho- tomultipliers are the standard sensor types. they are provided by specialised companies who cooperate with the experiments in developing and optimising sensors according to the respective specic needs . in the following, the detection principles of dierent types of cherenkov experiments in astroparticle physics are presented together with selected technical details and outstanding results.",
        "Subsections": [],
        "Groundtruth": "The text discusses the significance of the Nobel Prize awarded to Pavel Cherenkov, Ilya Frank, and Igor Tamm for their discovery and interpretation of the Cherenkov effect. This discovery has had a significant impact on astroparticle physics, enabling advancements in neutrino oscillations, cosmic neutrino detection, gamma-ray astronomy, and cosmic-ray physics. The text details the characteristics of Cherenkov radiation, including emission angle, intensity, and wavelength range. It explains how Cherenkov emission can be used to reconstruct particle trajectories and estimate energy levels. The text also highlights the importance of specialized detectors, such as photomultiplier tubes and silicon photomultipliers, for measuring Cherenkov photons with high spatial and time resolution. These detectors are crucial for experiments in astroparticle physics and are continuously developed and optimized in cooperation with specialized companies. The text concludes by mentioning the detection principles of various Cherenkov experiments in astroparticle physics and presents some notable technical details and results."
    },
    {
        "Section_Num": "2",
        "Section": "2 Ground-based gamma-ray detectors",
        "Text": "while the atmosphere is transparent to electromagnetic radi- ation in the radio and optical regimes, it is not for x-rays and gamma-rays. gamma-rays below gev are only accessible to satellite experiments. at signicantly higher energies, satel- lite instruments rapidly lose sensitivity to the steeply decreasing gamma-ray ux due to their limited collection area, and ground- based observations take over . imaging air cherenkov telescopes require clear, preferentially moon-less nights and sites with negligible light pollution and an elevation of typically km. they are pointing instruments with a eld of view of a few degrees in diameter. alternatively, tim- ing arrays at higher altitude can directly measure the gamma- induced particle cascade. they cover a signicant fraction of the sky, albeit with a higher energy threshold than iacts and inferior sensitivity at energies below about tev.",
        "Subsections": [
            {
                "Section_Num": "2_1",
                "Section": "2.1 Detection principle of Cherenkov telescopes",
                "Text": "upon entering the atmosphere, gamma-rays with gev en- ergies and above initiate electromagnetic cascades, extending longitudinally over several kilometres and having their maxi- mum typically at a height of km above sea level. the in- tegrated track length of all e in the cascade and therefore the overall cherenkov light yield is to a good approximation pro- portional to the initial gamma-ray energy, e. the cherenkov angle is about c = and increases with the air density, i.e. preprint submitted to nuclear instruments & methods in physics research, section a january , arxiv:v jan along the cascade. even though the basic processes in the cas- cade pair creation and bremsstrahlung are very close to be- ing collinear with the incoming gamma-ray, multiple scattering of the e widens the cherenkov light pool, which is concen- trated within a radius of about m at ground level. since all relevant particles in the cascade propagate with a speed very close to that of light, the cherenkov radiation ar- rives at ground in a ash of only a few nanoseconds duration and can thus be separated from the night-sky background. it is observed with one or several iacts that have a camera made of photomultiplier or sipm pixels in their focal plane. an im- portant parameter, governing e.g. the ability for large-area sky scans, is the eld of view of the camera . each camera pixel corresponds to a certain solid angle of the light ar- rival direction. a telescope pointed to a gamma-ray source thus sees the start of the cascade (small c, little multiple scatter- ing) close to its centre, from where it propagates outward. an example of a cascade observed by all ve h.e.s.s. telescopes is shown in fig. the gamma-ray direction and energy are reconstructed from the recorded light pattern and intensity. typical resolutions of and %, respectively, are achieved for stereoscopic observations by several cameras. atmospheric cascades induced by cosmic-ray protons or heav- ier nuclei are three orders of magnitude more frequent than gamma-rays, but can be eciently suppressed using the topol- ogy of the camera pictures . figure : camera pictures of a gamma-ray shower simultaneously ob- served by all ve h.e.s.s. telescopes. each hexagonal pixel corre- sponds to one photomultiplier. the colour code indicates the measured light intensity. picture provided by the h.e.s.s. collaboration. the observable e range is limited by the light intensity and the background separation at low energies (some gev for the largest telescopes in operation) and by the overall collection area at high energies (about tev for a large array of smaller telescopes). the sensitivities of current and future gamma-ray telescopes are compared in fig. as functions of e.",
                "Subsections": [],
                "Groundtruth": "Cherenkov telescopes utilize the detection principle based on the Cherenkov light yield produced by gamma-ray showers in the atmosphere. This light yield is proportional to the initial gamma-ray energy. The Cherenkov angle widens with air density as the cascade progresses. The Cherenkov radiation reaches the ground within nanoseconds, allowing for separation from background noise. Telescopes with cameras made of photomultiplier or SiPM pixels capture the light patterns to reconstruct the gamma-ray direction and energy. Atmospheric cascades from cosmic-ray protons can be efficiently distinguished from gamma-rays using camera topology. The observable energy range is limited by light intensity and background separation at low energies and by collection area at high energies. Typical resolutions of 0.1% and 1% are achieved for stereoscopic observations. Current and future gamma-ray telescope sensitivities are compared based on energy range."
            },
            {
                "Section_Num": "2_2",
                "Section": "2.2 Current Cherenkov telescopes",
                "Text": "after the feasibility of ground-based gamma-ray astronomy had been demonstrated in the late s and s by the whipple and hegra instruments, a second gener- ation of iacts became operational in the s. the main figure : dierential ux sensitivity of the current (h.e.s.s., magic, veritas) and the future ground-based iacts. also shown are the corresponding sensitivities for the timing array hawc. see sec- tions . for more details on these instruments. the green, dash- dotted lines indicate the sensitivity of the satellite instrument fermi- lat for two dierent directions of observation. see sections . for more details on these instruments. note that the vertical axis shows e ux. picture provided by the cta collaboration. properties of these telescopes are summarised in table ; a pho- tograph of the h.e.s.s. instrument is shown in fig. they have established gamma-ray astronomy as a major eld of ob- servational astrophysics and provided a wealth of scientic in- formation on high-energy processes in the universe. more than gamma-ray sources in the tev regime have been detected and their spectra, light curves and for galactic sources mor- phologies investigated. gamma-ray emission and thus particle acceleration beyond tev energies has been proven for a number of object classes, amongst them shell-type supernova remnants, pulsar wind nebulae, compact binaries and active galactic nu- clei. see and references therein for details. in addition to the iacts listed in table , a rst small tele- scope with a camera equipped with sipms, fact , has been constructed and is operated since in la palma for long-term monitoring purposes. fact has demonstrated that sipms can take very high rates, enabling observations even in full-moon nights.",
                "Subsections": [],
                "Groundtruth": "Ground-based gamma-ray astronomy was proven feasible in the late 20th century. Second-generation Imaging Atmospheric Cherenkov Telescopes (IACTs), like H.E.S.S., MAGIC, and VERITAS, became operational in the 2000s. These instruments, along with future IACTs and the HAWC timing array, have significantly advanced gamma-ray astronomy. Over a hundred gamma-ray sources in the TeV energy range have been detected, revealing insights into high-energy processes in the universe. Notable discoveries include gamma-ray emission from objects such as supernova remnants, pulsar wind nebulae, compact binaries, and active galactic nuclei. The construction and operation of the first small telescope equipped with SiPMs, FACT, have demonstrated high observation capabilities, even during full-moon nights."
            },
            {
                "Section_Num": "2_3",
                "Section": "2.3 Cherenkov Telescope Array",
                "Text": "the h.e.s.s., magic and veritas collaborations have joined forces to construct the next-generation iact, the cherenkov telescope array . cta will be installed in two sites, one in the northern hemisphere on la palma and the other in the southern hemisphere close to the eso paranal observatory in chile. cta will consist of telescopes of three sizes, lsts (large size telescopes), msts and ssts , with a eld of view of and , re- spectively. at the chile site, they will be arranged in concentric groups of lsts in the middle, followed by msts and ssts. on la palma, there will be lsts and msts, but figure : photograph of the h.e.s.s. telescope system in namibia. picture provided by the h.e.s.s. collaboration. table : main parameters of the currently operational iacts and ref- erences to their web pages. the second magic telescope came in operation , the large h.e.s.s. telescope in h.e.s.s. magic veritas site namibia la palma, arizona, us spain altitude a.s.l. m m m operation telescopes dish diameter m, m m m field of view photo-sensors pmts pmts pmts web page no ssts. this takes into account that on the northern site pre- dominantly extragalactic observations will be made, where the gamma-ray ux beyond some tev targeted by the ssts is strongly reduced through absorption by the extragalactic background light. the rst lst was recently inaugurated in la palma. cta is expected to start operation in with partial arrays and to be completed in the cta sensitivity (see fig. ) promises major progress in gamma-ray astronomy and high-energy astrophysics once cta will take data. the major scientic targets of cta are cosmic particle ac- celeration, probing extreme environments such as close to neu- tron stars and black holes, and fundamental physics such as in- vestigations into the nature of dark matter. cta will be op- erated as an observatory, with some key science projects re- served for the cta collaboration. these include surveys of the galactic centre, the galactic plane, the magellanic cloud, and extragalactic objects, as well as the investigation of cosmic- ray pevatrons, star-forming galaxies, active galactic nuclei and clusters of galaxies. also, the dark matter programme and the exploitation of cta data beyond gamma-rays are key science projects.",
                "Subsections": [],
                "Groundtruth": "The Cherenkov Telescope Array (CTA) is a next-generation observatory involving collaborations from h.e.s.s., MAGIC, and VERITAS. CTA will be located in two sites, one in La Palma in the northern hemisphere and the other near the ESO Paranal Observatory in Chile. It will consist of telescopes of three sizes (LSTs, MSTs, and SSTs) with specific field of view values. The telescopes in Chile will be arranged in concentric groups, while those in La Palma will only have LSTs and MSTs. CTA is expected to start operating in 2021 with partial arrays and be completed later. It promises significant progress in gamma-ray astronomy and high-energy astrophysics with major scientific targets including cosmic particle acceleration, extreme environments near neutron stars and black holes, investigations into dark matter, and more."
            },
            {
                "Section_Num": "2_4",
                "Section": "2.4 Timing arrays",
                "Text": "at altitudes exceeding about km above sea level, the parti- cle cascades induced by gamma-rays and cosmic rays at ener- gies beyond gev reach the ground and can be observed with arrays of suitable detectors, e.g. water tanks in which through-going charged particles generate cherenkov light de- tected by pmts. from measuring the arrival time of the shower front as a function of the horizontal position, the direction of the incoming particle can be determined. the intensity of the shower and the size of its footprint on ground yield an energy estimate. similarly to iacts, leptonic and hadronic showers are separated using the dierent event topologies and muon content in the detector array. figure : photograph of the hawc detector array in mexico. picture provided by the hawc collaboration. the currently most sensitive detector of this type is hawc near puebla in mexico, at an altitude of m. hawc consists of water tanks covering km, each lled with tons of ultra-pure water observed by pmts. a photograph of hawc is shown in fig. hawc reaches an angular reso- lution of about for gamma-ray energies e tev. even though, for e tev, hawc is less sensitive than cta for any given direction, it has the advantage of a large eld of view and a very high duty cycle. it is particularly well suited to observe very high-energy gamma-ray emission from extended objects. as an example, a recent measurement of the tev gamma-ray ux from the vicinity of two pulsars has strongly constrained the hypothesis that positrons from these/such pul- sars is responsible for the unexpectedly high ux of cosmic-ray positrons at earth .",
                "Subsections": [],
                "Groundtruth": "At altitudes above km, particle cascades induced by gamma-rays and cosmic rays reaching the ground can be detected using arrays of detectors such as water tanks with Cherenkov light detection. By measuring the arrival time of the shower front and its size on the ground, the direction and energy of incoming particles can be estimated. Different types of showers are distinguished based on event topologies. The High Altitude Water Cherenkov (HAWC) detector array in Mexico is currently the most sensitive, covering km with high-resolving power for gamma-rays. HAWC has advantages like a large field of view and high duty cycle, making it suitable for observing very high-energy gamma-ray emissions. Recent measurements from HAWC have helped constrain theories about cosmic-ray positron flux."
            }
        ],
        "Groundtruth": "Ground-based gamma-ray detectors offer an alternative to satellite experiments for detecting gamma-rays above a certain energy level. Imaging air Cherenkov telescopes are used for ground-based observations, requiring clear, dark nights and sites with minimal light pollution. These telescopes have a limited field of view but are effective at detecting gamma-rays at high energies. Timing arrays, located at higher altitudes, can also measure gamma-ray induced particle cascades, covering a larger portion of the sky but with a higher energy threshold and lower sensitivity compared to imaging air Cherenkov telescopes at lower energies."
    },
    {
        "Section_Num": "3",
        "Section": "3 Neutrino telescopes",
        "Text": "due to their notoriously small interaction cross section, neu- trinos are very good, long-range astrophysical messengers; on the other hand, they are dicult to detect. the basic princi- ple of neutrino telescopes is to observe cherenkov light from charged secondary particles emerging from neutrino reactions and passing a detector volume lled with a transparent dielec- tric medium and observed by an arrangement of pmts (due to their comparatively high noise rates, sipms are not suited for neutrino telescopes). for the low-energy regime (typically mevmulti-gev), detectors are installed in deep-underground caverns and the pmts cover a large percentage of the detector volume outer surface. for high energies , naturally abundant volumes of water or ice are instrumented with three-dimensional arrays of pmts .",
        "Subsections": [
            {
                "Section_Num": "3_1",
                "Section": "3.1 Low-energy neutrino detectors",
                "Text": "the neutrino uxes observed with theses detectors are those from the sun , from supernovae , at- mospheric neutrinos generated in cosmic-ray induced particle cascades , and also beam neutrinos from accel- erators for long-baseline experiments . the energy ranges indicate the typical observation windows of cherenkov detec- tors. note that the dominant neutrino interactions observed in these cases are dierent: solar e are detected via elas- tic eeeescattering, where the nal-state epreserves the e direction; supernova neutrinos are mostly visible via ep e+n, with poor directional information; atmospheric neutrinos produce high-energy e or , the direction and particle type of which are measured in the detector. the physics questions addressed through these measurements are neutrino oscillations, the processes in the sun and in supernovae, and searches for relic neutrinos from unresolved supernovae and for possible sterile neutrinos. two detectors have provided outstanding results: super- kamiokande in japan and the sudbury neutrino ob- servatory in canada. sk is installed in a cavern of a mine, with an overburden of m of rock. the detector volume is a stainless-steel vessel, about m in diameter and m in height, lled with ktons of water. the outermost ktons are used as veto layer against incoming charged particles. the inner volume is observed by more than -inch pmts, the veto layer by about -inch pmts. sk is operational since , with a period of reduced sensitivity in as the consequence of an ac- cident destroying more than % of the large pmts. sk has played a crucial role in the discovery and precision investiga- tion of neutrino oscillations. while one piece of evidence came from conrming the solar neutrino decit (i.e. the fact that less figure : zenith angle distributions for atmospheric neutrino events fully contained in the sk detector, for electron- and muon-like events. the upper plots are for visible energy below gev. the high-energy muons are combined with partially contained events . the error bars are the measured data, the red dotted lines show the prediction without oscillations and with the best-t oscillation scenario, respectively. plot taken from . solar neutrinos were measured than expected by the solar stan- dard model), the breakthrough was the observation of oscilla- tions of atmospheric neutrinos in the nal conrmation that the solar neutrino decit is a neutrino avour transition eect had to await the sno results . see for a summary of recent sk results. sno is located below an overburden of m of rock in a deep mine in ontario, canada. it uses a cavity with a di- ameter of m and a height of m. the core of the detec- tor is an acrylic vessel lled with kton of heavy water and observed by more than -inch pmts. the remaining volume of the cavern is lled with normal water , serv- ing as a veto volume against incoming charged particles. the experiment started data taking in and was operated until currently a follow-up experimental phase is in preparation, employing a liquid-scintillator lling doped with tellurium for the search for neutrino-less double beta decay. the deuteron target provides detectable reactions of all neu- trino avours: ed epp , xd xpn and xexe(elastic, also measurable by sk). the secondary electrons are observed via their cherenkov light, the neutrons by gamma radiation emitted when they are captured by deuterons or, in a later phase, by chlorine nuclei added through a nacl doping. the gamma radiation compton- scatters on electrons which then generate cherenkov light. the measured rates of the three reactions given above allow for a precise determination of both, the e and the + uxes from the sun. the overall neutrino ux is found to be consistent with the model expectation, but to consist only to about a third of electron neutrinos . this nding solved the puzzle of the solar neutrino decit and established a consistent stan- dard description of neutrino oscillations. figure : solar e vs. + uxes, with the experimental constraints by sno indicated by the coloured bands. the grey band shows the sk result. point and error contours are the combined result from the cc and nc measurements. the dashed lines represent the prediction by the standard solar model. plot taken from . in , the nobel prize in physics was awarded to takaaki kajita and arthur b. mcdonald for the discovery of neutrino oscillations, which shows that neutrinos have mass. note that the nobel committee selected the pictures reproduced in figs. and for the corresponding announcement.",
                "Subsections": [],
                "Groundtruth": "Neutrino detectors observe neutrino fluxes from various sources such as the sun, supernovae, atmospheric neutrinos, and beam neutrinos from accelerators. Different interactions are observed for each source, helping to study neutrino oscillations, solar processes, supernovae, and searches for sterile neutrinos. Notable detectors include Super-Kamiokande in Japan and the Sudbury Neutrino Observatory in Canada. Super-Kamiokande, located in a mine, played a crucial role in confirming neutrino oscillations. Sudbury Neutrino Observatory, also in a mine, helped resolve the solar neutrino deficit by observing neutrino oscillations. The experimental results support the standard models and theories related to neutrino oscillations. In 2015, the Nobel Prize in Physics was awarded for the discovery of neutrino oscillations, demonstrating that neutrinos have mass."
            },
            {
                "Section_Num": "3_2",
                "Section": "3.2 Deep-ice and deep-water neutrino telescopes",
                "Text": "for neutrino energies beyond some gev, the detectors discussed in the previous subsection lack sensitivity, simply be- cause the target volume is too small to yield sucient event rates and also because events at such energies are too large to be contained in the detector volume. in order to access this high-energy regime, in particular for the purpose of neutrino astronomy, instrumented volumes of at least several mtons, better gtons are required. they are constructed by deploying arrays of vertical struc- tures carrying pmts to the deep ice of the south pole or the deep water of the mediterranean sea or the lake baikal. the water/ice layer above the sensors completely shields the daylight. the pmts are included in pressure- resistant glass spheres which also house the voltage supplies, the front-end electronics and calibration instruments (optical modules). the strings are connected to surface/shore by ca- bles for data transport, operation control and electrical power supply. particle-induced events from neutrinos or atmospheric muons are recognised and reconstructed using the space-time pattern of cherenkov photons recorded by the optical modules. see for more details. figure : schematic of the icecube detector. picture provided by the icecube collaboration. the main science objectives of these neutrino telescopes in- clude neutrino astronomy (i.e. observing the sky in the light of neutrinos); multi-messenger astronomy (i.e. combining the neutrino results with electromagnetic and gravitational wave observations); the indirect search for dark matter; neutrino and other particle physics (neutrino oscillations, neutrino interac- tions, etc.), the search for phenomena beyond the standard model of particle physics (magnetic monopoles, violation of lorentz invariance, sterile neutrinos, etc.). note that the uni- verse is transparent to neutrinos of all energies, whereas the reach of electromagnetic radiation is severely constrained for energies exceeding some tev due to gamma-ray interaction with ubiquitous radiation elds. the currently most sensitive high-energy neutrino telescope is icecube at the south pole, operational in full congu- ration since it consists of strings carrying altogether downward-looking -inch pmts instrumenting one cu- bic kilometre of ice at a depth between m and m. a sub-volume is instrumented more densely than the rest to detect neutrinos with energies down to gev (deep core, ducial volume about mtons). the icetop cherenkov surface array serves for cosmic-ray studies and also provides some veto capability against muons and neutrinos from air showers. an in-ice hardware trigger re- quires hits in adjacent pmts within s. a schematic of ice- cube is shown in fig. the angular resolution for charged- current events above some tev is a few tenths of a degree; cas- cade events resulting from reactions of other neutrino avours or neutral-current interactions are reconstructed with a resolu- tion of about main systematics come from the inhomo- geneity of the optical ice properties and from light scattering, which blurs the space-time pattern of cherenkov photons. icecube has achieved two major breakthrough results: the rst identication and ux measurement of high-energy cosmic neutrinos and the rst association of high-energy neutri- nos to an astrophysical object, the blazar txs . this detection became possible by relating an icecube neutrino alert with electromagnetic observations, and was conrmed by archival icecube data. see for these and further ice- cube results. the icecube detector will be further developed and ex- tended. as a rst step, additional strings with newly devel- oped optical modules and calibration devices will be added to the deep core region. this project has currently been approved by the us national science foundation , and deployment is expected in / the main objectives are a better un- derstanding of the ice properties, entailing a reduction of the systematic uncertainties; improved investigation of neutrinos in the few-gev range; test of new hardware developments. as a next step, icecube-gen with a km deep-ice detector, a high-density core for low-energy neutrinos , a large cosmic-ray and veto surface array, and a radio detection array is planned. if all works well, icecube-gen could be installed see for details. complementing icecube in the eld of view and in the ma- jor systematic uncertainties, the antares neutrino telescope in the mediterranean sea othe french shore near toulon is operational in full conguration since it consists of strings carrying storeys with three -inch pmts each, downward-looking at an angle if to vertical. the strings are connected to a junction box on the sea bed and from there by an electro-optical cable to shore. all pmt hits exceeding a signal height corresponding to photo-electrons are read out and sent to shore, where the data are ltered by an online com- puter cluster. antares instruments a water volume of about km and is thus intrinsically signicantly less sensitive than icecube. angular resolutions for charged-current and for cascade events with neutrino energies exceeding tev are better than and , respectively. main instrumental sys- tematics are due to the inhomogeneity of detector and deep-sea environment in time, and due to background light from biolumi- nescence. in spite of its limited size, antares has provided a number of important results, in particular also in common anal- yses with icecube, which evidence the importance of full sky coverage for each neutrino avour and energy. see for a selection of important antares results. antares has proven the feasibility of deep-sea neutrino detection and has paved the way towards the next-generation neutrino telescope in the mediterranean sea, kmnet . the kmnet detector will consist of two installations, arca othe eastern sicilian shore and orca close to the antares site. arca will encompass two building blocks with strings each, where each string carries optical mod- ules; the overall instrumented volume will be km the prime objective of arca is neutrino astronomy in an energy range beyond a few tev. due to its larger size and improved detec- tor technology , the angular resolutions in kmnet astroparticle research with cosmics in the abyss oscillation research with cosmics in the abyss figure : photograph of a kmnet digital optical module during as- sembly/testing. picture provided by the kmnet collaboration. will be better than in antares (< for and < for cas- cade events at energies of about tev and above). orca will use the same basic detector technology as arca, but be much more densely instrumented ( strings, optical mod- ules per string, instrumented volume km). orca will focus on neutrino oscillation physics with atmospheric neu- trinos in the energy range of a few gev to a few gev, and in particular on measuring the neutrino mass ordering. an option to investigate cp violation by directing a neutrino beam from protvino to orca is under discussion. a number of new technical developments has been achieved for kmnet that improve functionality, cost-eectiveness, risk mitigation and construction time. amongst them are equi- pressure vertical cables (electrical leads and optical bres in an oil-lled hose), a new deployment strategy (strings wound on a spherical structure which is deployed to the sea oor and then unfurls upon release), and a multi-pmt digital optical module . the advantages of the latter are increased photo- cathode area per glass sphere at reduced overall pmt cost; re- duced risk due to less feed-through holes in the glass spheres; better photon-counting; angular information; almost isotropic sensitivity. the construction of both kmnet detectors has begun and is expected to be completed / a further ex- tension with four more arca blocks is envisioned but not yet negotiated. see for expected kmnet sensitivities, in par- ticular to a diuse cosmic neutrino ux as observed by icecube and to the neutrino mass ordering. a third large neutrino telescope, the gigaton volume de- tector , is currently under construction in lake baikal, russia. it will consist of -string clusters with a diameter of m and a height of m; the depth at the installation site is m. each string carries optical modules equipped with -inch, downward-looking pmts. in a rst phase, clusters will be deployed to instrument km of water; the nal goal are clusters covering a volume of km currently three clusters are operational, and two more are to be deployed per year. first results from gvd have been reported at the neu- trino conference .",
                "Subsections": [],
                "Groundtruth": "Deep-ice and deep-water neutrino telescopes are needed for detecting neutrinos at high energies beyond a few GeV. Instruments like IceCube at the South Pole and Antares in the Mediterranean Sea are crucial for neutrino astronomy, multi-messenger astronomy, dark matter searches, and beyond-standard-model physics. IceCube has successfully detected high-energy cosmic neutrinos and associated them with astrophysical objects. Plans for further development include IceCube-Gen with improved capabilities and a larger detector volume. Antares has provided important results despite its smaller size and paved the way for the KM3NeT project, which will consist of the ARCA and ORCA installations in the Mediterranean Sea. A third large neutrino telescope, the Gigaton Volume Detector in Lake Baikal, is also under construction. These projects aim to advance neutrino research and astroparticle physics."
            },
            {
                "Section_Num": "3_3",
                "Section": "3.3 Other neutrino detectors",
                "Text": "a new approach to detecting highest-energy is the use of iacts directed on the sea surface or other surface areas of the earth. the basic principle is that enter the earth under zenith angles close to , undergo a charged-current re- action and produce a that enters the atmosphere and initi- ates an extended air shower upon its decay. a rst study of this option has been performed with magic ; other ex- perimental approaches to detect tau neutrinos through the same mechanism are discussed in .",
                "Subsections": [],
                "Groundtruth": "A new approach to detecting high-energy neutrinos involves using Imaging Air Cherenkov Telescopes (IACTs) pointed towards the sea surface or other surface areas of the Earth. Neutrinos entering the Earth at close to zenith angles create charged-current reactions, producing tau neutrinos that decay in the atmosphere, initiating extended air showers. The use of IACTs such as MAGIC has been studied for this purpose, with other experimental approaches also exploring the detection of tau neutrinos through similar mechanisms."
            }
        ],
        "Groundtruth": "Neutrino telescopes utilize Cherenkov light from charged secondary particles created by neutrino reactions for detection. Low-energy detectors are placed in deep underground caverns with photomultiplier tubes (PMTs) covering the outer surface, while high-energy detectors use water or ice volumes with three-dimensional PMT arrays. PMTs are preferred over SiPMs due to their lower noise rates in neutrino telescopes."
    },
    {
        "Section_Num": "4",
        "Section": "4 Cosmic-ray and hybrid detectors",
        "Text": "in several large detector infrastructures, cherenkov detec- tion is combined with other measurement methods, such as the observation of uorescence light, direct particle detection with scintillator or other instruments, or radio detection. corre- sponding hybrid infrastructures are typically targeting cosmic rays, in several cases in combination with gamma-ray and/or neutrino measurements. figure : photograph of a water cherenkov detector and a building housing six uorescence telescopes of the auger infrastructure. picture provided by the auger collaboration. the largest hybrid detector system to date is the pierre auger observatory in argentina, combining pmt-equipped water tanks on a km area for the direct detection of charged particle from air showers with uorescence telescopes observing the atmosphere above the ground array . auger targets cosmic rays with en- ergies of about ev and searches for gamma-rays and neutrinos in the same energy interval. the water detec- tors yield a measurement of the footprint of an air shower, from which the energy can be inferred, and act as timing ar- rays for the direction measurement. they have a duty cycle of close to %. the uorescence telescopes mea- sure the light intensity along the shower and, in stereoscopic observations, determine shower position and direction. they thus provide an independent determination of energy and direc- tion, which is cross-calibrated against and combined with the array measurement. even though the uorescence telescopes can only be operated in clear, moon-less nights and thus have a limited duty cycle, their contribution to the control of systemat- ics and thus to the resulting experimental precision is essential. figure : cosmic-ray energy spectrum beyond ev measured by auger. the power-law behaviour below and above the ankle is indicated by the dashed lines. beyond ev the spectrum steeply decreases. picture from . auger started operation with a partial installation in and has achieved major breakthroughs, amongst others a precise measurement of the cosmic ray spectrum above ev indi- cating a clear cut-obeyond ev , and the rst detection of a dipole anisotropy of the arrival direction of cos- mic rays at highest energies . see for a summary of these and further recent results. still, important questions remain open, amongst them the chemical composi- tion of cosmic rays, the origin of said anisotropy, and the detec- tion of cosmogenic gamma-rays from interactions of highest- energy protons/nuclei with the cosmic microwave background. auger is currently upgraded to augerprime with new fast electronics and in particular with scintillator detectors on top of the cherenkov tanks to improve the discrimination between hadronic and leptonic shower components on ground. the tunka experiment in the tunka valley near lake baikal and its current extension stage taiga (tunka advanced instru- ment for cosmic ray physics and gamma astronomy) comprise ve dierent detector systems: an array of pmts observing the sky at clear moon-less nights (tunka- ), a radio array , an array of scintilla- tion stations recording charged particles (tunka-grande, see also ), wide-angle cherenkov stations providing im- proved cherenkov light detection and sev- eral iacts under construction . a dierent type of hybrid detector is nevod , a com- paratively small ground-level installation in moscow, russia. it is unique in combining a kton water volume observed by photomultipliers in multi-pmt optical modules with ad- ditional scintillator detector tiles for calibration and streamer tube tracking planes for precisely measuring the trajectories of muons or muon bundles. the combination of muon track- ing and cherenkov measurements oers promising options to cross-check simulations and calibrate e.g. acceptances of op- tical modules. also, the simultaneous measurement of muon directions and energies can be in- teresting for cosmic-ray studies. see for more details. future hybrid approaches are the lhaaso project in china, targeting gamma-rays , and the space missions jem-euso and poemma , both targeting ultra-high energy cos- mic rays and neutrinos.",
        "Subsections": [],
        "Groundtruth": "Cherenkov detection is often combined with other methods such as observing fluorescence light, direct particle detection, or radio detection in large detector infrastructures. Hybrid infrastructures usually focus on cosmic rays, sometimes with gamma-ray and/or neutrino measurements. The largest hybrid system, the Pierre Auger Observatory in Argentina, combines water tanks for direct charged particle detection with fluorescence telescopes. It measures cosmic rays with energies of about ev and also searches for gamma-rays and neutrinos. Auger started operation in and has made significant discoveries, but important questions remain open. The detector is being upgraded to AugerPrime. Other hybrid detectors include the Tunka experiment in Russia and the NeVod installation in Moscow. Future projects include the LHAASO project in China and the space missions JEM-EUSO and PoEMMA targeting ultra-high energy cosmic rays and neutrinos."
    },
    {
        "Section_Num": "5",
        "Section": "5 Conclusion and Outlook",
        "Text": "cherenkov detectors play a crucial role in gamma-ray, neu- trino and cosmic-ray astroparticle physics. many of the re- cent breakthrough-results would not have been possible without them, and they are essential for the future experiments being constructed or planned. references p.a. cherenkov, visible luminescence of pure liquids under the inuence of -radiation, dokl. akad. nauk sssr doi:/ufnr.n. i.m. frank and i.e. tamm, coherent visible radiation of fast electrons passing through matter, compt. rend. akad. sci. urss doi:/ufnr.o. f.c.t. barbato et al., another step in photodetection innovation: the -inch vsipmt prototype, in: this issue. s. vinogradov, status and perspectives of solid state photon detectors, in: this issue. m. ziembicky, photosensors and front-end electronics for the hyper-kamiokande experiment, in: this issue. a. sidorenkov et al., development of medium and small size photomultipliers for cherenkov and scintillation detectors in astroparticle physics experiments, in: this issue. s. funk, ground- and space-based gamma-ray astronomy, ann. rev. nucl. part. sci. doi:/annurev-nucl-- h.e.s.s. collaboration. h.e.s.s. web page . magic collaboration. magic web page . veritas collaboration. veritas web page . t.c. weekes et al., observation of tev gamma rays from the crab nebula using the atmospheric cerenkov imaging technique, astrophys. j. doi:/ s.m. bradbury et al., hegra collaboration, detection of -rays above tev from mkn , astron. astrophys. l fact collaboration. fact web page . j.a. barrio, for the cta consortium, status of the large size telescopes and medium size telescopes for the cherenkov telescope array observatory, in: this issue. m. heller, for the cta consortium, status and perspectives of the small size telescopes for the cherenkov telescope array southern observatory, in: this issue. hawc collaboration. hawc web page . a.u. abeysekara et al., hawc collaboration, extended gamma-ray sources around pulsars constrain the origin of the positron ux at earth, science doi:/science.aan u.f. katz and c. spiering, high-energy neutrino astrophysics: status and perspectives, prog. part. nucl. phys. doi:/j.ppnp.. super-kamiokande collaboration. super-kamiokande web page [online, cited nov ]. sno collaboration. sno web page . y. takeuchi, for the super-kamikande coll., recent results and future prospects of super-kamiokande, in: this issue. k.a. olive et al., particle data group, the review of particle physics, chin. phys. c b. aharmim, sno collaboration, electron energy spectra, uxes, and day-night asymmetries of b- solar neutrinos from measurements with nacl dissolved in the heavy-water detector at the sudbury neutrino observatory, phys. rev. c doi:/physrevc.. icecube collaboration. icecube web page . d. williams, for the icecube coll., status and prospects for the icecube neutrino observatory, in: this issue. d. williams, for the icecube collaboration, recent results from icecube, int. j. mod. phys. doi:/s t. chiarusi, for the antares and kmnet coll., neutrino astronomy and oscillation research in the mediterranean: antares and kmnet, in: this issue. s. adrin/martnez et al., kmnet collaboration, letter of intent for kmnet , j. phys. g doi:/-/// j. brunner, for the kmnet collaboration, kmnet-orca, pos neutel doi:/. d. zaborov for the kmnet collaboration, the kmnet neutrino telescope and the potential of a neutrino beam from russia to the mediterranean sea, contribution to th lomonosov conference on elementary particle physics, moscow, . url arxiv.org/abs/arxiv: z. dzhilkibaev for the baikal-gvd collaboration, baikal-gvd the next-generation neutrino telescope in lake baikal, contribution to th int. conf. on neutrino physics and astrophysics, , heidelberg, germany . doi:/zenodo. url zenodo.org/record/ r. mirzoyan et al., extending the observation limits of imaging air cherenkov telescopes toward horizon, in: this issue. m.l. ahnen et al., magic collaboration, limits on the uxes of tau neutrinos from pev to eev with the magic telescope, astropart. phys. doi:/j.astropartphys.. j. alvarez-mu niz et al., a comprehensive approach to tau-lepton production by high-energy tau neutrinos propagating through earth, phys. rev. d doi:/physrevd.. pierre auger collaboration. pierre auger web page [online, cited nov ]. pierre auger collaboration, the pierre auger observatory: contributions to the th international cosmic ray conference , collection of conference contributions . url arxiv.org/abs/arxiv: a. aab et al., pierre auger collaboration, lage-scale cosmic-ray anisotropies above eev measured by the pierre auger observatory doi:/-/aac i. maris, for the pierre auger coll., cherenkov detection at the pierre auger observatory, in: this issue. taiga collaboration. taiga web page . n. budnev et al., taiga a hybrid array for high-energy gamma astronomy and cosmic ray physics, epj web conf. doi:/epjconf/ a. vaidyanathan et al., optimization of electromagnetic and hadronic extensive air showers identication using muon detectors of taiga experiment, in: this issue. nevod collaboration. nevod web page . a. petrukhin, cherenkov water detector nevod and its further development, in: this issue. s. vernetto, for the lhaaso collaboration, gamma-ray astronomy with lhaaso, j. phys. conf. ser. doi:/-/// jem-euso-collaboration. jem-euso web page [online, cited nov ]. a. olinto et al., poemma: probe of extreme multi-messenger astrophysics, pos icrc doi:/.",
        "Subsections": [],
        "Groundtruth": "Cherenkov detectors are critical in gamma-ray, neutrino, and cosmic-ray astroparticle physics, enabling recent breakthrough results and essential for future experiments. Key references include studies on visible radiation, photodetection innovation, and the development of detectors for astroparticle physics experiments. Collaborations such as H.E.S.S., MAGIC, VERITAS, and others have made significant contributions, and projects like the Cherenkov Telescope Array (CTA), IceCube Neutrino Observatory, and Baikal-GVD are advancing research in this field. Future prospects include exploring neutrino astronomy, tau neutrino flux limits, and cosmic-ray anisotropies."
    },
    {
        "Section_Num": "Abstract",
        "Section": "Abstract",
        "Text": "accelerated mm algorithms for ranking scores inference from comparison data milan vojnovic department of statistics, london school of economics, london, united kingdom, wca ae, m.vojnovic@lse.ac.uk se-young yun graduate school of ai, kaist, daejeon, south korea, , yunseyoung@kaist.ac.kr kaifang zhou department of statistics, london school of economics, london, united kingdom, wca ae, k.zhou@lse.ac.uk the problem of assigning ranking scores to items based on observed comparison data, e.g., paired compar- isons, choice, and full ranking outcomes, has been of continued interest in a wide range of applications, including information search, aggregation of social opinions, electronic commerce, online gaming platforms, and more recently, evaluation of machine learning algorithms. the key problem is to compute ranking scores, which are of interest for quantifying the strength of skills, relevancies or preferences, and prediction of ranking outcomes when ranking scores are estimates of parameters of a statistical model of ranking outcomes. one of the most popular statistical models of ranking outcomes is the bradley-terry model for paired comparisons , and its extensions to choice and full ranking outcomes. the problem of computing ranking scores under the bradley-terry models amounts to estimation of model parameters. in this paper, we study a popular method for inference of the bradley-terry model parameters, namely the mm algorithm, for maximum likelihood estimation and maximum a posteriori probability estimation. this class of models includes the bradley-terry model of paired comparisons, the rao-kupper model of paired comparisons allowing for tie outcomes, the luce choice model, and the plackett-luce ranking model. we establish tight characterizations of the convergence rate for the mm algorithm, and show that it is essentially equivalent to that of a gradient descent algorithm. for the maximum likelihood estimation, the convergence is shown to be linear with the rate crucially determined by the algebraic connectivity of the matrix of item pair co-occurrences in observed comparison data. for the bayesian inference, the convergence rate is also shown to be linear, with the rate determined by a parameter of the prior distribution in a way that can make the convergence arbitrarily slow for small values of this parameter. we propose a simple modication of the classical mm algorithm that avoids the observed slow convergence issue and accelerates the convergence. the key component of the accelerated mm algorithm is a parameter rescaling performed at each iteration step that is carefully chosen based on theoretical analysis and characterisation of the convergence rate. our experimental results, performed on both synthetic and real-world data, demonstrate the identied slow convergence issue of the classic mm algorithm, and show that signicant efciency gains can be obtained by our new proposed method. key words: rank aggregation, generalized bradley-terry model, maximum likelihood estimation, bayesian inference, convex optimization, gradient descent, mm algorithm, rate of convergence",
        "Subsections": [],
        "Groundtruth": "The text discusses accelerated MM algorithms for ranking scores inference from comparison data. It addresses the problem of assigning ranking scores to items based on observed comparison data for various applications like information search, social opinions aggregation, and evaluation of machine learning algorithms. The focus is on computing ranking scores using the Bradley-Terry model, including methodological improvements and convergence rate analysis for maximum likelihood and Bayesian estimation. The study introduces an accelerated MM algorithm to improve convergence speed, with experimental results demonstrating significant efficiency gains compared to the classic MM algorithm. Key concepts include rank aggregation, maximum likelihood estimation, Bayesian inference, and gradient descent optimization."
    },
    {
        "Section_Num": "1",
        "Section": "1 Introduction",
        "Text": "rank aggregation is an important task that arises in a wide-range of applications, includ- ing recommender systems, information retrieval, online gaming, sport competitions, and evaluation of machine learning algo- rithms. given a set of items, the goal is to infer ranking scores of items or an ordering of items based on observed data that contains partial orderings of items. a common scenario is that of paired comparisons, e.g., player a defeats player b in a game, product a is preferred over product b by a user, and machine learning algorithm a outperforms machine learning algorithm b in an evaluation. in such scenar- ios, a common goal is not only to compute an aggregate ranking of items, but also to com- pute ranking scores, which represent strengths of individual items. such ranking scores are used for predicting outcomes of future ranking outcomes, such as predicting outcomes of matches in online games and sport contests, and predicting preference of users in product shopping or movie watching scenarios, among arxiv:v dec vojnovic, yun and zhou: accelerated mm algorithms for ranking scores inference from comparison data others. note that, importantly, observations are not restricted to paired comparisons, but may also include other types of comparison data, such as choice (e.g., product a chosen from a set of two or more products) or full ranking (e.g., a ranking list of players or teams participating in a competition). in this paper, our goals are to shed light on the efciency of one of the most popular iterative optimization methods for inference of ranking scores, namely the mm algorithm, where ranking scores correspond to param- eter estimates of popular bradley-terry fam- ily of models, and to propose an acceler- ated mm algorithm that resolves a slow con- vergence issue found to hold for a classic mm algorithm. related work statistical models of rank- ing data play an important role in a wide range of applications, including learning to rank in information retrieval (burges et al. , li ), skill rating in sport games ), online gaming platforms (her- brich et al. ), and evaluation of machine learning algorithms by comparing them with each other ). a common class of statistical models of ranking data are generalized bradley-terry mod- els, which accommodate paired comparisons with win-lose outcomes (zermelo , bradley and terry , bradley ), paired comparisons with win-lose-draw out- comes ), choices from comparison sets of two or more items, e.g., luce choice model ), full rank- ing outcomes for comparison sets of two or more items, e.g., plackett-luce ranking model ), as well as group compar- isons ). these mod- els can be derived from suitably dened latent variable models, where items are associated with independent latent performance random variables, which is in the spirit of the well- known thurstone model of comparative judg- ment ). an iterative optimization algorithm for the maximum likelihood parameter esti- mation of the bradley-terry model has been known since the original work of zermelo . lange et al. showed that this algorithm belongs to the class of mm optimization algorithms. here mm refers to either minorize-maximization or majorize- minimization, depending on whether the opti- mization problem is maximization or mini- mization of an objective function. lange provided a book on mm algorithms and hunter and lange provided a tutorial. mairal established some convergence results for incremental mm algorithms. in a seminal paper, hunter derived mm algorithms for generalized bradley-terry models as well as sufcient conditions for their convergence to ml estimators using the frame- work of mm optimization algorithms. for the bradley-terry model of paired comparisons, a necessary and sufcient condition for the exis- tence of a ml estimator is that the directed graph whose vertices correspond to items and edges represent outcomes of paired compar- isons is connected. in other words, the set of items cannot be partitioned in two sets such that none of the items in one partition won against an item in other partition. a bayesian inference method for general- ized bradley-terry models was proposed by caron and doucet , showing that clas- sical mm algorithms can be reinterpreted as special instances of expectation-maximization algorithms associated with suitably dened latent variables and proposed some original extensions. this amounts to mm algo- rithms for maximum a posteriori probabil- ity parameter estimation, for a specic family of prior distributions. this prior dis- tribution is a product-form distribution with gamma marginal distributions, where is the shape parameter and > is the rate parameter. importantly, unlike to the ml estimation, the map estimate is always guar- anteed to exist, for any given observation data. algorithms for tting bradley-terry model parameters are implemented in open source software packages, including bradleyterry ), bradleyterryscal- able ), and choix ). the rst package uses a fisher scoring algorithm (a second-order opti- mization method), while the latter two use mm algorithms (a rst-order optimization method). first-order methods are generally vojnovic, yun and zhou: accelerated mm algorithms for ranking scores inference from comparison data preferred over second-order methods for t- ting high-dimensional models using large training datasets. while the conditions for convergence of mm algorithms for generalized bradley-terry mod- els are well understood, to the best of our knowledge, not much is known about their convergence rates for either ml or map esti- mation. in this paper, we close this gap by providing tight characterizations of conver- gence rates. our results identify key properties of input data that determine the convergence rate, and in the case of map estimation, how the convergence rate depends on prior distri- bution parameters. our results show that mm algorithms, commonly used for map estima- tion for generalized bradley-terry models, can have a slow convergence for some prior distri- butions. recent research on statistical models of paired comparisons focused on characteriza- tion of the accuracy of parameter estimators and development of new, scalable parame- ter estimation methods, e.g., guiver and snel- son , wauthier et al. , hajek et al. , rajkumar and agarwal , chen and suh , shah et al. , vojnovic and yun , khetan and oh , negah- ban et al. , borkar et al. , chen et al. . note that the question about sta- tistical estimation accuracy and computation complexity tradeoff is out of the scope of our paper, and this was studied in the above cited papers. the focus of our work is on convergence properties of rst-order iterative optimization methods for parameter estimation of bradley- terry models. here rst-order refers to opti- mization methods that are restricted to value oracle access to gradients of the optimization objective function, thus not allowing access to second-order properties such as values of the hessian matrix. specically, we are inter- ested in convergence properties of rst-order methods for ml and map estimation objec- tives. it is noteworthy that some recently pro- posed algorithms show empirically faster con- vergence rate than mm, e.g., negahban et al. , maystre and grossglauser , agar- wal et al. , but it is hard to apply them for the map estimation objective. we thus restrict our attention to mm and gradient descent algorithms which are able to solve both mle and map optimization problems. a preliminary version of our paper was pub- lished in vojnovic et al. , which con- tains results on convergence rates of gradi- ent descent and mm algorithms . in the present paper, we extend our prior work by proposing new accelerated algorithms and establishing their theoretical guarantees (sec- tion ) as well as demonstrating their efciency through numerical evaluations . summary of our contributions we present tight characterizations of the rate of conver- gence of gradient descent and mm algorithms for ml and map estimation for generalized bradley-terry models. our results show that both gradient descent and mm algorithms have linear convergence with convergence rates differing only in constant factors. an iter- ative optimization algorithm that has linear convergence is generally considered to be fast in the space of rst-order optimization algorithms, and many rst-order algorithms cannot guar- antee a linear convergence. for example, stan- dard stochastic gradient descent algorithm is known to have sub-linear convergence, see, e.g., bubeck . we provide explicit bounds on convergence rates that provide insights into which properties of observed comparison data play a key role for the rate of convergence. specically, we show that the rate of con- vergence critically depends on certain proper- ties of the matrix of item pair co-occurrences, m, of input comparison data. we found that two key properties are: maximum num- ber of paired comparisons per item (denoted as d) and the algebraic connectivity of matrix m ). intuitively, a quanties how well is the graph of paired com- parisons connected. here a is the fiedler value , cf. fiedler , dened as the second smallest eigenvalue of the lapla- cian matrix lm = dm m, where dm is the diagonal matrix with diagonal elements equal to the row sums of m. the fiedler value of a matrix of paired comparison counts is known to play a key role in determining the mle accuracy, e.g., hajek et al. , shah et al. , khetan and oh , vojnovic and yun , negahban et al. . these works characterized the number of samples vojnovic, yun and zhou: accelerated mm algorithms for ranking scores inference from comparison data needed to estimate the true parameter vector within a statistical estimation error tolerance. this is different from the problem of charac- terizing the number of iterations needed for an iterative optimization algorithm to compute a ml or a map parameter estimate satisfying an error tolerance condition, which is studied in this paper. our results reveal the following facts about convergence time, dened as the number of iterations that an iterative optimization algo- rithm takes to reach the value of the underly- ing objective function within a given error tol- erance parameter > of the optimum value. for the ml objective, we show that the con- vergence time satises tml = o \u0012d a log \u0012 \u0013\u0013 which reveals that the rate of convergence crit- ically depends on the connectivity of the graph of paired comparisons in observed data. for the map estimation, we show that the convergence time satises tmap = o \u0012\u0012d + \u0013 log \u0012 \u0013\u0013 where, recall, > is the rate parameter of the gamma prior distribution. this bound is shown to be tight for some input data instances. we observe that the convergence time for the map estimation problem can be arbitrarily large for small enough parameter , where small values of parameter correspond to less informative prior distributions. our results identify a slow rate of conver- gence issue for gradient descent and mm algo- rithms for the map estimation problem. while the map estimation alleviates the issue of the non-existence of a ml estimator when the graph of paired comparisons is disconnected, it can have a much slower convergence than ml when the graph of paired comparisons is connected. perhaps surprisingly, the rate of convergence has a discontinuity at = , in the sense that for = and = , the mm algorithm for the map estimation corresponds to the classic mm algorithm for ml estima- tion, and in this case, the convergence bound holds, while for the map estimation, the convergence time grows arbitrarily large as approaches from above. we propose an acceleration method for the map estimation objective, with convergence time bounded as follows tmap acc = o \u0012 min \u001ad a, d \u001b log \u0012 \u0013\u0013 . this acceleration method resolves the slow convergence issue of classic mm algorithm for the map estimation for generalized bradley- terry models. this accelerated method does not have a discontinuity at = with respect to the rate of convergence: as approaches from above, the convergence time bound corresponds to that of the mm algorithm for ml estimation. the acceleration method nor- malizes the parameter vector estimate in each iteration of the gradient descent or mm algo- rithm using a particular normalization that ensures that the value of the objective func- tion is non decreasing along the sequence of estimated parameter vectors and that the objective function satises certain smoothness and strong convexity properties that ensure high convergence rate. this amounts to a slight modication of the classical mm algorithm that resolves the identied convergence issue. this acceleration method is derived by using a theoretical framework that may be of gen- eral interest. this framework can be applied to different statistical models of ranking data and prior distributions for bayesian inference of parameters of these models. we present numerical evaluation of the con- vergence time of different iterative optimiza- tion algorithms using input data comparisons from a collection of real-world datasets. these results demonstrate the extent of the slow convergence issue of the existing mm algo- rithm for map estimation and show signi- cant speed ups achieved by our accelerated mm algorithm. our theoretical results are established by using the framework of convex optimization analysis and spectral theory of matrices. in particular, the convergence rate bounds are obtained by using concepts of smooth and strongly convex functions. we derive acceler- ated iterative optimization algorithms based vojnovic, yun and zhou: accelerated mm algorithms for ranking scores inference from comparison data on a general approach that may be of inde- pendent interest. this approach transforms the parameter estimator in each iteration so that certain conditions are preserved for the gradi- ent vector and the hessian matrix of the objec- tive function. for generalized bradley-terry models, this transformation turns out to be simple, yielding a practical algorithm. organization of the paper in section , we present problem formulation and some background material. section contains our main results on characterization of conver- gence rates of gradient descent and mm algo- rithms; for simplicity of exposition, we focus only on the bradley-terry model of paired comparisons. section presents our acceler- ated algorithms for map estimation. section contains our numerical results. we conclude in section section contains all our proofs, additional discussions, and extensions to gen- eralized bradley-terry models.",
        "Subsections": [],
        "Groundtruth": "The text discusses the task of rank aggregation, which is important in various applications such as recommender systems, online gaming, and machine learning evaluation. The goal is to infer ranking scores of items based on observed data containing partial orderings. The text focuses on the efficiency of an iterative optimization method, the MM algorithm, for inferring ranking scores. It introduces an accelerated MM algorithm to address slow convergence issues. The section also touches on related work, statistical models of ranking data, and convergence properties of optimization algorithms for parameter estimation in the Bradley-Terry models. Key contributions include tight characterizations of convergence rates for MM and gradient descent algorithms, highlighting the importance of input data properties in determining convergence rates. The text presents theoretical guarantees for the new accelerated algorithms and demonstrates their efficiency through numerical evaluations."
    },
    {
        "Section_Num": "2",
        "Section": "2 Problem formulation",
        "Text": "according to the bradley-terry model of paired comparisons with win-lose outcomes, each com- parison of items i and j has an independent outcome: either i wins against j or j wins against i . the distribution of outcomes is given by pr = i i + j where = irn + is the parame- ter vector. the bradley-terry model of paired comparisons was studied by many, e.g., ford , dykstra, jr. , simons and yao and is covered by classic books on categorical data analysis, e.g., agresti . we will sometimes use the parametriza- tion i = ewi when it is simpler to express an equation or when we want to make a connection with the literature using this parametrization. using parameterization w = irn, we have pr = ewi ewi + ewj . all our convergence results are for the model with parameter w. the bradley-terry type models for paired comparisons with ties, choice, and full ranking outcomes, we refer to as generalized bradley-terry models, are dened in section . our results apply to all these different models. in the main body of the paper, we focus only on the bradley-terry model for paired comparisons in order to keep the presentation simple. maximum likelihood estimation the max- imum likelihood parameter estimation prob- lem corresponds to nding wthat solves the following optimisation problem: max wirn where is the log-likelihood function, = n i= j=i di,j ) with di,j denoting the number of observed paired comparisons such that i j. the maximum likelihood optimisation prob- lem is a convex optimization problem. note, however, that the objective function is not a strictly concave function as adding a common constant to each element of the parameter vector keeps the value of the objec- tive function unchanged. map estimation problem an alternative objective is obtained by using a bayesian infer- ence framework, which amounts to nding a maximum a posteriori estimate of the param- eter vector under a given prior distribution. we consider the bayesian method introduced by caron and doucet , which assumes the prior distribution to be of product-form with marginal distributions such that i has a gamma distribution where is the shape parameter and > is the rate parameter. note that and affect the scale of the parameter vector as with respect to the gamma prior distribution, i has the expected value and the mode equal to / and /, respectively. for any xed , the density of gamma distribution becomes more at as approaches zero which corre- sponds to a less informative prior. according to the assumed prior distribution, n i= i gamma and, hence, the mode of n i= i is /. we can interpret the mode of n i= i as the scale of the parameter vector. vojnovic, yun and zhou: accelerated mm algorithms for ranking scores inference from comparison data the log-a posteriori probability function can be written as = + where is the log-likelihood function in and is the log-likelihood of the prior distri- bution given by = n i= wi ewi) . note that for = and = , the log-a posteriori probability function corresponds to the log-likelihood function. for these values of parameters and , map and ml estimation problems are equivalent. mm algorithms the mm algorithm for minimizing a function f is dened by minimiz- ing a surrogate function that majorizes f. a surrogate function g is said to be a majorant function of f if f g and f = g for all x and y. the mm algo- rithm is dened by the iterative updates: x = argmin x g). for maximizing a function f, we can analo- gously dene the mm algorithm as minimiza- tion of a surrogate function g that minorizes function f. majorization surrogate functions are used for minimization of convex functions, and minorization surrogate functions are used for maximization of concave functions. the classic mm algorithm for the bradley- terry model of paired comparisons uses the following minorization function of : = n i= j=i ij, where ij = di,j \u0012 xi exi + exj eyi + eyj log + \u0013 . it is easy to observe that is a minoriza- tion surrogate function of by noting that log x and that equality holds if, and only if, x = , which is used to break log(exi + exj) terms in the log-likelihood function. the classic mm algorithm for the ml param- eter estimation of the bradley-terry model of paired comparisons , hunter ), is dened by the following iterative updates, for i = ,, . . . , n, i = n j= di,j n j= mi,j i + j . following caron and doucet , the mm algorithm for the map parameter estimation of the bradley-terry model of paired compar- isons is derived for the minorant surrogate function of function in , dened as = + where is the minorant surrogate func- tion of the log-likelihood function and is the prior log-likelihood function . the iterative updates of the mm algorithm are dened by, for i = ,, . . . , n, = + j=i di,j + j=i mi,j i + j . note that this iterative optimization algo- rithm corresponds to the classic mm algorithm for ml estimation when = and = we also consider gradient descent algorithm with constant step size > , which has itera- tive updates as x = x f ). our goal in this paper is to characterize the rate of convergence of mm algorithms for generalized bradley-terry models. we also derive convergence rates for gradient descent algorithms. it is natural to consider gradi- ent descent algorithms as they belong to the class of rst-order optimization methods (not requiring second-order quantities such as hes- sian of the objective function or its approxima- tions). intuitively, the rate of convergence of an iterative algorithm quanties how fast the value of the objective function converges to the optimum value with the number of iterations. for an iterative optimization method for minimizing function f, which outputs a sequence of points x, x, . . ., we say that vojnovic, yun and zhou: accelerated mm algorithms for ranking scores inference from comparison data there is an -improvement with respect to f at time step t if f ) f ) f ) where xis a minimizer of f. an iterative opti- mization method is said to have linear conver- gence if there exist positive constants and t such that the method yields an -improvement at each time step t t background on convex analysis we dene some basic concepts from convex analysis that we will use throughout the paper. function f : irn ir is -strongly convex on x if it satises the following subgradient inequality, for all x, y x : f f f ||x y|| f is -strongly convex on x if, and only if, f ||x|| is convex on x . function f is -smooth on x if its gradient f is -lipschitz on x , i.e., for all x, y x , ||f f || ||x y||. for any -smooth function f on x , we also have that, for all x, y x , | f f f | ||x y|| a proof of the last claim can be found in lemma in bubeck . function f satises the polyak-lojasiewicz inequality on x ) if there exists > such that for all x x , f f ||f || where xis a minimizer of f. when the pl inequality holds on x for a specic value of , we say that -pl inequality holds on x . if f is -strongly convex on x , then f satises the - pl inequality on x .",
        "Subsections": [],
        "Groundtruth": "The Bradley-Terry model of paired comparisons with win-lose outcomes is discussed, where each comparison has an independent outcome. Maximum likelihood estimation and MAP estimation problems are outlined, with a focus on the optimization algorithms used for parameter estimation. MM algorithms are introduced for minimizing and maximizing functions, with iterative updates explained. Gradient descent algorithms and their iterative updates are also considered. Convergence rates for these optimization algorithms are discussed, with an emphasis on characterizing the rate of convergence for generalized Bradley-Terry models. Basic concepts from convex analysis, such as strong convexity and smoothness, are defined and relevant inequalities are discussed."
    },
    {
        "Section_Num": "3",
        "Section": "3 Convergence rates",
        "Text": "in this section, we present results on the rate of convergence for gradient descent and mm algorithms for ml and map estimation for the bradley-terry model of paired compar- isons. we rst show some general convergence theorems that hold for any strongly convex and smooth function f, which characterize the rate of convergence in terms of the strong- convexity and smoothness parameters of f, and a parameter of the surrogate function used to dene the mm algorithm. these results are then used to derive convergence rate bounds for the bradley-terry model. the results of this section can be extended to other instances of generalized bradley-terry models, including the rao-kupper model of paired comparisons with tie outcomes, the luce choice model, and the plackett-luce ranking model. these extensions are estab- lished by following the same main steps as for the bradley-terry model of paired compar- isons. the differences lie in the characteriza- tion of the strong-convexity and smoothness parameters. the results provide characteriza- tions of the convergence rates that are equiv- alent to those for the bradley-terry model of paired comparisons up to constant factors. we provide details in section .",
        "Subsections": [
            {
                "Section_Num": "3_1",
                "Section": "3.1 General convergence theorems",
                "Text": "we rst present a known result that for any - smooth function satisfying the -pl inequality, gradient descent algorithm with suitable choice of the step size has a linear conver- gence with the rate of convergence / < this result is due to nesterov and a simple proof can be found in boyd and van- denberghe , chapter . this result fol- lows from the following theorem. theorem assume f is a convex -smooth function on x satisfying the - pl inequality on x x, xx is a minimizer of f, and x x is according to the gradient descent algorithm with step size = /. then, if x x and x x, there is an /-improvement with respect to f at time step t. note that if there exists t such that x x for all t t, then theorem implies a lin- ear convergence rate with rate /. such a t indeed exists as it can be shown that ||x x|| is non-increasing in t and is decreasing for every t such that ||f )|| = we next present a new result which shows that the mm algorithm also has linear con- vergence, for any smooth and strongly convex vojnovic, yun and zhou: accelerated mm algorithms for ranking scores inference from comparison data function f that has a surrogate function g sat- isfying a certain condition. theorem assume f is a convex - smooth function on x satisfying the -pl inequal- ity on x x, xx is a minimizer of f and x x is according to the mm algorithm . let g be a majorant surrogate function of f such that for some > , g f ||x y|| for all x, y x. then, if x x and x +f ) x, there is a /-improvement with respect to f at time step t. the proof of theorem is based on sepa- rating the gap between the objective function value at an iteration and the optimum function value in two components: one due to using a surrogate function and other due to a vir- tual gradient descent update. the remaining arguments are similar to those of the proof of theorem . it is worth noting that a different set of sufcient conditions for linear conver- gence of mm algorithms were found in propo- sition in mairal . these conditions require that g is a rst-order surrogate function of f, a notion we discuss further in section . from theorems and , we observe that the mm algorithm has the same rate of conver- gence bound as the gradient descent algorithm except for the smoothness parameter being enlarged for value . if c, for a constant c > , then the mm algorithm has the same rate of convergence bound as the gradient descent algorithm up to a constant factor.",
                "Subsections": [],
                "Groundtruth": "- For any -smooth function satisfying the -pl inequality, the gradient descent algorithm with a suitable choice of step size has linear convergence with a rate of <. This result is attributed to Nesterov and is proven in Boyd and Vandenberghe's book.\n- A new result shows that the MM algorithm also has linear convergence for smooth and strongly convex functions, under certain conditions involving a majorant surrogate function.\n- The MM algorithm's convergence rate is comparable to the gradient descent algorithm's, with the smoothness parameter being enlarged. If c>1, the MM algorithm's convergence rate is equivalent to the gradient descent algorithm's up to a constant factor."
            },
            {
                "Section_Num": "3_2",
                "Section": "3.2 Maximum likelihood estimation",
                "Text": "we consider the rate of convergence for the ml parameter estimation for the bradley-terry model of paired comparisons. this estima- tion problem amounts to nding a parame- ter vector that minimizes the negative log- likelihood function, with the log-likelihood function given in . recall that m denotes the matrix of the item-pair co-occurrence counts and lm denotes the associated lapla- cian matrix. for any positive semidenite matrix a, we let i denote the i-th smallest eigenvalue of a. lemma for any , the negative log- likelihood function for the bradley-terry model of paired comparisons is -strongly convex on w, = w {w irn : w = }, where w = {w irn : ||w||}, and -smooth on irn with = c and = n where c = / by lemma , the smoothness parameter is proportional to the largest eigenvalue of the laplacian matrix lm. by the gershgorin cir- cle theorem, e.g., theorem . in golub and loan , we have n d. thus, we can take = d/ we will express all our convergence time results in terms of d instead of n. this is a tight characteriza- tion up to constant factors. when m is a graph adjacency matrix, then n d + by grone et al. . in the context of paired comparisons, d has an intuitive interpre- tation as the maximum number of observed paired comparisons involving an item. the following lemma will be useful for showing that a function f satises the -pl inequality if it satises a -strong convexity condition. lemma assume that x is a convex set such that f is -strongly convex on x = x {x irn : x = }, and that for all c ir and x x , f ) = f and f ) = f where c = x + c then, f satises the -pl inequality on x . the proof of lemma follows by noting that if f is -strongly convex on x, then it sat- ises the -pl inequality on x since for every x x , x = x + c for some x x and c ir, by conditions and and denition of -pl inequality in , it follows that if -pl inequality holds on x, it holds as well on x . conditions and are satised by neg- ative log-likelihood functions for generalized bradley-terry models. since the negative log-likelihood function of the bradley-terry model of paired com- parisons satises conditions and of vojnovic, yun and zhou: accelerated mm algorithms for ranking scores inference from comparison data lemma , combining with lemma , we observe that it satises the -pl inequality on w with = ca. furthermore, by lemma , the negative log-likelihood func- tion is -smooth on irn with = d/ combining these facts with theorem , we have the following corollary: corollary assume that wis the maximum likelihood parameter estimate in w = {w : irn : ||w||}, for some , and w w is according to gradient descent algorithm with step size = /d. then, if w w, there is an m,- improvement at time step t where m, = c a d. the result in corollary implies a lin- ear convergence with the rate of convergence bound ca/d. hence, we have the following convergence time bound: t = o \u0012d a log \u0012 \u0013\u0013 . we next consider the classic mm algorithm for the ml estimation problem, which uses the surrogate function in . this surrogate func- tion satises the following property: lemma for any , for all x, y n, x y where = ed. by theorem and lemmas , , and , we have the following corollary: corollary assume that wis the max- imum likelihood parameter estimate in w = {w : irn : ||w||}, for some , and that w w is according to the mm algorithm. then, if w w, there is an m,- improvement with respect at time step t where m, = c a d and c = /. from corollaries and , we observe that both gradient descent and mm algorithms have the rate of convergence bound of the form ca/d for some constant c > the only difference is the value of constant c. hence, both gradient descent and mm algo- rithm have a linear convergence, and the con- vergence time bound . . maximum a posteriori probability estimation we next consider the maximum a posteriori probability estimation problem. we rst note that the negative log-a posteriori probability function has the following properties. lemma the negative log-a posteriori prob- ability function for the bradley-terry model of paired comparisons and the prior distribution gamma is -strongly convex and -smooth on w = {w irn : ||w||} with = e and = n + e. note that the strong convexity parameter is proportional to while as shown in lemma , for the ml objective is propor- tional to this has important implica- tions on the rate of convergence which we dis- cuss next. by theorem and lemma , we have the following corollary: corollary assume wis the maximum a posteriori parameter estimate in w = {w irn : ||w||}, for some , and w w is according to gradient descent algorithm with step size = / + e). then, if w w, there is a m,-improvement where m, = e d + e. the result in corollary implies a linear convergence with the convergence time bound t = o \u0012\u0012 + d \u0013 log \u0012 \u0013\u0013 . vojnovic, yun and zhou: accelerated mm algorithms for ranking scores inference from comparison data this bound can be arbitrarily large by taking parameter to be small enough. in section , we will show a simple instance for which this bound is tight. hence, the convergence time for map estimation can be arbitrarily slow, and much slower than for the ml case. we next consider the mm algorithm. first, note that since = , by lemma , we have lemma for all x, y n, x y where = ed. by theorem and lemmas and , we have the following corollary: corollary assume wis the maximum a posteriori parameter estimate in w = {w irn : ||w||}, for some , and w w is according to the mm algorithm. then, if w w, there is an m,- improvement at time step t where m,, = e d + e. from corollaries and , we observe that both gradient descent algorithm and mm algo- rithm have the rate of convergence bound )), and hence both have linear convergence and both have the convergence time bound . we next establish tightness of this rate of convergence by showing that it is achieved for a simple instance.",
                "Subsections": [],
                "Groundtruth": "The section discusses maximum likelihood estimation for the Bradley-Terry model of paired comparisons. It covers the rate of convergence for parameter estimation, highlighting the negative log-likelihood function and Laplacian matrix. The text delves into strong convexity and smoothness properties, presenting Lemmas and Corollaries to establish convergence time bounds, particularly focusing on the gradient descent and MM algorithms. Furthermore, it touches on Maximum A Posteriori Probability Estimation, emphasizing the properties of the negative log-A posteriori probability function and its implications on convergence rates compared to Maximum Likelihood estimation. The discussion includes findings on the rate of convergence and ties to specific algorithms, illustrating a comparison in convergence rates between the MAP and Maximum Likelihood cases."
            },
            {
                "Section_Num": "3_4",
                "Section": "3.4 Tightness of the rate of convergence",
                "Text": "consider an instance with two items, which are compared m times. note that d = m and a = m. this simple instance allows us to express the parameter vector estimate w and the log-a posteriori probability function ) in a closed form, with detailed deriva- tions provided in section . from this analy- sis, we have that ) satises lim t ) ) = \u0012 + m \u0013 . note that the rate of convergence is approx- imately /m for small by tak- ing such that = c, for a constant c > figure simple illustrative example: number of itera- tions until convergence versus parameter . note that the smaller the value of , the slower the convergence for map and the mm algorithm for map can be slower for several orders of magnitude than for ml. such that ||w||, we have the rate of con- vergence ). this establishes the tightness of the rate of convergence bound in corollary .",
                "Subsections": [],
                "Groundtruth": "In a simple instance with two items compared m times, parameter vector estimate w and log-a posteriori probability function are expressed in closed form. The rate of convergence is approximately 1/m for small values of a. The convergence speed is slower for smaller values of a, with the MM algorithm for MAP lagging behind ML by several orders of magnitude. The rate of convergence establishes the tightness of the bound in corollary."
            },
            {
                "Section_Num": "3_5",
                "Section": "3.5 A simple illustrative numerical example",
                "Text": "we illustrate the rate of convergence for a sim- ple example, using randomly generated obser- vations of paired comparisons. this allows us to demonstrate how the number of iterations grows as the value of parameter becomes smaller, and how the number of iterations is affected by the value of parameter . later, in section , we provide further validation by using real-world datasets. our example is for an instance with items with each distinct pair of items compared times and the input data generated according to the bradley-terry model of paired compar- isons with the parameter vector such that a half of items have parameter value and the other half of items have parameter value , for a parameter > we dene the convergence time t to be the smallest integer t such that ||w w||, for a xed parameter > in our experiments, we set = . the results in figure , obtained for = /, demonstrate that the mm algorithm for the map estimation problem with > can be much slower than the mm algorithm for the ml estimation problem. we further evaluate the convergence time of gradient descent and mm algorithms for vojnovic, yun and zhou: accelerated mm algorithms for ranking scores inference from comparison data different values of parameter , for each dis- tinct pair of items compared times. the numerical results in figure show the num- ber of iterations versus the value of parameter for gradient descent and mm algorithms, for different values of parameter . we observe that for small enough value of , convergence times of gradient descent and mm algorithms are nearly identical. both algorithms have the convergence time increasing by decreasing the value of for strictly positive values of this parameter. we also observe a discontinuity in convergence time, for = being smaller than for some small positive value of . the discontinuity at = originates from the fact that the log-likelihood function has innitely many solutions for = , but has a unique solution whenever > consider a simple illustrative example: f = (x ) + , for a parameter then, the gradient descent converges to the unique solution slowly when is close to when = , however, we just need to nd the minimum point of which can be solved in a few iterations.",
                "Subsections": [],
                "Groundtruth": "The text provides a simple illustrative numerical example to demonstrate the rate of convergence in algorithms for ranking scores inference from comparison data. The example involves paired comparisons with randomly generated observations according to the Bradley-Terry model. It shows how the number of iterations grows as a parameter value becomes smaller and how it is affected by the parameter value. The results show that the MM algorithm for Maximum a Posteriori (MAP) estimation can be slower than for Maximum Likelihood (ML) estimation. Gradient descent and MM algorithms are evaluated for different parameter values, with convergence times increasing as the parameter decreases. There is a discontinuity in convergence time at a certain parameter value due to the log-likelihood function having infinitely many solutions at that point. The text emphasizes the effectiveness of gradient descent in finding the minimum point of the function in just a few iterations."
            }
        ],
        "Groundtruth": "The text discusses the convergence rates for gradient descent and MM algorithms in maximum likelihood (ML) and maximum a posteriori (MAP) estimation for the Bradley-Terry model of paired comparisons. General convergence theorems are presented for strongly convex and smooth functions, with rates characterized by strong convexity, smoothness parameters, and a surrogate function parameter for the MM algorithm. These results are then applied to derive convergence rate bounds for the Bradley-Terry model and can be extended to other generalized models like the Rao-Kupper model, Luce choice model, and Plackett-Luce ranking model. Differences in these extensions lie in the characterization of strong convexity and smoothness parameters, with the convergence rates being equivalent up to constant factors."
    },
    {
        "Section_Num": "4",
        "Section": "4 Accelerated MAP inference",
        "Text": "in this section, we present a new acceler- ated algorithm for gradient descent and mm algorithms for map estimation. the key ele- ment is a transformation of the parameter vec- tor estimate in each iteration of an iterative optimization algorithm that ensures mono- tonic improvement of the optimization objec- tive along the sequence of parameter vector estimates and ensures certain second-order properties of the objective function hold along the sequence of parameter vector estimates. we rst introduce transformed versions of gradient descent and mm algorithms. given a mapping : irn irn, we dene the - transformed gradient descent algorithm by x = f )). similarly, we dene the -transformed mm algorithm by the iteration: x = (argmin x g)). importantly, function f and mapping have to satisfy certain conditions in order to provide a convergence rate guarantee, which we discuss in the following section.",
        "Subsections": [
            {
                "Section_Num": "4_1",
                "Section": "4.1 General convergence theorems",
                "Text": "assume that f and satisfy the following con- ditions, for a convex set x that contains opti- mum point x, and a vector d irn: f is -smooth on x ; f satises the -pl inequality on x = x {x irn : f d = } and f ) f for all x irn; x when x . condition is a standard smoothness con- dition imposed on x . condition is a stan- dard -pl condition imposed on the subset of points in x at which the gradient of the function f is orthogonal to vector d. condition means that applying to a point cannot increase the value of function f. this condition is crucial to ensure a monotonic improvement of the objective function value when transfor- mation is applied to an iterative optimiza- tion method. condition is satised when at any -transformed point, the gradient of f is orthogonal to vector d. this condition is crucial to ensure certain second-order properties hold when is applied to an iterative optimization method. we have the following two theorems. theorem assume that f satises and , satises , and = /. let x x be according to the - transformed gradient descent algorithm . then, if x, x x, there is an /- improvement with respect to f at time step t. the theorem establishes the same rate of convergence as for the gradient descent algo- rithm in theorem but with the strong con- vexity condition restricted to points at which the gradient of f is orthogonal to vector d. theorem assume that f satises and , satises , and g is a majorant sur- rogate function of f such that g f vojnovic, yun and zhou: accelerated mm algorithms for ranking scores inference from comparison data = = = figure number of iterations versus for gradient descent and mm algorithms, for different values of . ||x y|| let x x be according to the - transformed mm algorithm . then, if x, x x, there is an /- improvement with respect to f at time step t. the last theorem establishes the same rate of convergence as for classic mm algorithm in theorem , but with a strong convexity con- dition imposed only at the points at which the gradient of f is orthogonal to vector d. we next present a lemma which will be instrumental in showing that the pl condition in holds for the map estimation problem. lemma assume f is a convex, twice- differentiable function. let x be a convex set, x be dened by for a given vector d, and xx be a minimizer of f. if for some positive semidenite ax such that f ax for all x x , and uax v = for all u, v such that u = z and v = pdz, for z irn where pd = i ||d|| dd then, f satises the -pl inequality on x for all with := min xirn\\{}:dx= xax x ||x|| . note that pd is the projection matrix onto the space orthogonal to vector d. the value of is maximized when vector d is the eigenvec- tor corresponding to the smallest eigenvalue of ax . in this case, is the second smallest eigenvalue of ax .",
                "Subsections": [],
                "Groundtruth": "The text discusses general convergence theorems related to iterative optimization methods. The theorems rely on conditions such as smoothness, orthogonality of the gradient to a vector, and strong convexity. These conditions ensure a monotonic improvement in the objective function value during optimization. The theorems establish convergence rates for gradient descent and accelerated MM algorithms, with the strong convexity condition applied at points where the gradient of the function is orthogonal to a given vector. A lemma is presented to support the proof of a condition for map estimation problems, involving a convex, twice-differentiable function and a positive semidefinite matrix."
            },
            {
                "Section_Num": "4_2",
                "Section": "4.2 Convergence rate for the BT model",
                "Text": "in this section, we apply the framework devel- oped in the previous section to characterize the convergence rate for the map parameter esti- mation of the bradley-terry model of paired comparisons. the map parameter estimation problem amounts to nding a parameter vec- tor that maximizes the log-a posteriori proba- bility function dened in . let the trans- formation be dened as = x + c where c = log \u0012 n \u0013 log n i= exi ! . we next show that f and satisfy condi- tions , , , and for the direction vector d = this will allow us to apply theo- rems and to characterize the rate of con- vergence for -transformed gradient descent and mm algorithms. we rst show that f satises conditions and for the set w = {w irn : ||w||}. condition holds because, in lemma , we have already shown that f is -smooth on w with = d/ + e. con- dition can be shown to hold by lemma as follows. note that we have ) aw, for all w w, where aw = clm + ei. the assumptions of lemma hold: holds because aw is a positive semidef- inite matrix, and holds because ulmv = and uiv = uv = . since is the smallest eigenvalue of ax on the subspace orthogonal to vector , we have = c + e. vojnovic, yun and zhou: accelerated mm algorithms for ranking scores inference from comparison data hence, by lemma , it follows that f satises condition with = c + e. we next show that , dened in , sat- ises conditions and . these two con- ditions are shown to hold in the following lemma. lemma for all w irn, ) and ) = from theorem , we have the following corollary: corollary assume that w w is according to the -transformed gradient descent for the negative log-a posteri- ori probability function of the bradley-terry model of paired comparisons, with product-form prior dis- tribution such that ewi gamma , and > , and = /. then, there is an m,,-improvement with respect to at time step t where m,, = ca + e d + e and c = / from theorem , we have the following corollary: corollary assume that iterates are according to the -transformed mm for the negative log-a posteriori probability function of the bradley-terry model of paired comparisons, with product-form prior distribution such that ewi gamma , and > then, there is an m,,-improvement with respect to at time step t where m,, = ca + e d + e and c = / algorithm accelerated mm algorithm : initialization: , , prev : while || prev||> do : prev : for i = ,, . . . n do : temp i = +j=i di,j +j=i mi,j i+j standard mm : end for : for i = ,, . . . , n do : i = temp i n j= temp j n rescaling : end for : end while proof condition holds for because we have already shown that is -smooth with = d/ + e on w and = ed/ condition holds by lemma with = ca + e. conditions and hold by and in lemma , respectively. note that in the limit of small , the con- vergence rate bounds in corollaries and correspond to the bounds for the ml esti- mation in corollaries and , respectively. from corollaries and , it follows that for accelerated gradient descent and accelerated mm algorithm, the convergence time satises t = o \u0012 min \u001ad a, d \u001b log \u0012 \u0013\u0013 . for the bradley-terry model of paired comparisons with parametrization = , where i = ewi for i = ,, . . . , n, the transformation given by is equiv- alent to a rescaling as shown in a procedural form in algorithm this algorithm rst performs the standard mm update in eq. , which is followed by rescaling the resulting intermediate parameter vector such that the parameter vector at every iteration satises n i= i = c where c = n/. this can be interpreted as xing the scale of parameters to a carefully chosen scale that is dependent on the choice of the prior distribution. note that the scaling factor c cannot be arbitrarily xed while still preserving good convergence properties. in particular, selecting the scale c = can result in undesired convergence properties. we demonstrate this in section through numerical examples. vojnovic, yun and zhou: accelerated mm algorithms for ranking scores inference from comparison data figure the illustrative example revisited: acceler- ated mm resolves the convergence issue for map estimation: it has faster or equal con- vergence than for ml estimation. it turns out that the rescaling in algorithm is roughly of the same order as the ran- dom rescaling suggested in caron and doucet . therein, the authors suggested using independent identically distributed rescaling factors across different iteration steps with dis- tribution gamma. this ensures the dis- tribution of n i= i to remain invariant across different iterations, equal to gamma. the mode of this rescaling factor is /. this bears a similarity with the rescaling in algorithm , in particular, with respect to the dependence on parameter . our results show that it sufces to use a simple deterministic rescaling factor to ensure linear convergence. moreover, using a different rescaling than the one used in algorithm can result in a lack or slow convergence, which is shown by numeri- cal experiments in section our numerical example revisited we ran the accelerated mm algorithm for our numerical example and obtained the results shown in figure by comparing with the correspond- ing results obtained by the mm algorithm with no acceleration, shown in figure , we observe that the acceleration resolves the slow conver- gence issue and that it can yield a signicant reduction of the convergence time.",
                "Subsections": [],
                "Groundtruth": "The section discusses the convergence rate for the Bradley-Terry model parameter estimation of paired comparisons. By applying specific transformations and conditions, the convergence rates for gradient descent and MM algorithms are characterized. The study shows improvements in convergence time using accelerated algorithms compared to standard methods. A particular scaling factor is crucial for efficient convergence, as demonstrated through numerical examples and comparisons between accelerated and standard MM algorithms. The study highlights the importance of proper scaling for optimal convergence properties in parameter estimation algorithms."
            }
        ],
        "Groundtruth": "The section introduces an accelerated algorithm for gradient descent and MM algorithms in MAP estimation. It focuses on transforming the parameter vector estimate in each iteration to ensure monotonic improvement of the optimization objective and maintain certain second-order properties of the objective function. Transformed versions of gradient descent and MM algorithms are presented, where a specific mapping function is used to update the parameter vectors in each iteration. Convergence rate guarantees are dependent on the conditions that the function and mapping must satisfy, which are elaborated on in subsequent sections."
    },
    {
        "Section_Num": "5",
        "Section": "5 Numerical results",
        "Text": "in this section we present evaluation of convergence times of mm algorithms for different generalized bradley-terry models for a collection of real-world datasets. our goal is to provide empirical validation of table dataset properties. dataset m n d a gifgif: a , , gifgif: c , , gifgif: h , , gifgif: a gifgif: c gifgif: h chess , , chess , nascar , , some of the hypotheses derived from our theoretical analysis. overall, our numerical results validate that the convergence of the mm algorithm for map estimation can be much slower than for ml estimation, mm algorithm for map estimation has convergence time that increases as parameter of the prior distribution decreases, and a signicant reduction of the convergence time can be achieved by the accelerated mm algorithm dened in section the code and datasets for reproducing our experiments are available online at: https://github.com/gdmmbt/ acceleratedbradleyterry.",
        "Subsections": [
            {
                "Section_Num": "5_1",
                "Section": "5.1 Datasets",
                "Text": "we consider three datasets, which vary in the type of data, size and sparsity. the three datasets are described as follows. gifgif this dataset contains user evalu- ations of digital images by paired compar- isons with respect to different metrics, such as amusement, content, and happiness. the dataset was collected through an online web service by the mit media lab as part of the placepulse project rich et al. . this ser- vice presents the user with a pair of images and asks to select one that better expresses a given metric, or select none. the dataset con- tains ,, observations and covers met- rics. we used this dataset to evaluate conver- gence of mm algorithms for the bradley-terry model of paired comparisons. we did this for each of the three aforementioned metrics. chess this dataset contains game-by-game results for , matches among , chess players. the dataset was used in a kaggle vojnovic, yun and zhou: accelerated mm algorithms for ranking scores inference from comparison data = = = figure convergence of the mm algorithm with the input gifgif a training dataset for different values of parame- ter : log-a posteriori probability versus the number of iterations, and the difference between the maximum log-a posteriori probability and the log a-posteriori probability versus the number of itera- tions. the maximum a posteriori probability is approximated by taking the parameter vector woutput by the mm algorithm after a large number of iterations. the plots in the top row indicate that the algorithm converges. the plots in the bottom row indicate linear convergence. from the plots, we also observe that the convergence is slower for smaller values of parameter . chess ratings competition sonas . each observation contains information for a match between two players including unique iden- tiers of the two players, information about which one of the two players played with white gures, and the result of the match, which is either win, loss, or draw. this dataset has a large degree of sparsity. we used this dataset to evaluate convergence of the rao- kupper model of paired comparisons with ties. nascar this dataset contains auto racing competition results. each observation is for an auto race, consisting of the ranking of drivers in increasing order of their race nish times. the dataset is available from a web page main- tained by hunter . this dataset was pre- viously used for evaluation of mm algorithms for the plackett-luce ranking model by hunter and more recently by caron and doucet . we used this dataset to evaluate conver- gence times of mm algorithms for the plackett- luce ranking model. we summarise some key statistics for each dataset in table we use the shorthand nota- tion gifgif: a, gifgif: c, and gifgif: h to denote datasets for metrics amusement, con- tempt, and happiness, respectively. for full gifgif and chess datasets, we can split the items into two groups such that at least one item in one group is not compared with any item in the other group, i.e., the algebraic con- nectivity a of matrix m is zero. in this case, there exists no ml estimate, while an map estimate always exists. in order to con- sider cases when an mle exists, we consider sampled datasets by restricting to the set of items such that the algebraic connectivity for this subset of items is strictly positive. this subsampling was done by selecting the largest connected component of items.",
                "Subsections": [],
                "Groundtruth": "Three datasets are considered with varying data types, sizes, and sparsity levels. The first dataset, gifgif, consists of user evaluations of digital images using paired comparisons for metrics like amusement, content, and happiness. The second dataset, chess, contains game results among chess players. The third dataset, NASCAR, contains racing competition results. The datasets were used to evaluate algorithms for paired comparisons models such as Bradley-Terry, Rao-Kupper, and Plackett-Luce. Convergence of the algorithms was assessed based on unique characteristics of the datasets, such as sparsity and connectivity. Subsampling was performed to ensure the existence of maximum likelihood estimates in cases where the algebraic connectivity is positive."
            },
            {
                "Section_Num": "5_2",
                "Section": "5.2 Experimental results",
                "Text": "we evaluated the convergence time dened as the number of iterations that an algo- rithm takes until a convergence criteria is satis- ed. we use the standard convergence criteria based on the difference of successive param- eter vector estimates. specically, the conver- gence time t is dened as the smallest inte- ger t > such that ||w w||, for vojnovic, yun and zhou: accelerated mm algorithms for ranking scores inference from comparison data table number of iterations for the mm algorithm and accelerated mm algorithm . dataset algorithm = gifgif: a mm mle accmm non-existant gifgif: c mm mle accmm non-existant gifgif: h mm mle , accmm non-existant , gifgif: a mm accmm gifgif: c mm accmm gifgif: h mm , accmm chess mm mle , accmm non-existant , chess mm accmm nascar mm accmm nascar mm , , accmm nascar mm , , accmm nascar mm , , accmm xed value of parameter > , with initial value w = in our experiments, we used as the default value for parameter . for nascar dataset, we also present results for several other values of to demonstrate how the convergence time changes. in our exper- iments, we also evaluated the convergence time measured in real processor time units. we noted that they validate all the observations derived from the convergence times measured in the number of iterations, and hence we do not further discuss them. in our experiments we varied the value of parameter and, unless specied otherwise, we set the value of parameter such that = . this corresponds to xing the mode of the gamma prior marginal distributions to value note that the case = corresponds to ml estimation. before discussing numerical convergence time results, we rst show results validating that the mm algorithm converges and that this convergence is linear. this is shown in figure for gifgif a dataset for three different val- ues of parameter . for space reasons, we only include results for this dataset. we observe that in all cases the log-a posteriori probabil- ity monotonically increases with the number of iterations, thus validating convergence. we also observe that the gap between the maxi- mum log-a posteriori probability and the log-a posteriori probability decreases with the num- ber of iterations in a linear fashion for suf- ciently large number of iterations, when plot- ted using the logarithmic scale for the y axis, thus validating linear convergence. we next discuss our numerical results for convergence time evaluated for the mm algo- rithm and accelerated mm algorithm for dif- ferent datasets and choice of parameters. our numerical results are shown in table for gifgif datasets, we observe that the convergence time increases as the value of parameter decreases for > for the val- ues of considered, this increase can be for as much as two orders of magnitude. when the ml estimate exists , we observe that the mm algorithm for ml estima- tion converges much faster than the mm algo- rithm for map estimation for sufciently small vojnovic, yun and zhou: accelerated mm algorithms for ranking scores inference from comparison data = = = figure the log-a posteriori probability versus the number of iterations for the mm algorithm with normalization such that n i= ewi = at each iteration, using gifgif a as input dataset, for different values of . the results indicate that the log-a posteriori probability is not guaranteed to monotonically increase with the number of iterations. table number of iterations for the mm algorithm with normalization such that n i= ewi = in each iteration. dataset algorithm = gifgif: a mm norm no convergence gifgif: c mm norm no convergence gifgif: h mm norm no convergence values of parameter . we also observe that a signicant reduction of the convergence time can be achieved by the accelerated mm algo- rithm. this reduction can be for as much as order % of the convergence time of the mm algorithm without acceleration. these empiri- cal results validate our theoretical results. for chess datasets, all the observations derived by using the gifgif datasets remain to hold. for nascar dataset, we show results for different values of parameter , including the default value of again, all the obser- vations made for gifgif and chess datasets remain to hold. it is noteworthy that the mm algorithm for ml estimation converges much faster than for map estimation for sufciently small values of parameter . this is especially pronounced for smaller values of . for the cases considered, this can be for as much as three orders of magnitude. similarly, the accel- erated mm algorithm converges much faster than the classical mm algorithm. in the reminder of this section we highlight the importance of carefully changing the scale of the parameter vector in each iteration, as done in our accelerated mm algorithm, algo- rithm , as otherwise the monotonic conver- gence may not be guaranteed or the conver- gence may be slow. to demonstrate this, we examine the alternative change of scale such that the parameter vector w in each iteration satises n i= ewi = we present the results for the gifgif datasets. from figure , we observe that the algorithm does not guar- antee a monotonic increase of the a posteri- ori probability with the number of iterations, which is unlike to our accelerated mm algo- rithm for which this always holds. in table , we show the same quantities as in table but for the mm algorithm with the alterna- tive change of scale under consideration. we observe that our acceleration method can con- verge much faster, and that there are cases for which the alternative change of scale results in no convergence within a bound on the maxi- mum number of iterations.",
                "Subsections": [],
                "Groundtruth": "The experimental results section evaluated the convergence time of an algorithm based on the number of iterations needed until a convergence criteria is met. The standard convergence criteria was used, and the convergence time was defined as the smallest integer where a specified condition was satisfied. Different datasets and algorithms were tested, with observations showing that changing the scale of the parameter vector in each iteration could impact convergence time. Results also demonstrated that an accelerated version of the algorithm significantly reduced convergence time compared to the standard algorithm. Additionally, the importance of careful parameter scaling for achieving fast and guaranteed convergence was highlighted."
            }
        ],
        "Groundtruth": "The section presents evaluations of convergence times of MM algorithms for various generalized Bradley-Terry models across real-world datasets. The aim is to empirically validate the properties of the datasets. The study covers different models like GIFGIF and chess, derived hypotheses from theoretical analysis, and indicates that MM algorithm convergence for MAP estimation is slower than for ML estimation. It also notes that the convergence time increases as the prior distribution parameter decreases, and highlights that an accelerated MM algorithm in Section 6 can significantly reduce convergence time. The code and datasets for reproducing the experiments are available online."
    },
    {
        "Section_Num": "6",
        "Section": "6 Further discussion",
        "Text": "we have shown that for generalized bradley- terry models, gradient descent and mm algo- rithms for the ml estimation problem have a linear convergence with the convergence time bound o/a), where d is the maximum number of observed comparisons per item and a is the algebraic connectiv- ity of the matrix m of the observed counts of item-pair co-occurrences. we have also shown vojnovic, yun and zhou: accelerated mm algorithms for ranking scores inference from comparison data table parameters d and a for matrix m being the adjacency matrix of graph g with n vertices g d a d a complete n n star n circuit \u0000 cos \u0000 n \u0001\u0001 n path \u0010 cos \u0010 n \u0011\u0011 n that for generalized bradley-terry models, gra- dient descent and mm algorithms for the map estimation problem, with the prior product- form distribution with gamma marginal distributions, the convergence time is also lin- ear but with the convergence time bound o/). this bound is shown to be tight. our results identify a slow convergence issue for gradient descent and mm algorithms for the map estimation problem, which occurs for small values of parameter . the small values of parameter correspond to more vague prior distributions. our results identify a disconti- nuity of the convergence time at = , which corresponds to ml estimation. the pro- posed acceleration method for the map esti- mation problem resolves the slow convergence issue, and yields a convergence time that is bounded by the best of what can be achieved for the ml and map estimation problems. our results provide insights into how the observed comparison data affect the rate of convergence of gradient descent and mm algo- rithms. the two key parameters affecting the rate of convergence are d and a. for illustration purposes, in table we show val- ues of d and a for examples of matrix m with - valued entries, which correspond to graph adjacency matrices. we observe that when each distinct pair is compared the same number of times, i.e. for the complete graph case, the convergence time is t = o). for other cases, the convergence time is t = o), for some c we further consider the case of random design matrices where each distinct pair of items is either compared once or not com- pared at all, and this is according to indepen- dent bernoulli random variables with param- eter p across all distinct pairs of items. in other words, the item pair co-occurrence is according to the erd os-r enyi random graph gn,p and m is its adjacency matrix. d corresponds to the maximum degree of gn,p which has been extensively studied, with pre- cise results obtained for different scalings of p with n. in particular, by bollob as , d = pn + o( p pn log) with probability /n provided that p = /n). the algebraic connectivity for erd os-r enyi graphs has been studied as well. by coja-oghlan , a = pn + o( p pn log) with probability o, provided that p = /n). intuitively, if the expected degree np is large enough, d/a = . we can derive an upper bound for the con- vergence time, which depends only on some simple properties of the graph associated with matrix m. let a be the adjacency matrix of a graph g which has edge if, and only if, mi,j > let r = m/m where m = maxi,j mi,j and m = min{mi,j : mi,j > }. let d be the max- imum degree and d be the diameter of g. then, for both gradient descent and mm algo- rithms for the ml estimation, we have the con- vergence time bound : t = odn log). this implies that t = o) for every connected graph g, which follows by trivial facts d n and d n. the bound in follows from the lower bound on the algebraic connectivity of a laplacian matrix /), see theorem in merris .",
        "Subsections": [],
        "Groundtruth": "For generalized Bradley-Terry models, both gradient descent and MM algorithms for ML estimation exhibit linear convergence, with a convergence time bound of O(a/d), where d is the maximum number of comparisons per item and a is the algebraic connectivity of the observed item-pair co-occurrence matrix. The same linear convergence pattern is observed for MAP estimation with a prior product-form distribution using gamma marginal distributions. The convergence time is shown to be tight, with slow convergence issues identified for small parameter values corresponding to vague prior distributions. An acceleration method is proposed to address this slow convergence, leading to a convergence time that rivals the best achievable for ML and MAP problems.\n\nThe convergence rate is influenced by two key parameters, d and a, with a demonstration of how different values affect convergence time using examples of adjacency matrices. The impact of observed comparison data on the convergence rate is explored, considering cases of complete graphs and random design matrices modeled after Erdős-Rényi random graphs. For ML estimation, a convergence time bound of O(dn log n) is derived based on graph properties, emphasizing dependencies on the matrix's associated graph features such as maximum degree and diameter. Overall, the study provides insights into the factors affecting convergence in gradient descent and MM algorithms for both ML and MAP estimation problems."
    },
    {
        "Section_Num": "7",
        "Section": "7 Proofs and Additional Results",
        "Text": "",
        "Subsections": [
            {
                "Section_Num": "7_1",
                "Section": "7.1 Proof of Theorem 3.1",
                "Text": "let x be the output of the gradient descent iter- ation update for input x with step size . if x x and x x, then f f = f ) f f ||f || + ||f || f = f f \u0010 \u0011 ||f || f f \u0010 \u0011 f ) = f ) vojnovic, yun and zhou: accelerated mm algorithms for ranking scores inference from comparison data where the rst inequality is by the assump- tion that f is -smooth on x and the second inequality is by the assumption that f satis- es the -pl inequality on x. taking = /, which minimizes the above bound, establishes the claim of the theorem.",
                "Subsections": [],
                "Groundtruth": "The section proves Theorem 3.1 by showing that if x is the output of a gradient descent iteration with a certain step size, and if specific conditions on x are met, then certain inequalities involving the function f hold true. By setting the step size appropriately, the theorem is established."
            },
            {
                "Section_Num": "7_2",
                "Section": "7.2 Proof of Theorem 3.2",
                "Text": "let x be the output of the mm algorithm iter- ation update for input x. by the facts f g and g g for all z, for any , f f g f g; x) f = f ) f +g; x) f ). now, by the same arguments as in the proof of theorem , if x x and x f x, we have f ) f f ). next, if x x and x f x, g; x) f ) ||f || f ) where the rst inequality is by the smoothness condition on the majorant surrogate function and the second inequality is by the assumption that f satises the pl inequality with parame- ter on x. putting the pieces together, we have f f \u0000 + \u0001 f ). taking = / (which minimizes the factor involving in the last inequality) yields the asserted result. . comparison of theorem with proposition in mairal theorem ) suppose that f is a strongly convex function on x and xis a minimizer of f and that it holds xx. assume that g is a rst-order surrogate function of f on x with parameter > let x be the output of the mm algorithm for input x. then, if x x and x x, then we have f ) f c) f ) where c = \u001a , if > , if proof if g is a rst-order surrogate function on x with parameter , then f f + ||z y|| where x = argminz g. from this, it follows that f min z n f + ||z x||o min a \u001a f x) + a ||x x|| \u001b min a \u001a a f + f + a ||x x|| \u001b where the last inequality is by convexity of f. we have established the following inequal- ity f f min a \u001a f ) + a ||x x|| \u001b . by assumption that f is -strongly convex on x and x x, we have f f ||x x|| it follows that f f min a \u001a a + a \u001b f ). vojnovic, yun and zhou: accelerated mm algorithms for ranking scores inference from comparison data it remains only to note that min a \u001a a + a \u001b = c. the rate of convergence bound derived from theorem can be tighter than the rate of con- vergence bound derived from theorem . to show this consider the bradley-terry model for which we have shown in lemma that the surrogate function of the log- likelihood function satises condition of the- orem on n with = ed. it also holds that surrogate function is also a rst- order surrogate function of on n with = ed. hence in this case, we have = the convergence rate bound of theorem is tighter than the convergence rate bound of theorem if and only if + < since = , this is equivalent to < since by lemma we can take = d, the latter condition reads as < e which indeed holds true.",
                "Subsections": [],
                "Groundtruth": "The proof of Theorem 3.2 shows that under certain conditions on the majorant surrogate function, a specific inequality can be derived, leading to the final asserted result. Additionally, a comparison between Theorem 3.2 and a proposition in Mairal Theorem is presented, highlighting the implications for strongly convex functions and first-order surrogate functions in the context of mm algorithms. The text concludes by discussing the rate of convergence bounds derived from the theorems and presents a comparison scenario using the Bradley-Terry model, showcasing when one bound may be tighter than the other based on specific conditions."
            },
            {
                "Section_Num": "7_4",
                "Section": "7.4 Proof of Lemma 3.1",
                "Text": "the hessian of the negative log-likelihood function has the following elements: )i,j = ( v=i mi,v ewi ewv , if i = j mi,j ewi ewj , if i = j. we will show that for all i = j, wiwj ) cmi,j for all w n and mi,j wiwj ) for all w irn. from , we have ) clm for all w n. hence, for all w n and x x , x)x c||x|| where x = {x irn : x = }. this shows that is c-strongly convex on x . from , we have lm ) for all w irn. hence, for all x irn, x)x n||x|| this shows that is n-smooth on irn. it remains to show that and hold. for , we need to show that c xixj/(xi + xj) for all x n. note that xixj/(xi + xj) = z where z := xi/. note that z := [e/, e/(e + e)] for all x n. the function z achieves its minimum over the interval at a boundary of . thus, it holds minzz( z) = c. for , we can immediately note that for all w irn, wiwj = wi wi + wj \u0012 wi wi + wj \u0013",
                "Subsections": [],
                "Groundtruth": "The Hessian of the negative log-likelihood function shows specific elements based on different conditions. It is proven that the function is c-strongly convex and n-smooth within certain ranges. The proof demonstrates that specific conditions related to elements and ratios hold true for given sets of variables within the function."
            },
            {
                "Section_Num": "7_5",
                "Section": "7.5 Proof of Lemma 3.3",
                "Text": "let y be an arbitrary vector in n. let r = for x n. then, we have r = ,xr = , and xr = ) + a where a is a n n diagonal matrix with diag- onal elements ai,i = ji mi,j exi eyi + eyj e||m||. since ) is a positive semi-denite matrix and a is a diagonal matrix, for all x, y n and w n, we have x xrx ||m|| e ||x|| = ||x|| by limited taylor expansion, for all x n, r r + xr + min a xry; y) = min a xry) ||x y|| by the denition of r, we have ||x y|| vojnovic, yun and zhou: accelerated mm algorithms for ranking scores inference from comparison data . surrogate function for the bradley-terry model is a rst-order surrogate function we show that the surrogate function of the log-likelihood function of the bradley-terry model, given by , is a rst-order surro- gate function on x = n with = ed. we need to show that the error function h = is a -smooth function on x. by a straightforward calculus, we note h = + d where d is a diagonal matrix with diago- nal elements du = j=u mu,j exu eyu + eyj . we can take = max x,yx max{|)|,|n)|}. for any a = b + d where b is a n n matrix and d is a n n diagonal matrix with diagonal elements d, d, . . . , dn, we have + min u du i n + max u du. it thus follows that max x,yx max{|)| + min u du|,|n) + max u du|}. now note that for all x, y x, d ) n) = and e min u ju mu,j min u du max u du ed. we have |n) + max u du| = max u du ed and |) + min u du| = ) + min u du) i)+minu du +) min u du) i)+minu du< min u du i)+minu du ) i)+minu du< ed i)+minu du + d i)+minu du< ed.",
                "Subsections": [],
                "Groundtruth": "In section 7.5, the text discusses the proof of Lemma 3.3 involving vectors and matrices in a mathematical framework. It introduces various calculations and manipulations involving diagonal matrices, limited Taylor expansion, and error functions. The text also mentions the accelerated MM algorithms for ranking scores inference and the surrogate function for the Bradley-Terry model. Lastly, it delves into demonstrating that the error function is a -smooth function on x in the context of the Bradley-Terry model's log-likelihood function."
            },
            {
                "Section_Num": "7_7",
                "Section": "7.7 Proof of Lemma 3.4",
                "Text": "we consider the log-a posteriori probability function = + + const where is the log-likelihood function given by and is the prior log-likelihood function given by . note that ) is a diag- onal matrix with diagonal elements equal to ewi, for i = ,, . . . , n. it can be readily shown that for w w, clm + ein ) lm + ein. from , for all w w and x irn, x)x ||x|| = e||x|| hence, is e-strongly convex on w. similarly, from , for all w w, and x irn, x)x n( lm + ein)||x|| (n( lm) + n)||x|| = ( n + e)||x|| hence, is -smooth on w with = n + e.",
                "Subsections": [],
                "Groundtruth": "The log-a posteriori probability function considered includes the log-likelihood function and the prior log-likelihood function. The function is shown to be e-strongly convex and -smooth on a specific set, with specific properties demonstrated for the matrix involved in the calculations."
            },
            {
                "Section_Num": "7_8",
                "Section": "7.8 The asymptote in Section 3.4",
                "Text": "we consider the case of two items, compared m times. suppose that the observed data is such that the number of comparisons won by items and are d and d, respectively. vojnovic, yun and zhou: accelerated mm algorithms for ranking scores inference from comparison data the mm algorithm iterates w are such that ew i = di + m + s s, for i = , where s = ew + ew . observe that s evolves according to the following autonomous nonlinear dynamical system: s = m + m + s s. the limit point of s as t goes to innity is /. note that / is the mode of gamma. let us dene a by s = ( + a). note that a goes to as t goes to innity. by a tedious but straightforward calculus, we can show that ) = log)). from , note that /a evolves accord- ing to a linear dynamical system, which allows us to derive the solution for a in the explicit form given as follows: a = s \u0012 + m \u0013t . from , ) = )( + o) for large t, and thus ) = \u0010 s \u0011 \u0010 + m \u0011t ). it follows that the rate of convergence of the log-a posteriori probability function is given as follows: lim t ) ) = \u0012 + m \u0013 .",
                "Subsections": [],
                "Groundtruth": "The text discusses the asymptotic behavior of a nonlinear dynamical system in the context of ranking scores inference from comparison data. It presents the evolution of a parameter 's' and its limit point as 't' approaches infinity. By defining a new parameter 'a', the text shows that 'a' approaches a specific value as 't' goes to infinity. Through calculations, it is demonstrated that a logarithmic function evolves according to a linear dynamical system, allowing for the derivation of an explicit solution. The rate of convergence of the log-a posteriori probability function is determined as 't' approaches infinity."
            },
            {
                "Section_Num": "7_9",
                "Section": "7.9 Proof of Theorem 4.1",
                "Text": "since f ) f for all x irn, f ) = f f ))) f f )). by the same steps as those in the proof of the- orem , we can show that f f )) f \u0012 \u0013 ) f ). hence, it follows that f ) f \u0012 \u0013 ) f ).",
                "Subsections": [],
                "Groundtruth": "The text provides a proof for Theorem 4.1 by showing that if f is a continuous function and f(x) equals f(f(x)) for all x in R^n, then f(x) is equal to f(f(f(x))). This is demonstrated through a series of steps similar to the proof of the theorem, resulting in the conclusion that f(x) equals f(f(f(x)))."
            },
            {
                "Section_Num": "7_10",
                "Section": "7.10 Proof of Lemma 4.1",
                "Text": "by a limited taylor expansion, for any x, y irn, we have f f + f + mina f x). let u = and v = pd where pd = i ||d|| dd. notice that u + v = y x, and u and v are orthogonal, i.e., uv = from now on, assume that x and y are such that x, y x and y = x. by denition of x, we have df = , which together with u + v = xx, implies f = f v. now, note that for any a , we have the following relations: f x) = f x) ax vax v \u0012 min y:dy= yax y ||y|| \u0013 ||v|| ||v|| vojnovic, yun and zhou: accelerated mm algorithms for ranking scores inference from comparison data where is by assumption and is by assumption that ax is a positive semidef- inite matrix and . hence, we have shown that, for all a , f x) ||v|| next, note that f v + ||v|| min zirn \u0012 f z + ||z|| \u0013 ||f || combining with -, we obtain f f ||f ||",
                "Subsections": [],
                "Groundtruth": "For any x, y in R^n, a limited Taylor expansion shows that f(x + y) = f(x) + f'(x) + min_a [f''(x) a]. Let u = x and v = ρd where ρd = ||d||d. Noting that u + v = y - x and u and v are orthogonal, we assume x and y satisfy certain conditions. Using the definitions, we derive f = f'(x) v. Further relations show that for all a, f(x) ≤ f(x) + a ||v||. By combining these relations, it is proven that ||f(x)|| ≤ f'(x) ||v||. Additionally, considering f(v) + ||v|| ≤ min_z [f(z) + ||z||] ||f||, we deduce f' f ≤ ||f||."
            },
            {
                "Section_Num": "7_11",
                "Section": "7.11 Proof of Lemma 4.2",
                "Text": "proof of since = ) for all w irn, we have that ) is equivalent to ) now, note ) = ) = nc ec n i= ewi + n i= ewi = n i= ewi ! \u0012 n n i= ewi c ec + \u0013 = n i= ewi ! ec \u0000c + ec\u0001 where the last inequality holds by the fact x + ex for all x ir. proof of indeed, = + it is readily checked that = for all w irn. we next show that ) = for all w irn. note that wi = ewi for i = ,, . . . , n. hence, = n n i= ewi. now, by denition of the mapping given by and , for all w irn, ) = n ec n i= ewi =",
                "Subsections": [],
                "Groundtruth": "The text provides a proof of Lemma 4.2, showing that a certain equation is equivalent. It demonstrates that a specific expression equals another expression by manipulating variables and utilizing the properties of addition. The proof confirms that the two expressions are equal for all values of the variables involved."
            },
            {
                "Section_Num": "7_12",
                "Section": "7.12 Proof of Lemma 7.1",
                "Text": "let ti,j be the number of paired comparisons in the input data with tie outcome for items i and j. note that ti,j = tj,i. the log-likelihood func- tion can be written as follows: = n i= j=i di,j ) + n i= j=i ti,j log + log \u0001 . let di,j be the number of paired comparisons of items i and j such that i j, i.e., di,j = di,j + ti,j. by a straightforward calculus, we can write = n i= j=i di,j ) + n i= ti,j log. now, we note when i = j, wiwj ) = di,j ewiewj dj,i ewiewj and w i ) = j=i wuwj ). for any i = j, it indeed holds ewiewj hence, when i = j, wiwj ) ( di,j + dj,i) mi,j. it follows that lm ) for all w irn. hence, x)x n for all x irn. vojnovic, yun and zhou: accelerated mm algorithms for ranking scores inference from comparison data this implies that is a n-smooth func- tion on irn. on the other hand, we can show that for all w n, ewiewj := c,. this can be noted as follows. let z = ewj/. note that ewiewj = z and that z := . the function z is convex and thus achieves its minimum value over the interval at one of its boundary points. it can be read- ily checked that the minimum is achieved at z= /, which yields z = c,. hence, when i = j, wiwj ) c,( di,j + dj,i) c,mi,j. it follows that ) c,lm. from this, we have that for all w n and x x , x)x c, where x = {x irn : ||x|| and x = }. this implies that is c,-strongly convex on x .",
                "Subsections": [],
                "Groundtruth": "The text discusses the proof of Lemma 7.1, focusing on defining the log-likelihood function based on paired comparisons in input data. It highlights the relationship between paired comparisons, tie outcomes, and calculus concepts to establish properties of the function. The proof demonstrates that the function is n-smooth on IR^n and c,-strongly convex on X, providing insights into its behavior and characteristics under different conditions."
            },
            {
                "Section_Num": "7_13",
                "Section": "7.13 Proof of Lemma 7.5",
                "Text": "it can be easily shown that for all w n, s n such that |s| , and u, v s such that u = v, we have e |s| ewuewv e |s| . combining with , we have wuwv ) yt dy wuwv ( k j= e wyj) u,v{y,y,,yk} e k yt du,v{y,y,,yk} = e k mu,v. from this it follows that for all x irn such that x = , x)x e k ||x|| similarly, we have wuwv ) yt dy k l= wuwv ( k j=l e wyj) u,v{y,y,,yk} e k l= mu,v = e k l= l mu,v e \u0012 + z k dx x \u0013 mu,v = e \u0012 k \u0013 mu,v. from this it follows that for all x, x)x e \u0012 k \u0013 n||x|| . derivation of the convergence time bound first note that ma m ma where the inequalities hold elementwise. from this, it follows that lm mla and mla lm, where recall a is the adjacency matrix induced by matrix m. now, note d = ||m|| md and a = m where d is the maximum degree of a node in graph g. hence, we have d a rd by theorem in merris , for any graph g with adjacency matrix a and diameter d, /). vojnovic, yun and zhou: accelerated mm algorithms for ranking scores inference from comparison data it thus follows that d a rddn which implies the convergence time bound t = odn log).",
                "Subsections": [],
                "Groundtruth": "Lemma 7.5 proof demonstrates that for certain values of w, n, s, u, and v, a specific mathematical equality holds. By combining these equations, it leads to important conclusions regarding the magnitude of various expressions. Furthermore, the text provides insights into convergence time bounds for ranking scores inference based on comparison data, showcasing the relationship between graph properties and the speed of convergence in specific algorithms."
            }
        ],
        "Groundtruth": ""
    },
    {
        "Section_Num": "Abstract",
        "Section": "Abstract",
        "Text": "arxiv:v jan sterile neutrino. a short introduction. dmitry v. naumov, joint institute for nuclear research abstract. this is a pedagogical introduction to the main concepts of the sterile neutrino - a hypothetical particle, coined to resolve some anomalies in neutrino data and retain consistency with observed widths of the w and z bosons. we briey review existing anomalies and the oscillation parameters that best de- scribe these data. we discuss in more detail how sterile neutrinos can be observed, as well as the consequences of its possible existence. in particular, we pay attention to a possible loss of coherence in a model of neutrino oscillations with sterile neutrinos, where this eect might be of a major importance with respect to the model. the current status of searches for a sterile neutrino state is also briey reviewed.",
        "Subsections": [],
        "Groundtruth": "The text provides a pedagogical introduction to the concept of the sterile neutrino, a theoretical particle proposed to address anomalies in neutrino data and maintain consistency with observed widths of W and Z bosons. It discusses existing anomalies, oscillation parameters, potential methods for observing sterile neutrinos, and the consequences of their existence. The text highlights the importance of coherence loss in a model of neutrino oscillations with sterile neutrinos, emphasizing its significance. Additionally, it briefly reviews the current status of searches for a sterile neutrino state."
    },
    {
        "Section_Num": "1",
        "Section": "1 Introduction",
        "Text": "there are three generations of leptons in the standard model e ! l , ! l , ! l grouped in sul doublets. the sub-index l indicates that the quantum elds i and are eigenstates of the pl = left-handed helicity operator. the elds i and have denite masses and they obey the dirac equation. there are special linear combinations of i elds known as avor neutrinos e = v , where v is unitary pontecorvo-maki-nakagawa-sakata matrix . the elds of avor neutrino have denite lepton numbers l, . if the masses of i are all dierent, the avor neutrino eld does not obey the dirac equation, nor is the lepton number is conserved. therefore, there is not much sense in using the avor neutrino elds, as they are not fundamental objects of the sm. respectively, they can be abandoned in any consideration. e-mail: dnaumov@jinr.ru in the sm the interactions of leptons with the w-boson mix all generations of the former, as can be seen from the corresponding lagrangian term l = g x =e,, x i=,, viiplw + h.c., where vi is the matrix element of the pmns matrix v, i, , w are quantum elds of the neutrino with mass mi, lepton of avor and w-boson, respectively and is a dirac matrix. the smallness of the masses of neutrinos and their mixture in interactions with w-boson and charged leptons from dierent generations give rise to a spectacular quantum eect ob- served at macroscopic scales oscillation of lepton avor, or neutrino oscillation . this eect manifests itself as a quasi-periodic probability to observe charged leptons and in, respectively, the source and the detector of neutrino. simplifying the consideration to only two neutrino types and , the corresponding probability in the plane wave model reads p = sin sin ml p , , , p = sin sin ml p . in eq. is the mixing angle, parameterizing the pmns matrix v in eq. , m = m m , l is the distance between the source and the detector and p is the absolute value of neutrino three-momentum. in a two-neutrino model the oscillation probability is strictly periodic as a function of l/p. the corresponding oscillation length losc = p/m = km p mev ev m is of macroscopic magnitude for observed m and typical neutrino momenta. the plane-wave model of neutrino oscillation being used elsewhere is not self-consistent and leads to a number of paradoxes . a consistent model adopts wave-packets. the oscillation probability in eq. is modied and in the wave-packet model it depends on three more parameters having dimension of length coherence, dispersion lengths and spatial size x of the neutrino wave-packet . the interference term in eq. is suppressed by e \u0010 + \u0011 e \u0010 x/losc\u0011 factors, where lcoh = p pm , ld = p pm , x = p . one consequence of the wave-packet model of neutrino oscillation is that for l lcoh the neutrino is not in a coherent superposition and the probability of observation p depends neither on distance, nor on momentum. for any realistic assumption about p neutrinos traveling astrophysical distances are incoherent. real analyses of neutrino oscillation data use a three-neutrino model. analyses of solar, atmospheric, reactor and accelerator neutrino data yield m ev, |m | ev",
        "Subsections": [],
        "Groundtruth": "The text discusses the three generations of leptons in the Standard Model, specifically focusing on flavor neutrinos and their interactions. The mixing of neutrinos with different masses leads to neutrino oscillation, observed as a periodic probability of detecting charged leptons and neutrinos. The discussion moves from a simplified two-neutrino model to a more accurate wave-packet model, introducing coherence and dispersion lengths as additional parameters affecting oscillation probabilities. Realistic analyses use a three-neutrino model, with observed neutrino masses in the range of a few eV."
    },
    {
        "Section_Num": "2",
        "Section": "2 Anomalies in neutrino data",
        "Text": "the motivation for proposing a sterile neutrino state is driven by the existence of some anoma- lies in neutrino data which cannot be described by a three-neutrino model with values of m and |m | given by eq. . there are two groups of the corresponding anomalies seen as appearance and disappear- ance of neutrinos.",
        "Subsections": [
            {
                "Section_Num": "2_1",
                "Section": "2.1 Appearance and disappearance data",
                "Text": "appearance data include lsnd and miniboone observations. lsnd observed an excess of e in their study of decays + e+ + e + of positively charged muons at rest . the mean neutrino energy was about mev. the statistical signicance of the excess was about three standard deviations. miniboone, with a beam having neutrino energy of mev and the same l/p as in lsnd, observed an excess of e and e with a signicance of about standard deviations . disappearance data include an about % decit of e and e from calibration sources of sage and gallex experiments and a % decit of e from reactors when compared to a calculation . we note that none of these anomalies show a distinct l/p oscillation dependence pre- dicted by eq. .",
                "Subsections": [],
                "Groundtruth": "LSND and MiniBooNE observations show an excess of electron neutrinos in their studies. LSND observed an excess in the decay of positively charged muons at rest, with a mean neutrino energy around MeV and a statistical significance of about three standard deviations. MiniBooNE, with similar conditions to LSND, also observed an excess of electron neutrinos and antineutrinos at a significance level of about standard deviations. Disappearance data include about a 98% deficit of electron neutrinos and antineutrinos from calibration sources in SAGE and GALLEX experiments, and a 95% deficit of electron neutrinos from reactors when compared to theoretical calculations. None of these anomalies exhibit a clear oscillation dependence as predicted by the relevant equation.\n"
            },
            {
                "Section_Num": "2_2",
                "Section": "2.2 Interpretation of anomalies within the hypothesis of neutrino oscillation",
                "Text": "if these anomalies are interpreted as due to neutrino oscillation then m ev and sin are best to describe these observations. the required m ev does not t into the framework with measured values of m given by eq. . a straightforward extension of the lepton sector of the sm by adding an additional doublet of leptons is impossible because of the measured widths of w = mev and of z bosons = mev. each new neutrino from the sul doublet adds to the widths of w and z the following contributions mev and mev which are much larger than the experimental uncertainties in eqs. and . these are essential, minimally required ingredients to review the concept of sterile neu- trino.",
                "Subsections": [],
                "Groundtruth": "The anomalies indicating neutrino oscillation can be best described using the parameters m_ev and sin. However, the required m_ev value does not align with the measured m values from equations. Adding an additional doublet of leptons to the Standard Model (SM) to explain these anomalies is not feasible due to the measured widths of W and Z bosons. The additional neutrinos from the doublet would significantly increase the widths of W and Z bosons beyond the experimental uncertainties, necessitating a review of the concept of sterile neutrinos."
            }
        ],
        "Groundtruth": "The section discusses anomalies in neutrino data that have led to the proposal of a sterile neutrino state. These anomalies cannot be explained by the conventional three-neutrino model and manifest as anomalies related to both the appearance and disappearance of neutrinos."
    },
    {
        "Section_Num": "3",
        "Section": "3 Concept of Sterile Neutrino",
        "Text": "masses of fermions in the sm the main trick to create a sterile neutrino state is to add to the sm a neutrino eld without adding the fourth left-handed elds of leptons. in order to explain it, let us recall how the masses of fermions appear in the sm. we simplify our consideration by examining only the dirac-type particles in order to keep the analysis simple and clear. a fundamental idea of the sm is a requirement of gauge invariance of the lagrangian under a particular symmetry, sulu. all fermions in the sm are massless because the corresponding mass-term is not gauge invariant. to illustrate this statement let us consider rst the mass-term of only one generation of charged leptons m \u0010 lr + h.c. \u0011 equation is not gauge invariant because the left-handed l and right-handed r elds have dierent gauge symmetries: sulu and u, respectively. the masses of fermions are acquired via the so-called yukawa interactions of a scalar eld with two elds of fermions. originally, this interaction was proposed to explain an attractive potential of two nucleons interacting with each other by exchange of a scalar massive eld the pion. in the sm the same terminology is used to describe interactions of fermions with the only fundamental scalar eld of the model the higgs eld . the yukawa term in the lagrangian for charged leptons reads ly = \u0010 l, l \u0011 ! r + h.c. the replacement v + , where v is a non-zero vacuum expectation value of the higgs eld, generates the mass term in eq. with m = v and a term describing an interaction of fermions with an excitation of the higgs eld above vev. the masses of the neutrinos are generated in a similar way replacing ! by c ! , where c is a charge-conjugated higgs eld, and replacing r by r. the right-handed neu- trino eld r does not interact with w and z, and thus is often called sterile. this is not the sterile eld one needs in interpretations of anomalies in neutrino data mentioned in sec. in order to generalize our consideration of the case of generations of leptons one should add three right-handed elds for charged leptons er, r, r and three right-handed elds for neutrinos er, r, r. the most general yukawa lagrangian gives non-diagonal terms for charged leptons and neutrinos m \u0010 lr + h.c. \u0011 m \u0010 lr + h.c. \u0011 . mand m are non-diagonal matrices with matrix elements m and m , respectively. both terms in eq. must be diagonalized in order to be interpreted as mass-terms. the diagonalization can be done with the help of four matrices u l, u r, u l and u r, rotating l, r, l and r, respectively, where t = \u0010 e, , \u0011 and t = \u0010 e, , \u0011 . the rotation matrices make u l mu r and u l mu r diagonal. as a result, the elds with denite masses and belonging to dierent sul doublets mix in their interactions with w boson as shown in eq. . the corresponding mixing pmns-matrix reads v = u l u l. this matrix should not be attributed, as done often in the literature, to neutrino elds. instead, v is a mixing matrix of both charged leptons and neutrinos. the reviewed mass-generation mechanism used elds for three charged leptons and three neutrinos.",
        "Subsections": [
            {
                "Section_Num": "3_2",
                "Section": "3.2 Four right-handed neutrinos and three left-handed doublets",
                "Text": "the sterile neutrino emerges if there are four right-handed neutrino and still three left-handed doublets. let us label the fourth right-handed neutrino eld as s r. assuming that this new eld interacts with the higgs and left-handed neutrino elds in yukawa interactions, one arrives at non-diagonal terms like in eq. in which m is replaced by a matrix and an additional fourth left-handed eld is constructed as \u0010 s r \u0011c. making the diagonalization, one nds that mixing matrix v is replaced by a matrix v k u m, ! in which the dimension of each sub-block is displayed. the unitarity of this matrix yields the following relationships vv + uu = , kk + mm = , vv + kk = , uu + mm = , vk + um = , vu + km = grouping avor neutrino elds into f = \u0010 e, , \u0011t and massive neutrino elds into m = t, the relationship in eq. is generalized as f l (s r)c ! = v k u m ! m l l ! . the left-handed neutrino eld s l (s r)c being made of a sterile right-handed eld, remains sterile in terms of interactions with w and z-bosons. at the same time elds m l and l do interact with w and z bosons. what can be said about widths of w and z bosons in the presence of new elds l and s l? the corresponding amplitude, assuming coherence of the sterile neutrino state and us- ing eq. , reads for w aw(s + w ) = x i= u iviaw i + m kaw aw = , where aw i , aw are interaction amplitudes for massive neutrinos and aw is the corresponding amplitude assuming zero neutrino mass. similarly, one can show that for the z boson, within the same assumptions, the amplitudes read az = x j u jaz ji + m az i az \u0010 v h vu + kmi\u0011 i = , az = x j u jaz j + m az az \u0010 k h vu + kmi\u0011 = equations and show that the interaction amplitudes of a sterile neutrino state with w and z are both vanishing. therefore, decays of w and z involving sterile neutrinos should also vanish in the end. we made an important assumption that a coherent superposition of neutrino states |iand |making up the sterile state |s= p i= u i|i+ m |can be produced. as mentioned in section use of states with denite momentum is an approximation which fails in describing neutrino oscillation phenomenon. a more consistent approach is based on using the wave-packet model which necessarily imposes a non-zero incoherency in the production of states with dierent masses. the term suppressing the coherence of neutrino states is the second term in eq. . the coherence is suppressed if the wave-packet spatial dimension x is comparable to or larger than the oscillation length losc. let us note that in the plane wave model x = . what happens to w and z widths if the |sstate cannot be produced as a coherent su- perposition of |iand |states? let us focus only on the z-boson decay width. one can observe that the diagonal in the sm vertex g cos w x =e,, ,l,lz = g cos w x i= i,li,lz of z-boson interaction with neutrino is no longer diagonal in an extension of the sm with sterile neutrinos. keeping only essential factors, this vertex now reads x x i, j \u0010 vv \u0011 i j i j + x i \u0010 vk \u0011 i i + x i \u0010 kv \u0011 i i + \u0010 kk \u0011 the rst term in eq. suggests that the decay width z i j is proportional to |i j| and therefore, summation over i, j yields x i, j tr \u0010 vvvv \u0011 . a similar calculation can be applied to all other possible nal states. therefore, the width p i, j + p i + p i + of a z-boson decaying into neutrino and anti-neutrino is proportional to tr \u0010 vvvv + vkkv + kvvk + kkkk \u0011 = tr \u0010 vvvv + vvkk + kvvk + kkkk\u0011 = tr \u0010 vv + kk\u0011 = , where for the last equality we used rst unitarity relation in the second line of eq. . thus, according to eq. , the expected width of a z boson decaying into all possible combina- tions of four neutrino and anti-neutrino states is determined by three generations of leptons avoiding inconsistency with the observed decay width of z boson. in order to get a simpler understanding of how this magic happened, it is instructive to consider only one lepton generation, rather than three, for example \u0000e,l, el \u0001. let us now add two right-handed neutrino elds and a yukawa interaction term which should be diago- nalized, similar to considerations with three generations of leptons. instead of eq. one gets e s ! = cos sin sin cos ! ! . the avor part of the lagrangian given by eq. for z boson interaction with e and e is replaced by cos + sin cos + sin therefore, the width of z boson decaying into any of i j, is proportional to cos |{z} + sin cos | {z } + sin cos | {z } + sin |{z} = \u0010 cos + sin \u0011 = thus, we nd that the corresponding decay width is proportional to one, while there are two massive neutrinos and both interacting with z. the original coupling of zee vertex is redistributed over four combinations of two elds conserving the total strength of the interaction as illustrated by eqs. and . let us briey summarize key elements of the concept of sterile neutrinos. assume a disparity between the numbers of neutrino elds interacting and non- interacting directly with w and z bosons in the sm. for example, assuming dirac neu- trinos, there are three left-handed interacting and four right-handed non-interacting with w and z, neutrino elds. assume a mechanism of mixing active and inert neutrino elds in a generally non- diagonal mass-term. an example of such mechanism is the yukawa interaction. other mech- anisms also exist. the sterile neutrino eld emerges after the diagonalization of the mass-term. the sterile eld is a superposition of at least four massive neutrino elds with nearly zero interac- tion amplitude with w and z. this amplitude corresponds to a coherent superposition of all four neutrino states. the widths of w and z bosons decaying into any possible combination of neutrino and anti-neutrino states is proportional to the number of active neutrino elds (three in the sm).",
                "Subsections": [],
                "Groundtruth": "The presence of four right-handed neutrinos and three left-handed doublets leads to the emergence of a sterile neutrino. When interactions with the Higgs and left-handed neutrino fields are considered, non-diagonal terms appear in the equations. The mixing matrix is modified, and the sterile neutrino field does not interact with W and Z-bosons, while other fields do. The decay widths of W and Z bosons involving sterile neutrinos are found to be vanishing. The coherence of neutrino states affects the decay widths, with a focus on the Z-boson decay width. The expected decay width of a Z boson into all possible combinations of neutrino and anti-neutrino states is determined by three generations of leptons. The concept of sterile neutrinos involves a disparity in the numbers of neutrino fields interacting with w and z bosons, mixing active and inert neutrino fields, and the emergence of a sterile neutrino field with nearly zero interaction amplitude with W and Z-bosons."
            },
            {
                "Section_Num": "3_3",
                "Section": "3.3 How sterile neutrino state can be observed",
                "Text": "in neutrino oscillation as a decit of the event rate and as l/p pattern in both appear- ance and disappearance channels. remarkably, these eects are expected for both charged and neutral currents. this is in contrast to neutrino oscillation without the sterile neutrino, in which only charged currents display an oscillatory pattern and the event rates in neutral currents remains unchanged. since the matrix in eq. is unitary, its sub-block v, which is the pmns matrix, must be non-unitary unless k and u non-diagonal sub-blocks are identically zero. in the latter case, the oscillation to a sterile neutrino state is impossible and no event rate decit due to sterile neutrinos can be expected. therefore, a measurement of unitarity of v is a direct probe of the existence of sterile neutrinos. beta-decays of tritium provide measurements of m = x i= |vei|m i / , sensitive to m and to |ve| neutrino-less beta decays of unstable nuclei, if the neutrino is a majorana particle, provide measurements of m = x i v eimi , sensitive to m and to ve in cosmology, is a relativistic degree-of-freedom in primordial plasma, which is an observable. the sterile neutrino state aects big-bang-nucleosynthesis because a larger number of neutrino species means a faster expansion rate of the universe. there are many other ways in which the sterile neutrino impacts the evolution of the universe. for example, an additional eld would add to a contribution of relic neutrinos into the energy density of the universe. this contribution is determined by p i mi.",
                "Subsections": [],
                "Groundtruth": "The sterile neutrino state can be observed in neutrino oscillation through a deficit in event rate and specific patterns in appearance and disappearance channels, affecting both charged and neutral currents. The unitarity of the PMNS matrix is essential for sterile neutrino existence, with non-unitarity indicating the possibility of oscillation to a sterile neutrino state. Measurement methods include beta decays of tritium and neutrino-less beta decays of unstable nuclei. In cosmology, sterile neutrinos impact big-bang nucleosynthesis and contribute to the energy density of the universe."
            },
            {
                "Section_Num": "3_4",
                "Section": "3.4 Loss of coherence for sterile neutrino",
                "Text": "the argument of the second exponential in eq. is responsible for the coherency of neutrino states at production and detection. it is not at all guaranteed that any combination of |i could be coherently produced or detected, while this is usually silently assumed in a plane wave model. these states would be coherent if \u0010 x/losc\u0011 this condition is relativistically invariant, which can be seen writing \u0010 x/losc\u0011 = m m ! , where m = pp can be interpreted as uncertainty of measurement of m thus, the coherency of |istates is possible if the uncertainty in the determination of m is much larger than m this is in agreement with the principles of quantum physics if an intermediate state cannot be determined, one should sum amplitudes with all possible intermediate states, yielding interference terms in the absolute value squared of the total amplitude. consider neutrinos from pion decays. this reaction is important for atmospheric and accelerator neutrinos. since m is a relativistic invariant one can make an estimate of this quantity in the pions rest-frame, taking p = mev and p = = ev, where is the decay width of meson in its rest-frame. therefore, one can estimate m = ev this value is much larger than the observed m given by eq. for a three neutrino model. taking the largest |m | ev, one can see that / \u0010 / \u0011 and the second exponential in eq. is very close to unity, which means no signicant suppression of the interference term. a posteriori, this calculation conrms the assumption of neutrino coherence made in the plane wave model. taking m ev required to explain anomalies in neutrino data briey reviewed in section , one can observe that the factor in eq. is approximately equal to and the second exponential in eq. takes values , sizably aecting the oscillation pattern. for example, for m ev the oscillation amplitude is suppressed by %. correspondingly, an erroneous statement about the mixing angle of sterile neutrinos could be drawn in a plane wave model in which these suppressions are ignored. a careful analysis of neutrino oscillation data requires a wave packet model. as a side remark, let us note that arguments similar to these considerations explain why charged leptons do not oscillate, their m is much lager than m thus, the interference terms vanish .",
                "Subsections": [],
                "Groundtruth": "The section discusses how the coherence of neutrino states is crucial for their production and detection, and how this coherency can be affected by the uncertainty in the determination of mass. In the context of neutrinos from pion decays, it is shown that the uncertainty in mass measurement must be much larger than the mass itself for the coherence of neutrino states to be maintained. In the plane wave model, incorrect assumptions about neutrino coherence could lead to significant errors in interpreting oscillation patterns. A wave packet model is recommended for a more accurate analysis of neutrino oscillation data. Additionally, an explanation is provided for why charged leptons do not oscillate."
            },
            {
                "Section_Num": "3_5",
                "Section": "3.5 Confusions in terminology",
                "Text": "one might be confused by the use of the same terminology sterile neutrino with dierent meaning by physicists working with neutrino oscillations and by cosmologists. for the former sterile neutrino means a coherent superposition of mass eigenstates |i like in our notation |s= p i= u i|i+ m |= p i= v si|i. cosmologists often use sterile neutrino to refer to the fourth state |silently assuming that |v|",
                "Subsections": [],
                "Groundtruth": "Physicists and cosmologists use the term \"sterile neutrino\" with different meanings. Physicists refer to it as a superposition of mass eigenstates, while cosmologists often use it to represent a fourth state. This difference in terminology can lead to confusion for those working with neutrino oscillations and cosmology."
            },
            {
                "Section_Num": "3_6",
                "Section": "3.6 Current status and perspectives",
                "Text": "a world-wide research program is carried out examining the possible existence of a sterile neutrino state. here we very briey review the current status suggesting an interested reader to follow dedicated reviews . in there are several hints in favor of the existence of sterile neutrinos, and there is a bulk of data excluding possible parameter space for this still hypothetical particle. a decit of reactor e with respect to calculations is known now as the reactor antineutrino anomaly. its interpretation as due to e s oscillation with m = ev and |ve| = was addressed by a number of experiments. danss and neos excluded the best-t parameters of oscillation model with sterile neutrino, but their data fa- vors at about condence level m = ev and |ve| . daya bay measured rates of reactor e due to two dominant isotopes u and pu . it was found that the observed decit is caused mainly by a larger model contribution of u, while the ex- pected and measured rates of e due to pu are found to be consistent. a model with sterile neutrinos suggesting equal decit for any nuclear isotope is excluded by daya bay data at . appearance data from lsnd and miniboone , interpreted as due to ster- ile neutrino oscillation, is in strong tension with recent disappearance data from mi- nos/minos+ , nova , icecube . the compatibility of appearance and dis- appearance datasets is less than . therefore, the sterile neutrino interpretation of lsnd and miniboone anomalies is unlikely. cosmology provides other strong constraints on the existence of sterile neutrino. after a large number of collisions in early plasma the coherence of massive neutrinos would be lost and all four neutrino species i, will be in a thermal equilibrium if |v| is not vanishingly small. an additional relativistic degree of freedom is disfavored by constraints from big bang nucleosynthesis and from recombination epoch . the sum of neu- trino masses p i mi is signicantly constrained by the cosmic microwave background and structure formation data, which disfavor extra neutrino species with masses larger than ev.",
                "Subsections": [],
                "Groundtruth": "A world-wide research program is investigating the possible existence of a sterile neutrino state. Current evidence suggests hints in favor of sterile neutrinos, with data excluding certain parameter spaces for this hypothetical particle. The reactor antineutrino anomaly, attributed to neutrino oscillation, has been studied by various experiments. Data from experiments like Daya Bay indicate discrepancies in rates of reactor antineutrinos due to different isotopes. Appearance data from LSND and MiniBooNE supporting sterile neutrino oscillation is in tension with recent disappearance data. Cosmological constraints also weigh against the presence of sterile neutrinos, with neutrino mass sums and structure formation data disfavoring additional neutrino species with masses above a certain threshold."
            }
        ],
        "Groundtruth": "The concept of sterile neutrino involves adding a neutrino field without introducing additional left-handed fields of leptons in the Standard Model (SM). Fermion masses in the SM are generated through yukawa interactions with the Higgs field. Masses of neutrinos are similarly generated by replacing certain terms in the yukawa interaction. A right-handed neutrino field that does not interact with W and Z bosons is referred to as sterile. The discussion extends to multiple generations of leptons by adding additional right-handed fields. Diagonalization of matrices is required to interpret mass terms, facilitated by rotation matrices. The mixing matrix applies to both charged leptons and neutrinos, not just neutrinos."
    },
    {
        "Section_Num": "4",
        "Section": "4 Summary",
        "Text": "we reviewed main concepts of sterile neutrinos a yet hypothetical particle, coined to resolve some anomalies in neutrino data. in the framework of an extension of the sm, a sterile neutrino eld is a superposition of elds of massive neutrinos. the corresponding interaction amplitude of the sterile neutrino state vanishes if states of massive neutrinos are coherent. even in the case when massive neutrinos are in incoherent mixture, the widths of decays of w i and z i j, where i, j , are proportional to the number of active neutrino species , thus avoiding inconsistencies with observations. this elegant theoretical construction appears when there is a disparity between active and inert numbers of neutrino elds in an extension of the sm. currently, there are data consistent with sterile neutrino hypothesis at about with m = ev and |ve| . on the other hand there is a bulk of accelerator and atmo- spheric data strongly disfavoring lsnd and miniboone anomalies as due to sterile neutrino oscillation. finally, cosmology also disfavors a fourth neutrino with ev mass and |v| the only currently available domain of parameters for sterile neutrino hypothesis of reactor e data. still, the allowed parameter space is within the sensitivity region of currently running experiments .",
        "Subsections": [],
        "Groundtruth": "Sterile neutrinos are hypothetical particles proposed to address anomalies in neutrino data. In the context of an extension of the Standard Model, a sterile neutrino field is a combination of fields of massive neutrinos. The interaction amplitude of the sterile neutrino state becomes zero when the states of massive neutrinos are coherent. Decays of certain particles are influenced by the number of active neutrino species, matching observations. The theoretical framework arises from a difference in the numbers of active and inert neutrino fields in the extended Standard Model. Data partially support the existence of sterile neutrinos with a mass of around 1 eV and specific parameters, while other data contradict LSND and MiniBooNE anomalies caused by sterile neutrino oscillations. Additionally, cosmological findings do not align with a fourth neutrino with a 1 eV mass, leaving the reactor data domain as the only compatible range for sterile neutrino hypotheses. However, current experiments are sensitive to exploring the allowed parameter space."
    },
    {
        "Section_Num": "5",
        "Section": "5 Acknowledgments",
        "Text": "we sincerely thank v. a. naumov and c. t. kullenberg for reading the manuscript and mak- ing important comments. references b. pontecorvo, sov. phys. jetp , . z. maki, m. nakagawa and s. sakata, high-energy physics. proceedings, th interna- tional conference, ichep, geneva, switzerland, jul -, , - y. fukuda et al. , phys. rev. lett. , doi:/physrevlett. . m. h. ahn et al. , phys. rev. lett. , doi:/physrevlett. . q. r. ahmad et al. , phys. rev. lett. , doi:/physrevlett. . q. r. ahmad et al. , phys. rev. lett. , doi:/physrevlett. . s. abe et al. , phys. rev. lett. , doi:/physrevlett. ]. f. p. an et al. , phys. rev. lett. , doi:/physrevlett. ]. f. p. an et al. , phys. rev. d , no. , doi:/physrevd. ]. e. k. akhmedov and a. y. smirnov, phys. atom. nucl. , doi:/s ]. d. v. naumov and v. a. naumov, j. phys. g , doi:/- /// ]. f. p. an et al. , eur. phys. j. c , no. , doi:/epjc/s---y ]. a. aguilar-arevalo et al. , phys. rev. d , doi:/physrevd. . a. a. aguilar-arevalo et al. , phys. rev. lett. , no. , doi:/physrevlett. ]. j. n. abdurashitov et al. , phys. rev. c , doi:/physrevc. . j. n. abdurashitov et al., phys. rev. c , doi:/physrevc. . j. n. bahcall, p. i. krastev and e. lisi, phys. lett. b , doi:/- -w . p. huber, phys. rev. c , erratum: doi:/physrevc., /physrevc. [arxiv: ]. t. a. mueller et al., phys. rev. c , doi:/physrevc. ]. e. k. akhmedov, jhep , doi:/-/// ]. m. dentler, . hernndez-cabezudo, j. kopp, p. a. n. machado, m. maltoni, i. martinez-soler and t. schwetz, jhep , doi:/jhep ]. s. gariazzo, c. giunti, m. laveder and y. f. li, jhep , doi:/jhep ]. i. alekseev et al. , phys. lett. b , doi:/j.physletb.. ]. y. j. ko et al. , phys. rev. lett. , no. , doi:/physrevlett. ]. f. p. an et al. , phys. rev. lett. , no. , doi:/physrevlett. ]. d. adey et al. , arxiv: . p. adamson et al. , arxiv: . p. adamson et al. , phys. rev. d , no. , doi:/physrevd. ]. m. g. aartsen et al. , phys. rev. lett. , no. , doi:/physrevlett. ]. r. h. cyburt, b. d. fields, k. a. olive and t. h. yeh, rev. mod. phys. , doi:/revmodphys. ]. y. akrami et al. , arxiv: .",
        "Subsections": [],
        "Groundtruth": "The authors express gratitude towards V.A. Naumov and C.T. Kullenberg for their valuable feedback on the manuscript. Additionally, various references are mentioned throughout the text, including works by Pontecorvo, Maki, Nakagawa, and Sakata, as well as contributions from numerous researchers and publications in the field of high-energy physics."
    },
    {
        "Section_Num": "Abstract",
        "Section": "Abstract",
        "Text": "arxiv:v jan on fusible rings m. tamer kos an, jerzy matczuk abstract. we answer in negative two of questions posed in . we also establish a new characterization of semiprime left goldie rings by showing that a semiprime ring r is left goldie iit is regular left fusible and has nite left goldie dimension.",
        "Subsections": [],
        "Groundtruth": "In this text, the authors provide answers to two questions posed in a previous study. They also introduce a new characterization of semiprime left Goldie rings. The authors show that a semiprime ring R is left Goldie if it is regular, left fusible, and has finite left Goldie dimension."
    },
    {
        "Section_Num": "1",
        "Section": "1. Introduction",
        "Text": "throughout this paper, r denotes an associative ring with unity. for a nonempty subset s r, lannr stands for the left annihilator of a in r, i.e. lannr = {r r | aa = }. an element a r is a left zero-divisor if lannr = elements which are not left zero-divisor are called left regular. ghashghaei and mcgovern in introduced and investigated fusible rings. a nonzero element r of a ring r is called left fusible if r can be presented in a form r = c+w, where c is a left zero-divisor and w is left regular. in such situation we will say that r = c + w is a left fusible decomposition of r. our ring r is called left fusible if every nonzero element of r is left fusible. let us point out that our notation diers from the one introduced in left zero divisors are right zero divisors in the meaning of . thus our left fusible rings are right fusible in the language of . clearly both domains and clean rings are examples of fusible rings . it was proved in that polynomial rings and matrix rings over left fusible rings are left fusible. the aim of the paper is to continue a study of fusible and related rings. in particular, we answer in negative questions and posed in . we also introduce regular left fusible rings, a class which is slightly wider than that of left fusible rings and discuss relations between those two classes. as a result a new description of semiprime left goldie rings is given in theorem . we also prove that the regular left fusible ring property lifts to matrix rings for rings having left quotient rings. some questions are formulated. date: january , mathematics subject classication. n, u key words and phrases. fusible ring, the classical quotient ring, goldie ring . kos an, matczuk",
        "Subsections": [],
        "Groundtruth": "The text discusses fusible rings in the context of associative rings with unity. A left fusible ring is defined as a ring where every nonzero element can be decomposed into a left zero-divisor and a left regular element. The study aims to explore fusible and related rings, addressing questions posed in previous work. Regular left fusible rings, a broader class than left fusible rings, are introduced and their relationship is discussed. The paper offers a new description of semiprime left goldie rings and proves that the regular left fusible ring property extends to matrix rings for rings with left quotient rings. Some questions for further exploration are also presented."
    },
    {
        "Section_Num": "2",
        "Section": "2. Results",
        "Text": "we begin with the following denition. denition . a ring r is said to be regular left fusible if for any nonzero element r r there exists a regular element s r such that the element sr is left fusible, i.e. sr = c + w, where c is a left zero divisor and w is left regular. since every left regular element is left fusible, our denition reduces to the requirement that for every nonzero left zero divisor a there exists a regular element s in r such that the element sa is left fusible. it is also clear that every left fusible ring is regular left fusible (it is enough to take s = ). the following example shows that the class of left fusible rings is strictly smaller than the one of regular left fusible rings. example . let f be a eld and set r = f < x, y | x = >. by , the set of all left zero divisors of r is equal to xr. in particular, a dierence of any two left zero divisors is a left zero divisor as well. thus r is not left fusible. notice that y is a regular element of r and yr = + yr is a left fusible decomposition of yr, for any nonzero element r r. thus r is a regular left fusible ring. similarly, one can see that r is not right fusible but it is regular right fusible. we will use the above example to answer question of . before doing so let us recall some notions. a ring r is a left p.p. ring if any principal left ideal of r is projective and r is said to be left p.q.-baer if the left annihilators of principal left ideals are generated as left ideals by an idempotent. birkenmeier et al. in proved that a ring r is an abelian left p.p. ring if and only if r is a reduced p.q.-baer ring. it was proved in [, corollary ] that every abelian left p.p. ring is fusible and that, in general, p.q.-baer rings do not have to be left fusible . question of asks whether every abelian p.q.-baer ring is fusible. the following example shows that the answer to this question is negative. example . let r = f < x, y | x = > be the ring dened in example . since elements from yr are left regular, we get ayb = , for any nonzero a, b r. thus r is a prime ring, so it is also a p.q.-baer ring. the only idempotents of r are and (so it is abelian). this can be checked directly or one can apply a classical result of herstein . by , the set of all nilpotent elements of r is equal to xrx. the subring f is invariant under all inner automorphisms adjoint to elements of the form , n xrx but f is neither contained in the center nor contains a nonzero ideal of r. therefore, by , the ring r does not contain nontrivial idempotents. thus r is an abelian p.q.-baer ring. we have already seen in example that r is neither left nor right fusible. example also suggests that although abelian p.q.-baer rings need not be fusible but maybe they have to be regular fusible. we are unable to answer the following question: question . is every abelian left p.q.-baer ring regular left fusible? on fusible rings now we will move to some basic properties of regular left fusible rings. recall that a left ideal i of a ring r is essential if a i = for every nonzero left ideal a of r. lemma . let a, b r be such that lannr <e rr and lannr = then lannr = in particular, every element element a such that lannr <e rr can not be left fusible. proof. the imposed assumptions imply that lannr lannr = , so the thesis is clear. the left singular ideal of a ring r is singl = {x r | lannr <e rr}. it is well known that singl is a two-sided ideal of r. a ring r is called left nonsingular if singl = the following lemma generalizes . lemma . every regular left fusible ring r is left nonsingular. proof. assume = a singl. since r is regular left fusible, we can pick a regular element s r such that sa is left fusible. since singl is a two-sided ideal, sa singl. thus, by lemma , sa is not left fusible, a contradiction. let s denote the left ore set consisting of regular element of the ring r and let sr be the left ore localization of r. writing ql we will mean that r possesses a classical left quotient ring which is equal to ql. lemma . let s s and a r. then lannsr = if and only if lannr = proof. suppose bsa = , for some nonzero b sr. eventually multiplying on the left by a suitable element from s, we may assume = b r. since s is the left ore set, there exists t s and = c r such that bs = tc. then = c lannr. suppose ca = , for some = c r. then sa = , so sa is a left zero divisor in sr. proposition . let r be a ring. if r is left fusible, then so is sr. if r is regular left fusible, then ql is left fusible. if sr is regular left fusible, then r is regular left fusible. suppose every regular element of r is invertible. then r is regular left fusible i r is left fusible. proof. let sr sr. as r is left fusible, r has a left fusible decomposition, say r = a + w. lemma shows that sr = sa + sw is a left fusible decomposition of sr. suppose r is regular left fusible. let = sr ql. then there exists a regular element t r such that tr has a fusible decomposition tr = a + w. by lemma , sr = a + w is a fusible decomposition of sr in ql. kos an, matczuk let r r. since sr is regular left fusible, there exists a regular element s t sr such that the element z = str has a left fusible decomposition, say str = sa + sw taking the left common denominator, we may write z = str = sa + sw for suitable s s, t, a, w r. by lemma , t is regular, w is left regular and a is a left zero divisor of r. this implies that the element tr is left fusible in r and hence r is regular left fusible. by assumption, r = ql. let r r. suppose r is regular left fusible. there exists a regular element t r such that tr has a fusible decomposition tr = a + w. then, by lemma , r = ta + tw is a fusible decomposition of r. this yields the proof of . the above proposition gives immediately the following: corollary . suppose r has the left quotient ring ql. the following conditions are equivalent: r is regular left fusible. ql is left fusible. ql is regular left fusible. in , the authors observed that the statement of proposition holds when r is a commutative ring and asked whether the reverse implication holds. in the context of corollary , their question can be restated as: question . let r be a regular left fusible ring having the classical left quotient ring. is r left fusible? notice that, although the ring r in example is not left fusible, the ring r is regular left fusible. thus the above question has a negative answer without the assumption that r possesses the classical left quotient ring. using the same arguments as in and making use of the fact that in a commutative ring r an element r is nilpotent if and only if the ideal rr is nilpotent we get the following modication of . lemma . every commutative regular fusible ring is reduced. in , the authors showed that a commutative ring with only nitely many minimal prime ideals is fusible if and only if it is reduced. using this characterization we obtain a positive, partial answer to question : proposition . let r be a commutative ring with only nitely many minimal prime ideals. the following conditions are equivalent: r is fusible. r is regular fusible. r is reduced. proof. the implication is a tautology. the implications and are given by lemma and , respectively. on fusible rings oers a commutative ring which is reduced but is not fusible, i.e. the equivalence does not hold for arbitrary commutative rings. the ring from this example is also not regular fusible. we do not know of any example of a commutative ring which is regular fusible but not fusible. a ring r is called unit-regular if for each a r there exists a unit u u such that aua = a. in , the following proposition was proved. we oer a short direct argument. proposition . every unit-regular ring is left fusible. proof. let r be a unit-regular ring and r r, i.e., = r = rur where u r. then e = ur is an idempotent and e = + is its fusible decomposition. this shows that r is regular left fusible. now, since every regular element in r is invertible, the thesis is a direct consequence of proposition . proposition and corollary enable us to give the following new characterization of semiprime left goldie rings: theorem . for a semiprime ring r, the following conditions are equivalent: r is left goldie. r is regular left fusible and has nite left goldie dimension proof. suppose r is left goldie. then, by goldies theorem, ql is a semisimple artinian ring. in particular it has nite left goldie dimension and it is a unit-regular ring. thus, by proposition , ql is left fusible. now corollary yields that r is regular left fusible. this completes the proof of . let r be as in . then, by lemma , r is left nonsingular. now the implication is a consequence of goldies theorem. recall that a ring r a right complemented, if for each a r, there is a b r such that ab = and a + b is regular . by , every right complemented ring is left fusible. asks whether a right complemented ring have always possesses a right classical quotient ring? since any domain is complemented on both sides and there are domains having no a classical right quotient ring, the answer to this question is no in general. however the above theorem gives immediately the following: corollary . every right complemented ring of nite left goldie dimension has a classical left quotient ring. proof. as observed in , every right complemented ring is reduced, so it is semiprime. now the thesis is a consequence of the left version of theorem . one can easily adopt the proof of to get the following: proposition . suppose that for any nite set r, . . . , rn r\\{} there exists a regular element s r such that the elements sr, . . . , srn are left fusible. then the matrix ring mn is regular left fusible. with the help of the above proposition we get: kos an, matczuk theorem . suppose r is a regular left fusible ring having the classical left quotient ring. then the matrix ring mn is regular left fusible. proof. by corollary , ql is left fusible and proposition shows that mn) is regular left fusible. let us notice that mn) = smn, where s denotes the set of all diagonal matrices diag, where s ranges over all regular elements of r. now the thesis is a direct consequence of proposition . let r be the ring from example . then y r is regular and, for any = r r, yr is left regular, so r satises the assumptions of proposition . therefore we have: example . let r be the ring from example . then the matrix ring mn is regular fusible, for any n we were unable to answer the following: question . is the matrix ring mn over a regular left fusible ring r itself regular left fusible? we close the paper with an observation that regular left fusible property behaves well under polynomial extensions. essentially the same proof as in gives the following proposition . if r is regular left fusible, then so are the rings r, r and r], where is an automorphism of r. references g. f. birkenmeier, j. y. kim, j. k. park: principally quasi-baer rings, comm. algebra , - . m. chebotar, p. h. lee, e. r. puczylowski: on prime rings with commuting nilpotent elements, proc. ams , - p. m. cohn: prime rings with involution whose symmetric zero-divisors are nilpotent, proc. ams , - e. ghashghaei, w. wm. mcgovern: fusible rings, comm. algebra, , - i. n. herstein: a theorem on invariant subrings, j. algebra , - . department of mathematics, gazi university, ankara, turkey e-mail address: tkosan@gmail.com department of mathematics, university of warsaw, ul. banacha , - warsaw, poland e-mail address: jmatczuk@mimuw.edu.pl",
        "Subsections": [],
        "Groundtruth": "The text discusses the concept of regular left fusible rings in algebra. It introduces the definition of regular left fusible rings and provides examples to illustrate the concept. The text also explores related properties such as left p.p. rings, left q.q.-baer rings, and abelian p.q.-baer rings. It presents questions related to the fusibility of rings and provides propositions, lemmas, and theorems to characterize and analyze different types of rings. Additionally, the text mentions unit-regular rings, semiprime left Goldie rings, and right complemented rings. It concludes with observations on the behavior of regular left fusible property under polynomial extensions."
    },
    {
        "Section_Num": "References",
        "Section": "References",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "Abstract",
        "Section": "Abstract",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "Occupation",
        "Section": "Occupation time statistics of a gas of interacting diffusing particles",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "Abstract",
        "Section": "Abstract",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "Hexagonal",
        "Section": "Hexagonal MASnI3 exhibiting strong absorption of ultraviolet photons",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "Abstract",
        "Section": "Abstract",
        "Text": "., , . intel xeon phi knights landing .. , .. . - , - : , , . - intel xeon phi knights landing , . . - openmp. : . , - intel xeon phi knl. . , - . : , , , openmp, intel xeon phi, knights landing, .",
        "Subsections": [],
        "Groundtruth": "The text discusses the Intel Xeon Phi Knights Landing processor, emphasizing its features such as high parallelism and performance. It highlights the use of OpenMP for programming on the Knights Landing platform and mentions the importance of optimizing applications for Intel Xeon Phi KNL architecture."
    },
    {
        "Section_Num": "1",
        "Section": "1 Введение.",
        "Text": "- : , , . . , - . - , . , - . , , - , , , . hotsax . hotsax . , , , , . - , - , . , , , ; , e-mail: apolyakov@naumen.ru - , - , . , , , ; ..-.., , e-mail: mzym@susu.ru arxiv:v jan intel xeon phi knights landing , , phidd . . hotsax, - . phidd. , . , .",
        "Subsections": [],
        "Groundtruth": "The text introduces the topic of using the hotsax and phidd algorithms on Intel Xeon Phi Knights Landing processor. It provides contact information for the authors for further inquiry. The focus is on experimentation with these algorithms on the mentioned processor to optimize performance."
    },
    {
        "Section_Num": "2",
        "Section": "2 Постановка задачи.",
        "Text": "",
        "Subsections": [
            {
                "Section_Num": "2_1",
                "Section": "2.1 Формальные определения и нотация.",
                "Text": "- . t : t = , ti \\in \\bbbr . m | t | . ti, n t - t n , i: ti, n = , \\leqslant n \\leqslant m, \\leqslant i \\leqslant m - n + t, n, n = m - n + ti, n tj, n t , | i - j | \\geqslant n. , c, mc. dist : \\bbbr n \\times \\bbbr n ightarrow \\bbbr , (\\forall x = ti, n, y = tj, n dist = dist = dist ). d t , \\forall c, mc \\in t min) > min). , , - . x y n t - : ed = \\sqrt{} n \\sum i=",
                "Subsections": [],
                "Groundtruth": "The section introduces formal definitions and notations related to distance metrics and computational calculations. It defines variables and functions such as t, ti, m, n, dist, c, and mc, and discusses properties of these elements within the context of distance calculations in a mathematical formulation. Key concepts include defining distances between points in n-dimensional space and computation of distances based on specific criteria."
            },
            {
                "Section_Num": "2_2",
                "Section": "2.2 Последовательный алгоритм.",
                "Text": "hotsax - z- . z- t \\^ t = (\\^ t, . . . , \\^ tm), : \\^ ti = ti - \\mu \\sigma , \\leqslant i \\leqslant m; \\mu = m m \\sum i= ti; \\sigma = m m \\sum i= t i - \\mu , - (paa, piesewise aggregate approximation) . - - c = c = , w \\leqslant n : ci = w n \\cdot n w \\cdot j \\sum j= n w \\cdot + cj. paa- . - c = \\^ c = (\\^ c, \\^ c, . . . , \\^ cw), . \\scra = , | \\scra | (\\alpha =a, \\alpha =b ..). \\^ ci = \\alpha i \\leftrightarrow \\beta j - \\leqslant \\^ ci < \\beta j. \\beta i , - \\scrb = , \\beta = - \\infty \\beta | \\scra | = +\\infty , n \\beta i \\beta i+ | \\scra | . . , . , . ( sn t t, n). discorddiscovery : for all ci \\in sn t do : min \\leftarrow \\infty : for all cj \\in sn t and | i - j| \\geqslant n do : dist \\leftarrow ed : if dist < distbsf then : break : if dist < min then : min \\leftarrow dist : if min > distbsf then : distbsf \\leftarrow min : posbsf \\leftarrow i : return \\{ posbsf, distbsf\\}",
                "Subsections": [],
                "Groundtruth": "The hotsax algorithm involves a method to compute PAA (piecewise aggregate approximation) and uses data aggregation techniques. It defines calculations for PAA coefficient and signal representation as well as determining discord patterns. The algorithm aims to discover discord patterns within a given dataset by iteratively computing distances and identifying the minimum distance points."
            }
        ],
        "Groundtruth": ""
    },
    {
        "Section_Num": "3",
        "Section": "3 Параллельный алгоритм поиска диссонансов PhiDD",
        "Text": "",
        "Subsections": [
            {
                "Section_Num": "3_1",
                "Section": "3.1 Проектирование алгоритма",
                "Text": "- . phidd - . sn t \\in \\bbbr n\\times : sn t := \\~ ti+j - . intel xeon phi (.. , ), w. n w, - . - pad = w - , \\~ ti,n : \\~ ti,n = \\left\\{ ti, ti+, . . . , ti+n - , , , . . . , \\underbrace{} \\underbrace{} pad , if n mod w > ti, ti+, . . . , ti+n - , otherwise. , - . , , , , , . paan, w t \\in \\bbbr n\\times w paa- , . sax-, saxn, \\scra t \\in \\bbbn n\\times w, \\scra , . cand \\in \\bbbn n sn t , sax- saxn, \\scra t : candi = k \\leftrightarrow freq = min \\leqslant j\\leqslant n freq \\forall i < j candi < candj. freq \\in \\bbbn n , saxn, \\scra t : freqi = k \\leftrightarrow | \\{ j : saxn, \\scra t = saxn, \\scra t \\} | = k. , . , w\\scra \\in \\bbbn dictsize\\times w, w, - \\scra . dictsize \\scra w , dictsize = \\= aw | \\scra | =| \\scra | w. \\scra , , . . . , | \\scra | . , ( ) . - h : \\bbbn w \\leftarrow \\{ , , . . . , dictsize\\} , : h = w+ \\sum j= aj \\cdot ww - j - , w = | \\scra | = , , - , . . \\scra sax- indexw ord \\in \\bbbn dictsize\\times n: indexw ord = k \\leftrightarrow w\\scra = saxn, \\scra t . , .",
                "Subsections": [],
                "Groundtruth": "The text discusses algorithm design for a specific technical application involving computation and data manipulation. It includes equations and explanations related to the algorithm's structure, calculations, and computational requirements. Key elements include data organization, frequency calculations, and dictionary size considerations for efficient processing."
            },
            {
                "Section_Num": "3_2",
                "Section": "3.2 Реализация алгоритма",
                "Text": "phidd . , - . - . openmp . . . , w, \\scra . paa- . phidd \\triangleleft : sn t \\leftarrow znormalize(sn t ) : w\\scra \\leftarrow makewordmatrix : paan, w t \\leftarrow paa(sn t , w) : saxn, \\scra t \\leftarrow sax(paan, w t , \\scra ) : cand \\leftarrow makecandidates(saxn, \\scra t ) : indexw ord \\leftarrow makeindexword(saxn, \\scra t ) \\triangleleft : \\{ posbsf, distbsf\\} \\leftarrow potentialdiscord : \\{ posbsf, distbsf\\} \\leftarrow refinediscord : return \\{ posbsf, distbsf\\} #pragma omp parallel for openmp, . sax- paa-, . paa-, , #pragma omp parallel for. sax- - cand . freq cand sax-, . freq - #pragma omp parallel for reduction, . indexw ord, - . sax-. -, . - , -, sax-. sax- #pragma omp parallel for openmp, . . - , , , , . - - . , . . cand - , , , . , - . . . - indexw ord, - potentialdiscord : for all ci \\in cand do : min \\leftarrow \\infty : for all cj \\in indexw ord(saxn, \\scra t ) and | i - j| \\geqslant n do : dist \\leftarrow ed : if dist < distbsf then : break : if dist < min then : min \\leftarrow dist : #pragma omp parallel for schedule : for all cj / \\in indexw ord(saxn, \\scra t ) and | i - j| \\geqslant n do : dist \\leftarrow ed : if dist < distbsf then : break : if min > distbsf then : distbsf \\leftarrow min : posbsf \\leftarrow i : return \\{ distbsf, posbsf\\} , sax-. , . , , , - . - , . bsfdist, - . , , , , . , , - . , . - , . . , - . . - openmp #pragma omp parallel for. , - schedule , . , , .",
                "Subsections": [],
                "Groundtruth": "The text details the implementation of an algorithm using OpenMP for parallel processing. It involves operations such as data normalization, matrix creation, feature extraction, candidate generation, and distance calculation. Utilizing parallel for loops and reduction techniques for optimization, the algorithm aims to identify potential anomalies or discord in the data. The algorithm involves a series of steps including potential discord identification, refined discord analysis, and final output generation of the most significant discord. The parallel processing techniques like OpenMP play a key role in improving the efficiency of the algorithm."
            }
        ],
        "Groundtruth": ""
    },
    {
        "Section_Num": "4",
        "Section": "4 Вычислительные эксперименты.",
        "Text": "",
        "Subsections": [
            {
                "Section_Num": "4_1",
                "Section": "4.1 Цели, аппаратная платформа и наборы данных экспериментов",
                "Text": "- . refinediscord : #pragma omp parallel for schedule : for all ci / \\in cand do : min \\leftarrow \\infty : for all cj \\in indexw ord(saxn, \\scra t ) and | i - j| \\geqslant n do : dist \\leftarrow ed : if dist < distbsf then : break : if dist < min then : min \\leftarrow dist : for all cj / \\in indexw ord(saxn, \\scra t ) and | i - j| \\geqslant n do : dist \\leftarrow ed : if dist < distbsf then : break : if dist < min then : min \\leftarrow dist : if min > distbsf then : distbsf \\leftarrow min : posbsf \\leftarrow i : return \\{ \\sqrt{} distbsf, posbsf\\} - , . : intel xeon intel xeon phi e-v e-v sex - . \\times \\times \\times \\times \\times \\times - . , vpu, -, tflops , . pdd . : | t| = m n scd-m , , , , scd-m , , , , phidd. - . - (, , .) , . , k , - s = t tk e = s k , t tk k . n . phidd - hotsax pdd .",
                "Subsections": [],
                "Groundtruth": "The section outlines a code snippet that utilizes OpenMP for parallel programming and describes a specific algorithm for calculating distances between data points. The code snippet includes a loop structure with pragma directives and variables such as dist, min, and distbsf. The hardware platform mentioned is Intel Xeon Phi E5. The text also refers to the performance capabilities of the hardware platform in terms of TFLOPS. Additionally, there is mention of sets of experimental data used in the context of the algorithm."
            },
            {
                "Section_Num": "4_2",
                "Section": "4.2 Результаты экспериментов",
                "Text": "- intel xeon phi knights landing . . n = n = n = n = n = ) , % n = n = n = n = n = ) . : phidd scd-m , phidd % , , , intel xeon phi. n = n = n = n = n = ) , % n = n = n = n = n = ) . : phidd scd - m , , , . - m n , . , . % n = , n = % . - intel xeon phi. , phi knl phi knc xxeon_v xxeon_v ) scd-m , phi knl phi knc xxeon_v xxeon_v ) scd-m . : phidd . , phidd , , intel xeon phi, intel xeon. knights landing; knights corner - intel xeon n \\leqslant . . intel xeon phi knl - intel xeon . : phidd , hotsax pdd phidd hotsax cpu intel xeon phi intel core i pdd phidd cpu scd-m \\times \\cdot \\times \\cdot \\times scd-m \\geqslant \\geqslant \\geqslant \\cdot \\times \\geqslant \\cdot \\times phidd . - pdd . hotsax . intel core i , pdd phidd hotsax. phidd - , , pdd. , phidd .",
                "Subsections": [],
                "Groundtruth": "The section presents experimental results related to Intel Xeon Phi Knights Landing performance metrics. The text includes data on various configurations and comparisons involving Phi KNL, Phi KNC, Xeon V processors, and different performance measurements like Phidd and SCD-M. It discusses the performance of Intel Xeon Phi processors in comparison to other models such as Intel Xeon and Intel Core i."
            }
        ],
        "Groundtruth": ""
    },
    {
        "Section_Num": "5",
        "Section": "5 Заключение.",
        "Text": ". - . : , - , . intel xeon phi knights landing , , phidd. , phidd. ( --), . ( a.) . . .., .. . .: -, - . : . url: http://www.sscc.icmmg.nsc.ru/hardware.html. bacon d.f., graham s.l., sharp o.j. compiler transformations for high-performance computing. acm comput. surv. , no. p. doi: /. huang t., zhu y., mao y. et al. parallel discord discovery // proc. of the th pacific-asia conference on advances in knowledge discovery and data mining, pakdd , auckland, new zealand, april , p. doi: /----_ keogh e., lin j., fu a. hot sax: efficiently finding the most unusual time series subsequence // proc. of the th ieee int. conf. on data mining, icdm, houston, texas, november , p. doi: /icdm.. keogh e.j., lin j., lee s., herle h.v. finding the most unusual time series subsequence: algorithms and applications // knowl. inf. syst. vol. , no. p. doi: /s--- lin j., keogh e.j., lonardi s., chiu b.y. a symbolic representation of time series, with implications for streaming algorithms // proc. of the th acm sigmod workshop on research issues in data mining and knowledge discovery, dmkd , san diego, california, usa, june , p. doi: /. mattson t. s - introduction to openmp // proc. of the acm/ieee sc conf. on high performance networking and computing, november , , tampa, fl, usa. p. doi: /. sodani a. knights landing : nd generation intel xeon phi processor // ieee hot chips th symposium , cupertino, ca, usa, august , ieee, wu y., zhu y., huang t. et al. distributed discord discovery: spark based anomaly detection in time series // proc. of the th ieee int. conf. on high performance computing and communications, hpcc , th ieee int. symp. on cyberspace safety and security, css , and th ieee int. conf. on embedded software and systems, icess , new york, ny, usa, august , p. doi: /hpcc-css- icess.. parallel algorithm for time series discords discovery on the intel xeon phi knights landing many-core processor a.v. polyakov and m.l. zymbler south ural state university, department of system programming; prospekt lenina , chelyabinsk, , russia; master student, e-mail: apolyakov@naumen.ru south ural state university, department of system programming; prospekt lenina , chelyabinsk, , russia; cand. sci., associate professor, e-mail: mzym@susu.ru received abstract: discord is a refinement of the concept of anomalous subsequence of a time series. the task of discords discovery is applied in a wide range of subject domains related to time series: medicine, economics, climate modeling, etc. in this paper, we propose a novel parallel algorithm for discords discovery for the intel xeon phi knights landing many-core systems for the case when input data fit in main memory. the algorithm exploits the ability to independently calculate euclidean distances between the subsequences of the time series. computations are paralleled through openmp technology. the algorithm consists of two stages, namely precomputations and discovery. at the precomputations stage, we construct the auxiliary matrix data structures, which ensure efficient vectorization of computations on intel xeon phi knl. at the discovery stage, the algorithm finds discord based upon the structures above. experimental evaluation confirms the high scalability of the developed algorithm. keywords: time series, discords discovery, parallel algorithm, openmp, intel xeon phi, knights landing, data layout, vectorization. references v. v. voevodin, and vl. v. voevodin, the parallel computing. . hardware specifications of the siberian supercomputing center. accessed: . url: http://www.sscc.icmmg.nsc.ru/hardware.html. d. f. bacon, s. l. graham, and o. j. sharp, compiler transformations for high-performance computing, acm comput. surv. , . doi: /. t. huang, y. zhu, y. mao et al., parallel discord discovery, in proc. of the th pacific-asia conference on advances in knowledge discovery and data mining, pakdd , auckland, new zealand, april , , pp. doi: /----_ e. keogh, j. lin, and a. fu, hot sax: efficiently finding the most unusual time series subsequence, in proc. of the th ieee int. conf. on data mining, icdm, houston, texas, november , , pp. doi: /icdm.. e. j. keogh, j. lin, s. lee, and h. v. herle, finding the most unusual time series subsequence: algorithms and applications, knowl. inf. syst. , . doi: /s--- j. lin, e. j. keogh, s. lonardi, and b. y. chiu, a symbolic representation of time series, with implications for streaming algorithms, in proc. of the th acm sigmod workshop on research issues in data mining and knowledge discovery, dmkd , san diego, california, usa, june , , pp. doi: /. t. mattson, introduction to openmp, in proc. of the acm/ieee conf. on supercomputing, tampa, fl, usa, november , , pp. doi: /. a. sodani, knights landing : nd generation intel xeon phi processor, in ieee hot chips th symposium , cupertino, ca, usa, august , , pp. y. wu, y. zhu, t. huang et al., distributed discord discovery: spark based anomaly detection in time series, in proc. of the th ieee int. conf. on high performance computing and communications, hpcc , th ieee int. symp. on cyberspace safety and security, css , and th ieee int. conf. on embedded software and systems, icess , new york, ny, usa, august , , pp. doi: /hpcc-css-icess..",
        "Subsections": [],
        "Groundtruth": "The section concludes with a summary of a novel parallel algorithm for discords discovery on the Intel Xeon Phi Knights Landing many-core processor. This algorithm is designed for cases where the input data can fit in main memory and leverages OpenMP technology for parallel computations. The algorithm has two stages: precomputations and discovery, with a focus on efficient vectorization of computations on the Intel Xeon Phi KNL. Experimental evaluations show high scalability of the algorithm. Key aspects include time series, discords discovery, parallel algorithm, OpenMP, Intel Xeon Phi, Knights Landing, data layout, and vectorization."
    },
    {
        "Section_Num": "Abstract",
        "Section": "Abstract",
        "Text": "arxiv:v nov cluster editing with vertex splitting faisal n. abu-khzam emmanuel arrighi matthias bentert p al grn as drange judith egan serge gaspers alexis shaw peter shaw blair d. sullivan petra wolf abstract cluster editing, also known as correlation clustering, is a well-studied graph mod- ication problem. in this problem, one is given a graph and the task is to perform up to k edge additions or deletions to transform it into a cluster graph, i.e., a graph consisting of a disjoint union of cliques. however, in real-world networks, clusters are often overlapping. for example in social networks, a person might belong to several communitiese.g. those corresponding to work, school, or neighborhood. other strong motivations come from biological network analysis and from language networks. trying to cluster words with similar usage in the latter can be confounded by homonyms, that is, words with multiple meanings like bat. in this paper, we introduce a new variant of cluster editing whereby a vertex can be split into two or more vertices. first used in the context of graph drawing, this operation allows a vertex v to be replaced by two vertices whose combined neighborhood is the neighborhood of v (and thus v can belong to more than one cluster). we call the new problem cluster editing with vertex splitting and we initiate the study of it. we show that it is np-complete and xed-parameter tractable when parameterized by the total number k of allowed vertex- splitting and edge-editing operations. in particular, we obtain an o-time algorithm and a k-vertex kernel. introduction cluster editing is dened as follows. given a graph g and a non-negative integer k, one is asked whether g can be turned into a disjoint union of cliques by a sequence of at most k edge-editing operations . the problem is known to be np-complete since the work of k riv anek and mor avek and it is xed-parameter tractable when parameterized by k, the total number of allowed edge-editing operations . over the last decade, cluster editing has been well studied from both theoretical and practical perspectives . in general, clustering results in a partitioning of the input graph. hence, it forces each vertex to be in one and only one cluster. this can be a limitation when the entity represented by a vertex plays a role in multiple clusters. this situation is recorded in work on gene regulatory networks , where enumeration of maximal cliques was considered a viable alternative to clustering. moreover, such vertices can eectively hide clique-like structures and also greatly increase the computational time required to obtain an optimal solution . in this paper, we introduce a new variant, which we call cluster editing with vertex splitting in an attempt to allow for such overlapping clusters we show the new problem to be np-complete and investigate its parameterized complexity. we obtain a polynomial kernel using the notion of a critical cliques as introduced by lin et al. and applied to cluster editing by guo . this paper is structured as follows: in section , we give some basic denitions and notation used throughout the paper. in section , we prove that our problem is np-complete, even on graphs of bounded maximum degree. in section , we study the order of operations in an optimal solution and section is devoted to critical cliques. in section , we show how to obtain a k-vertex author e-mail addresses: faisal.abukhzam@lau.edu.lb, emmanuel.arrighi@gmail.com, matthias.bentert@uib.no, pal.drange@uib.no, judith.egan@cdu.edu.au, serge.gaspers@unsw.edu.au, alexis.shaw@student.unsw.edu.au, peter- shaw@ojlab.ac.cn, sullivan@cs.utah.edu, and mail@wolfp.net. preliminary versions of parts of this paper have been presented at isco and ipec . a b c d v a b c d v v figure : an illustration of a vertex-split operation. the vertex v is replaced by v and v, with the vertices a, c, and d being adjacent to exactly one of the two vertices and b being adjacent to both. kernel in linear time. section presents a xed-parameter tractable algorithm and we conclude in section with some open problems and future directions. preliminaries for a positive integer n, we use to denote the set {, , . . ., n} of all positive integers up to n. all logarithms in this paper use as their base. we use standard graph-theoretic notation and refer the reader to the textbook by diestel for commonly used denitions. all graphs in this work are simple, unweighted, and undirected. we denote the open and closed neighborhoods of a vertex v by n and n, respectively. for a subset v of vertices in a graph g, we denote by g the subgraph of g induced by v . for an introduction to parameterized complexity, xed-parameter tractability, and kernelization, we refer the reader to the textbooks by flum and grohe , niedermeier , and cygan et al. . the exponential-time hypothesis , formulated by impagliazzo, paturi, and zane , states that there exists some positive real number s such that -sat on n variables and m clauses cannot be solved in s time. a cluster graph is a graph in which the vertex set of each connected component induces a clique. equivalently, a graph is a cluster graph if and only if the graph does not have p as an induced subgraph. problem denition given a graph g = , an edit sequence of length k is a sequence = of k operations, where each ei is one of the following operations: do nothing, add an edge to e, delete an edge from e, and split a vertex, that is, replace a vertex v by two vertices v, v such that nn = n. an example of the splitting operation is given in figure and we call the two new vertices v and v copies of the original vertex v (and if v or v are further split in the future, then the resulting vertices are also called copies of v). we denote the graph resulting from applying an edit sequence to a graph g by g|. cluster editing with vertex splitting is then dened as follows. given a graph g and an integer k, is there an edit sequence of length k such that g| is a cluster graph? n p-hardness in this section, we show that cluster editing with vertex splitting is np-complete. the graph w . one of the three ways of trans- forming w into two ks using six operations. figure : the graph wt requires t edits and any solution with exactly t edits results in a disjoint union of ks. theorem cluster editing with vertex splitting is np-complete. moreover, assuming the exponential-time hypothesis, there is no o-time or o poly-time algorithm for it. proof. since containment in np is obvious (non-deterministically guess the sequence of operations and check that the resulting graph is indeed a cluster graph), we focus on the np-hardness and present a reduction from -sat. therein, we will use two gadgets, a variable gadget and a clause gadget. the variable gadget is a wheel graph with two center vertices. an example of this graph is depicted on the left side of figure we call this graph with t vertices on the outside wt and we will only consider instances with t mod = , that is, t = a for some positive integer a. the clause gadget is a crown graph as depicted in figure more precisely, for each variable xi, we construct a variable gadget gi which is a wa where a is the number of clauses that contain either xi or xi. for each clause cj, we construct a clause gad- get hj as depicted in figure , that is, a k with the edges of a triangle removed. we arbitrarily assign each of the three vertices of degree two in hj to one literal in cj. finally, we connect the vari- able and clause gadgets as follows. if a variable xi appears in a clause cj, then let u be the vertex in hj assigned to xi . moreover, let b be the number such that cj is the bth clause contain- ing either xi or xi and let c = let the vertices on the outer cycle of gi be v, v, . . . , va. if cj contains the literal xi, then we add the three edges {u, vc+}, {u, vc+}, {u, vc+}. if cj contains the literal xi, then we add the three edges {u, vc+}, {u, vc+}, {u, vc+}. to complete the reduction, we set k = m n, where m is the number of clauses and n is the number of variables. we next show that the reduction is correct, that is, the constructed instance of cluster editing with vertex splitting is a yes-instance if and only if the original formula of -sat is satisable. to this end, rst assume that is satisable and let be a satisfying assignment. for each variable xi, we will partition gi into ks as follows. let a be the value such that gi is isomorphic to wa. if sets xi to true, then we remove the edge {vj, vj+} and add the edge {vj+, vj+} for each integer j a . if sets xi to false, then we remove the edge {vj+, vj+} and add the edge {vj+, vj+} for each j a. moreover, we split the two center vertices a times. in total, we use a modications to transform gi into a collection of ks. since each clause contains exactly three literals and we add six vertices for each variable appearance, the sum of lengths of cycles in all variable gadgets combined is m. hence, in all variable gadgets combined, we perform m n modications. next, we modify the crown graphs. to this end, let cj be a clause and let hj be the constructed clause gadget. since is a satisfying assignment, at least one variable appearing in cj satises it. if y x z the crown graph y x z good solution: one added edge and two deleted edges y x z bad solution : three added edges y x z bad solution : two splits and one added edge y x z bad solution : one split, one deleted edge, and one added edge figure : the crown graph with its four solutions of size the good solution is the only solution with three operations that creates at least one isolated vertex. multiple such variables exist, then we pick one arbitrarily. let xi be the selected variable and let u be the vertex in hj assigned to xi. we rst turn hj into a k and an isolated vertex by removing the two edges incident to u in hj and add the missing edge between the two vertices assigned to dierent variables. finally, we look at the edges between variable gadgets and clause gadgets. for the vertex u, note that by construction the three vertices that u is adjacent to in gi already belong to a k and hence we can add two edges to the two centers of the variable gadget to form a k for the two other vertices in hj that have edges to vertices in variable gadgets, we remove all three such edges, that is, six edges per clause. hence, we use ++ = modications for each clause. since the total number of modications is m n and the resulting graph is a collection of ks, ks, and ks, the constructed instance of cluster editing with vertex splitting is a yes-instance. for the other direction, suppose the constructed instance of cluster editing with vertex splitting is a yes-instance. we rst show that m n modications are necessary to transform all variable gadgets into cluster graphs and that this bound can only be achieved if each time exactly three consecutive vertices on the cycles are contained in the same k to this end, consider any variable gadget gi. by construction, gi is isomorphic to wa for some integer a. by the counting argument from above, we show that at least a modications are necessary. note rst that some edge in the cycle has to be removed or some vertex on the cycle has to be split as otherwise any solution would contain a clique with all vertices in the cycle and this would require at least a a > a edge additions (since the degree of each of the a vertices in the cycle would need to increase from to a ). we next analyze how many modications are necessary to separate b vertices from the outer cycle into a clique. we require at least two modications for the center vertices (either splitting them or removing the edges between them and the rst vertex that we want to separate) and one operation to separate the cycle on the other end (either splitting a vertex or removing an edge of the cycle). for b {, } these operations are enough. for b , we need to add \u0000b \u0001 edges (all edges in a clique of size b minus the already existing edges of a path on b vertices). note that the average cost per separated vertex (number of operations divided by b) is minimized with b = with a cost of for three vertices. hence, to separate all but c vertices from the cycle, we require at least / operations. the cost for making the remaining c vertices into a clique requires again \u0000c \u0001 edge additions. analogously, the optimal solution is to have c = with just a single edge addition. thus, the minimum number of required operations is at least / + = a (where the + comes from the initial edge removal and the nal edge addition between the last c = vertices) and this value can only be reached by partitioning the cycle into triples which each form a k with the two center vertices. note that it is always preferable to delete an edge on the outer cycle and not split one of the two incident edges as splitting a vertex increases the number of vertices on the cycle and thus invokes a higher overall cost. next, we analyze the clause gadget and the edges between the dierent gadgets. we start with the latter. let u be a vertex in a clause gadget hj with incident edges to some variable gadget. the only way to not use at least three operations to deal with the three edges is if u is an isolated vertex or if the three neighbors do not have two more neighbors in the current solution. in the former case, we can add the two edges between u and the two centers of the respective variable gadgets to build a k in the latter case, we have used at least three operations more in the variable gadget than intended (either by removing edges between neighbors of u and the center vertices or by splitting all neighbors of u). since each vertex in a variable gadget is only adjacent to at most one vertex in a clause gadget, this cannot lead to an overall reduction in the number of operations and we can therefore ignore this latter case. we are now in a position to argue that at least eleven modications are necessary for each clause gadget. first, note that at least three operations are required to transform a crown into a cluster graph. possible ways of achieving this are depicted in figure in each of these possibilities, at most one vertex becomes an isolated vertex. to make two vertices independent, at least four operations are required and for three isolated vertices, at least ve operations are required. as shown above, at least two operations are required for each isolated vertex with edges to variable gadgets and at least three operations are required for non-isolated vertices with edges to variable gadgets. thus, at least eleven operations are required for each clause gadget and eleven operations are sucient if and only if the three vertices incident to one of the vertices in hj belong to the same k in the variable gadget. by the argument above, at least m n + m = k operations are necessary and since the constructed instance is a yes-instance, there is a way to cover all variable gadgets with ks such that for each clause there is at least one vertex whose three neighbors in a variable gadget belong to the same k let cj be a clause, let u be a vertex with all three neighbors in the same k, and let xi be the variable corresponding to this variable gadget. if xi appears positively in cj, then vi+, vi+, and vi+ belong to the same k for each i and we set xi to true. if xi appears negatively in cj, then vi+, vi+, and vi+ belong to the same k for each i and we set xi to false. note that we never set a variable to both true and false in this way. we set all remaining variables arbitrarily to true or false. by construction, the variable xi satises cj and since we do the same for all clauses, all clauses are satised, that is, the original formula is satisable. thus, the constructed instance is equivalent to the original instance of -sat. since the reduction can clearly be computed in polynomial time, this concludes the proof for the np-hardness. for the eth-based hardness, observe that k, n, m o. this implies that there are no o-time or o poly-time algorithms for cluster editing with vertex splitting unless the eth fails . in contrast to the reduction for cluster editing , our reduction does not produce in- stances with constant maximum degree. we instead observe that in our reduction, the maximum degree of the produced instances depends only on the maximum number of times a variable appears in a clause. combining this with the fact that -sat remains np-hard when restricted to instances where each variable appears in at most four clauses , we obtain the following corollary: corollary cluster editing with vertex splitting remains np-hard on graphs with maximum degree",
        "Subsections": [],
        "Groundtruth": "The text introduces a new variant of the Cluster Editing problem called Cluster Editing with Vertex Splitting, where a vertex can be split into multiple vertices, allowing for overlapping clusters. It proves that this new problem is NP-complete and parameterized tractable when considering the total number of allowed vertex-splitting and edge-editing operations. The paper provides a polynomial kernel and presents a fixed-parameter tractable algorithm. The NP-hardness is established through a reduction from -SAT, showcasing that there is no polynomial-time algorithm unless the Exponential Time Hypothesis fails. The reduction demonstrates that a correct instance of Cluster Editing with Vertex Splitting is equivalent to a satisfiable -SAT formula. The complexity is highlighted, with the necessity of several operations to transform variable and clause gadgets into cluster graphs, emphasizing the challenge of solving this problem efficiently."
    },
    {
        "Section_Num": "Introduction",
        "Section": "Introduction",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "Preliminaries",
        "Section": "Preliminaries",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "NP-hardness",
        "Section": "NP-hardness",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "The",
        "Section": "The Edit-Sequence Approach",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "Critical",
        "Section": "Critical Cliques",
        "Text": "as introduced by lin et al. and applied to cluster editing by guo . this paper is structured as follows: in section , we give some basic denitions and notation used throughout the paper. in section , we prove that our problem is np-complete, even on graphs of bounded maximum degree. in section , we study the order of operations in an optimal solution and section is devoted to critical cliques. in section , we show how to obtain a k-vertex author e-mail addresses: faisal.abukhzam@lau.edu.lb, emmanuel.arrighi@gmail.com, matthias.bentert@uib.no, pal.drange@uib.no, judith.egan@cdu.edu.au, serge.gaspers@unsw.edu.au, alexis.shaw@student.unsw.edu.au, peter- shaw@ojlab.ac.cn, sullivan@cs.utah.edu, and mail@wolfp.net. preliminary versions of parts of this paper have been presented at isco and ipec . a b c d v a b c d v v figure : an illustration of a vertex-split operation. the vertex v is replaced by v and v, with the vertices a, c, and d being adjacent to exactly one of the two vertices and b being adjacent to both. kernel in linear time. section presents a xed-parameter tractable algorithm and we conclude in section with some open problems and future directions. preliminaries for a positive integer n, we use to denote the set {, , . . ., n} of all positive integers up to n. all logarithms in this paper use as their base. we use standard graph-theoretic notation and refer the reader to the textbook by diestel for commonly used denitions. all graphs in this work are simple, unweighted, and undirected. we denote the open and closed neighborhoods of a vertex v by n and n, respectively. for a subset v of vertices in a graph g, we denote by g the subgraph of g induced by v . for an introduction to parameterized complexity, xed-parameter tractability, and kernelization, we refer the reader to the textbooks by flum and grohe , niedermeier , and cygan et al. . the exponential-time hypothesis , formulated by impagliazzo, paturi, and zane , states that there exists some positive real number s such that -sat on n variables and m clauses cannot be solved in s time. a cluster graph is a graph in which the vertex set of each connected component induces a clique. equivalently, a graph is a cluster graph if and only if the graph does not have p as an induced subgraph. problem denition given a graph g = , an edit sequence of length k is a sequence = of k operations, where each ei is one of the following operations: do nothing, add an edge to e, delete an edge from e, and split a vertex, that is, replace a vertex v by two vertices v, v such that nn = n. an example of the splitting operation is given in figure and we call the two new vertices v and v copies of the original vertex v (and if v or v are further split in the future, then the resulting vertices are also called copies of v). we denote the graph resulting from applying an edit sequence to a graph g by g|. cluster editing with vertex splitting is then dened as follows. given a graph g and an integer k, is there an edit sequence of length k such that g| is a cluster graph? n p-hardness in this section, we show that cluster editing with vertex splitting is np-complete. the graph w . one of the three ways of trans- forming w into two ks using six operations. figure : the graph wt requires t edits and any solution with exactly t edits results in a disjoint union of ks. theorem cluster editing with vertex splitting is np-complete. moreover, assuming the exponential-time hypothesis, there is no o-time or o poly-time algorithm for it. proof. since containment in np is obvious (non-deterministically guess the sequence of operations and check that the resulting graph is indeed a cluster graph), we focus on the np-hardness and present a reduction from -sat. therein, we will use two gadgets, a variable gadget and a clause gadget. the variable gadget is a wheel graph with two center vertices. an example of this graph is depicted on the left side of figure we call this graph with t vertices on the outside wt and we will only consider instances with t mod = , that is, t = a for some positive integer a. the clause gadget is a crown graph as depicted in figure more precisely, for each variable xi, we construct a variable gadget gi which is a wa where a is the number of clauses that contain either xi or xi. for each clause cj, we construct a clause gad- get hj as depicted in figure , that is, a k with the edges of a triangle removed. we arbitrarily assign each of the three vertices of degree two in hj to one literal in cj. finally, we connect the vari- able and clause gadgets as follows. if a variable xi appears in a clause cj, then let u be the vertex in hj assigned to xi . moreover, let b be the number such that cj is the bth clause contain- ing either xi or xi and let c = let the vertices on the outer cycle of gi be v, v, . . . , va. if cj contains the literal xi, then we add the three edges {u, vc+}, {u, vc+}, {u, vc+}. if cj contains the literal xi, then we add the three edges {u, vc+}, {u, vc+}, {u, vc+}. to complete the reduction, we set k = m n, where m is the number of clauses and n is the number of variables. we next show that the reduction is correct, that is, the constructed instance of cluster editing with vertex splitting is a yes-instance if and only if the original formula of -sat is satisable. to this end, rst assume that is satisable and let be a satisfying assignment. for each variable xi, we will partition gi into ks as follows. let a be the value such that gi is isomorphic to wa. if sets xi to true, then we remove the edge {vj, vj+} and add the edge {vj+, vj+} for each integer j a . if sets xi to false, then we remove the edge {vj+, vj+} and add the edge {vj+, vj+} for each j a. moreover, we split the two center vertices a times. in total, we use a modications to transform gi into a collection of ks. since each clause contains exactly three literals and we add six vertices for each variable appearance, the sum of lengths of cycles in all variable gadgets combined is m. hence, in all variable gadgets combined, we perform m n modications. next, we modify the crown graphs. to this end, let cj be a clause and let hj be the constructed clause gadget. since is a satisfying assignment, at least one variable appearing in cj satises it. if y x z the crown graph y x z good solution: one added edge and two deleted edges y x z bad solution : three added edges y x z bad solution : two splits and one added edge y x z bad solution : one split, one deleted edge, and one added edge figure : the crown graph with its four solutions of size the good solution is the only solution with three operations that creates at least one isolated vertex. multiple such variables exist, then we pick one arbitrarily. let xi be the selected variable and let u be the vertex in hj assigned to xi. we rst turn hj into a k and an isolated vertex by removing the two edges incident to u in hj and add the missing edge between the two vertices assigned to dierent variables. finally, we look at the edges between variable gadgets and clause gadgets. for the vertex u, note that by construction the three vertices that u is adjacent to in gi already belong to a k and hence we can add two edges to the two centers of the variable gadget to form a k for the two other vertices in hj that have edges to vertices in variable gadgets, we remove all three such edges, that is, six edges per clause. hence, we use ++ = modications for each clause. since the total number of modications is m n and the resulting graph is a collection of ks, ks, and ks, the constructed instance of cluster editing with vertex splitting is a yes-instance. for the other direction, suppose the constructed instance of cluster editing with vertex splitting is a yes-instance. we rst show that m n modications are necessary to transform all variable gadgets into cluster graphs and that this bound can only be achieved if each time exactly three consecutive vertices on the cycles are contained in the same k to this end, consider any variable gadget gi. by construction, gi is isomorphic to wa for some integer a. by the counting argument from above, we show that at least a modications are necessary. note rst that some edge in the cycle has to be removed or some vertex on the cycle has to be split as otherwise any solution would contain a clique with all vertices in the cycle and this would require at least a a > a edge additions (since the degree of each of the a vertices in the cycle would need to increase from to a ). we next analyze how many modications are necessary to separate b vertices from the outer cycle into a clique. we require at least two modications for the center vertices (either splitting them or removing the edges between them and the rst vertex that we want to separate) and one operation to separate the cycle on the other end (either splitting a vertex or removing an edge of the cycle). for b {, } these operations are enough. for b , we need to add \u0000b \u0001 edges (all edges in a clique of size b minus the already existing edges of a path on b vertices). note that the average cost per separated vertex (number of operations divided by b) is minimized with b = with a cost of for three vertices. hence, to separate all but c vertices from the cycle, we require at least / operations. the cost for making the remaining c vertices into a clique requires again \u0000c \u0001 edge additions. analogously, the optimal solution is to have c = with just a single edge addition. thus, the minimum number of required operations is at least / + = a (where the + comes from the initial edge removal and the nal edge addition between the last c = vertices) and this value can only be reached by partitioning the cycle into triples which each form a k with the two center vertices. note that it is always preferable to delete an edge on the outer cycle and not split one of the two incident edges as splitting a vertex increases the number of vertices on the cycle and thus invokes a higher overall cost. next, we analyze the clause gadget and the edges between the dierent gadgets. we start with the latter. let u be a vertex in a clause gadget hj with incident edges to some variable gadget. the only way to not use at least three operations to deal with the three edges is if u is an isolated vertex or if the three neighbors do not have two more neighbors in the current solution. in the former case, we can add the two edges between u and the two centers of the respective variable gadgets to build a k in the latter case, we have used at least three operations more in the variable gadget than intended (either by removing edges between neighbors of u and the center vertices or by splitting all neighbors of u). since each vertex in a variable gadget is only adjacent to at most one vertex in a clause gadget, this cannot lead to an overall reduction in the number of operations and we can therefore ignore this latter case. we are now in a position to argue that at least eleven modications are necessary for each clause gadget. first, note that at least three operations are required to transform a crown into a cluster graph. possible ways of achieving this are depicted in figure in each of these possibilities, at most one vertex becomes an isolated vertex. to make two vertices independent, at least four operations are required and for three isolated vertices, at least ve operations are required. as shown above, at least two operations are required for each isolated vertex with edges to variable gadgets and at least three operations are required for non-isolated vertices with edges to variable gadgets. thus, at least eleven operations are required for each clause gadget and eleven operations are sucient if and only if the three vertices incident to one of the vertices in hj belong to the same k in the variable gadget. by the argument above, at least m n + m = k operations are necessary and since the constructed instance is a yes-instance, there is a way to cover all variable gadgets with ks such that for each clause there is at least one vertex whose three neighbors in a variable gadget belong to the same k let cj be a clause, let u be a vertex with all three neighbors in the same k, and let xi be the variable corresponding to this variable gadget. if xi appears positively in cj, then vi+, vi+, and vi+ belong to the same k for each i and we set xi to true. if xi appears negatively in cj, then vi+, vi+, and vi+ belong to the same k for each i and we set xi to false. note that we never set a variable to both true and false in this way. we set all remaining variables arbitrarily to true or false. by construction, the variable xi satises cj and since we do the same for all clauses, all clauses are satised, that is, the original formula is satisable. thus, the constructed instance is equivalent to the original instance of -sat. since the reduction can clearly be computed in polynomial time, this concludes the proof for the np-hardness. for the eth-based hardness, observe that k, n, m o. this implies that there are no o-time or o poly-time algorithms for cluster editing with vertex splitting unless the eth fails . in contrast to the reduction for cluster editing , our reduction does not produce in- stances with constant maximum degree. we instead observe that in our reduction, the maximum degree of the produced instances depends only on the maximum number of times a variable appears in a clause. combining this with the fact that -sat remains np-hard when restricted to instances where each variable appears in at most four clauses , we obtain the following corollary: corollary cluster editing with vertex splitting remains np-hard on graphs with maximum degree the edit-sequence approach in this section, we show that we can always assume that a solution to cluster editing with vertex splitting has a specic structure, that is, it rst adds edges, then removes edges, and nally splits vertices. we mention that removing edges can also be moved to the front or to the back and that we do not use the fact that we want to reach a cluster graph at any point. the statement therefore holds for any graph-modication problem which only adds edges, deletes edges, and splits vertices. to start, we say that two edit sequences and are equivalent if g| and g| are isomorphic. we show rst that all vertex splittings can be moved to the back of the edit sequence. lemma for any edit sequence = where ei is a vertex splitting and ei+ is an edge addition, an edge deletion, or a do-nothing operation, there is an equivalent edit sequence = (e, e, . . . , ei, e i, e i+, ei+, . . . , ek) of the same length where e i is an edge addition, an edge deletion, or a do-nothing operation and e i+ is a vertex splitting. proof. if ei+ is a do-nothing operation or the edge added or removed by it is not incident to one of the vertices introduced by the vertex split ei, then the edit sequence where e i = ei+ and e i+ = ei is equivalent to and of the same length. so assume without loss of general- ity that ei splits vertex v into v and v and ei+ adds or deletes the edge {v, w} for some vertex w. if ei+ adds the edge {v, w} and the edge {v, w} exists after performing the edit subsequence = , then we set e i to be the do-nothing operation. if ei+ adds the edge {v, w} and the edge {v, w} does not exist after performing , then we let e i add the edge {v, w}. in both cases, we let e i+ = ei with the modication that v is also adjacent to w. note that this is possible as v is adjacent to w after performing the edit sequence (e, e, . . . , ei, e i). if ei+ deletes the edge {v, w}, then we know that the edge {v, w} exists after performing and we can assume without loss of generality that the edge {v, w} does not exists after performing the edit sequence (e, e, . . . , ei, e i, e i+). hence, we let e i remove the edge {v, w} and let e i+ = ei with the modication that v is no longer adjacent to w. in all cases, the graphs reached after performing the edit sequences and (e, e, . . . , ei, e i, e i+) are identical and hence performing the edit sequence = afterwards shows that and are equivalent . we show next that moving edge additions to the front results in an equivalent edit sequence. lemma for any edit sequence = where ei is an edge deletion and ei+ is an edge addition, at least one of the edit sequences = (e, e, . . . , ei, e i, e i+, ei+, . . . , ek) where e i = ei+ and e i+ = ei or both are do-nothing operations is of the same length and equivalent to . proof. note that if ei+ adds the edge that ei removed, then replacing these two operations with do-nothing operations results in the same graph. if ei+ adds a dierent edge than ei removed, then the graphs reached after the edit subsequences = and = are identical. hence, performing the edit sequence = afterwards results in isomorphic graphs for both starting edit sequences. note that performing rst and then is equivalent to performing and performing rst and afterwards is equivalent to performing where e i = ei+ and e i+ = ei. this concludes the proof. we can easily deduce the following theorem from the two above lemmata. theorem for any edit sequence = , there is an edit sequence = (e , e , . . . , e k) such that k k, and are equivalent, e i is not a do-nothing operations for any i . if e i is an edge addition and e j is an edge deletion or a vertex splitting, then i < j, if e i is an edge deletion and e j is a vertex splitting, then i < j, and proof. let = be any edit sequence. if ei is a do-nothing operation for some i , then the edit sequence is equivalent and shorter. hence, we can as- sume that does not contain any do-nothing operations. if there exists a vertex-splitting opera- tion ei and an operation ej with j > such that ej is not a vertex-splitting operation, then let i be the last index of a vertex-splitting operation such that ei+ is not a vertex-splitting operation. by lemma , we can modify into an equivalent edit sequence (e , e , . . . , e k) where e j = ej for all j \\ {i, i + } and ei+ is a vertex-splitting operation and ei is not. if e i is a do- noting operation, we can again remove it. performing this procedure repeatedly until no longer applicable results in an edit sequence that is equivalent to , does not contain any do-nothing operations and all edge-addition and edge-deletion operations come before all vertex-splitting op- erations. if performs an edge-deletion operation ei before an edge-addition operation ej , then there is also an index i such that ei is an edge-deletion operation and ei+ is an edge- addition operation. we can then use lemma to obtain a new sequence in which the number of index pairs with i < j, ei is an edge-deletion operation, and ei is an edge-addition operation is reduced by at least one. if the application of lemma introduces a do-nothing operation, then we again remove it. repeatedly applying lemma nally results in an equivalent sequence where the number of mentioned index pairs is zero, that is, all edge-addition operation come before all edge-deletion operations. note that now satises all requirements of the theorem statement. we refer to an edit sequence satisfying the statement of theorem as an edit sequence in standard form. critical cliques originally introduced by lin et al. , critical cliques provide a useful tool in understanding the clique structure in graphs. denition a critical clique is a subset of vertices c that is maximal with the properties that c is a clique there exists u v s.t. n = u for all v c. equivalently, two vertices u and v belong to the same critical clique if and only if they are true twins, that is, n = n. hence, each vertex v appears in exactly one critical clique. the critical clique quotient graph c of g contains a node for each critical clique in g and two nodes are adjacent if and only if the two respective critical cliques c and c are adjacent, that is, there is an edge between each vertex in c and each vertex in c note that by the denition of critical cliques, this is equivalent to the condition that at least one edge {u, v} with u c and v c exists. to avoid confusion, we will call the vertices in c nodes and in g vertices. the following lemma is adapted from lemma by guo with a careful restatement in the context of our new problem. lemma . let be a yes-instance of cluster editing with vertex splitting. then, there exists a solution of length at most k such that for each critical clique ci in g and each clique sj in g|, either sj contains exactly one copy of each vertex in ci or sj does not contain any copy of a vertex in ci. proof. let be an optimal solution for in standard form. for each critical clique ci, we select a representative vertex ri ci by picking any vertex in ci with fewest appearances in . if it holds for each critical clique ci in g and each clique sj in the resulting cluster graph g| that sj contains exactly one copy of each vertex in ci or sj does not contain any copy of a vertex in ci, then satises all requirements of the lemma statement and we are done. otherwise, there exists a clique sj in g| which contains two copies of some vertex v or there exists a critical clique ci and two vertices u, v ci such that sj contains a copy of u but no copy of v. in the former case, note that removing any operations involving one of the two copies results in a solution of strictly shorter length, contradicting the fact that was an optimal solution. in the latter case, there also exists such a pair of vertices where one of the two vertices is ri. let w be the other vertex. we nd a new optimal solution by removing all operations involving w and copying all opera- tions including ri and replacing ri by w in the copy. for the sake of notational convenience, we we mention in passing that we claimed a slightly stronger lemma in a previous version of this paper. firbas et al. observed that the stronger version does not hold and conjectured that this weaker version is true. we conrm this here. say that that if some operation involves the jth copy of ri, then the very next operation will be the copy and will use the jth copy of w. since w is involved in at least as many operations as ri (recall that ri was picked as having fewest appearances in ), the resulting sequence will be at most as long as . we next show that is also a solution. if the resulting graph g| is not a cluster graph, then it contains an induced p let x be an induced p in g|. since we only modied operations for w, some copy of w has to be part of x. let j be the index such that x contains the jth copy of w. if x contains another copy of w, then the two copies are the two ends. the center cannot be a copy of ri as each copy of w is only adjacent to the corresponding copy of ri and vice versa. hence, we have another induced p where we replace the two copies of w by their respective copies of ri. this contradicts that is a solution. so assume that x contains only the jth copy of w. if x does not contain the jth copy of ri, then the graph contains also a p where we replace the jth copy w by the jth copy of ri. this again contradicts the assumption that is a solution. thus, we may assume that x contains both the jth copy of ri and the jth copy of w. since these two vertices are adjacent, one of the two vertices is the center of x. however, all other vertices have either both or none of the two vertices as neighbors. this contradicts that x exists. hence, is also a solution. note that the number of vertices that behave dierently than their representative is one smaller in compared to . hence, repeating the above procedure at most n times results results in an optimal solution as stated in the lemma. in the following two sections, we will use lemma to develop a linear-vertex kernel and a o-time algorithm. a k-vertex kernel in this section, we prove a problem kernel for cluster editing with vertex splitting with at most k vertices based on the critical clique lemma and a similar kernel for cluster editing by guo . to this end, we propose three reduction rules, prove that they are safe, that they can be performed exhaustively in linear time, and that their application gives a kernel as required. we start with the simplest one. reduction rule remove all isolated cliques. lemma reduction rule is safe. proof. notice that for any optimal solution , each clique in g| only contains copies of vertices of one connected component in g. hence, if a clique contains a copy of a vertex in an isolated clique k in g, then it only contains copies of vertices in k. since k by denition does not require any operations to be transformed into a clique with no outgoing edges, removing k results in an equivalent instance of cluster editing with vertex splitting. we next bound the size of each critical clique in terms of the sizes of adjacent critical cliques. reduction rule if there is a critical clique k such that |k| > | s cn c| + , then reduce the size of k to | s cn c| + lemma reduction rule is safe. proof. let k be a critical clique with |k| > | s cn c| and let be an optimal solution. we show that does not split any vertex corresponding to k and does not add or delete any edge incident to such a vertex. by lemma , we may assume that all cliques in g| contain no or exactly we only point out here that since we removed any operations involving w, the original edge between ri and w is not removed. moreover when splitting another vertex , then we treat each copy of w the same as the corresponding copy of ri. when splitting the jth copy of ri to create the jth and jth copies, then we keep both new vertices adjacent to the jth copy of w and when splitting the jth copy of w, then we make the jth copy of w adjacent to the jth copy of ri and the jth copy of w adjacent to the jth copy of ri. note that each copy of w is adjacent to the respective copy of ri and not adjacent to any other copy of ri . a reduction rule is safe if its application results in an equivalent instance. one copy of vertices in k. let h be a clique in g| that contains exactly one copy of each vertex corresponding to k. let a be the set of nodes in n in the critical clique quotient graph c of g whose corresponding vertices have a copy in h. analogously, let b be the set of nodes not in n in c whose corresponding vertices have a copy in h. note rst that we can assume b = as otherwise adds at least |k| edges between a vertex corresponding to a node in b and all vertices in k. splitting all vertices corresponding to nodes in a instead (and therefore splitting the clique h into two cliques, one containing a copy of each vertex corresponding to a node in ka and the other containing a copy of each vertex corresponding to a node in ab) results in another cluster graph reached by a shorter edit sequence as |a| | s cn c| < |k|. this contradicts the fact that is an optimal solution. we next show that a = n. assume towards a contradiction that a = n. then, there exists a node v n \\ a. modifying to split each vertex corresponding to v once, add all missing edges between vertices corresponding to v and vertices corresponding to a node in a, and removing all edge removals between vertices corresponding to k and v results in another cluster graph. moreover, the newly acquired edit sequence is in fact shorter as |v| + |v| | s ca c| |v| (| s cn\\{v} c| + ) |v| (| s cn c|) < |v| |k|. this again contradicts the fact that is optimal. we have shown that a = n and b = . since h contains a copy of each vertex adjacent to a vertex in k, we can assume without loss of generality that no vertex in k is split and no edge incident to such a vertex is added or deleted. since the above argument holds as long as |k| > | s cn c|, we can reduce the size of k to | s cn c| + and still have an equivalent instance of cluster editing with vertex splitting. we next show that if more than k vertices are left after performing reduction rules and exhaustively, then we have a no-instance. lemma if there is a solution to cluster editing with vertex splitting on , then there are at most k vertices and k critical cliques left after performing reduction rules and exhaustively. proof. we follow an approach similar to that taken by guo . let be an optimal solution of cluster editing with vertex splitting that satises lemma let g be the graph obtained from g after applying reduction rules and exhaustively. partition the node set of the critical clique quotient graph c of g into sets w, x, y, and z as follows. let w be the set of nodes whose corresponding vertices are the endpoint of some edge added by . let x be the subset of nodes not contained in w whose corresponding vertices are the endpoint of some edge deleted by . let y be the subset of nodes not in w or x whose corresponding vertices are split by . finally, let z be all other nodes in c. as each vertex corresponding to a node in w, x, and y is aected by some operation in and each operation can aect at most vertices, it holds that | s cwxy c| || k. let us now consider z. assume towards a contradiction that there is a clique h in g| that contains the vertices corresponding to a node in z but no vertex corresponding to a node in w x y or c contains vertices corresponding to two nodes in z. in the former case, let v z be a node whose corresponding vertices are contained in h. by denition of z, the vertices corresponding to v are adjacent to all vertices in h . hence, none of these vertices correspond to a node in w x y . since reduction rule removed all isolated critical cliques, v is not an isolated node and there is therefore a second node u z whose corresponding vertices are contained in h. hence, we are in the second case where h contains the vertices of at least two critical cliques u, v z. by denition of z, the vertices corresponding to u and v are adjacent to all vertices in h and not adjacent to any other vertices. moreover, since h is a clique and no edges incident to vertices corresponding to u or v were added by , all vertices corresponding to u and v are pairwise adjacent in g. note that this means that u and v are actually the same critical clique, a contradiction. hence, each clique h in g| contains vertices corresponding to at most one node in z and if it does, then it contains vertices of at least one node in w x y . this also means that the number of nodes in z is upper bounded by |w x y | k. thus, g contains at most |w x y z| k critical cliques. consider now the graph g|. the number of vertices that are copies of vertices corresponding to nodes in w x y is at most k. by denition of z, the vertices corresponding to nodes in z are not split during . hence, the number of vertices in g| that are copies of vertices corresponding to nodes in z is the same as the number of vertices in g that correspond to nodes in z. assume towards a contradiction that this number is more than k. then, there exists by the pigeonhole principle a clique h in g| and an integer such that h contains vertices which are copies of vertices corresponding to nodes in w x y and at least + vertices that are (copies of vertices) corresponding to nodes in z. let a be the set of vertices in g which correspond to a node in w x y and have a copy in h and let b be the set of vertices in g which correspond to a node in z and are contained in h. by denition of z, all vertices in b are adjacent to all vertices in a in g. moreover, as shown above, all vertices in b belong to the same critical clique c in g. hence, |c| | s cn c| + | s cn c| + this contradicts the fact that c was reduced with respect to reduction rule thus, the number of vertices in g that correspond to a node in z is at most k and the total number of vertices in g is at most k. the above lemma can be converted into the following reduction rule. note that a c is a cycle on four vertices and a c cannot be transformed into a cluster graph with a single operation. reduction rule if there are more than k vertices or k critical cliques left after applying reduction rules and exhaustively, then reduce the graph to a c and set k = based on the previous three reduction rules, it is easy to derive a problem kernel with k vertices in linear time. theorem one can compute a kernel with at most k vertices and k critical cliques for clus- ter editing with vertex splitting in linear time. proof. computing the critical clique quotient graph c of g takes linear time . applying reduc- tion rule exhaustively also takes linear time and this removes all isolated nodes in c. applying reduction rule exhaustively takes linear time as shown next. first, we can sort all critical cliques by their size in linear time using bucket sort. applying reduction rule to a critical clique c takes deg time. by the handshaking lemma, this procedure takes time linear in the number of edges in c which is upper-bounded by the number of edges in the input graph. note that if we iterate over the critical cliques in increasing order of size, then an application of reduction rule can never aect previously considered critical cliques. this is due to the fact that an application of reduction rule does only depend on adjacent critical cliques. since, whenever we reduce the size of a critical clique, we only reduce it to a size larger than all of its adjacent critical cliques, this can never lead to a situation where we can reduce a critical clique that was initially smaller than the current critical clique. hence, applying reduction rule exhaustively takes linear time. afterwards, we can compute the critical clique quotient graph c of the resulting graph g and count the number of vertices in g and c in linear time. if the number of vertices in g is at most k and the number of vertices in c is at most k, then we are done. otherwise, we apply reduction rule this is correct by lemma , can be performed in constant time, and the resulting graph has k vertices and k critical cliques, where k = is the newly set parameter. this concludes the proof. we leave it as an open problem whether the size of the kernel can be improved and mention that there is a k-vertex kernel for cluster editing . an fpt algorithm the result in theorem implies that cluster editing with vertex splitting is xed- parameter tractable. by lemma , we can assume that all vertices in the same critical clique belong to the same clique in a solution. it is easy to see that the nal solution consists of at most k cliques and guessing these for each of the at most k critical cliques takes ok) = o time. checking whether a given solution can be reached in k operations takes o time and com- bined with the time for computing the kernel, this results in a running time in o. this is, however, far from optimal as shown next. theorem cluster editing with vertex splitting can be solved in o(k log k + n + m) time. proof. first, we compute the critical clique of each vertex and the critical clique quotient graph c of g in linear time . next, we compute the kernel from theorem in linear time. note that c contains at most k vertices. by lemma , we can also assume that all vertices in a critical clique belong to the same clique in the graph g| reached after performing an optimal solution . let x = {s, s, . . . , s} be the set of cliques in g|. note that x contains k cliques as each operation can complete at most two cliques of the solution (removing an edge between two cliques or splitting a vertex contained in both cliques) and reduction rule removed all isolated cliques . hence, if there are more than k cliques in the solution, then we cannot reach the solution with k operations. to streamline the following argumentation, we will cover the nodes in c by cliques s, s, . . . , s=k and assume that an optimal solution contains exactly k cliques by allowing some of the cliques to be empty. next, we iterate over all possible colorings of the nodes in c using + colors , , , . . ., . note that there are at most k ok) such colorings. the idea behind the coloring is the following. all colors , , . . ., will correspond to the cliques s, s, . . . , s, that is, we try to nd a solution where all (vertices in critical cliques corre- sponding to) nodes of the same color belong to the same clique in the solution. the color indicates that the node will belong to multiple cliques in the solution, that is, all vertices in the respective critical clique will be split. since each such split operation reduces k by one, we can reject any coloring in which the number of vertices in critical cliques corresponding to nodes with color is more than k. in particular, we can reject any coloring in which more than k nodes have color next, we guess two indices i , j [] and assume that the ith node of color belongs to sj or we guess that all nodes of color have been assigned to all cliques they belong to. note that in each iteration, there are k+ possibilities and since each guess reduces k by at least one, is the last guess corresponding to one of the nodes of color , or the last guess in general, we can make at most k + guesses. hence, there are at most k+ = k+ ok+) such guesses. it remains to compute the best solution corresponding to each possible set of guesses. to this end, we rst iterate over each pair of vertices and add an edge between them if this edge does not already exist and we guessed that there is a clique si which contains a copy of each of the two vertices. moreover, we remove an existing edge between them if we guessed that the two vertices do not appear in a common clique. finally, we perform all split operations. therein, we iteratively split one vertex v into two vertices u and u where u will be the vertex in some clique si and u might be split further in the future. the vertex u is adjacent to all vertices that are guessed to belong to si. the vertex u is adjacent to all vertices that u was adjacent to, except for vertices that are adjacent to u and not guessed to also belong to some other clique sj which (some copy of) u belongs to. since our algorithm basically performs an exhaustive search, it will nd an optimal solution. it only remains to analyze the running time. we rst compute the kernel in o time. we then try ok) possible colorings of c and for each coloring ok+) guesses. afterwards, we compute the solution in o time as n o by reduction rule thus, the overall running time is in ok+ k + n + m) o. we should note in passing that, while the constants in the running time of our algorithm can probably be improved, a completely new approach is required if one wants a single-exponential- time algorithm. this is due to the fact that the number of possible partitions of o critical cliques into clusters grows super-exponentially even if no vertex-splitting operations are allowed. conclusion by allowing a vertex to split into two vertices, we extend the notion of cluster editing in an attempt to better model clustering problems where a data element may have roles in more than one cluster. on the one hand, we show that this new problem, which we call cluster editing with vertex splitting, is np-complete and, assuming the eth, there are no o-time or o poly-time algorithms for it. on the other hand, we give",
        "Subsections": [],
        "Groundtruth": "The text introduces a problem called cluster editing with vertex splitting and discusses various aspects related to it. The paper presents different reduction rules and lemmas to show that the problem is NP-complete and that there is no polynomial-time algorithm for it, assuming certain hypotheses. It also provides a kernelization algorithm that can reduce the problem to at most k vertices and k critical cliques in linear time. Furthermore, the paper demonstrates an algorithm to solve the cluster editing with vertex splitting problem in O(k log k + n + m) time, where k is the number of critical cliques and n and m are the number of vertices and edges in the graph, respectively. The approach involves guessing and computing the best solution based on different colorings of nodes in the critical clique quotient graph. The proposed algorithm takes into consideration different scenarios to find an optimal solution efficiently."
    },
    {
        "Section_Num": "A",
        "Section": "A 6k-vertex kernel ",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "An",
        "Section": "An FPT algorithm",
        "Text": "the result in theorem implies that cluster editing with vertex splitting is xed- parameter tractable. by lemma , we can assume that all vertices in the same critical clique belong to the same clique in a solution. it is easy to see that the nal solution consists of at most k cliques and guessing these for each of the at most k critical cliques takes ok) = o time. checking whether a given solution can be reached in k operations takes o time and com- bined with the time for computing the kernel, this results in a running time in o. this is, however, far from optimal as shown next. theorem cluster editing with vertex splitting can be solved in o(k log k + n + m) time. proof. first, we compute the critical clique of each vertex and the critical clique quotient graph c of g in linear time . next, we compute the kernel from theorem in linear time. note that c contains at most k vertices. by lemma , we can also assume that all vertices in a critical clique belong to the same clique in the graph g| reached after performing an optimal solution . let x = {s, s, . . . , s} be the set of cliques in g|. note that x contains k cliques as each operation can complete at most two cliques of the solution (removing an edge between two cliques or splitting a vertex contained in both cliques) and reduction rule removed all isolated cliques . hence, if there are more than k cliques in the solution, then we cannot reach the solution with k operations. to streamline the following argumentation, we will cover the nodes in c by cliques s, s, . . . , s=k and assume that an optimal solution contains exactly k cliques by allowing some of the cliques to be empty. next, we iterate over all possible colorings of the nodes in c using + colors , , , . . ., . note that there are at most k ok) such colorings. the idea behind the coloring is the following. all colors , , . . ., will correspond to the cliques s, s, . . . , s, that is, we try to nd a solution where all (vertices in critical cliques corre- sponding to) nodes of the same color belong to the same clique in the solution. the color indicates that the node will belong to multiple cliques in the solution, that is, all vertices in the respective critical clique will be split. since each such split operation reduces k by one, we can reject any coloring in which the number of vertices in critical cliques corresponding to nodes with color is more than k. in particular, we can reject any coloring in which more than k nodes have color next, we guess two indices i , j [] and assume that the ith node of color belongs to sj or we guess that all nodes of color have been assigned to all cliques they belong to. note that in each iteration, there are k+ possibilities and since each guess reduces k by at least one, is the last guess corresponding to one of the nodes of color , or the last guess in general, we can make at most k + guesses. hence, there are at most k+ = k+ ok+) such guesses. it remains to compute the best solution corresponding to each possible set of guesses. to this end, we rst iterate over each pair of vertices and add an edge between them if this edge does not already exist and we guessed that there is a clique si which contains a copy of each of the two vertices. moreover, we remove an existing edge between them if we guessed that the two vertices do not appear in a common clique. finally, we perform all split operations. therein, we iteratively split one vertex v into two vertices u and u where u will be the vertex in some clique si and u might be split further in the future. the vertex u is adjacent to all vertices that are guessed to belong to si. the vertex u is adjacent to all vertices that u was adjacent to, except for vertices that are adjacent to u and not guessed to also belong to some other clique sj which (some copy of) u belongs to. since our algorithm basically performs an exhaustive search, it will nd an optimal solution. it only remains to analyze the running time. we rst compute the kernel in o time. we then try ok) possible colorings of c and for each coloring ok+) guesses. afterwards, we compute the solution in o time as n o by reduction rule thus, the overall running time is in ok+ k + n + m) o. we should note in passing that, while the constants in the running time of our algorithm can probably be improved, a completely new approach is required if one wants a single-exponential- time algorithm. this is due to the fact that the number of possible partitions of o critical cliques into clusters grows super-exponentially even if no vertex-splitting operations are allowed. conclusion by allowing a vertex to split into two vertices, we extend the notion of cluster editing in an attempt to better model clustering problems where a data element may have roles in more than one cluster. on the one hand, we show that this new problem, which we call cluster editing with vertex splitting, is np-complete and, assuming the eth, there are no o-time or o poly-time algorithms for it. on the other hand, we give a k-vertex kernel and an o-time algorithm. this leaves the following gaps. open problem does there exist a o poly-time algorithm for cluster editing with vertex splitting? open problem does there exist a linear-size kernel, that is, a kernel in which the number of vertices plus the number of edges is in o? however, even resolving these questions should only be seen as a starting point for a much larger undertaking. while we do understand the parameterized complexity of cluster editing with vertex splitting with respect to k reasonably well, there are still a lot of open questions regarding structural parameters of the input graph. future work may also consider a bound on the number of allowed edge edits incident to each vertex as used by komusiewicz and uhlmann and abu-khzam . moreover, one might also study the approximability of cluster editing with vertex splitting as the trivial constant-factor approximation of cluster editing does not carry over if we allow vertex splitting. in case cluster editing with vertex splitting turns out to be hard to approximate, one might then continue with studying fpt-approximation and approximation kernels . the vertex splitting operation may also be applicable to other classes of target graphs, including bipartite graphs, chordal-graphs, comparability graphs, perfect graphs, or disjoint unions of graphs like complete bipartite graphs , s-cliques, s-clubs, s-plexes, k-cores, or -quasi-cliques. we note that the results in section are directly applicable to these settings as well. especially bicluster editing has received signicant attention recently . to the best of our knowledge, nothing is known about bicluster editing with vertex splitting. we should also note that there are two natural versions in the bipartite case and both of them seem worth studying. the two versions dier in whether or not one requires that all copies of a split vertex lie on the same side of a bipartition in a solution. on the one hand, the additional requirement makes sense if the data is inherently bipartite. this happens for example if each vertex represents either a researcher or a paper and an edge represents an authorship. on the other hand, if edges reect something like a seller-buyer relationship, then it is plausible that a person both sells and buys. finally, we believe that it also makes sense to study a variant of the vertex-splitting operation where the neighborhood of the two newly introduced vertices are a partition of the neighborhood of the split vertex rather than a covering. that is, when we split a vertex v into v and v, instead of allowing a neighbor w of v to be adjacent to both v and v, we require that w must be adjacent to exactly one of the two. this operation is called exclusive vertex splitting in the literature, and can be seen to be closely related to the clique partitioning problem. acknowledgments matthias bentert is supported by the european research council under the european unions horizon research and innovation programme . p al grn as drange was partially supported by the research council of norway, grant parameterized complexity for practical computing . serge gaspers is the recipient of an australian research council future fellowship and acknowledges support under the arcs discovery projects funding scheme . alexis shaw is the recipient of an australian government research training program scholarship. references f. n. abu-khzam. on the complexity of multi-parameterized cluster editing. journal of discrete algorithms, :, f. n. abu-khzam, n. e. baldwin, m. a. langston, and n. f. samatova. on the relative e- ciency of maximal clique enumeration algorithms, with applications to high-throughput com- putational biology. in proceedings of the international conference on research trends in science and technology, pages , f. n. abu-khzam, j. egan, s. gaspers, a. shaw, and p. shaw. cluster editing with vertex splitting. in international symposium on combinatorial optimization, pages springer, e. arrighi, m. bentert, p. g. drange, b. d. sullivan, and p. wolf. cluster editing with overlap- ping communities. in proceedings of the th international symposium on parameterized and exact computation . schloss dagstuhl leibniz-zentrum f ur informatik, to appear. s. b ocker. a golden ratio parameterized algorithm for cluster editing. journal of discrete algorithms, :, l. cai. fixed-parameter tractability of graph modication problems for hereditary properties. information processing letters, :, j. chen and j. meng. a k kernel for the cluster editing problem. journal of computer and system sciences, :, j. chen, x. huang, i. a. kanj, and g. xia. strong computational lower bounds via parame- terized complexity. journal of computer and system sciences, : , m. cygan, f. v. fomin, l. kowalik, d. lokshtanov, d. marx, m. pilipczuk, m. pilipczuk, and s. saurabh. parameterized algorithms. springer, m. daddario, d. kopczynski, j. baumbach, and s. rahmann. a modular computational framework for automated peak extraction from ion mobility spectra. bmc bioinformatics, :, f. dehne, m. a. langston, x. luo, s. pitre, p. shaw, and y. zhang. the cluster editing problem: implementations and experiments. in proceedings of the nd international work- shop on parameterized and exact computation , pages springer berlin heidelberg, r. diestel. graph theory. springer, p. g. drange, f. reidl, f. s. villaamil, and s. sikdar. fast biclustering by dual parame- terization. in proceedings of the th international symposium on parameterized and exact computation , pages schloss dagstuhl leibniz-zentrum f ur informatik, a. fadiel, m. a. langston, x. peng, a. d. perkins, h. s. taylor, o. tuncalp, d. vitello, p. h. pevsner, and f. naftolin. computational analysis of mass spectrometry data using novel combinatorial methods. in proceedings of the ieee/acs international conference on computer systems and applications , pages ieee computer society, m. fellows, m. langston, f. rosamond, and p. shaw. ecient parameterized preprocessing for cluster editing. in proceedings of the th international symposium on fundamentals of computation theory , pages springer, a. firbas, a. dobler, f. holzer, j. schafellner, m. sorge, a. villedieu, and m. wimann. the complexity of cluster vertex splitting and company. corr, abs/, j. flum and m. grohe. parameterized complexity theory. springer, j. gramm, j. guo, f. h uner, and r. niedermeier. graph-modeled data clustering: exact algorithms for clique generation. theory of computing systems, :, j. guo. a more eective linear kernelization for cluster editing. theoretical computer science, : , j. guo, f. h uner, c. komusiewicz, and y. zhang. improved algorithms for bicluster editing. in proceedings of the th international conference on theory and applications of models of computation , pages springer, f. h uner, c. komusiewicz, h. moser, and r. niedermeier. fixed-parameter algorithms for cluster vertex deletion. theory of computing systems, :, r. impagliazzo, r. paturi, and f. zane. which problems have strongly exponential complex- ity? journal of computer and system sciences, :, c. komusiewicz and j. uhlmann. cluster editing with locally bounded modications. discrete applied mathematics, :, m. k riv anek and j. mor avek. np-hard problems in hierarchical-tree clustering. acta infor- matica, :, g.-h. lin, t. jiang, and p. e. kearney. phylogenetic k-root and steiner k-root. in proceedings of the th international symposium on algorithms and computation , pages springer, r. niedermeier. an invitation to fixed-parameter algorithms. oxford university press, m. radovanovi c, a. nanopoulos, and m. ivanovi c. hubs in space: popular nearest neighbors in high-dimensional data. journal of machine learning research, :, n. tomasev, m. radovanovic, d. mladenic, and m. ivanovic. the role of hubness in clustering high-dimensional data. ieee transactions on knowledge and data engineering, : , c. a. tovey. a simplied np-complete satisability problem. discrete applied mathematics, :, d. tsur. faster parameterized algorithm for bicluster editing. information processing letters, , m. xiao and s. kou. a simple and improved parameterized algorithm for bicluster editing. information processing letters,",
        "Subsections": [],
        "Groundtruth": "Cluster editing with vertex splitting is a fixed-parameter tractable problem. The algorithm computes a critical clique quotient graph and a kernel in linear time, solving the problem in O(k log k + n + m) time. It involves guessing possible solutions and optimizing for each guess. Despite potential improvements in constants, a completely new approach is needed for a single-exponential-time algorithm. The text highlights open problems, such as finding a polynomial-time algorithm and linear-size kernel for this problem. Future work includes exploring other types of target graphs and variants of the vertex-splitting operation."
    },
    {
        "Section_Num": "Conclusion",
        "Section": "Conclusion",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "A",
        "Section": "A 6k-vertex kernel",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "Abstract",
        "Section": "Abstract",
        "Text": "arxiv:v feb the university of oklahoma arxiv: ou-hep- december charming top decays with flavor changing neutral higgs boson and ww at hadron colliders rishabh jainand chung kao homer l. dodge department of physics and astronomy, university of oklahoma, norman, ok , usa abstract we investigate the prospects for discovering a top quark decaying into one light higgs boson along with a charm quark in top quark pair production at the cern large hadron collider and future hadron colliers. a general two higgs doublet model is adopted to study the signature of avor changing neutral higgs interactions with t ch, followed by h ww ++ e /t , where h is the cp-even higgs boson and e /t stands for missing transverse energy from neutrinos. we study the discovery potential for this fcnh signal and physics background from dominant processes with realistic acceptance cuts as well as tagging and mistagging eciencies. promising results are found for the lhc running at tev and tev center-of-mass energy as well as future pp colliders at tev and tev. pacs numbers: .fr, mm, .ec, .ha e-mail address: rishabh.jain@ou.edu e-mail address: chung.kao@ou.edu",
        "Subsections": [],
        "Groundtruth": "The text discusses the prospects of discovering a top quark decaying into a light Higgs boson and a charm quark, involving flavor-changing neutral Higgs interactions, at the CERN Large Hadron Collider and future hadron colliders. The study uses a two Higgs doublet model to analyze the signature of these interactions leading to the production of Higgs boson pairs along with missing transverse energy. Promising results are found for the LHC at 14 TeV and future proton-proton colliders at 100 TeV and 33 TeV center-of-mass energies."
    },
    {
        "Section_Num": "I",
        "Section": "I Introduction",
        "Text": "the standard model has been very successful in explaining almost all experimental data to date, culminating in the discovery of the the long awaited standard higgs boson at the cern large hadron collider . the most important experimental goals of the lhc, future high energy hadron colliders, and the international linear collider are to study the higgs properties and to search for new physics beyond the standard model including additional higgs bosons and avor changing neutral higgs interactions. in the standard model there is one higgs doublet, which generates masses for both vector bosons and fermions. there is no explanation for the large dierences among yukawa couplings of fermions with the higgs boson. in addition, there are no avor changing neutral currents mediated by gauge interactions or by higgs interactions at the tree level. the top quark is the most massive elementary particle ever discovered. the fact that the higgs boson is lighter than the top quark makes it possible for the top quark to decay into the higgs boson along with a charm quark kinematically. at the one loop level, the branching fraction of t ch is for mh = gev . if this decay mode is detected in the near future, it would indicate a large tree-level coupling or a signicant enhancement from beyond sm loop eects. a general two higgs doublet model usually contains avor changing neutral higgs interactions if there is no discrete symmetry to turn otree-level fcnc . in , it was pointed out that top-charm fcnh coupling could be prominent if the yukawa couplings of fermions and the higgs boson are comparable to the geometric mean of their mass . a special two higgs doublet model for the top quark might provide a reasonable explanation why the top quark is much more massive than other elementary fermions. in the thdm, top quark is the only elementary fermion acquiring its mass from a special higgs doublet with a large vacuum expectation value . since the up and charm quarks couple to another higgs doublet , there are fcnh interactions among the up-type quarks. the down type quarks have the same interactions as those in the sm. in a general two higgs doublet model, there are ve physical higgs bosons: two cp-even scalars h and h , a cp-odd pseudoscalar , and a pair of singly charged higgs boson . to study fcnh interactions in a general hdm, we employ the following lagrangian with higgs bosons and fermions , ly = x f=u,d,l f nh fs + fc i h + h fc fs i h i sgnfao prf u h v dpr uv pl i dh+ h lpr i lh+ + h.c. where pl,r /, c = cos, s = sin, is the mixing angle between neutral higgs scalars, tan v/v , qf is the charge of a fermion,and matrices are diagonal and xed by fermion masses to f = mf/v with v gev, while matrices have both diagonal and o-diagonal elements with free parameters. the lhc has become a top quark factory. the production cross section of top quark pair is approximately pb in pp collisions with a tev center-of-mass energy energy , and it becomes tt pb at s = tev . for an integrated luminosity of l = fb at s = tev, the lhc has produced more than top quark pairs (t t) for mt gev . for the same integrated luminosity at s = tev, the number of (t t) pairs generated would increase to about thus, the lhc will provide great opportunities to study electroweak symmetry breaking as well as other important properties of the top quark and the higgs boson. most atlas and cms measurements of the gev higgs boson are consistent with expectations for the standard model. the branching fractions of the standard higgs boson are presented in table i . in a general two higgs doublet model, let us consider the light higgs scalar as the sm higgs boson in the alignment limit . table i: branching fractions and partial decay widths of the light cp-even higgs boson of a general two higgs doublet model in the alignment limit (h h sm). for simplicity, let us take ff f = mf/v. widths are in mev units, with sm h mev . decay channel bsm comment bb % bb b ww % sin gg % tt t % zz % sin % w-loop and fermion loops. it is clear that the most probable decay channels are b b and ww with branching fractions b(h b b) and b as show in table i. however, the light higgs boson was rst discovered with h and h zz, because these channels have less background and better mass resolutions. in the past few years, several theoretical studies and experimental searches have been completed for the charming top fcnh decay t ch with h b b , h zz, h , and higgs decays into multileptons . recently, the atlas collaboration has placed tight limits on the fcnh branching fraction for t ch and the yukawa coupling tch with higgs boson decaying into multileptons b % , and tch , for the eective lagrangian le= tch cth + h.c. . the lhc limits for the branching ratios can be translated to a limit on the avor changing yukawa coupling by a simple rescaling. it is a good approximation to consider a simple numerical relation between the fcnh yukawa coupling and the branching fraction of t ch tch q b . in this article, we focus on the discovery potential of the lhc in the search for the fcnh top decay t ch followed by h ww + . we have evaluated production rates with full tree-level matrix elements including breit-wigner resonances for both the signal and the physics background. in addition, we optimize the acceptance cuts to eectively reduce the background with realistic b-tagging and mistagging eciencies. promising results are presented for the lhc with s = tev and s = tev as well as for future hadron colliders at s = tev and tev, for high luminosities of l = fb and fb section ii shows the production cross sections for the higgs signal and the dominant background, as well as our strategy to determine the reconstructed masses for the top quark and the higgs boson. realistic acceptance cuts are discussed in section iii. section iv presents the discovery potential at the lhc for s = tev and tev, as well as for future hadron colliders with for s = tev and tev. our optimistic conclusions are drawn in section v.",
        "Subsections": [],
        "Groundtruth": "The standard model, successful in explaining experimental data, focuses on studying Higgs properties and seeking new physics beyond. Interesting findings include the top quark's massiveness compared to other elementary particles and the relevance of Higgs bosons in interactions. The search for flavor-changing neutral Higgs (FCNH) interactions is emphasized, with implications for possible models. LHC serves as a top quark production hub, aiding in studying electroweak symmetry breaking and properties of the Higgs boson. Experimental tests, such as the discovery potential of FCNH decay, are highlighted for current and future collider energies. Strategies for mass reconstruction and discovery potential at LHC are discussed, pointing to promising results in the search for FCNH top decay."
    },
    {
        "Section_Num": "II",
        "Section": "II The Higgs Signal and Physics Background",
        "Text": "in this section we present the cross section for the fcnh higgs signal in pp collisions (pp t t tch bjj c + x, = e, ) as well as for the dominant physics background processes. figure shows the feynman diagram of top quark pair production in pp collisions from gluon fusion and quark-antiquark fusion, followed by one top quark decaying into a higgs boson and a charm quark, while the other top quark decays into bw bjj. p p c b + j j t t h w w + w + fig. : feynman diagram for pp t t bjj ch + x bjj c+ x, where = e or .",
        "Subsections": [
            {
                "Section_Num": "A",
                "Section": "A The Higgs Signal in Top Decay",
                "Text": "applying the lagrangian in eq. with general yukawa interactions for the light higgs boson and fermions, we obtain the decay width of t ch tch = c mt \" ( + r c r h) + rc( tc ct + tcct) # /(, r c, r h) where c = cos, rh = mh/mt, rc = mc/mt, and = x + y + z xy xz yz . let us dene two variables, tc = s |tc| + |ct| , and c = tc ct + tcct , combining lhc higgs data and b physics, a recent study found constraints tc and ct . that implies c < tc for tc hence we can write our decay width as, tch = c mt [( + r c r h)| tc| + crc] /(, r c, r h) . using mt = gev, mh = gev and mc = gev , we obtain tch = c mt [| tc| + c)] /(, r c, r h) . since we have mc mt, rc , and c < | tc|, it is a very good approximation to consider tch c mt [( r h)| tc|] /(, r c, r h) . for typical values of parameters cos = , |tc| and |ct| , we have tch (c | tc|) gev , and b . for simplicity, we may adopt the following eective lagrangian to study fcnh yukawa interactions for the light cp-even higgs boson with the top quark and the charm quark l = ghtc cth + h.c., where ghtc = tc cos = tch . it is the eective coupling of the fcnh yukawa coupling. then the decay width for t ch becomes = |ghtc| [ + r c r h] q q . we assume that the total decay width of the top quark is t = + . then the branching fraction of t ch becomes b = t . as a case study, let us take the fcnh yukawa couplings to be the geometric mean of the yukawa couplings of the quarks that is also known as the cheng-sher ansatz ghtc = mtmc v , or tch = ghtc = mtmc v , with mt = gev and mc = gev. then the branching fraction of t ch becomes b = for mh = gev. in general, we will consider ghtc = tc cos( )/ with tc and cos as free parameters. we employ the programs madgraph and helas to evaluate the exact matrix element for the fcnh signal in top decays from gluon fusion and quark-antiquark annihilation, gg, q q t t t ch bjj c+ , and , gg, q q tt tch bjjc+ , where = e or . the cross section of the higgs signal in fcnh top decays at the lhc and future hadron colliders for pp t t tch bjj c+ + x is evaluated with the parton distribution functions of ctlo with a common value q = mt t = the invariant mass of t t, for the renormalization scale and the factorization scale . this choice of scale leads to a k factor of approximately for top quark pair production. we have used the computer program top++ to evaluate higher order corrections. in addition, we have checked the tree-level signal cross section with narrow width approximation. that is, the cross section (pp t t tch bjj c+ + x) is calculated as the product of cross section times branching fractions: (pp t t bjj t + x) b b . in our analysis, we consider the fcnh signal from both t t t ch bjj c+ and t t ch t bjj c+ . in every event, we require that there should be one b jet and three light jets . in addition, there are two leptons and neutrinos, which will be lead to missing transverse energy (e /t). unless explicitly specied, q generally denotes a quark or an anti-quark ( q) and will represent a lepton or anti-lepton . that means our fcnh signal leads to the nal state of bjj c+ or bjjj++ e /t.",
                "Subsections": [],
                "Groundtruth": "The section analyzes the Higgs signal in top decay by utilizing the Lagrangian equation with Yukawa interactions for the Higgs boson and fermions. By defining specific variables and constraints based on data from the Large Hadron Collider (LHC) and B physics, the decay width of the top quark into a charm quark and a Higgs boson is determined. The study focuses on the effective coupling of the Yukawa interactions for the light Higgs boson with the top quark and the charm quark. The decay width, branching fraction, and cross section of the Higgs signal in top decays are evaluated for various processes at the LHC and future hadron colliders using specific programs and calculations. The analysis considers specific final states with requirements for jets, leptons, and missing transverse energy."
            },
            {
                "Section_Num": "B",
                "Section": "B The Physics Background",
                "Text": "the dominant physics background to the nal state of bjjc+ comes from top quark pair production along with two light jets (t tjj), pp t tjj b bjjww b bjj+ + x, where every top quark decays into a bquark as well as a w boson and a b- jet is mis-identied as a c-jet. we have also considered backgrounds from pp t tw b bjjww b bjj+ + x with one w boson decaying into jj, and pp b bjjww b bjj+ + x, excluding the contribution from t tjj and t tw. in addition, we have included pp c cjjww c cjj+ + x and pp jjjjww jjjj+ + x where j = u, d, s, or g. we evaluate the cross section of physics background in pp collisions with proper tagging and mistagging eciencies. in our analysis, we adopt updated atlas tagging eciencies : the b tagging eciency is%, the probability that a c-jet is mistagged as a b-jet is approximately %, while the probability that any other jet is mistagged as a b-jet is %.",
                "Subsections": [],
                "Groundtruth": "The main physics background contributing to the final state of bjjc+ arises from top quark pair production with two light jets (ttjj), resulting in top quark decay into a b-quark and a W boson. Additionally, backgrounds from processes like W boson decays and other jet combinations are considered. These backgrounds are evaluated in proton-proton collisions, taking into account proper tagging and mistagging efficiencies. Current Atlas tagging efficiencies are used in the analysis, with a b-tagging efficiency of % and specific probabilities for c-jet and other jet mistagging as b-jets."
            },
            {
                "Section_Num": "C",
                "Section": "C Mass Reconstruction",
                "Text": "in this subsection, we demonstrate that the proposed higgs signal comes from top quark pair production with t t bjj ch bjj c++ e /t. we discuss our strategy to determine the reconstructed top mass as the invariant mass of bjj from t bw bjj along with another top quark decays into a higgs boson and a charm quark t ch furthermore, we employ cluster transverse mass distributions for +and c+with missing transverse energy (e /t) from neutrinos. these distributions have broad peaks near mh and mt respec- tively as the kinematic characteristics of t ch c++ e /t. applying suitable cuts on the cluster transverse mass mt (, e /t) as well as mt(c, e /t), we can greatly reduce the physics background and enhance the statistical signicance for the higgs signal. fig. : invariant mass distributions of jj , and bjj , for the higgs signal in pp collisions, d/dm(pp t t tch tcww bjjc++ e /t + x , with basic cuts dened in eq. . also shown are the invariant mass distributions d/dmjj and d/dmbjj for the dominant physics background from t tjj. in our analysis, we assume that the fcnh signal comes from top quark pair production with one top quark decaying into a charm quark and a higgs boson (t ch cww c+ ) while the other decays hadronically . in every event, there is one tagged b-jet and three light jets. let us choose the pair of light jets that minimize |mjj mw| and |mbjj mt| as jj and label the other jet as j c. that means, for a correctly reconstructed event, j and j are the products of a w decay such that their invariant mass distribution peaks at mjj mw. for a background event, one b is likely coming from the top decay t bw bjj while the other is either a mistagged c or a light quark jet coming from w decay, or a real b quark coming from the decay of t. we present the invariant mass distributions for mjj and mbjj in fig. for the higgs signal (t t tch) and the dominant background (t tjj) with basic cuts from cms : pt > gev , pt > gev , pt > gev , e /t > gev , ||| < , and |r| > , where pt pt and r q + it is clear to see that mjj distribution peaks at mw while d/dmbjj has a peak at mt. in a good reconstruction, the remaining light jet, j c should reproduce the top quark mass with the momenta of charged leptons and neutrinos. to reconstruct the higgs mass and top mass for t ch c++ e /t, we use cluster transverse mass mt(, e /t) and mt(c, e /t) , dened below, m t(, e /t) = \u0012q p t + m + e /t \u0013 ( pt + e /t) , and m t(c, e /t) = \u0012q p t + m c+ e /t \u0013 ( pt + e /t) , where pt or pt is the total transverse momentum of all the visible particles and m or mcis the invariant mass. figure presents the cluster transverse mass distributions (d/dmt(, e /t) and (d/dmt(c, e /t) for the higgs signal in pp collisions, d/dmt(pp t t tch tcww bjjc++ e /t + x , with basic cuts dened in eq. , as well as |mjj mw| mw and |mbjj mt| mt. the cluster transverse mass distribu- tions for +and c+for the higgs signal (t t tch) and the dominant background (t tjj) with basic cuts dened in eq. as well as invariant mass cuts note that d/dmt(, e /t peaks near mh while d/dmt(c, e /t has a peak near mt. it is clear that there are pronounced peaks at mw and mt in the invariant mass distribu- tions of jets as shown in fig. we can also see broad peaks near mh and mt in the cluster transverse mass distributions: m jj mw , m bjj mt , m t(, e /t) mh , m t(c, e /t) mt , fig. : cluster transverse mass distributions of + and c+(blue solid) for the higgs signal in pp collisions, d/dmt (pp t t tch tcww bjjc++e /t +x , with basic cuts dened in eq. , as well as |mjj mw| mw and |mbjj mt| mt. also shown are the cluster transverse mass distributions d/dmt (, e /t ) and d/dmt (c, e /t ) for the dominant physics background from t tjj. where mis the value of invariant mass or cluster transverse mass with a peak of the distri- bution. these distributions provide powerful selection tools to remove physics background while maintaining the higgs signal.",
                "Subsections": [],
                "Groundtruth": "The subsection focuses on reconstructing the mass of particles in the context of the proposed Higgs signal originating from top quark pair production. The strategy involves determining the reconstructed top mass as the invariant mass of certain particle combinations. Utilizing cluster transverse mass distributions with missing transverse energy, they aim to enhance the statistical significance of the Higgs signal by applying suitable cuts. The presentation includes invariant mass distributions for relevant particle combinations and demonstrates the effectiveness of cluster transverse mass distributions in distinguishing between the Higgs signal and background physics events. Peaks in the distributions correspond to specific masses, aiding in accurate mass reconstruction."
            }
        ],
        "Groundtruth": "The section presents the cross section for the FCNH Higgs signal in proton-proton collisions, as well as for the main physics background processes. It includes a Feynman diagram depicting top quark pair production in proton-proton collisions through gluon fusion and quark-antiquark fusion, with one top quark decaying into a Higgs boson and a charm quark, and the other top quark decaying into a top quark and bottom quark jet."
    },
    {
        "Section_Num": "III",
        "Section": "III Realistic Acceptance Cuts",
        "Text": "to study the discovery potential of this charming fcnh signal from top decays at the lhc, we have applied realistic basic cuts listed in eq. and tagging eciencies for bjets. in addition to basic cuts we apply cuts on invariant mass of jets and cluster transverse mass of and cto eectively veto the background events: |mjj mw| mw, |mbjj mt| mt, gev mt(, e /t) gev, and gev mt(c, e /t) gev. these selection requirements remove more than % of the total background. measurement uncertainties in jet and lepton momenta as well as missing transverse mo- mentum give rise to a spread in the reconstructed masses about the true values of mt and m. based on the atlas and the cms specications we model these eects by fig. : the cross section in fb of pp t t tch bjjc++ e /t + x at s = tev and tev as a function of tc, along with total and most dominant background after applying all the cuts, tagging and mistagging eciencies and higher order qcd corrections. the blue dash line and green dash line shows the minimum cross section needed for signicance at l = fb and ab respectively for center of mass energy of tev. where as for tev, we present l = ab only. the current atlas-limit is shown as a black dash vertical line. gaussian smearing of momenta: e e = q e , for jets and e e = q e , for charged leptons with individual terms added in quadrature.",
        "Subsections": [],
        "Groundtruth": "The text discusses applying realistic acceptance cuts to study the discovery potential of a signal from top decays at the LHC. These cuts include basic criteria and efficiencies for tagging b-jets, as well as additional requirements based on jet invariant mass and cluster transverse mass to reduce background events. Various selection criteria are listed, which lead to the removal of a significant portion of total background. The impact of measurement uncertainties on jet and lepton momenta and missing transverse momentum is considered when reconstructing masses. The text also mentions modeling effects based on ATLAS and CMS specifications, showing the cross section as a function of certain parameters and comparing to background levels. Gaussian smearing of momenta is discussed to account for uncertainties in measurements."
    },
    {
        "Section_Num": "IV",
        "Section": "IV Discovery Potential at the LHC",
        "Text": "applying all realistic cuts, we present our results for the higgs signal at the lhc with s = tev and s = tev as well as cross sections for future hadron colliders with s = tev and s = tev in table ii. here we have kept cos = . later we will vary it from to for discovery contours. cross sections for dominant background processes are presented in table iii. to estimate the discovery potential at the lhc we include curves that correspond to the minimal cross section of signal required by our discovery criterion described in the following. we dene the signal to be observable if the lower limit on the signal plus background is larger than the corresponding upper limit on the background with statistical uctuations l n q l lb + n q lb, fig. : similar to fig. , but for s = tev, and tev. table ii: cross section of higgs signal pp t t tch bjj c+e /t +x in fb with cos = for the lhc and future hadron colliders. tc tev tev tev tev or equivalently, s n l \u0014 n + q lb \u0015 . here l is the integrated luminosity, s is the cross section of the fcnh signal, and b is the background cross section. the parameter n species the level or probability of discovery. we take n = , which corresponds to a signal. for lb , this requirement becomes similar to nss = ns nb = ls lb , table iii: cross section in fb for dominant physics background processes, with k factors and tagging eciencies at the lhc and future hadron colliders. background tev tev tev tev ttjj ttw bbjj bbjjww ccjjww wwjjjj e- e- e- where ns is the signal number of events, nb is the background number of events, and nss is the statistical signicance, which is commonly used in the literature. if the background has fewer than events for a given luminosity, we employ the poisson distribution and require that the poisson probability for the sm background to uctuate to this level is less than , i.e. an equivalent probability to a -sigma uctuation with gaussian statistics. figure shows the higgs signal cross section as a function of tc, along with cross section of total background and the most dominant background process for the cern large hadron collider with s = and tev. we have also shown, minimum cross section required for signicance at l = fb and higher luminosities for the future hl lhc , i.e l = and fb all tagging eciencies and k factors discussed above are included. our analysis suggests an improvement in the reach of atlas at a luminosity of fb, which gets better at higher energies, i.e s = and tev, as shown in figure we present the discovery reach at the lhc for s = tev and s = tev in fig. , in the parameter plane of [cos, tc]. we have chosen l = and fb figure shows the discovery contours for s = and tev. high energy lhc with high luminosity is quite promising as it nearly covers the entire parameter space that we have used in our analysis. dot: fb- dash: fb- dotdash : atlas- cos \u0003tc \u0004 s = tev p p \u0005 t t \u0005 t c h \u0005 b j j c l l\u0006 + x dot: fb- dash: fb- dotdash : atlas- cos \u0003tc \u0004 s = tev p p \u0005 t t \u0005 t c h \u0005 b j j c l l\u0006 + x fig. : the discovery contours at the lhc in the plane of [cos, tc] for s = tev and s = tev. for l = fb and l = fb . also shown is the current limit on tch = tc cos set by atlas . the shaded region above this curve is excluded at % cl.",
        "Subsections": [],
        "Groundtruth": "The text discusses the discovery potential of the Higgs signal at the Large Hadron Collider (LHC) and future hadron colliders with different energy levels, cross sections, and background processes. It includes tables presenting cross sections for the Higgs signal and dominant background processes. The analysis considers signal observability based on statistical fluctuations and luminosity requirements. Figures illustrate the cross section of the Higgs signal, discovery reach at the LHC for different energies, and discovery contours in the parameter plane of [cos, tc]. The study suggests improved reach at higher luminosities and energies, with the high energy LHC showing promise in covering the analyzed parameter space. Visual representations display the discovery contours for different luminosities and energies, showing the exclusion of certain regions based on current limits set by ATLAS."
    },
    {
        "Section_Num": "V",
        "Section": "V Conclusions",
        "Text": "it is a generic possibility of particle theories beyond the standard model to have contri- butions to tree-level fcnh interactions, especially for the third generation quarks. these contributions arise naturally in models with additional higgs doublets, such as the special two higgs doublet model for the top quark , or a general hdm. in the alignment solid: fb- dot: fb- dash: fb- dotdash : atlas- cos \u0003tc \u0004 s = tev p p \u0005 t t \u0005 t c h \u0005 b j j c l l\u0006 + x solid: fb- dot: fb- dash: fb- dotdash : atlas- cos \u0003tc \u0004 s = tev p p \u0005 t t \u0005 t c h \u0005 b j j c l l\u0006 + x fig. : the discovery contours at future pp colliders in the plane of [cos, tc] for s = tev, and s = tev, for l = fb , l = fb and l = fb . also shown is the current limit on tch = tc cos set by atlas . the shaded region above this curve is excluded at % cl. limit, the light higgs boson resembles the standard higgs boson, and it has a mass below the top mass. this could engender the rare decay t ch we investigated the prospects for discovering such a decay at the lhc, focusing on the channel where tt are pair produced and subsequently decay, one haronically and the other through the fcnh mode. the primary background for this signal is a ttjj with both top quarks decaying leptonically. this background involves one bjet mis-tagged as a c jet,and two other light jets, along with two leptons and missing transverse energy. nonetheless, by taking advantage of the available kinematic information, we can reconstruct the resonances of the signal and reject much of the background. based on our analysis, we nd that lhc at s = tev, with l = fb, can probe to as low as b , tch = tc cos . it gets better with s = tev and s = tev, which can reach upto b , tch and b , tch respectively. we look forward to being guided by more new experimental results as we explore inter- esting physics of electroweak symmetry breaking and fcnh interactions. while the properties of the higgs boson goes under further scrutiny as data accumulate, perhaps a dedicated fcnh t ch search should be undertaken, for upcoming hl lhc and further he-lhc as well as future high energy hadron collider with a cm energy of tev",
        "Subsections": [],
        "Groundtruth": "Particle theories beyond the standard model can have contributions to tree-level fcnh interactions, especially for third-generation quarks. These contributions are natural in models with additional higgs doublets. The alignment solid discovery contours at future pp colliders show potential limits and exclusions on the tch interaction. The rare decay tch could be observed at the LHC, with a focus on tt pair production and subsequent decay. By utilizing kinematic information, the signal can be reconstructed and background rejected. Analysis suggests that the LHC at different energies can probe tch interactions, paving the way for further exploration of electroweak symmetry breaking and fcnh interactions in high-energy colliders in the future."
    },
    {
        "Section_Num": "Acknowledgments",
        "Section": " Acknowledgments",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "References",
        "Section": " References",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "Acknowledgments",
        "Section": "Acknowledgments",
        "Text": "we are grateful to kai-feng jack chen for benecial discussions. c.k. thanks the in- stitute of physics at the academia sinica and the high energy physics group at national taiwan university for excellent hospitality, where part of the research was completed. this research was supported in part by the u.s. department of energy. g. aad et al. [atlas collaboration], phys. lett. b , doi:/j.physletb.. ]. s. chatrchyan et al. [cms collaboration], phys. lett. b , doi:/j.physletb.. ]. j. a. aguilar-saavedra, acta phys. polon. b , . b. mele, s. petrarca and a. soddu, phys. lett. b , . g. eilam, j. l. hewett and a. soni, phys. rev. d , ; [erratum-ibid. d , ]. s. l. glashow and s. weinberg, phys. rev. d , . doi:/physrevd. j. f. gunion, h. e. haber, g. l. kane and s. dawson, front. phys. , . w. s. hou, phys. lett. b , . doi:/--m t. p. cheng and m. sher, phys. rev. d , . doi:/physrevd. a. k. das and c. kao, phys. lett. b , . s. davidson and h. e. haber, phys. rev. d , [phys. rev. d , ]. f. mahmoudi and o. stal, phys. rev. d p. nason, s. dawson and r. k. ellis, nucl. phys. b , . doi:/- - n. kidonakis, phys. rev. d , doi:/physrevd. ]. v. ahrens, a. ferroglia, m. neubert, b. d. pecjak and l. l. yang, phys. lett. b , doi:/j.physletb.. ]. m. cacciari, m. czakon, m. mangano, a. mitov and p. nason, phys. lett. b , doi:/j.physletb.. ]. m. czakon, p. fiedler and a. mitov, phys. rev. lett. , doi:/physrevlett. ]. m. aaboud et al. , phys. lett. b , erratum: [phys. lett. b , ] doi:/j.physletb.., /j.physletb.. ]. a. m. sirunyan et al. [cms collaboration], jhep , doi:/jhep ]. s. heinemeyer et al. , doi:/cern-- arxiv: . g. aad et al. [atlas collaboration], eur. phys. j. c , no. , doi:/epjc/s---y ]. the atlas and cms collaborations, atlas-conf-- j. f. gunion and h. e. haber, phys. rev. d , doi:/physrevd. . m. carena, i. low, n. r. shah and c. e. m. wagner, jhep , doi:/jhep ]. c. kao, h. y. cheng, w. s. hou and j. sayre, phys. lett. b , doi:/j.physletb.. ]. d. atwood, s. k. gupta and a. soni, jhep , doi:/jhep ]. a. m. sirunyan et al. [cms collaboration], jhep , doi:/jhep ]. k. f. chen, w. s. hou, c. kao and m. kohda, phys. lett. b , doi:/j.physletb.. ]. m. aaboud et al. [atlas collaboration], jhep , doi:/jhep ]. s. banerjee, m. chala and m. spannowsky, eur. phys. j. c , no. , doi:/epjc/s--- ]. n. craig, j. a. evans, r. gray, m. park, s. somalwar, s. thomas and m. walker, phys. rev. d , doi:/physrevd. ]. cms collaboration , cms-pas-hig-- v. khachatryan et al. [cms collaboration], phys. rev. d , doi:/physrevd. ]. m. aaboud et al. , phys. rev. d , no. , doi:/physrevd. ]. the atlas collaboration, atlas-phys-pub-- w. barletta, m. battaglia, m. klute, m. mangano, s. prestemon, l. rossi and p. skands, arxiv: . r. tomas.et.al doi:/j.nuclphysbps.. f. zimmermann, icfa beam dyn. newslett. , . v. shiltsev, doi:/jacow-napac-tupob arxiv: . b. altunkaynak, w. s. hou, c. kao, m. kohda and b. mccoy, phys. lett. b , doi:/j.physletb.. ] j. alwall, m. herquet, f. maltoni, o. mattelaer and t. stelzer, jhep , doi:/jhep ]. k. hagiwara, j. kanzaki, q. li and k. mawatari, eur. phys. j. c , doi:/epjc/s---x ]. k. hagiwara,j kanzaki,h. murayama and i. watanabe helas : helicity amplitud subrou- tines for feynman diagram evaluations arxiv:. s. dulat et al., phys. rev. d , no. , doi:/physrevd. ]. j. gao et al., phys. rev. d , no. , doi:/physrevd. ]. the atlas collaboration , atlas-conf-- l. scodellaro , arxiv: . v. barger and r. j. n.phillips collider physics, isbn -- -, t. han and r. j. zhang, phys. rev. lett. , doi:/physrevlett. . c. kao and j. sayre, phys. lett. b , doi:/j.physletb.. ]. cms collaboration , cms-pas-hig-- the atlas collaboration, atlas-phys-pub-- the cms collaboration, arxiv:",
        "Subsections": [],
        "Groundtruth": "The section acknowledges Kai-Feng Jack Chen for beneficial discussions and thanks various institutions for their support and hospitality during the research. It also includes references to numerous publications and collaborations in the field of high-energy physics. The acknowledgments highlight the contributions of individuals and groups involved in the research."
    },
    {
        "Section_Num": "Abstract",
        "Section": "Abstract",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "Hard",
        "Section": "Hard X-ray spectroscopy of the itinerant magnets RFe4Sb12 (R=Na, K, Ca, Sr, Ba)",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "Abstract",
        "Section": "Abstract",
        "Text": "arxiv:v feb puzzles of the cosmic ray anisotropy a.d. erlykin , s.k. machavariani and a.w. wolfendale p n lebedev physical institute, moscow , russia. physics department, durham university, durham, dh le, uk february , abstract we discuss three of the known puzzles of the cosmic ray anisotropy in the pev and sub-pev energy region. they are ) the so called inverse anisotropy, ) the irregularity in the energy dependence of the amplitude and phase of the rst harmonic and ) the contribution of the single source. keywords: inverse anisotropy, amplitude and phase, single source",
        "Subsections": [],
        "Groundtruth": "The text discusses three cosmic ray anisotropy puzzles in the pev and sub-pev energy range: the inverse anisotropy, irregularity in the energy dependence of the first harmonic amplitude and phase, and the contribution of a single source."
    },
    {
        "Section_Num": "1",
        "Section": "1 Introduction",
        "Text": "one of the most important and simultaneously most dicult studies of the origin of cosmic rays is the study of their anisotropy. the diculty is due to the extremely low level of the anisotropy and the steeply falling energy spectrum of cr. in their seminal book origin of cosmic rays issued in v.l.ginzburg and s.i.syrovatskii could only give an upper limit of the anisotropy as < % . more than years later v.l.ginzburg and his co-authors on the basis of data obtained by linsley j. could show that the amplitude and phase of the anisotropy are not constant, but vary with the energy . this variability is the consequence of the non-uniformity of the spatial distribution of cr sources and properties of the interstellar medium . sources of dierent locations and ages contribute in dierent energy regions and magnetic elds of dierent strength and orientations tend to isotropise the arrival directions of cr particles. one of the most interesting regions of the cr energy spectrum is sub-pev and pev region where the well-known knee is observed. the study of this region is dicult because of the very small intensity of particles and low anisotropy level. that is why there are many unsolved problems and puzzles here which still wait for their solution. below we describe only three of them. corresponding author: tel + e-mail address: erlykin@sci.lebedev.ru",
        "Subsections": [],
        "Groundtruth": "The study of cosmic ray anisotropy is challenging due to the low level of anisotropy and steep energy spectrum. Initial limits to anisotropy were given by Ginzburg and Syrovatskii, with later work showing variability with energy, attributed to varying source distribution and interstellar medium properties. The cosmic ray energy spectrum, especially in sub-peV and PeV regions, presents complexities and unsolved questions, including the observed knee. Many puzzles remain in this area awaiting resolution."
    },
    {
        "Section_Num": "2",
        "Section": "2 Puzzle 1: an inverse anisotropy",
        "Text": "due to cr scattering on the chaotic magnetic elds their motion in the galaxy is like a diusion from regions of higher to lower cr density. since the solar system is located at about kpc from the galactic center then the main source of cr - supernova remnants and pulsars are concentrated in the inner galaxy. the cr density is higher towards the galactic center and their gradient is directed from the inner to outer galaxy. hence we have to expect some anisotropy with the maximum ux from the inner galaxy. the expected and observed situation with the rst harmonic of the anisotropy in the sub-tev region is shown in figure ihe upper panel shows the amplitudes and it is seen that they do not exceed the value of the lower panel presents measurements of the phase in equatorial coordinates. if transferred to galactic coordinates they occupy the region delimited by two dashed lines. it is seen that when the expected phase of the maximum cr ux is from the inner galaxy with , the observed phase occupies the longitude -region between and , i.e. cr come preferentially from the outer galaxy. we call this phenomenon an inverse anisotropy. one of the possible explanations of an inverse anisotropy is to assume that it is a local phenomenon caused by a spatial orientation of the magnetic eld or the ism density uctuation. the sun is located in the local bubble at the inner edge of the orion spiral arm. the strength of the magnetic eld and of the ism density in the arm is higher than in the interarm region where the local bubble is located. the diusion in the nearby outer side of the galaxy is slower than locally. cr moving from the inner galaxy meet like a wall, a part of them reect from it, accumulate in a number and create an inverse gradient . there might be an alternative explanation . recent measurements of cr at sub- tev - sub-pev energies demonstrated the irregular non-power law shape of the energy spectrum. as an example the spectral hardening at - gevnoticed in cream, pamela, ams- experiments can be mentioned, the steepening at the magnetic rigid- ity of tv in the nucleon experiment and others. the observed irregularities point to the possible role of local sources. however, to be responsible for an observed inverse anisotropy these local sources should be located mostly in the outer galaxy. we should underline that these explanations are only a possible assumption with many internal uncertainties. however, we should emphasize that the inverse anisotropy is most likely a local phenomenon, which is caused by the reection of cr from a nearby region of higher ism density or the dominance of some local sources in the outer galaxy.",
        "Subsections": [],
        "Groundtruth": "Chaotic magnetic fields in the galaxy cause cosmic rays (CRs) to diffuse from regions of higher density to lower density, resulting in an anisotropy with higher flux from the inner galaxy. This phenomenon, known as inverse anisotropy, is observed in the sub-TeV region with the phase of maximum CR flux located in the outer galaxy rather than the inner galaxy as expected. Possible explanations include local effects such as magnetic field orientation or ISM density fluctuations. Measurements of CRs at sub-TeV to sub-PeV energies show irregularities in the energy spectrum, suggesting the influence of local sources, potentially located mostly in the outer galaxy. Overall, the inverse anisotropy is likely a local phenomenon influenced by nearby regions of higher ISM density or dominant local sources in the outer galaxy."
    },
    {
        "Section_Num": "3",
        "Section": "3 Puzzle 2: peculiarity of the amplitude and phase",
        "Text": "in general there are several features noticable in figure : a good consistency of the results at energies up to a few pev; figure : the observed amplitude and equatorial phase of the rst harmonic of the cr anisotropy. data denoted as are taken from the survey presented in , - super-kamiokande-i , - tibet iii , - baksan , - andyrchi , - eas-top . thick line above loge = with an arrow - upper limits of the amplitude, given by kascade . here and in the text below the energy e is in gev. the full line in the upper panel relates to calculations with our model described in section . the area between thin dashed lines in the lower panel denotes the region in galactic coordinates corresponding to the ra region occupied by experimental points in equatorial coordinates assuming that the observations were made at declinations about - the extremely small amplitude of the anisotropy; the visible rise of the amplitude a with energy e up to loge ; a moderate fall of the amplitude above loge up to a minimum at loge ; the rise of the amplitude beyond this minimum up to a few pev; the approximately constant phase at low energies which suddenly changes its direc- tion at about the same energy of loge . where the amplitude has a minimum; in the pev region, where the rise of the amplitude is observed, the phase has an apparent trend to recover up to its previous direction close to ra in what follows we shall endeavour to build a model which can reproduce these features with the minimum number of assumptions. this model contains three basic ingredients: the galactic disk, the halo and the single source . although we separate here the role of the single source, we understand that, in fact, it is just part of cr in the disk. the details of the model suggested to explain all these peculiar features are given in . here we give just its main features and they are illustrated in figure we assume that the anisotropy appears only in the vicinity of the sources, i.e. in the disk. the amplitude of the anisotropy a is connected with the cr intensity i, its gradient, gradi, and the diusion coecient d as a = dgradi ci . if the relative gradient gradi i = and d ed then a in the disk rises with energy as ad ed. the anisotropy of cr in the halo is postulated as being ah = and the isotropy of cr re-entrant from the halo back into the disk dilutes the anisotropy of cr produced and trapped in the disk. in this treatment we just consider the rst harmonic. later work will deal with higher multipoles. we calculate the amplitude of the rst harmonic for the case where only disk and halo contribute to cr as a = adid id + ih the result is shown in figure b by the dotted line. the rise of the amplitude at energies above gev is due to the rise of the diusion coecient in the expression for ad mentioned above. the slow decrease of a above tev is explained by the rising fraction of isotropic cr from the halo, which overcomes the rise of ad. however, this scenario does not reproduce the remarkable dip in the amplitude visible in the experimental data at loge = and the subsequent rise of the amplitude above this dip . we think that these features, if they are real, are connected with the existence of the single source, from which the cr energy spectrum is schematically shown by the dotted line in figure a and denoted as ss. however, this idea alone is not enough to reproduce the experimental data and here the examination of the phase of the rst harmonic could help. in figure b it is seen that after the moderate decrease in the energy interval - tev the phase suddenly changes to its opposite. we consider this change seriously and propose that cr from the single source have a phase opposite to that of the background at lower energies. this is a necessary complementary requirement in our model. the rising part of cr coming from the opposite direction would reduce the figure : schematic formation of the cr energy spectrum from the disk, the halo and the single source. the amplitude of the rst harmonic of the cr anisotropy obtained with contributions from disk + halo + single source and the same with contributions only from disk + halo without single source . anisotropy of the background from the disk and halo as a = |adid assiss id + ih + iss | the result of the calculations with the contribution of the single source is shown in figure b by the full line. a comparison with the experimental data is shown also in figure a by the full line. it is seen that after minimum in the dip the amplitude of the anisotropy starts rising again and it is caused by the rising contribution of the single source, which has the opposite phase. we think that the described model with three basic ingredients: disk, halo and single source is reasonable and is worthy of discussion. the new features advocated here are: the dominance of the halo component in the sub-pev region. it means that the cr which we observe and study in spite of being ourselves inside the disk come mostly from the halo. it is disputable but helps to understand the low anisotropy, small radial gradient of cr intensity and small level of irregularities in the regular power law energy spectrum. the idea about the single source, which has to be nearby and young and creates the knee, usually raises questions: if it is nearby why dont we see it in the anisotropy ?. it is a very reasonable question and this work gives the answer. the single source causes the stronger decrease and the dip in the amplitude of the dipole anisotropy at sub-pev energies. it is also seen in the change of the phase of the anisotropy at the same energies. it means that the single source should deliver cr from the direction opposite to the direction of cr from the background and it is a new assumption in the single source scenario. above the dip energy the amplitude starts to rise again with the opposite phase, as expected. the dip in this model appears as the result of subtraction of two bigger values. its position and shape are extremely sensitive to the choice of parameters participating in expression . the relatively good agreement with the experimental data is the result of the tting procedure, but, nevertheless: it demonstrates the possibility of achieving agreement within the framework of our simplistic model and the high sensitivity of the dip to the input parameters of the expression gives the possibility of investigating these parameters when precise results in this energy region are obtained. the phase of the rst harmonic in the pev region, where the contribution of the single source is big enough, could help to locate it on the sky. the present experimental data have a too big spread to make a conclusion. we understand that this scenario raises more questions than gives answers. for instance, the main questions are: do the halo and the single source really exist ? arguments for positive answers are given in the , but more supportive arguments are needed. why numerical estimates for cr intensity in the disk and the halo, calculated with the expression , are smaller than in the observations ? why is the energy spectrum of cr in the disk steeper than the spectrum in the halo ? according to our conception developed in the spectrum in the disk, with its higher turbulence in the interstellar medium due to sn explosions, should be atter than in the halo where there are no such powerful sources of turbulence as sn. to what extent are the simplied assumptions about the shapes and normali- sation of the disk, halo and single source spectra as well as other parameters: ad, ah and ass, reasonable and what will the more sophisticated approach do for the result ? the answers to these and other puzzling questions are the subject of further work.",
        "Subsections": [],
        "Groundtruth": "The text discusses the peculiarities of the amplitude and phase of cosmic ray (CR) anisotropy observations. Key features include the rise and fall of the amplitude with energy, changes in phase direction, and a dip in amplitude at loge = . The text proposes a model with three basic components—galactic disk, halo, and single source—to explain these features. The model suggests that the dip in amplitude and phase change may be due to the influence of the single source. The model is highlighted by its ability to reproduce these observed features and offers insights into the origin of CR anisotropy. Major questions raised include the existence of the halo and single source, discrepancies in intensity estimates, and the need for a more sophisticated approach to address uncertainties."
    },
    {
        "Section_Num": "4",
        "Section": "4 Puzzle 3: the nature of the Single Source",
        "Text": "the existence of the single source has been proposed by us to explain the puzzling sharpness of the knee in the size spectrum of extensive air showers ( see and later publications ). the physical basis of this proposal is the evident non-uniformity of the spatial and temporal distributions of sn explosions and subsequent snr. as a result one sn could explode not very long ago and close to the solar system.. its contribution to the cr intensity is rather high and it gives rise to a small peak above the background from other snr - it is our single source. to search for the single source authors used the so called dierence metod. it is dierent from the traditional study of the cr intensity. its main idea is that the dierence between properties of eas coming from the direction of the single source and from the opposite direction should be maximum. the method is stable against random experimental errors and allows to separate anomalies connected with the laboratory coordinate system from anomalies in the celestial coordinate system. the method allows to study the whole celestial sphere including regions outside of the line of sight of the experimental installation. to search for an anisotropy authors used experimental data obtained with the gamma experimental eas array and have taken the eas age parameter as a char- acteristic of the eas property. the dierence between eas age distributions in two opposite directions was quantied by the reduced value . the result of the search is given in figure the maximum dierence in galactic coordinates has been found figure : two-dimensional prole of the reduced of the dierence between age distributions of eas coming from opposite directions in galactic coordinates. at = , b = the closest source to this location is cluster vela and therefore it is a good candidate for the role of the single source, which is responsible for the knee in the cr spectrum and the minimum in the amplitude of the anisotropy. however, a rm conclusion can be drawn only if it will be conrmed by the analysis of data from other eas arrays. an alternative explanation is the inuence of the regular magnetic eld in the area surrounding the earth especially taking into account the nearby spiral arm ( see puzzle of this paper ). the only point we should like to stress is that the eect of the regular magnetic eld in pev and sub-pev energies should be small compared with the eect of the single source in order not to destroy the diusive character of cr propagation. additional asguments in favour of the dominance of the single source in the formation of the maximum dierence in age distributions of eas coming from the opposite directions could be the vicinity of vela source and the younger age of eas coming from the direction of the maximum since magnetic elds do not change the energy spectrum of eas and their ages. in the section of our paper we speculated that a minor shift of the maximum dierence from vela source seen in figure could be due to the inuence of this small regular magnetic eld.",
        "Subsections": [],
        "Groundtruth": "The single source hypothesis has been proposed to explain the sharp knee in the size spectrum of extensive air showers. This hypothesis suggests that a supernova (SN) explosion not long ago and close to the solar system contributes significantly to cosmic ray intensity, creating a peak above the background from other SN remnants. Researchers used the \"difference method\" to search for this single source, which compares properties of air showers from the single source direction and the opposite direction. The analysis pointed towards the Vela cluster as a potential single source candidate, but further data analysis from other arrays is needed for confirmation. The influence of the regular magnetic field in the Earth's vicinity is also considered, although the impact of the single source on cosmic ray propagation is believed to outweigh that of the magnetic field. The study speculates that minor deviations from the Vela source could be attributed to the influence of the regular magnetic field."
    },
    {
        "Section_Num": "5",
        "Section": "5 Conclusion",
        "Text": "we mentioned only puzzles existing in the pev and sub-pev energy region. however, there are many other there and after prince hamlet we can conclude that there are many things in heaven and earth, horatio, than are dreamt of in your philosophy. references ginzburg v.l., syrovatskii s.i., , the origin of cosmic rays, pergamon press, oxford berezinsky v.s.,bulanov s.v., ginzburg v.l., dogiel v.a., ptuskin v.s., , astrophysics of cosmic rays, north holland, amsterdam guillian g. for the sk coll., observation of the anisotropy of tev pri- mary cosmic ray nuclei ux with the super-kamiokande- detector, phys. rev. d:; astro-ph/ amenomori m. et al., tibet as-gamma coll., northern sky galactic cosmic ray anisotropy between - tev with the tibet air shower array, astrophys.j., , ; arxiv: kozyarivsky v.a. et al., mean diurnal variations of cosmic ray intensity mea- sured by andyrchi air shower array and baksan underground scintillation tele- scope, astro-ph/ aglietta m. et al., evolution of the cosmic ray anisotropy above ev., astro- phys. j. lett., l, ; arxiv: antoni t. et al. for kascade coll., large scale cosmic ray anisotropy with kas- cade, astrophys. j., , ; astro-ph/ erlykin a.d., wolfendale a.w. the anisotropy of galactic cosmic rays as a product of stochastic supernova explosions, astropart. phys., , , ; astro-ph/ erlykin a.d., sibatov r.t., uchaikin v.v., wolfendale a.w. a non-local relativis- tic transport approach to the cosmic ray anisotropy problem, proc int. cosm. ray conf., hague, , proc.of sci., pos ahlers m., desiphering the dipole anisotropy of galactic cosmic rays, phys. rev. lett., , , erlykin a.d., wolfendale a.w. a single source of cosmic rays in the range ev j.phys.d: nucl. part. phys., , , pavlyuchenko v.p., martirosov r.m., nikolskaya n.m., erlykin a.d., dierence method to search for the anisotropy of primary cosmic radiation, j. phys. g: nucl. part. phys, , ,",
        "Subsections": [],
        "Groundtruth": "The section concludes by highlighting the puzzles existing in the pev and sub-pev energy region of cosmic rays. It references various works on cosmic rays, emphasizing that there is much more to be discovered beyond our current understanding. References include studies on cosmic ray anisotropy, observations of cosmic ray nuclei flux, diurnal variations in cosmic ray intensity, and the evolution of cosmic ray anisotropy. Additionally, sources discuss the role of supernova explosions, relativistic transport approaches, and potential single sources of cosmic rays. Further research is needed to fully comprehend the complexities of cosmic ray phenomena."
    },
    {
        "Section_Num": "Abstract",
        "Section": "Abstract",
        "Text": "arxiv:v mar the lowest two-sided cell of weighted coxeter groups of rank jianwei gao abstract. in this paper, we give precise description for the low- est lowest two-sided cell c and the left cells in it for a weighted coxeter group of rank then we show conjectures p p and e p hold for c and do some calculation for the based ring of c",
        "Subsections": [],
        "Groundtruth": "The text provides a precise description of the lowest two-sided cell and its left cells in a weighted Coxeter group of rank. It also discusses conjectures P, P', and E' that hold for this cell and presents calculations for the associated based ring."
    },
    {
        "Section_Num": "0",
        "Section": "0. Introduction",
        "Text": "let be a weighted coxeter group. in , g.lusztig conjectured that the maximal weight value of the longest elements of the nite parabolic subgroups of w is a bound for . this property is referred as boundness of a weighted coxeter group([l, ]). when the rank of w is or , this conjecture is clear. in [gao, ], i proved this conjecture when rank = as a consequence, w has a lowest two-sided cell c, see . in this paper, we give precise description for c and the left cells in it. then we can show conjectures p p and e p hold for c at last, we do some calculation for the based ring of c",
        "Subsections": [],
        "Groundtruth": "A weighted Coxeter group has a conjectured property known as boundness, where the maximal weight value of the longest elements of finite parabolic subgroups functions as a bound for it. The conjecture holds for groups with a rank of 0 or 1, and has been proven for rank 2 in a study. Detailed descriptions are provided for the lowest two-sided cell C and the left cells within it in this paper. Additional calculations for the based ring of C show that conjectures P P and E P are satisfied."
    },
    {
        "Section_Num": "1",
        "Section": "1. Preliminaries",
        "Text": ". in this paper, for any coxeter group , we assume the gen- erating set s is nite. we call |s| the rank of and denote it by rank. we use l for the length function and for the bruhat order of w. the neutral element of w will be denoted by e. for x w, we set l = {s s|sx < x}, r = {s s|xs < x}. for s, t w, let mst z s{} be the order of st in w. for any i s, let wi = i. then is also a coxeter group, called a parabolic subgroup of . denote the longest element of wi by wi if |wi| < . for s, t s, s = t, we use wst instead of w{s,t} and wst instead of w{s,t}. for w, w, , wn w, we often use the notation w w wn instead of ww wn if l = l + l + + l. key words and phrases. weighted coxeter group, hecke algebra, two-sided cell, left cell, based ring. jianwei gao . let be a coxeter group. a map l : w z is called a weight function if l = l + l for any w, w w with l = l+l. then we call a weighted coxeter group. in this paper, the weight function l for any weighted coxeter group is assumed to be positive, that is, l > for any s s. . let be a weighted coxeter group and z be the ring of laurent polynomials in an indeterminate v with integer coe- cients. for f = p nz anvn z \\ {}, we dene deg f = max nz an= n. complementally, we dene deg = . for w w, set vw = vl z. the hecke algebra h of is the unital associative zalgebra dened by the generators ts and the relations (ts + v s ) = , s s. tsttts | {z } mst factors = tttstt | {z } mst factors , s, t s, mst < . obviously, te is the multiplicative unit of h. for any w w, we dene tw = tsts tsn h, where w = ss sn is a reduced ex- pression of w in w. then tw is independent of the choice of reduced expression and {tw|w w} is a zbasis of h, called the stan- dard basis. we dene fx,y,z z for any x, y, z w by the identity txty = x zw fx,y,ztz. the following involutive automorphism of rings is useful, called the bar involution: : h h vn vn. ts t s . we have tw = t w for any w w. we set h = m ww ztw, h< = m ww vztw. we can get the following facts by easy computation. lemma . for any x, y, w, we have fx,y,e = x,y for any x, y, z w, we have deg fx,y,z min{l, l, l}. for any x, y, z w, we have fx,y,z = fy,z,x = fz,x,y the lowest two-sided cell of weighted coxeter groups of rank for any nite parabolic subgroup wi of w, x wi, we have deg fwi,wi,x = l. dene the degree map deg : h z {} x ww fwtw max{deg fw|w w}. and we set n = max is |wi |< l. g.lusztig gave the following conjecture in . conjecture . let be a weighted coxeter group, s is nite and l is positive, then n is a bound for . namely, deg n for all x, y w. remark . when w is a nite coxeter group, this conjecture can be proved using lemma . in , g.lusztig proved this conjecture when w is an ane weyl group. in , j.shi and g.yang proved this conjecture when w has complete coxeter graph. in , i proved this conjecture when the rank of w is . for any w w, there exists a unique element cw h such that cw = cw and cw tw h< the elements {cw|w w} form a zbasis of h, called the kazhdan-lusztig basis. we dene hx,y,z, px,y z for any x, y, z w such that cxcy = x zw hx,y,zcz. cy = x xw px,ytx. these polynomials px,y are called kazhdan-lusztig polynomials. for w w, i = r, j = l, we have the factorization w = x wi = wj y for some x, y w. then we set ex = x xx l=l+l pxwi,wtx. fy = x yy l=l+l pwjy,wty. . using kazhdan-lusztig basis, we can dene the preorders l, r, lr on w. these preorders give rise to equivalence relations l, r, lr on w respectively. the equicalence classes are called left cells, right cells and two-sided cells of w. then we have partial orders l , r, lr jianwei gao on the sets of left cells, right cells and two-sided cells of w respectively. for x, y w, we have x l y if and only if x r y now we assume conjecture holds and w has a lowest two-sided cell c it is easy to see that deg hx,y,z n for any x, y, z w, so we can dene the a-function a : w n w max x,yw deg hx,y,w. for any x, y, z w, we dene x,y,z, x,y,z z such that fx,y,z = x,y,zvn + lower degree terms. hx,y,z = x,y,zva + lower degree terms. lemma . let x, y, z w. we have x,y,z = y,z,x = z,x,y. we have x,y,z = y,x,z if x,y,z = , then x l y, y l z, z l x, a = a = a = n, and x,y,z = x,y,z = y,z,x = z,x,y. if x,y,z = and a = n, then x,y,z = x,y,z = . in , g.lusztig gave some conjectures about the hecke algebra, which are called p p and e p. when these con- jectures hold for c, we can study the based ring j of c, which is an associative ring with zbasis {tw|w c} and multiplication txty = x zc x,y,ztz for any x, y c in section of this paper, we will do some calculation for j to study its structure when the rank of w is the boundness of from now on, we assume is a weighted coxeter group of rank and l is positive. we set n = max is |wi |< l. in , i proved conjecture in this case. theorem . we have deg n for all x, y w. remark . in the proof of , i set s = {r, s, t}, mrt = , msr mst, w is innite and w is not an ane weyl group, otherwise this conjecture has been proved. when msr = and mst = , or msr = mst = , the conjecture is easy to check, so i only need to consider the following three cases in . case : msr = > mst r s t mst the lowest two-sided cell of weighted coxeter groups of rank case : > msr mst , msr r s t msr mst case : > msr , mst = r s t msr now we set m = {wj|j s, |wj| < , l = n}. = {x u y|x, y w, u m}. for any wj m, we set bj = {x w|r s \\ j}. uj = {y w|l s \\ j and swjy / for all s j}. then theorem has the following corollaries. corollary . if x, y w satisfy deg = n, then both x and y . for any wj m, q wj, x, y w, r, l s \\ j, we have deg n l. in particular, txwjty = txwjy. for any wj m, the left cell of w containing wj is {x wj|x w} = {y w|r = j}, the right cell of w containing wj is {wj x|x w} = {y w|l = j}. proof. see . corollary . let wj m, x bj, q < wj. if y b j , l , then deg < n l for the three cases listed in remark . if y uj, then deg < n l for any weighted coxeter group of rank proof. by the proofs in , we have the following tables. case wj q deg wst q < wst, l case jianwei gao wj q deg wsr sr max{l, l} wsr rs max{l, l} wsr rsr l wsr srs max{l, l} wsr q < wsr, l wst st l wst ts l wst tst wst sts l wst q < wst, l case wj q deg wsr sr l wsr rs l wsr srs l wsr rsr l wsr srsr l wsr rsrs l wsr rsrsr l wsr srsrs wsr q < wsr, l we have deg < n l in all these three cases. it is obvious when w is a nite coxeter group. we can get this inequality by when w is an ane weyl group and by [xie, ] when w has complete coxeter graph. now we only consider other case. if l = , by corollary , we get deg < n since y / . if l = , by corollary , we get deg < n since qy / , so deg < n l. if l , then we must in the three cases listed in remark , we get deg < n l by .",
        "Subsections": [],
        "Groundtruth": "The text discusses weighted Coxeter groups, the Heckle algebra, two-sided cells, left cells, based rings, and Kazhdan-Lusztig polynomials. It presents various conjectures and theorems related to the structure and properties of these algebraic structures. A key focus is on proving conjectures about the degrees of elements in the Heckle algebra. The study involves calculations and analysis of the Heckle algebra and the based ring, with particular attention to cases related to the rank of the weighted Coxeter group. The text provides corollaries and tables that illustrate the relationships between elements in different contexts within the algebraic framework."
    },
    {
        "Section_Num": "2",
        "Section": "2. The boundness of (W,S,L)",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "3",
        "Section": "3. The lowest two-side cell c0",
        "Text": "now we x an element wj m and let c be the two-sided cell of w containing wj. in , i proved the following consequence. proposition . the two-sided cell c is the lowest two-sided cell of w. we have = {w w|a = n} c by corollary and corollary , we can prove the following propo- sition using the same method used in . proposition . for wj m, x bj, y uj, we have cxwjy = excwjfy. the lowest two-sided cell of weighted coxeter groups of rank for wj m, y uj, x u j , the set j,y = bjwjy is a left cell of w and x,j = xwjb j is a right cell of w. now we can give precise description for c and the left cells in it. theorem . we have c = = {w w|a = n}. the lowest two-sided cell c can be decomposed into left cells as c = a wjm,yuj j,y . proof. it is clear that = s wjm bjwjuj = s wjm u j wjb j c we claim that for any x w, y , if x l y or x r y, then x . we only prove the x l y case. first, we assume y = zwjz for some wj m, z bj, z uj, hs,y,x = for some s s. by proposition , we have csczwjz = csczwjfz = x zbj hs,zwj,zwjczwjfz = x zbj hs,zwj,zwjczwjz we get x = zwjz for some z bj and z uj, the claim is proved. for w c, we have w lr wj and wj , so w by the claim. on the other hand, we have = {w w|a = n} c by proposition . this conclusion follows. it is clear that c = s wjm,yuj j,y . if wj, wj m, y uj, y uj such that j,y = j,y, then there exists x bj such that xwjy = wjy. if x = e, we take s l, then s l = l = j and swjy = sxwjy , contradict to y uj. so we have x = e. thus, j = l = l = j, y = y. therefore, it is a disjoint union. proposition . we have the following table for weighted coxeter groups of rank w the number of left cells in c nite coxeter group ane weyl group |w| msr = , mst = mrt = msr = mst = , > mrt or mrt msr = mst = mrt = or or other cases jianwei gao when w is an ane weyl group, w denotes the nite weyl group corresponding to w. when the number of left cells in c has various possibilities, it depends on the weight function l. proof. it is clear when w is a nite coxeter group. see when w is an ane weyl group. then we assume msr mst mrt and consider the following cases. msr = mst = mrt = . we have n = max{l, l, l}. we may assume l l l. if l = l = l, then {r},e,{s},e and {t},e are all the left cells in c if l = l > l, then {r},e,{s},e,{r},t and {s},t are all the left cells in c if l > l l, then for dierent k n, {r},k are dierent left cells in c msr = mst = , > mrt we have n = max{l, l}. if l > l, then for dierent k n, {r,t},k are dierent left cells in c if l = l, then {r,t},e and {s},w, w wrt \\ {wrt} are all the mrt left cells in c if l > l, then {s},w, w wrt are all the mrt left cells in c msr = , mst = mrt = now we have n = max{l, l}. if l > l, then {s,t},e and {s,t},r are all the left cells in c if l = l, then {s,t},e and {r,t},e are all the left cells in c if l > l, then {r,t},e and {r,t},s are all the left cells in c msr = , > mst , mrt = we have n = max{l, l}. if l > l, then for dierent k n, {s,t},k are dierent left cells in c if l l, then for dierent k n, {r,t},k are dierent left cells in c msr = , > mst, mrt we have n = max{l, l}. we may assume n = l, then for dierent k n, {s,t},k are dierent left cells in c > msr mst , msr , mrt = we have n = max{l, l} > l. if l l, then for dier- ent k n, {s,r},k are dierent left cells in c if l > l, then for dierent k n, {s,t},k are dierent left cells in c > msr , mst = , mrt = we have n = l > max{l, l}. for dierent k n, {s,r},k are dierent left cells in c > msr mst mrt if l = n, then for dierent k n, {s,r},k are dierent left cells in c l = n and l = n are similar. > msr mst , mrt = , l = n. for dierent k n, {r,t},k are dierent left cells in c > msr mst , mrt = , l = n. for dierent k n, {s,r},k are dierent left cells in c > msr , mst = mrt = now we have n = l > the lowest two-sided cell of weighted coxeter groups of rank l = l. for dierent k n, {s,r},k are dierent left cells in c for n z, dene n : z z x kz akvk an. for z w, dene n and nz z \\ {}, such that pe,z = nzv + lower degree terms. let d = {z w|a = }. proposition . we have the following propositions p p and e p . in particular, conjectures p p and e p hold for c p for z c, we have a . p for d d c, x, y c, if x,y,d = , then x = y p for y c, there exists unique d d c such that y,y,d = p for z, z w, if z c or z c, and z lr z, then a a. p for d d c, y c, if y,y,d = , then y,y,d = nd = p for d d c, we have d = e. p for x, y, z w, if x c or z c, then x,y,z = y,z,x. p for x, y w, z c, if x,y,z = , then x l y, y l z, z l x p for z, z w, if z c or z c, z l z, and a = a, then z l z. p for z, z w, if z c or z c, z r z, and a = a, then z r z. p for z, z w, if z c or z c, z lr z, and a = a, then z lr z. p if y c wi for some i s, then a computed in terms of wi is equal to a computed in terms of w. p for any left cell in c, we have |d| = assume d = {d}, then x,x,d = for all x . p for any z c, we have z lr z p for x, x w, y, w c, we have the equality p yc hx,y,y hw,x,y = p yc hx,w,y hy,x,y in z z z. e p for x, y, z w, z c, if x,y,z = , z l z, then there exists x c, such that a = jianwei gao proof. by proposition and theorem , we can prove p p using the same method used in . here we only give the proof of e p . since x,y,z = , z c, by p, we have y l z. since z l z, z c, we get z l z by p so y, z, z are in the same left cell in c by theorem , we assume y = awjb, z = awjb, z = awjb for some wj m, b uj, a, a, a bj. then we have deg = deg = n. thus there exists x w, such that deg fy,z,x = n, by lemma and theorem , we get x c we have deg fx,y,z = deg fy,z,x = n, so deg hx,y,z = n, that is, a =",
        "Subsections": [],
        "Groundtruth": "Summary: The text discusses the lowest two-sided cell in weighted Coxeter groups and provides detailed proofs and propositions related to left cells within this cell. It presents a decomposition of the lowest two-sided cell into left cells and examines the number of left cells in the group for different cases and weight functions. It also delves into the relationships between elements in the cells, proving various conjectures and propositions related to their interactions. The analysis involves specific calculations and steps to determine the properties and structure of left cells within the Coxeter group."
    },
    {
        "Section_Num": "4",
        "Section": "4. The based ring of c0",
        "Text": "in this section, for a weighted coxeter group of rank , we study the based ring j of its lowest two-sided cell c we want to calculate txty for any x, y c, but it is quite a dicult problem. for wj, wj m, we set pj,j = {x w|l = j, r = j}. let p = [ wj,wjm pj,j. by , we have the following lemma. lemma . for any w c, there exists unique wj, wj m, pw pj,j, x u j , y uj, such that w = x pw y. moreover, we have cw = excpwfy. let w, w, w c we have the following decomposition as in . w = x p y, w = x p y, w = x p y if y = x , y = x , y = x , r = l, r = l, r = l, then w,w,w = p,p,p otherwise, w,w,w = the lemma above simplify our work to calculate txty for x pj,j, y pj,j, but it is still not easy. so we assume x indecomposable at rst. for x pj,j p, we call x indecomposable, if x / m, and there doesnt exist wj m, x pj,j \\ m, x pj,j \\ m such that x = xwjx then we have the following proposition. proposition . let be a weight coxeter group of rank but not an ane weyl group. assume wj, wj, wj m, x pj,j and indecomposable, y pj,j, then txty = txwjy + twjxy. if there exists y w such that y = x y, then = , otherwise = to prove this proposition, we need two lemmas. the lowest two-sided cell of weighted coxeter groups of rank lemma . let x = x wj pj,j and indecomposable, r j. then we have x r / c or x r wjuj. proof. we assume xr c since xwj r xr, we get xwj r xr by p, so l = l = j. assume x r = wj x for some x b j . we claim that x uj. otherwise, there exists r j such that rwj x c by theorem , we may assume rwj x = x wj x for some wj m, x bj, x b j . then we have x = x wj = x r rwj = r rwj x rwj = r x wj x rwj = wj. since x r r r x wj, we get x r r r x wj by p, so l = l = j. thus, we get r x wj pj,j. on the other hand, since x r l wj x, by p, we get r r(wj x) = r, so r = j. thus, wj x rwj pj,j, contradict to the indecomposability of x. lemma . let x pj,j with j = {r, r} and r = r assume x = x wj for some x bj. if l = j, then we have l = j or l = j. proof. if l = l = j, we assume x r = wj x for some x b j . x r = wj x for some x b j . then we have x = wj x rwj = wj x rwj. so x rwj = x rwj. since l = j, we get x, x bj. thus, rwj = rwj and then r = r, a contradiction. . now we prove proposition . finite coxeter groups dont have any indecomposable elements. if w has complete coxeter graph, see . for x, y, z c, we have x,y,z = x,y,z, so we may compute txty. it is much easier than computing cxcy. if msr = and mst = mrt = , we can list all the indecomposable elements and check this proposition directly. jianwei gao weight function indecomposable elements l > l srst l = l srt, rst l > l rsrt the case of msr = mst = , mrt = is also easy. weight function indecomposable elements l > l rtsrt l = l rts, srt, srs, sts l > l srs, sts, srts from now on, we only consider the three cases listed in remark . we assume x = xwj for some x bj, y = wj y for some y b j . then we have txty = x qwj fwj,wj,qtxqty. when q = wj, by lemma and corollary , we have deg fwj,wj,wj = l = n and txqty = txwjy. since fwj,wj,wj is a monic polynomial, we know fwj,wj,wjtxwjty contributes txwjy for txty. when q < wj and l , by corollary , we have deg < n. so x q<wj, l fwj,wj,qtxqty has no contribution for txty. now we assume j = {r, r} s with r = r then we have x l fwj,wj,qtxqty = (vr v r )txrty + (vr v r )txrty + txty. if deg ((vr v r )txrty) = n or deg = n, then we have deg = n, so xr c by corollary . moreover, we have xr wjuj by lemma . assume fxr,ry,z = n for some z w, then xr,ry,z = xr,ry,z = by lemma , there exists y w, such that y = xy. in this case, we have z = and xr,ry, = xr,ry, = , so precisely one of deg ((vr v r )txrty) and deg is n. similarly, we know precisely one of deg ((vr v r )txrty) and deg is n. now we claim that precisely one of deg ((vrv r )txrty), deg ((vr v r )txrty) and deg is n. otherwise, we must have deg ((vr the lowest two-sided cell of weighted coxeter groups of rank v r )txrty) = deg ((vr v r )txrty) = n and deg < n. thus l = j. by lemma , we have l = j or l = j. since xwj r xr, xwj r xr, we get xr / c or xr / c by p, contradict to corollary . summarizing the arguments above, we know x l fwj,wj,qtxqty contributes twjxy for txty. if there exists y w such that y = xy, then = , otherwise = we have completed the proof of proposition . references a.bjorner, f.brenti: combinatorics of coxeter groups, springer . k.bremke: on generalized cells in ane weyl groups, j.algebra j.gao: the boundness of weighted coxeter groups of rank , arxiv:v, j.e.humphreys: reection groups and coxeter groups, cambridge studies in advanced mathematics . d.kazhdan, g.lusztig: representations of coxeter groups and hecke al- gebras, invent. math. g.lusztig: cells in ane weyl groups, algebraic groups and related top- ics, adv. stud. pure math. g.lusztig: cells in ane weyl groups, ii, j.algebra g.lusztig: hecke algebras with unequal parameters, crm monograph se- ries . j.shi, g.yang: the boundness of the weighted coxeter group with complete graph, proc. amer. math. soc. n.xi: representations of ane hecke algebras, lect. notes math., springer . n.xi: lusztigs a-function for coxeter groups with complete graphs, bull. inst. math. acad. sinica x.xie: the based ring of the lowest generalized two-sided cell of an ex- tended ane weyl group, j.algebra x.xie: the lowest two-sided cell of a coxeter group with complete graph, j.algebra p.zhou: lusztigs a-function for coxeter groups of rank , j.algebra jianwei gao, beijing international center for mathematical re- search, peking university, no. yiheyuan road, beijing, , peo- ples republic of china e-mail address: gaojianwei@bicmr.pku.edu.cn",
        "Subsections": [],
        "Groundtruth": "For a weighted Coxeter group of rank n, we examine the based ring of its lowest two-sided cell. Calculating txty for any x, y in this cell is a challenging task. By introducing various definitions and lemmas, we simplify the computations, particularly with the assumption that x is indecomposable initially. Several propositions and lemmas are presented to aid in these calculations. Finite Coxeter groups are discussed, highlighting the absence of indecomposable elements. The proposition is proven for weight Coxeter groups not being affine Weyl groups. Various references and mathematical concepts are cited to support the arguments presented in the section."
    },
    {
        "Section_Num": "References",
        "Section": "References",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "Abstract",
        "Section": "Abstract",
        "Text": "arxiv:v jan a field-size independent code construction for groupcast index coding problems mahesh babu vaddi and b. sundar rajan department of electrical communication engineering, indian institute of science, bengaluru , ka, india e-mail: {vaddi, bsrajan}@iisc.ac.in abstractthe length of an optimal scalar linear index code of a groupcast index coding problem is equal to the minrank of its side information hypergraph. the side-information hypergraph becomes a side-information graph for a special class of groupcast index coding problems known as unicast index coding problems. the computation of minrank is an np-hard problem. there exists a low rank matrix completion method and clique cover method to nd suboptimal solutions to the index coding problem represented by a side-information graph. however, both the methods are np-hard. the number of computations required to nd the minrank depends on the number of edges present in the side-information graph. in this paper, we dene the notion of minrank-critical edges in a side-information graph and derive some properties of minrank, which identies minrank- non-critical edges. using these properties we present a method for reduction of the given minrank computation problem into a smaller problem. also, we give an heuristic algorithm to nd a clique cover of the side-information graph by using some binary operations on the adjacency matrix of the side-information graph. we also give a method to convert a groupcast index coding problem into a single unicast index coding problem. combining all these results, we construct index codes (not necessarily optimal length) for groupcast index coding problems. the construction technique is independent of eld size and hence can be used to construct index codes over binary eld. in some cases the constructed index codes are better than the best known in the literature both in terms of the length of the code and the minimum eld size required.",
        "Subsections": [],
        "Groundtruth": "The text presents a technique for constructing index codes for groupcast index coding problems that is field-size independent. The method involves defining minrank-critical edges in a side-information graph and deriving properties to identify minrank-non-critical edges, reducing the computation problem. An algorithm for finding a clique cover of the side-information graph is provided, along with a method to convert groupcast index coding problems into unicast index coding problems. The constructed index codes may be better than existing codes in terms of length and minimum field size required."
    },
    {
        "Section_Num": "I",
        "Section": "I Introduction",
        "Text": "an index coding problem , comprises of a transmitter that has a set of k messages {x, x, . . . , xk}, and a set of m receivers {r, r, . . . , rm}. each receiver, rk = , knows a subset of messages, kk x, called its side- information, and demands another subset of messages, wk kc k, called its want-set. the transmitter can take cognizance of the side-information of the receivers and broadcast coded messages, called the index code, over a noiseless channel. the objective is to minimize the number of coded transmissions, called the length of the index code, such that each receiver can decode its demanded message using its side-information and the coded messages. an index coding problem with no restrictions on want- set and side-information is called a groupcast index coding problem. without loss of generality a groupcast index coding problem with m receivers and want-set wk for k can be converted into another groupcast index coding problem with p k |wk| receivers such that every receiver wants exactly one message. a groupcast index coding problem with k messages {x, x, . . . , xk} can be represented by a hyper- graph h with k vertices {x, x, . . . , xk} and p k |wk| number of hyperedges . consider a groupcast index coding problem with k mes- sages, m receivers each wanting one message and side infor- mation hypergraph h. let ek = ( . . . | {z } k . . . | {z } kk ) fk q . the support of a vector u fk q is dened to be the set supp = \b k : uk = . let e . we denote ue whenever supp e. then, the minrankq over fq is dened as min{rankfq({vk +ek}k : vk fk q , vk kk}. in , it was shown that for any given index coding problem, the length of an optimal scalar linear index code over fq is equal to the minrankq of its side- information hypergraph. however, nding the minrank for any arbitrary side-information hypergraph is np-hard . there exists a low rank matrix completion method to nd the rank of a binary matrix which is also np-hard . an index coding problem is unicast if the demand sets of the receivers are disjoint. an index coding problem is called single unicast if the demand sets of the receivers are disjoint and every receiver wants only one message. any unicast index problem can be equivalently reduced to a single unicast index coding problem . in a single unicast index coding problem, the number of messages is equal to the number of receivers. any suicp with k messages {x, x, . . . , xk} can be expressed as a side-information graph g with k vertices {x, x, . . . , xk}. in g, there exists an edge from xi to xj if the receiver wanting xi knows xj. in a unicast index coding problem with k messages and k receivers, the side- information graph has p k |kk| number of edges. a matrix a = ts g if ai,i = for all i and ai,j= whenever is not an edge of g. let rkq denote the rank of this matrix over fq. the minrankq is dened as minrankq min{rkq : a fits g}. in a side-information graph, if receiver ri knows xj and receiver rj knows xi, then the vertices xi and xj in the side-information graph are connected with an undirected edge. the undirected edges in the side-information graph contribute towards cliques in the side-information graph. all the receivers which want a message symbol in a clique can be satised by one index code symbol which is the xor of all message symbols present in the clique. all the receivers which wants a message symbol in a cycle of length k can be satised by k index code symbols. as nding the minrank of a side-information graph is np- hard, many researchers have proposed heuristic methods to solve the minrank problem. birk et al. proposed least difference greedy clique cover algorithm to nd the cliques in the side-information graph. ldg algorithm works by computing all the possible distances between the rows of the tting matrix. kwak et al. proposed extended least difference greedy clique cover algorithm to nd the cliques in the side-information graph. eldg algorithm works by computing all possible distances between the rows and columns of the tting matrix. eldg algorithm also gives a method to nd directed cycles of length three. awais et. al proposed an algorithm to piggyback a message which is sparsely connected to cycles of length k on the given graph g. for a side-information graph g with k vertices, the ad- jacency matrix a = is a binary square matrix of size k k with at the th position if there exist a directed edge from xi to xj and else. note that an undirected edge from xi to xj implies that there exists two directed edges, one from xi to xj and the other from xj and xi. hence, in the side-information graph if there exists an undirected edge from xi to xj, there exist in th and th positions in the adjacency matrix. for any positive integer n, the nth power of the adjacency matrix a gives some information about the paths of length n in the graph g. the th entry of the matrix an gives the number of paths of length n from the vertex xi to xj . similarly, the th entry of the matrix an gives the number of cycles of length n, which pass through xk. these properties of adjacency matrix were used in to give an algorithm to piggyback a message which is sparsely connected to cycles of length n on the given side-information graph g. in the given side-information graph g with k vertices, if there exists no cycles of length less than or equal to n for some positive integer n, the presence of a cycle of length n can be determined by computing an. in an, if all diagonal elements are zero, this implies that there exists no cycles of length n. index coding is motivated by wireless broadcasting appli- cations where the side-information may be a random quantity. let p be the probability that the receiver rk, k knows the message xj as side-information for j \\ k. then, g is a random graph with vertices {x, x, . . . , xk}, such that each edge between any two vertices occurs with probability p, independently of all other edges. the size of cliques and cycles in random graphs were extensively studied in the literature. grimmett et. al. proved that as the number of vertices in g tends to innity, the size of maximum clique in g would of log log( p ) with probability one. being the size of the cliques is small ), heuristic algorithms may be very useful to give a solution to the index coding problem. in this paper, we give a heuristic approach to nd the clique cover and a heuristic method to convert the groupcast index coding problem into a unicast index coding problem. in a given index coding problem with side-information graph g, an edge e is said to be critical if the removal of e from g strictly reduces the capacity region. the index coding problem g is critical if every edge e is critical. tahmasbi et al. studied critical graphs and analyzed some properties of critical graphs with respect to capacity region. in this paper, we analyze properties of minrank by dening the notion of minrank-critical edges. denition in a given index coding problem with side- information graph g, an edge e is said to be minrank-critical if the removal of e from g strictly increases the minrank of the graph g. an edge e e is said to be minrank-non-critical if the removal of e from g does not change the minrank of the graph g. the computation of minrank over binary eld requires the computation of the rank of p k |kk| number of binary ma- trices of size k k. hence, identication of every minrank- non-critical edge can reduce the number of computations required to compute the minrank by half. a directed graph g with k vertices is called -partial clique iff every vertex in g knows atleast messages as side-information and there exits atleast one ver- tex in g which knows exactly messages as side-information. for an index coding problem whose side information graph is a -partial clique, maximum distance separable code of length k and dimension +, over a nite eld fq for q k, can be used as an index code. -partial clique method provides a savings of k transmissions when compared with the naive technique of transmitting all k messages. tehrani et. al in proposed a partition multicast technique to address the groupcast index coding problem. in the partition multicast, one divides the messages into partitions and consider each partition as a partial clique. the messages are partitioned in such a way that the sum of savings of all partitions are maximized. however, the proposed partition multicast technique is suboptimal and np- hard and the required eld size depends on the number of messages in the partition and the number of messages known to each receiver in the partition. throughout we assume a nite eld with characteristic and use the xor operation for convenience. however the results are easily extendable to nite elds with any characteristic. a. contributions the main contributions of this paper are summarized as follows. we give a method to construct index codes for groupcast index coding problems which is independent of eld size. partition multicast index codes is the best known in the literature for groupcast index coding problems and they do not exist for all elds. we give instances of groupcast index coding problem where the length of index code obtained by using proposed method is less than that of partition multicast. to give a method to construct index codes for groupcast index coding problem, we develop many tools to address single unicast index coding problems. we dene the no- tion of minrank-critical edges in a side-information graph and derive some properties of minrank, which identify minrank-non-critical edges in a side-information graph. by using the properties of minrank, we give an algorithm to convert the given minrank computation problem into a smaller problem. we give a heuristic algorithm to nd a clique cover of the side-information graph. we also give a sub-optimal method to convert a groupcast index coding problem into a single unicast index coding problem. the remaining part of this paper is organized as follows. in section ii, we derive some properties of the minrank of a side- information graph and give a method to reduce the complexity of the minrank computation problem. in section iii, we give a method to construct index codes for groupcast index coding problems which works over every nite eld. we conclude the paper in section iv. in the appendix we give a heuristic algorithm to nd the clique cover of side-information graph. ii. properties of minrank of a side-information graph in this section, we derive some properties of the minrank of the index coding problem. by using the derived properties, we provide a method to identify minrank-non-critical edges of a side-information graph. as the number of computations required to nd exact value of the minrank is exponential in the number of edges present in the side-information graph, identication of every minrank-non-critical edge can reduce the number of computations required to compute the minrank by half. lemma let g be the side-information graph of an suicp with k messages. let g be the side-information graph after removing all the incoming and outgoing edges associated with a vertex xk for any xk v . then, the minrank of g is atmost one greater than the minrank of g. proof. the tting matrix a of g is a k k matrix. let gk be the induced subgraph of vertices v \\{xk} in g. let ak be the tting matrix of gk. the minrank of gk is dened as minrankq min{rkq : ak ts in gk}. the matrix ak is a matrix which can be obtained from a by removing the row and column corresponding to xk. thus the minrank of gk can not be greater than the minrank of g. the graph g is the union of gk and the isolated vertex xk. the minrank of the isolated vertex is one and the minrank of a union of disjoint subgraphs is equal to the sum of the minrank of the subgraphs. thus the minrank of g is atmost one greater than the minrank of g. lemma consider the side-information graph g in fig. in which v = v v v and there are no edges between the vertex sets v and v . then, we have minrank + minrank minrank minrank + minrank + minrank. g g g . . . . . . . . . . . . fig. proof. the tting matrix a of g is a v v matrix. let g be the induced subgraph of the vertices v v in g. let a be the tting matrix of g the minrank of g is dened as minrank min{rk : a ts in g}. the matrix a can be obtained from a by removing the rows and columns corresponding to the vertices in v . thus the minrank of g can not be greater than the minrank of g. but the graph g is the disjoint union of the graphs g and g hence, we have minrank = minrank + minrank minrank. if we remove all the incoming and outgoing edges to v in g, then the resulting graph is a disjoint union of g, g, g and hence the minrank of this resulting graph is the sum of the minrank of g, g and g hence, the minrank of g can not be more than the sum of minrank of g, g and g this along with completes the proof. as a special case of lemma the following lemma is obtained. lemma consider the graph g in fig. in which v = v v xk and there are no edges between the vertex sets v and v . then, we have minrank + minrank minrank minrank + minrank + theorem let g be a side-information graph and gk be the induced subgraph of g with the vertex set v \\ {xk} for any xk v . if xk is not present in any directed cycle in g and the minrank of gk is m , then the minrank of g is m. g g xk . . . . . . . . . . . . fig. proof. from lemma , minrank of g is either m or m. consider the case when the minrank of g is m that is, the row and the column corresponding to the vertex xk in the tting matrix of g are in the span of rows and columns corresponding to the remaining vertices of the graph respectively. let lj be the row corresponding to xj in the tting matrix of g for any j . let lk be in the span of li, li, . . . , lit for some i, i, . . . , it \\ {k}. that is, lk + li + li + . . . + lit = the rows li, li, . . . , ltt and lk have s in i, i, . . . , it and kth positions respectively = a = a = a = ). the linear dependence condition in indi- cates that there exist atleast one more non zero element in k, i, i, . . . , it positions in li, li, . . . , ltt and lk such that the ones in the diagonal positions gets canceled. note that a non zero element in jth position of ls for any j, s indicates that there is an edge from xs to xj in g. this implies that there exists a cycle xk xi xi . . . xit xk. this is a contradiction to our assumption that there exists no cycle through xk. hence, the rank of g is m. lemma in the side-information graph g, if xk is not present in any directed cycle in g, then all the incoming and outgoing edges from xk are minrank-non-critical. proof. let gk be the induced subgraph with the vertex set v \\ {xk} in g. let the minrank of gk be m if xk is not present in any directed cycle in g, from theorem , the minrank of g is m. let g be the graph after removing all the incoming and outgoing edges from xk in g. hence, the graph g is the union of gk and isolated vertex {xk}. as the minrank of an isolated vertex is one, we have minrank) = minrank + = m = minrank. hence, removing all incoming and outgoing edges from xk does not reduce the minrank of g and these edges are minrank- non-critical. let a be the adjacency matrix of g with k vertices. from the properties of the adjacency matrix, if at is zero, then there exist no cycles in g with length t and which contains xk. hence the presence of xk in any directed cycle can be obtained from pk i= ai. in pk i= ai, if th element is zero, this indicates that xk is not present in any directed cycle in g. hence by computing pk i= ai, one can identify all the vertices which are not present in any cycle. example consider the side-information graph g given in fig. the adjacency matrix a of g and p k= ak are shown below. the element in p k= ak is zero indicates that the vertex x is not present in any directed cycle. hence, from theorem , all the incoming and outgoing edges from x are minrank-non-critical. let a be the matrix after deleting the third row and third column of a. we have minrank=minrank+ and we can compute the minrank of a by computing the minrank of a x x x x x x x fig. a = , x k= ak = . lemma let c be the set of vertices in any clique of size t in the graph g. let gc be the side-information graph after removing all the incoming and outgoing edges associated with the t vertices in c, i.e., gc = v \\ c. then, the minrank of gc is atmost one greater than the minrank of g. proof. let g be the subgraph induced by the t vertices in the clique c in g and g be the subgraph induced by the remaining vertices in g. we have v = v v . the minrank of g is one as it is a clique. hence, we have minrank minrank + minrank = minrank = + minrank + minrank. the last inequality in the above equation follows from the fact that g is a subgraph of g. this completes the proof. denition let g be a side-information graph. let ci = {xi, xi, . . . , xi|ci|} and cj = {xj, xj, . . . , xj|cj |} be two cliques in g. let vr = v \\ ({xi, xi, . . . , xi|ci|} {xj, xj, . . . , xj|cj |}). we say that the cliques ci and cj are cycle-free if there exists atleast two vertices xk ci and xk cj such that there is no cycle consisting of vertices only from a non-trivial subset of {xk, xk} and any subset of vr. the three examples given below illustrate denition example consider the side-information graph g given in fig. in g, an undirected edge represents a clique of size two. in g, there exist two cliques {x, x, x} and {x, x}. let vr = v \\ = {x, x, x, x, x, x}. in g, every vertex in the clique {x, x, x} is having an out going edge to every vertex in the clique {x, x}. the vertex x is present in a cycle which comprises of vertices only from the set vr (x x x x). similarly, the vertex x is present in a cycle which comprises of vertices only from the set vr . the vertex x is not present in any cycle comprising of vertices only from the set vr. in the clique {x, x}, both the vertices x and x are not present in any directed cycle which comprises of vertices only from the set vr. but, there exists a directed cycle comprising of x from clique {x, x, x} along with x from clique {x, x} and vertices only from the set vr . there also exists a directed cycle comprising of x from clique {x, x, x} along with x from clique {x, x} and vertices only from the set vr . hence, according to denition , the clique {x, x, x} and the clique {x, x} are not cycle-free. x x x x x x x x x x x x x x x fig. : side-information graph g example consider the side-information graph g given in fig. the graph in fig. is same as that of the graph in fig. , except one edge from x to x is removed. in g, there exist two cliques {x, x, x} and {x, x}. in the graph g, the vertex x in the clique {x, x, x} is not present in any cycle comprising of vertices only from the set vr. the vertex x in the clique {x, x} is not present in any directed cycle which comprises of vertices only from the set vr. there also does not exist a directed cycle comprising of x from the clique {x, x, x} along with x from the clique {x, x} and vertices only from the set vr. hence, according to denition , the clique {x, x, x} and the clique {x, x} are cycle-free. example consider the side-information graph g given in fig. the graph in fig. is same as that of the graph in fig. , except the direction of one edge from x to x is reversed. in g, there exist two cliques {x, x, x} and {x, x} and every vertex in the clique {x, x, x} is having an outgoing edge with every vertex of clique {x, x}. in the graph g, the vertex x in the clique {x, x, x} is not present in any cycle comprising of vertices only from the set vr. the vertex x x x x x x x x x x x fig. : side-information graph g x in the clique {x, x} is not present in any directed cycle which comprises of vertices only from the set vr. there also does not exist a directed cycle comprising of x from the clique {x, x, x} along with x from the clique {x, x} and vertices only from the set vr. hence, according to denition , the clique {x, x, x} and the clique {x, x} are cycle-free. x x x x x x x x x x x fig. : side-information graph g theorem given below identies the minrank-non-critical edges between cliques. theorem let g be a side-information graph. let ci = {xi, xi, . . . , xi|ci|} and cj = {xj, xj . . . , xj|cj |} be any two cliques in g that are cycle-free. then all the edges between ci and cj are minrank-non-critical. proof. let the number of edges between ci and cj be . we prove that all these edges are minrank-non-critical. let g be the side-information graph after deleting edges between ci and cj in g. we prove that the minrank of g can not be less than the minrank of g. let vr = v \\ . let g be the induced graph of vr in g. from lemma , we have minrank( g) minrank + we show that the minrank of g can not be less than minrank + let xk ci and xk cj be the two vertices such that there is no cycle consists of vertices from non-trivial subset of {xk, xk} and any subset of vr. let g be the induced graph of the vertices {xk, xk} in g. let g be the induced graph of the vertices ci cj \\ {xk, xk} in g. we have v = v v v . from denition and lemma , all incoming and outgoing edges from v to vr are minrank-non-critical. hence, the vertices in g are connected to g via g figure is useful to illustrate g. from denition , there exists no cycle among xk and xk. hence, we have minrank= from lemma , we have |{z} minrank + minrank minrank |{z} minrank +minrank + minrank. hence, from and , we have minrank( g) + minrank minrank. this completes the proof. . . . . . . . . . . . . xk xk g g g ci n fxkg cj n fxkg vr fig. note that denition and theorem are also applicable if clique ci is a single vertex or cj is a single vertex or both ci and cj are single vertices. theorem let g be a side-information graph with k vertices {x, x, . . . , xk}. let g be the graph obtained from g by the following reduction procedure: find a set of cliques {c, c, . . . , ct} in g such that all the t cliques partition v . note that any vertex is also a trivial clique of size one. if the cliques ci and cj are cycle-free, delete all the edges between ci and cj for every i, j . then, minrank = minrank( g). proof. let ci and cj be two cycle-free cliques. from the- orem , if ci and cj are cycle-free, then all the directed edges between ci and cj are minrank-non-critical. hence, the construction procedure given in the construction of g would not increase the minrank and we have minrank( g) = minrank. example given below illustrates theorem example consider the side-information graph g given in fig. in g, there exist two cliques {x, x, x} and {x, x} and every vertex in the clique {x, x, x} is having an outgoing edge with every vertex of clique {x, x}. ac- cording to denition , the clique {x, x, x} and the clique {x, x} are cycle-free. hence, from theorem , the six edges from clique {x, x, x} to clique {x, x} are minrank-non- critical. graph g after removing the six minrank-non-critical edges is shown in fig. from theorem , the minrank of g and g is same. x x x x x x x x x x x fig. theorem reduces the minrank computation problem into a smaller problem in terms of number of edges (number of vertices remain same after reduction). construction i given in next subsection reduces the minrank computation problem into a smaller problem in terms of both the number of vertices and the number of edges. a. a heuristic method to reduce the minrank computation problem in the following three steps, we give a heuristic approach to reduce the minrank computation problem into a smaller problem. we refer the following three steps as construction i in the rest of the paper. construction i step : let g be the graph obtained from theorem with {c, c, . . . , ct} being the set of t cliques in g. these t cliques partition v ( g) = {x, x, . . . , xk} (the cliques need not satisfy the cycle-free condition). note that any vertex is also a trivial clique of size one. let gr be the graph obtained from g after the following next two step: step : let {xi, xi, . . . , xi|ci|} be the vertices in the ith clique for i . if | ci |, combine these |ci| vertices into one new vertex yi. else, leave the vertex in ci as it is. step :now the number of vertices is equal to the number of cliques in g, that is t. if the number of directed edges from ci to cj in g are |ci|.|cj|, then introduce a directed edge from yi to yj for i, j . otherwise, there does not exist a directed edge from yi to yj for i, j . the reduction of minrank computation problem by using theorem and construction i is summarized below. g |{z} theorem g |{z} construction i gr minrank = minrank( g) minrank. in lemma , we give a sufcient condition when the minrank of the graphs g and gr are equal. the necessary and sufcient conditions that the side-information graph g need to satisfy such that the minrank of g is equal to the minrank of gr needs further investigation. for a graph g, the order of an induced acyclic sub-graph formed by removing the minimum number of vertices in g, is called maximum acyclic induced subgraph ). in , it was shown that mais lower bounds the minrank of g. that is, minrank mais. lemma let g be a side-information graph. let {c, c, . . . , ct} be a set of t cliques in g obtained in theorem if every pair of these t cliques are cycle-free, then the minrank of g is equal to the minrank of gr. proof. as there exist t cliques in gr and every clique requires one index code transmission, we have minrank t. given every pair of t cliques being cycle-free, from the- orem , all the incoming and outgoing edges to every pair of cliques are minrank-non-critical. let g be the graph after removing all the incoming and outgoing edges from every pair of cliques. we have minrank = minrank. in g, if we choose one vertex from each clique, then all these t vertices form an acyclic induced subgraph. hence, we have mais t. by combining -, we have t mais minrank = minrank minrank t. this completes the proof. lemma given below gives the relation between the index code for gr and the index code for g. lemma let g be the side-information graph of a single unicast icp. let gr be the graph obtained from g by using construction i. an index code c for the icp represented by gr can be used as an index code for the icp represented by g after replacing yi with the xor of vertices present in ci for i . proof. in the icp represented by g, receiver rj wants to decode xj for every j . let xj ci for some i [ : t]. from construction i, receiver rj knows all the messages corresponding to out neighbourhood of yi in gr. hence, rj computes yi from c and from yi it computes xj. the following three examples illustrate construction i. example consider the index coding problem represented by the side-information graph g given in fig. in g, the vertices {x, x, x} form a clique of size three and the vertices {x, x} form a clique of size two. in the graph g, there exists a directed edge from every vertex in the clique {x, x, x} to every vertex in the clique {x, x} except an edge from x to x from construction i, we can combine the vertices {x, x, x} into one vertex and the vertices {x, x} into another vertex in the reduced side-information graph gr. the reduced side-information graph gr is given in fig. in this example, the minrank of the graph g is four and the minrank of graph gr is four. the index code for the index coding problem represented by gr is {y, y, x, x} and the index code for the index coding problem represented by g is {x + x + x | {z } y , x + x | {z } y , x, x}. x x x x x x x fig. y y x x fig. : reduced side-information of g given in fig. example consider the index coding problem represented by the side-information graph g given in fig. from construction i, the reduced side-information graph gr is given in fig. in this example, the minrank of the graph g is three and the minrank of graph gr is three. the index code for the index coding problem represented by gr is {y + y, y + x, x + x} and the index code for the index coding problem represented by g is {x + x + x | {z } y + x + x | {z } y , x + x | {z } y +x, x + x}. in example and example , the minrank of side- information graph before and after reduction by using con- struction i is same. however, the reduction method given in construction i may not necessarily keep the rank same. for some graphs, the reduction procedure given in construction i x x x x x x x fig. y y x x fig. : reduced side-information of g given in fig. may increase the minrank of reduced side-information graph. example given below is useful to understand this. example consider the index coding problem represented by the side-information graph g given in fig. in g, the vertices {x, x, x} form a clique of size three and the vertices {x, x} form a clique of size two. in the graph g, there exists a directed edge from every vertex in the clique {x, x, x} to every vertex in the clique {x, x}. but, there does not exists an edge from every vertex in the clique {x, x} to x from construction i, the reduced side- information graph gr is given in fig. in this example, the minrank of the graph g is nine, but the minrank of graph gr is ten. x x x x x x x x x x x x x x fig. : side-information graph g with minrank nine in theorem and construction i, it is assumed that the cliques in the graph are known. note that nding a clique cover of a graph is an np-hard problem. there exist various heuristic algorithms to nd clique covers. in the appendix, we give a heuristic algorithm to nd the cliques by using binary operations on the adjacency matrix. there exist polynomial time cycle detection algorithms to check the cycle among a given set of vertices . hence, given two cliques, one can y y x x x x x x x x x fig. : reduced side-information of g given in fig. nd whether two cliques are cycle-free or not in polynomial time. iii. code construction for groupcast index coding problems in this section, we give a method to convert a groupcast index coding problem into a single unicast index coding problem. this method, along with the other techniques given in this paper leads to a construction of index code for groupcast index coding problem. a. converting groupcast icp into single unicast icp consider a groupcast index coding problem with k messages {x, x, . . . , xk} and a set of m receivers {r, r, . . . , rm}. let wk be the want-set and kk be the side-information of receiver rk for k . in groupcast index coding problem, there are no restrictions on want-set and side-information of each receiver. theorem consider a groupcast index coding problem with k messages and m receivers. let k be the set of receivers wanting the message xk for k and kk = \\ rjk kj. consider a single unicast index coding problem with k mes- sages {x, x, . . . , xk} and k receivers { r, r, . . . , rk}. the kth receiver rk wanting xk and having the side- information kk. then, any index code for this single unicast icp is also an index code for the groupcast icp. proof. let c be the index code for the converted single unicast index coding problem. from c, every receiver rk for k [ : k] can decode its wanted message xk. from the denition of kk, we have kk [ xjwk kj for k . hence, receiver rk for k can decode all its wanted messages in wk from the given index code c. b. steps to construct index code for groupcast index coding problems in the following four steps, we give a heuristic approach to construct an index code for groupcast index coding problems. we refer the following four steps as construction ii in the rest of the paper. construction ii step convert the given groupcast index coding problem into a single unicast index coding problem by using theorem step find the clique cover by using algorithm reduce the given minrank problem into a smaller problem by using construction i. step find the cycle cover in the reduced minrank problem by using any cycle cover algorithm. step construct the index code by using the clique cover and cycle cover found in step and step the other method that can be used to construct index codes for groupcast problems is partition multicast . however, the partition multicast is np-hard and requires higher eld size. the eld size required in partition multicast depends on the number of messages in a partition and the number of messages known to each receiver in the partition. whereas, construction ii can be used to construct index code in poly- nomial time and this method is independent of eld size. note that both partition multicast and construction ii are suboptimal in the length of index code. example consider a groupcast index coding problem with seven messages and ten receivers as given in table i. the single unicast index coding problem corresponding to the groupcast index coding problem obtained from theorem is given in table ii. cliques in this single unicast index coding problem can be found by using algorithm and this single unicast index coding problem can be converted into a reduced index coding problem by using construction i. the side-information graph of single unicast icp given in table ii and its reduced side-information graph are shown in fig. and fig. the minrank of gr and hence the minrank of g is four. the index code for the index coding problem represented by gr is {y + x, x + y, y + x, x + x} and the index code for the index coding problem represented by g is {x + x | {z } y +x, x + x + x | {z } y , x + x | {z } y +x, x + x}. note for the groupcast index coding problem given in table i, the index code length obtained by using partition multicast is ve, whereas, by using construction ii, we can construct index code of length four as shown in example example some of the groupcast index coding problems in which the length of the index code given by construction ii is less than the length obtained from partition multicast are given in table iii. in table iii, we use k,m,land lp m to denote number of messages, number of receivers, length of index code by using construction ii and length of index code by using partition multicast respectively. the minimum rk wk kk r x, x x, x, x, x r x, x x, x, x, x r x, x x, x, x, x r x, x x, x, x, x r x x, x r x, x x, x, x r x x, x r x, x x, x, x, x r x, x x, x, x r x, x x, x, x r x, x x, x, x, x table i: rk wk kk r x x, x r x x, x r x x, x r x x, x r x x, x, x r x x r x x, x table ii: single unicast icp obtained from theorem for the groupcast icp given in table i eld size required to construct the index code is mentioned with the length of index code in both the methods. for the groupcast index coding problem given in s. no. of table iii, the minimum eld size required in partition multicast is f, whereas construction ii gives the index code in f and the length of index code given by construction ii is one less than that of partition multicast.",
        "Subsections": [],
        "Groundtruth": "The text discusses the index coding problem, where a transmitter has a set of messages and receivers have side-information and a want-set of messages. The objective is to minimize the number of transmissions so that each receiver can decode its desired message. The problem can be simplified into a single unicast index coding problem. The minrank over a finite field is key in determining the optimal linear index code length. Various heuristic methods have been proposed to tackle the NP-hard minrank problem. The text also presents algorithms to convert groupcast index coding problems into single unicast ones, reducing the complexity of minrank computation. Additionally, a method to reduce the complexity of the minrank computation problem is provided, along with a heuristic algorithm to find a clique cover of the side-information graph. The section concludes by giving a method to construct index codes for groupcast index coding problems. It emphasizes the use of construction methods and heuristic algorithms to tackle complex index coding problems efficiently, leading to reduced transmission lengths in comparison to existing methods."
    },
    {
        "Section_Num": "II",
        "Section": "II Properties of minrank of a side-information graph",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "III",
        "Section": "III Code construction for groupcast index coding problems",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "IV",
        "Section": "IV conclusion and discussions",
        "Text": "in this paper, we give a method to nd some of the minrank- non-critical edges in the side-information graph. we give a simple heuristic method to nd the clique cover by using binary operations on adjacency matrix. we presented a method to address groupcast index coding problems. it is interesting to analyze more properties of minrank and design algorithms to compute the minrank of a side-information graph in a more efcient way. acknowledgement this work was supported partly by the science and engi- neering research board of department of science and technology , government of india, through j.c. bose national fellowship to b. sundar rajan. references y. birk and t. kol, coding-on-demand by an informed-source (is- cod) for efcient broadcast of different supplemental data to caching clients, in ieee trans. inf. theory,, vol. , no., pp.-, june s.no k m wk kk index code l lp m w = {x, x}, w = {x, x} k = {x, x, x}, k = {x, x, x} c = {x + x + x + x, w = {x}, w = {x} k = {x, x, x}, k = {x} x + x, x} w = {x, x}. k = {x, x, x}. w = {x, x}, w = {x, x} k = {x, x, x}, c = {x + x, w = {x, x}, w = {x} k = {x, x, x, x}, k = {x, x}, x + x + x, w = {x, x}, w = {x}. k = {x, x}, k = {x, x, x}, x + x + x, w = {x, x}, w = {x, x}. k = {x, x}, k = {x, x}, x + x + x, k = {x, x, x}, x + x} w = {x, x}, w = {x, x} k = {x, x}, k = {x, x, x} c = {x + x, w = {x, x}, w = {x, x} k = {x, x, x}, k = {x, x, x} x + x + x, w = {x}, w = {x, x}. k = {x}, k = {x, x}. x + x + x, } w = {x}, w = {x}. k = {x, x}, k = {x}. x + x} w = {x, x, x}, k = {x, x, x, x, x} c = {x + x + x, w = {x, x, x}, k = {x, x, x, x, x, x}. x + x + x, w = {x, x}, k = {x, x, x, x} x + x + x, w = {x, x, x}, k = {x, x, x, x, x}. x + x, x + x, w = {x, x}, w = {x, x}. k = {x, x, x, x}, k = {x, x}. x + x + x} w = {x, x, x}, k = {x, x, x, x, x} c = {x + x + x, w = {x, x, x}, k = {x, x, x, x, x, x}. x + x + x, w = {x, x}, k = {x, x, x, x} x + x + x, w = {x, x, x}, k = {x, x, x, x, x}. x + x, x + x, w = {x, x}, w = {x, x}. k = {x, x, x, x}, x + x + x} k = {x, x, x, x}. w = {x, x, x}, k = {x, x, x}, c = {x + x, w = {x, x, x}, k = {x, x, x, x}, x + x + x, w = {x, x}, w = {x}, k = {x, x, x}, k = {x, x, x}, x + x + x, w = {x, x}, w = {x}, k = {x, x, x}, k = {x, x, x}, x + x + x, w = {x, x}, w = {x, x}. k = {x, x, x}, k = {x, x, x}, x + x, x} table iii: some instances of the groupcast index coding problem where the length of the index code given by construction ii is less than the length obtained from partition multicast. x x x x x x x x x x x x fig. : side-information graph of single unicast icp given in table ii l ong and c k ho, optimal index codes for a class of multicast networks with receiver side information, in proc. ieee icc, , pp. - z. bar-yossef, z. birk, t. s. jayram, and t. kol, index coding with side information, in ieee trans. inf. theory,, vol. , no., pp.- , mar. s. h. dau, v. skachek, and y. m. chee, error correction for index coding with side information, in ieee trans. inf. theory,, vol. , no., pp.-, mar. y x y x x fig. : reduced side-information of g given in fig. m. tahmasbi, a. shahrasbi and a. gohari, critical graphs in index coding, in ieee journal in selected areas of communications, vol. , no., pp.-, feb. b. recht. m. fazel and p. parrilo, guaranteed minimum-rank solutions on linear matrix equations via nuclear norm minimization, siam review, vol. , no. , pp. -, h. esfahanizadeh, f. lahouti, and b. hassibi, a matrix completion approach to linear index coding problem, itw , australia, november s. kwak, j. so, and y. sung, a extended least difference greedy clique-cover algorithm for index coding, isit , australia, koshy. t, discrete mathematics with applications, academic press, m. awais and j. qureshi, efcient coding for unicast ows in opportunistic wireless networks, iet communications., , vol. , iss. , pp. b. bollobas and p. erdosw, cliques in random graphs, mathematical proceedings of the cambridge a. s. tehrani, a. g. dimakis, and m. j. neely, bipartite index coding, in proc. ieee isit , pp. j. a. bondy and u. s. r. murty, graph theory, graduate texts in mathematics , springer. g. r. grimmett and c. j. h mcdiarrnd, on colouring random graphs, math. proc. cambridge philos. soc. , , pp. - appendix a heuristic algorithm to find a clique cover birk et al. proposed least difference greedy clique cover algorithm to nd the cliques in side-information graph. kwak et al. improved the ldg algorithm by proposing extended least difference greedy clique cover algorithm to nd the cliques in side-information graph. however, the way the cliques are found in both ldg and eldg algorithms depends also on the directed edges in the side-information graph which do not contribute to cliques. in this section, we give a method for the heuristic search of the cliques in the side-information graph by using binary operations on the adjacency matrix. ldg and eldg algorithms use tting matrix to nd cliques, whereas the algorithm presented in this paper use the adjacency matrix. ldg and eldg algorithms are given below for continuity and completeness of presentation. a. least difference greedy clique-cover algorithm the distance between two rows in a tting matrix is dened as the sum of the inter-entry distance, where the inter-entry distance d is dened as d = d = d = , d = d = , and d = . based on the dened inter-row distance, the ldg algorithm nds the minimum distance among all possible pairs of two rows in a tting matrix and merges two rows with the minimum inter-row distance, and then iterates this procedure until all inter-row distances become innite. b. extended least difference greedy clique-cover algorithm the eldg algorithm is based on the observation that the minrank of the tting matrix a and at is same. in eldg algorithm, the ldg algorithm is applied on both the rows and columns of the tting matrix and the algorithm chooses the row/column merging requiring less *s. in both ldg and eldg algorithms, the inter-row distance given in depends on directed edges which do not contribute to any cliques. hence, some of the directed edges present in the graph can mislead the row/column merging in eldg algorithm. consider the side information graph g given in fig. the row inter entry distances for the ith and jth rows) and the column inter entry distances (denoted by dc for the ith and jth columns) are given in table iv and v. in table iv and v, the inter row and inter column distances is the minimum between rows and and columns and hence, in g, the eldg algorithm merges the two sets of rows and . hence, the rank reduction by eldg algorithm is two, whereas, one can reduce the rank by three by merging the rows and . x x x x x x x x x x x x x x x fig. dr = dr = dr = for j dr = dr = dr = for j dr = dr = dr = for j dr = dr = dr = for j table iv: row inter-entry distances of g given in fig. dc = dc = dc = for j dc = dc = dc = for j dc = dc = dc = for j dc = dc = dc = for j table v: column inter-entry distances of g given in fig. the hadamard product of two matrices a and b is denoted . fig. : fitting matrix of side-information given in fig. by a b and dened by = a b. that is, the hadamard product of two matrices is the element wise binary and operation of its elements. lemma let a be the adjacency matrix of the graph g, dene b = at a. the matrix b is a symmetric matrix and represents the location of undirected edges in g. proof. if there exists a directed edge from xi to xj, we have a = , a = , at = and at = hence, we have b = at a = , and b = at a = if there exists an undirected edge from xi to xj, we have a = , a = , at = and at = hence, we have b = at a = , and b = at a = this completes the proof. by using lemma , in algorithm , we give a heuristic method to nd the cliques in the side-information graph. in the lth iteration, algorithm nds cliques of size two in the adjacency matrix given by th iteration. note that every row of the adjacency matrix in th iteration corresponds to a clique detected in th iteration. c. algorithm description in the rst iteration, the algorithm nds cliques of size two in g. that is, after the rst iteration every row (or column) in a represents a vertex in g or a clique of size two in g. similarly, after second iteration, every row (or column) in a represents a vertex in g or a clique of size // in g. the iterations continue until there exist no more combinable rows and columns in the matrix a obtained from the previous iteration . in step , the row indices are arranged in the ascending order of their hamming weights to ensure that the algorithm rst combines the vertices which are connected with less number of undirected edges (the vertices which are connected with more number of undirected edges have more options for combining, hence they are treated later in the order). the purpose of different sets used in algorithm are given below. the set s keeps track of two tuples of row indices that form a clique in the present iteration the set d keeps track of the indices of rows which would be merged with the other rows. the rows and columns corresponding to the indices present in d would be deleted before going to the next iteration. the set e keeps track of the rows in b whose hamming weight is zero. the rows and columns corresponding to the indices present in e would be deleted before going to the next iteration. the sets ck, ck, . . . , ckn give the cliques identied till the present iteration. algorithm heuristic algorithm to nd a clique cover of a side-information graph : a is the adjacency matrix of graph g of order v v . : ci = {i}, i = , , . . . , v , c = and n = v . : b = at a and s = , d = , e = . : if b is a zero matrix then, goto step : let r = {k, k, . . . , kn} be the set of row indices in the ascending order of their hamming weights (a is a n n matrix). : i = and t = : if wt then, : e = e {ki} and i i + , repeat step . : else if b = s s {}, cki = cki ckj and ckj = . r r\\{ki, kj}. i i + and t = t + if i < n then, repeat step . : s = s. : if s then, a = a a. s = s\\{}. if s = then, repeat step . : s = s. : if s then, a = a a. s = s\\{}. d = d {j} if s = then, repeat step . : delete the rows and columns in the n n matrix a whose indices are present in the set d and e. output the cjs for every j d. : do { : let dki be the number of rows deleted in a above ki . ckidki = cki and t = t : while {t > }. : n = n| d | | e | . : goto step . d. computational complexity of algorithm by using algorithm , all cliques of size two can be found by using n binary and operations. if the graph consists of a clique of maximum size l, after atmost l iterations the matrix b becomes zero. therefore the computational complexity of nding all m-cliques is less than ln binary and operations. for the given graph g, algorithm nds all cliques of size two and combines them in the rst iteration. similarly, in lth iteration, algorithm nds and combines all cliques of size two in the graph corresponding to the adjacency matrix obtained from th iteration. however, algorithm can be modied to combine only one clique of size two in every iteration. this might yield less number of cliques in some side-information graphs at the cost of more iterations. example for the side-information graph given in fig. , in the rst iteration, algorithm nds all the cliques of size two and merges the cliques {, } and {, }. in the second iteration, algorithm combines the cliques {, } and {}. after second iteration, b is zero and algorithm outputs the cliques {, , } and {, }.",
        "Subsections": [],
        "Groundtruth": "The paper proposes a method to identify minrank-non-critical edges in a side-information graph by using binary operations on the adjacency matrix to find the clique cover. They address groupcast index coding problems and suggest further research to analyze minrank properties and design more efficient algorithms. The work was supported by the Science and Engineering Research Board of the Department of Science and Technology, Government of India. Previous work in this area is referenced, and a heuristic algorithm for finding a clique cover in the side-information graph using binary operations on the adjacency matrix is outlined. The computational complexity of the algorithm for finding all m-cliques is discussed, along with an example demonstrating the application of the algorithm to a specific side-information graph."
    },
    {
        "Section_Num": "References",
        "Section": "References",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "Appendix",
        "Section": "Appendix",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "Abstract",
        "Section": "Abstract",
        "Text": "arxiv:v jan equivalent conditions for existence of three solutions for a problem with discontinuous and strongly-singular terms carlos alberto santos universidade de bras lia, departamento de matem atica -, bras lia - df - brazil e-mail: csantos@unb.br lais santos universidade federal de vi cosa, departamento de matem atica , vi cosa - mg - brazil e-mails: matmslais@gmail.com marcos l. m. carvalho universidade federal de goi as, instituto de matem atica -, goi ania - go - brazil e-mails: marcos leandro carvalho@ufg.br abstract in this paper, we are concerned with a kirchhoproblem in the presence of a strongly-singular term perturbed by a discontinuous nonlinearity of the heaviside type in the setting of orlicz- sobolev space. the presence of both strongly-singular and non-continuous terms bring up diculties in associating a dierentiable functional to the problem with nite energy in the whole space w , . to overcome this obstacle, we established an optimal condition for the existence of w , -solutions to a strongly-singular problem, which allows us to constrain the energy functional to a subset of w , to apply techniques of convex analysis and generalized gradient in clarke sense. mathematics subject classications: j, j, j, j, d, b key words: non-local kirchhoproblems, strongly-singular nonlinearity, discontinuous perturbation, -laplacian operator. carlos alberto santos acknowledges the support of capes/brazil proc. n o /",
        "Subsections": [],
        "Groundtruth": "The paper addresses a Kirchhoff problem with a strongly-singular term perturbed by a discontinuous nonlinearity of the Heaviside type in the Orlicz-Sobolev space. The presence of these terms poses challenges in associating a differentiable functional to the problem with finite energy in the whole space. To address this, the authors establish an optimal condition for the existence of solutions to this strongly-singular problem in a subset of the space, allowing the application of convex analysis and generalized gradient techniques in the Clarke sense. The research focuses on non-local Kirchhoff problems, strongly-singular nonlinearity, discontinuous perturbation, and the -Laplacian operator."
    },
    {
        "Section_Num": "1",
        "Section": "1 Introduction",
        "Text": "in this paper, we are concerned in presenting equivalent conditions for the existence of three solutions for the quasilinear problem m \u0012z dx \u0013 u = bu + f in , u > in , u = on , which are linked to an optimal compatibility condition between for existence of solution to the strongly-singular problem \u001a u = bu in , u > in , u = on with the boundary condition still in the sense of the trace. here, m : [, ) [, ) is a continuous function, f : is of heaviside type, < b l, > , , > are real parameters. moreover, u = divu) stands for the -laplacian operator, where a : is a c-function that denes the increasing homeomorphism : r r given by = \u001a at if t = , if t = , whose the associated n-function : r r is given by = r |t| ds. the issue about existence of three solutions for a suitable range of parameters , > , for particular forms of probem , has been considered in the literature recently, principally in the context of non-singular problems and in the case in which f is continuous, see for instance , , , , , and references therein. there are few works for singular nonlinearities, we quote for example , and who considered = |t|p/p, t > , < p < and m in . in , a singular problem for low dimensions was studied, while in and a singular problem for high dimensions was treated, but in both cases f has been considered a carath eodory function with suitable assumptions. more specically, in , the singular perturbation was considered in the weak sense , while in they permitted > by balancing the size of this with the existence of a < u c such that the product bu in l. in this paper, we establish an optimal condition to the relationship between the power > and the potential b > to existence of three solutions to the singular problem , independent of the dimension n, in the presence of both a discontinuous nonlinearity of the heaviside type and a non-local term. more precisely, we prove how the existence of three solutions to is associated to the existence of solutions still in w , to the problem . our approach is based on the existence of positive solution to the problem , which provides a non-empty eective domain for the energy functional associated to and enable us to apply techniques of the generalized gradient in clarke sense to get a multiplicity result. besides this, we prove qualitative results about these three solutions. we highlight how the non- local term m should be to the discontinuity of the function f be eectively attained by the solutions and how the level set of these solutions behaves exactly at the discontinuity point of f. to our knowledge, both the results of equivalent conditions and qualitative information on solutions are new in literature. as our main results will be obtained via variational methods, we need to introduce the energy functional associated to problem . to do this, let us denote by w , the orlicz-sobolev space associated to and extend the function f to r as f = a.e in and for all t from these, the functional naturally associated to is i : w , r dened by i = m \u0012z dx \u0013 z fdx + z gdx, where m = z t mds, f := z t fds and g : r (, ] is dened by g = ( bt for x and t > , +for x and t to ease our future references, let us rewrite i as i = + , where = m \u0012z dx \u0013 z fdx and = z gdx. the main diculty in treating strongly-singular problems consists in the fact that the energy functional associated to the equation neither belongs to c, in the sense of fr echet dierentiability, nor is dened in the whole space w , . in fact, when > the functional may not be proper, i.e. it may occur = , for all u w , . another diculty exploited in this work is the presence of a more general quasilinear operator, which may be even nonhomogeneous. to deal with this situation, we approach the problem in orlicz-sobolev space setting. below, let us state the assumptions about that we will assume throughout this paper. : a c, ) and is an increasing odd homeomorphisms from r onto r; : < a:= inf t> t sup t> t := a+ < . let us denote by the function whose inverse is given by = r t s/nds, t > in order to be a n-function, we need to require z s/nds < and z s/nds = . in this case, is a n-function given by = r |t| ds for some increasing odd homeomorphisms : r r. about , we will consider : + < := inf t> t , where < := a+ a+ + := +. as another consequence of and , the orlicz space l coincides with the set of measurable functions u : r such that r dx < and it is a banach space endowed with the luxemburg norm u := inf \u001a > : z \u0012|u| \u0013 dx \u001b . associated to the space l, we can set the orlicz-sobolev space w , by w , = \b u l : uxi l, i = , , n and deduce that it is a banach space with respect to the norm uw , = u + u. the orlicz-sobolev space w , is naturally dened as the closure of c in w ,-norm, under the hypothesis . for more information about the orlicz and orlicz-sobolev spaces, we refer , and . about m, let us assume : m mt for all t and for some > such that , that is, lim t = for all > , where := . to conclude our assumptions, let us suppose that f : r+ is a measurable function such that f = a.e. in (, ] and : f c (r { a}) for some a > , < f(x, a ) < f(x, a + ) < , x , where f(x, a ) := lim s af, f(x, a + ) := lim s a+ f, : there exists an odd increasing homeomorphism h from r onto r and nonnegative constants a, a and a such that || a + a e h h for all f, t r and x , where h = r |t| hds is a n-function satisfying ( h is the its complementary function) such that h and th h h+ for all t t with < h+ + , for some t > , : lim t+ supf t+ = , : lim t supf t = before stating the main results, let us clarify what we mean by a solution of . denition a function u w , is a solution to the problem if u > a.e in , bu l and m \u0012z dx \u0013 z audx = z \u0014 b u + f \u0015 dx for all w , . under the hypothesis , a solution < u w , of the problem has to satisfy z \u0000f ) f + ) \u0001 u{x: u= a}dx = for all w , , where {x: u= a} stands for the characteristic function of the set {x : u = a}. next, we state that is satised, under additional assumptions on f and b, by showing that meas{x : u = a} = , where meas stands for the lebesgue measure. theorem assume f satises , and < b l holds. if u w , is such that: i) u is either a local minimum or a local maximum of i, then meas{x : u = a} = , ii) u is a critical point of i and b l loc, then meas{x : |u| = } = in particular, meas{x : u = c} = for each c > moreover, if u satises i) or ii) above, then: u is a solution of problem , there exists c > such that u cd for x , where d stands for the distance function to the boundary , u solves almost everywhere in if in addition bd l h. about multiplicity, our main result can be stated as follows. theorem assume > , b l l loc, , and hold. then, the below claims are equivalents: i) there exists < u w , such that z bu dx < , ii) the problem admits a weak solution u w , such that u cd for x for some c > independent of u, iii) for each > , there exists > such that for admits at least three solutions, being two local minima and the other one a mountain pass critical point of the functional i, where = inf m \u0012z \u0013 z fdx : u w , and z fdx > . moreover, for each of such solutions the meas{x : u = a} = besides this, u solves almost everywhere in if in addition bd l h and if: iv) either m is non-decreasing and f = f for all < t < and a.e. x , v) or m is such that a comparison principle holds to problem and > , then there exists a> such that meas{x : u > a and u is a solution of } > for each < a < agiven. remark about the above theorem, we still highlight the following facts: the equivalency between and holds true without assuming b l loc, each one of such solutions given by iii) is such that u cd for x , for some c > dependent on u. in , lazer and mckenna has proven that problem admits solution still in h if, and only if, < when < b b l and = |t|/ in . mohammed, in , considered = |t|p/p in and proved that the sharp power in this case is given by /. as a consequence of theorem , we are able to nd a q > such that the problem still admits a solution in w , for all < q, where q depends on the summability lq of b. this is the content of the next corollary. corollary assume , and hold. if < b lq for some q > and < < q + q := q, then the problem admits weak solution. although no answer about q > be the sharp power for the existence of solution still in w , has been provided, we observe that q / as q and this limit is the sharp value obtained both by and for the cases = |t|/ and = |t|p/p , respectively. in particular, as a consequence of theorem and corollary , we have the following. corollary assume , , , and hold. if b lq for some q > and < < q, then for each > given, there exists > such that for (, ] the problem admits at least three weak solutions with the same properties as those found in itemiii) in theorem . it is worth mentioning that the above theorems improve or complement the related results in the literature both by the presence of the kirchhoterm, by the summability assumption on the potential b, the strongly-singular term and the non-homogeneity of the operator. our results contribute to the literature principally by: i) theorem unify some results on p-laplacian operator, with < p < , to -laplacian operator, see for instance and . ii) theorem establishes necessary and sucient conditions for existence of multiple solutions for the problem , by connecting and extending the principal result in yijing to a non-homogeneous operator; iii) theorem extends the principal result in faraci et.al and complements the main result in , principally by considering a non-homogeneous operator, an optimal condition on the pair to existence of three solutions, a discontinuity of the heaviside type and including a kirchhoterm; iv) corollary gives us an explicit range of variation of , in which the existence of solution in w , for is still guaranteed. in particular, when = |t|p/p and b b l for some constant b > , the value q coincides with the sharp values obtained in and ; v) corollary complements the principal result in by showing an explicit variation to , where the multiplicity is still ensured, namely, < < p n = , to ease the reading, from now on let us assume the assumptions , , , and gather below some functional that appear throughout the paper. m = z t mds, t r, = m \u0012z dx \u0013 z fdx, = z gdx, p = z dx, j := \u0010 m p \u0011 = m \u0012z dx \u0013 , j = z fdx, i = + = j j + , : w , \u0010 w , \u0011 is understood as u, := z audx, w , . this paper is organized as follows. in section , we present some preliminary knowledge on the orlicz-sobolev spaces and some results of non-smooth analysis related to our problem. the section is reserved to prove theorem , while in section we prove theorem .",
        "Subsections": [],
        "Groundtruth": "The section discusses the equivalent conditions for the existence of three solutions for a quasilinear problem. The problem involves a singular term, a discontinuous nonlinearity of the heaviside type, and a non-local term. The text presents new results on the relationship between parameters for the existence of three solutions, offers qualitative information on solutions, and introduces variational methods. The main results are obtained using Orlicz-Sobolev spaces and generalized gradient techniques. The text also defines the energy functional associated with the problem and establishes necessary and sufficient conditions for the existence of multiple solutions. The paper contributes to the literature by unifying results on operators, extending principal results, and providing explicit ranges for solution existence. The organization of the paper involves presenting preliminary knowledge, proving a theorem in one section, and proving another theorem in a separate section."
    },
    {
        "Section_Num": "2",
        "Section": "2 Non-smooth analysis for locally Lipschitz functional",
        "Text": "in this section, we are going to remember some facts related to non-smooth analysis. however, one of the principal contribution of this section is establishing appropriated assumptions under the n- function , the non-local term m and the discontinuous function f that make possible to approach (ii = iii), in theorem , via ricceris theorem . under our hypotheses and the decomposition of the functional i into plus , that is, i = + , we have written i as a sum of a locally lipschitz functional and a convex one and ). below, let us recall few notations and results on the critical point theory for the functional and we refer the reader to carl, le & motreanu , chang , clarke and references therein for more details about this issue. let us begin by remembering that the generalized directional derivative of at u w , in the direction of v w , is dened by = lim sup h + and the subdierential of at z w , is given by = \u001a \u0010 w , \u0011 : + , v zfor all v w , \u001b , since is a convex function. in particular, is named by the generalized gradient of at u and denoted by about the functional , its eective domain is dened by dom = {u w , : < } and a point u dom is called a critical point of the functional i if + , v w , . in this context, we say that i satises the palais-smale condition for short) if: {un} w , is such that i c and + nv un, v w , , where n +, then {un} possesses a convergent subsequence. in order to prove the next lemma, let us dene the functionals j := m) and j := z fdx, where p is dened by p = z dx. it is well know that, under the hypotheses and , the functional p is sequentially weakly lower semicontinuous and c with p, = z audx, w , . moreover, p : w , w , is a strictly monotonic operator of the type . thus, we can rewrite i as i = + = j j + , where j is c, j is locally lipschitz and is a convex functional. lemma suppose , , and holds. then, i) j c(w , , r)) and j , = m) z audx, w , , ii) j liploc(w , , r) and j n w \u0000lh \u0001 : w f) a.e. x o . in particular, for each w j, there exists a unique l h such that a.e. x and w, = z dx, w , , iii) j is of type , that is, if un u and lim nsup j , un u, then un u in w , . iv) if un u in w , , then j and n, un u= z ndx , n j, v) if un u in w , , then j j, vi) j is sequentially weakly lower semicontinuous in w , , vii) liploc(w , ; r) is sequentially weakly lower semicontinuous and is of the type . proof first, we note that the item i) is an immediate consequence of assumptions on m and properties of p. next, we present a summary proof of the other items. ii) let e j : lh r be a functional dened by e j = r fdx, u lh. so, it follows from theorem in that e j liploc; r) and e j n w \u0000lh \u0001 : w f) a.e. x o . since w , lh = lh, we are able to apply to conclude that j = e j w , is locally lipschitz continuous and j e j n w \u0000lh \u0001 : w f) a.e. x o . the conclusion of the proof is a direct consequence of theorem in and classical riesz theorem for orlicz spaces, see for instance . iii) this conclusion is a consequence of item i) and the fact that p is of the type . iv) let un u and n j since n \u0000lh \u0001, the riez theorem for orlicz spaces implies that there exists a unique n l e h, still denoted by n, such that n, un u= z ndx. besides this, by using , h and youngs inequality, we obtain |n| a|un u| + a e h h|un u| c), which leads us to conclude that |n| g for some g l, after using the compact embedding w , lh and lemma in . as un u a.e in , the rst claim follows by lebesgue theorem. to end the proof, it follows from proposition in that there exists e n j such that j = e n, v, for all v w , . hence, we obtain from above conclusion that j = e n, un u v) as in the previous item, by using and dominated convergence the result follows. vi) this item is a consequence of the continuity and monotonicity of m and the fact that p is sequentially weakly lower semicontinuous in w , . vii) by items i) and ii) above, we have liploc(w , ; r). besides this, we get from item iv) and that is sequentially weakly lower semicontinuous. let un u such that lim supn then, and above lead us to lim sup n \u0000m p \u0001 un, un u = lim sup n \u0000m p \u0001 un, un u lim nj = lim sup n , which implies the claimed, after using the iii). this ends the proof. the next lemma gives us some properties regarding lemma assume < b l if problem admits a solution in w , , then is a proper functional. besides this, is convex, sequentially weakly lower semicontinuous and = for all < u w , . proof first, note that g +in for all u w , , so = . moreover, if u w , is a solution of , then u dom, which proves dom = . the convexity follows directly from the denition of finally, by the fatous lemma, we conclude that is sequentially weakly lower semicontinuous. lemma suppose , , and hold. then, i is a coercive functional. proof first, by the assumption and lemma in , we have m \u0010 p \u0011 m u for all u w , with u moreover, by taking > small enough, it follows from and that f c + |t|for all x , t r and for some c > thus, by the embedding w , l, which follows from the hypothesis , we conclude c \u0010 u \u0011 for all u w , with u for some c > since > , we have thus, after all these information and , we conclude i as u , that is, i is coercive. this ends the proof. lemma suppose admits a solution in w , and the assumptions , ,, hold. then i satises the condition. proof let w , and be sequences such that i c r, n and + \u0010 \u0011 n for all w , and n n. it follows from the coercivity of i, obtained in the previous lemma, that is bounded in w , . thus, passing to a subsequence if necessary, we may assume that un u. so, by lemmas -vii) and , we obtain that i is sequentially weakly lower semicontinuous, which yields i lim inf ni = c < , whence < . so, by taking = u in , we obtain \u0010 \u0011 + n for n n. therefore, by using the previous inequality and the lower semicontinuity of , we get lim inf n , which leads to lim inf n lim inf n \u0002 + j \u0003 = lim inf nj ; un u+ lim nj = lim sup nj ; un u, after applying lemma -iv). thus, lemma -iii) implies that un u in w , to a subsequence that ends the proof of lemma. proposition assume , , and hold. then, any strict local minimum of the functional = j j in the strong topology of w , is so in the weak topology. proof we just need verify that, under these assumptions, the conditions of theorem c in are fullled. since w , is a reexive and separable space, j and j are sequentially weakly lower semicontinuous and the functional is coercive ), we just need to check that j ww , , that is, if un u and lim ninf j j, then un u up to a subsequence to conclude the proof of the proposition, in this direction, let us assume un u and lim ninf j j since j is sequentially weakly lower semicontinuous, we have lim nj = j for some subsequence, still denoted by . thus, from this fact, continuity and monotonicity of m in r+, we obtain lim np = p. therefore, by the hypothesis we can apply to conclude that un u in w , . this ends the proof. below, let us connect the existence of solution to problem with existence of two local minima to the functional i. lemma suppose admits a w , -solution, , and hold. then, for each > there exists > such that for (, ] the functional i has two local minima. proof : fix > , where > was dened at . since is lower semicontinuous and coercive and ), there exists a global minimum u w , of in w , and, in particular, = if = , we would have j j = = for all u w , , which would yield , but this is impossible. let us denote by c > the best embedding constant of w , l+ and take < < /. thus, it follows from the assumptions and that f t+ for all t for some m > small enough and m > large enough. besides this, if u < , then we have m+ \u0010 z u+dx \u0011/+ u+ cu c, that is, c/m+. so, it follows from the above information and assumption that z fdx = z fdx + z fdx + z fdx z \\ u+dx + sup mtm f c m+ z u+dx for some > small enough, which shows j u+ + for all u w , with u . therefore, we obtain from this fact, hypothesis and lemma in that m u+ u+ + mc+ u+ + u+ + > = holds, whenever u < with > such above, that is, is a strict local minimum of in the strong topology. hence, we obtain from proposition that is a local strict minimum of in the weak topology as well, i.e, there exists a weak neighborhood vw of such that = < for all u vw \\ {}. after these information and the assumption that the problem admits a solution in w , , we are able to follow the same strategy of the proof of theorem in to build disjoint open sets d and d, in the strong topology, such that d, u d and to nd i di such that and are distinct local minima of i. this ends the proof. by applying corollary of for functional of the type locally lipschiz plus convex (it is a version of corollary in that considers functional of the type c plus convex), lemma and lemma , we have. corollary suppose , , and hold. in addition, assume that problem admits a w , -solution. then, for each > there exists > such that for (, ] the functional i has three critical points, being two of them local minima and the other one a mountain pass point to the functional i.",
        "Subsections": [],
        "Groundtruth": "This section discusses non-smooth analysis, focusing on establishing suitable assumptions for approaching certain theorems via Ricceri's theorem. The section introduces notations related to critical point theory and directional derivatives. Lemmas and proofs are provided to demonstrate various properties of the functionals involved. Coerciveness and sequential weak lower semicontinuity are discussed in relation to critical points. A proposition relates strict local minima of a functional to the strong and weak topologies. Another lemma connects the existence of solutions to a problem with the presence of local minima. Lastly, a corollary states conditions for functional i to have three critical points, including local minima and a mountain pass point."
    },
    {
        "Section_Num": "3",
        "Section": "3 Proof of Theorem ??",
        "Text": "before starting the proof of theorem , let us prove the two below lemmas. lemma assume , , , , < b l and u w , be a critical point of i. then: u > a.e. in and there exist a and a l e h such that \u0000m p \u0001 z audx = , + z dx for all w , , where stands for the subdierential of the convex functional at u, bu l for any w , . besides this, = {} and , = z budx for all w , . in particular, the equation turns into \u0000m p \u0001 z audx = z \u0002 bu + \u0003 dx for all w , , there exists a c > , dependent on u, such that u cd for x , + bu l loc if in addition b l loc. proof of . since u is a critical point of i ), in particular, we have u dom, which implies r |g|dx < , that is, g) is nite a.e. in . therefore, by the denition of g, we have u > a.e in . again, by u w , be a critical point of i, it follows from , that \u0000m p \u0001 u j + , where j stands for the generalized gradient of the locally lipschiz continuous functional j at u. thus, there exist j and such that \u0000m p \u0001 u, v= , v, vfor all v w , . so, it follows from lemma - that there exists a unique l h, with , such that the equality holds true. this ends the proof of i). let us prove ii). by and , we have \u0000m p \u0001 z audx , for all w , , which implies, by denition of , that \u0000m p \u0001 z audx z hg g t i dx for all w , . hence, by u > a.e. in and fatous lemma, we obtain z budx lim inf t+ + z b + u+ t ! dx m ) z audx < for all w , , that proves that bu l for any w , . to nish the proof of ii), let then for , we have , u, which can be rewritten as + z bu+dx , u. so, by doing + in the previous inequality, we obtain z bu+dx , u. on the other hand, again by the fact that , one has , = + z b \u0012u+ + \u0013 dx, for all w , and > given, which yields z budx , , after using fatous lemma. by taking = u in and combining this with , we obtain , u= z bu+dx. besides this, by letting w , , testing with + and using , we get z budx , , u , , which lead us to z budx , , , due , that is, , u by using that || as , the inequality yields z budx , , for all w , , that is, , = z budx, for all w , . this ends the proof of item ii). now, we are ready to prove iii). first, let us denote by c := m ) > and consider the problem v = c b in , v = on , where b = min{, b}. we know from lemmas and in that there exist a unique solution of , say u w , , and c = cu > such that u cd in . on the other hand, we obtain from that z audx z c budx z c bdx for all w , , that is, u is a supersolution for the problem . hence, z (a(| u|) u au) ( u u)+ dx c z b \u0000( u + ) \u0001 ( u u)+ dx , which implies that cd u u in and this proves . let us prove . by and property h ) h = h for all t r (the equality is due h being continuous), we obtain || a e h h) + a ah + a c \u0000 + uh+\u0001 , for some c > , where the last inequality is a consequence of in . hence, we obtain from , w , l and h+ / + that l loc. so, combining the fact that l loc together with above, the proof of follows. this ends the proof of lemma. lemma assume , , , and bd l h. let u w , be a critical point of i and l e h as in lemma . then: \u0000m p \u0001 u \u0000lh \u0001, there exists a unique representative of \u0000m p \u0001 u in l e h, still denoted by \u0000m p \u0001 u, such that \u0000m p \u0001 u = + bu a.e. in . proof of i) we have from that \u0000m p \u0001 u, = , , for all w , , where \u0010 w , \u0011 and j \u0000lh \u0001 with this last inclusion due to the lemma -. since bd l h, we obtain from lemma - and that \u0000lh \u0001 as well. thus, we obtain from these information and w , h = lh that \u0000m p \u0001 u \u0000lh \u0001. this proves i). let us prove ii). it follows from item i) and riesz theorem for orlicz spaces that there exist a unique element in l e h, still denoted by \u0000m p \u0001 u, such that m pu, = z u) dx for all w , , which implies by that z \u0010 m pu bu\u0011 = for all w , . this ends the proof of lemma. proof of theorem -conclusion. the proof of item i) is inspired on ideas from , while for the proof of ii) we borrow strategies from . the item - are consequences of lemmas and . proof of i): we just consider the case when u is a local minimum for i. similar arguments work when u is a local maximum for i. in this case, it is readily that z f f dx z g g dx z m) m) dx holds for any w , and any > given. below, let us consider two cases. first, x c . so, we obtain from lebourgs theorem that there exist t and f such that f f = , for each x . by using , we are able to estimate by || a e h h) + a := g, where g l is independent of > hence, coming back to , we obtain f f g l for every > small enough. besides this, the right derivative of f at u is given by lim + f f = f + ) a.e. x , because so, we are in position to apply lebesgues theorem, combined with fatous lemma and lemma , in to show that z \u0012 f + ) + b u \u0013 dx \u0000m p \u0001 z udx = z \u0000m p \u0001 udx holds for any w , , that is, \u0000m p \u0001 u f + ) + b u a.e. x . on the other hand, it follows from and lemma - that \u0000m p \u0001 u f + ) + b u a.e. x , due to the fact that u is a critical point of i. after these two inequalities, we obtain \u0000m p \u0001 u = f + ) + b u , a.e. x . secondly, let us x c with by similar arguments as those done to prove the case , we are able to show that \u0000m p \u0001 u = f ) + b u a.e. x . holds. finally, if meas{x : u = a} > , then it would have from and that f = f(x, a + ) a.e. x {x : u = a}, but this is impossible by so meas{x : u = a} = this ends the proof of i). proof of ii): since u w , is a critical point of i, we obtain from lemmas -ii) that \u0000m p \u0001 u = + bu := h a.e. in . with [f, f]. so, it follows from lemma and , that a|u| w , loc . besides this, we have \u0012 a|u| + a|u| \u0013 = |u|)| ||u|)|, which shows that a|u| + a|u| w , loc for each > given and so a|u| + a|u| w , can be used as a test function for any > and any c given. by doing this, we get from that z h \u0010 a|u| + a|u| \u0011 = \u0000m p \u0001 z au \u0012 a|u| + a|u| \u0013 dx = \u0000m p \u0001 z a a|u| + a|u|udx + \u0000m p \u0001 z a |u|)udx. since, a |u|)u || + |u|) ||u|)| ||u|)| holds for any > , we are able to apply lebesgue theorem to the equalities in to infer that z \\{u=} hdx = \u0000m p \u0001 z \\{u=} au = \u0000m p \u0001 z au = z hdx, holds, which lead us to have h = a.e. in {x : u = }. as we already know from lemma - that h > in , we obtain that meas{x : u = } = so, it follows from morey-stampacchias theorem that {x : u = c} {x : u = } for any real constant c given, which shows that meas{x : u = c} = so, as a consequence of and above, = f) if u = a and if u = a, we obtain that = f) a.e. in . finely, by applying lemma , we have and , while lemma implies . this ends the proof.",
        "Subsections": [],
        "Groundtruth": "The text discusses the proof of a theorem by first proving lemmas relating to critical points of a functional. The lemmas establish relationships between the subdifferential of a convex functional, the gradient of a function, and the uniqueness of solutions. The proof of the theorem involves showing that a function is a critical point of the functional and establishes various inequalities and equalities. The conclusion of the theorem proof involves applying different strategies and theorems to verify the critical points satisfy certain conditions."
    },
    {
        "Section_Num": "4",
        "Section": "4 Proof of Theorem ??",
        "Text": "in this section, let us begin proving the equivalences among , and . to prove (i = ii), we borrow ideas from yijing , who treated this situation in the context of homogeneous operators. the principal diculty in doing this is to nd appropriated assumptions under the n-function to become possible to obtain compactness results for minimizing sequences on nehari sets type, while the main obstacles to prove (ii = iii) were already got over in the last section. the (iii = i) is immediately. we will end this section ensuring that the discontinuity of the nonlinearity f may be attained. let us begin by dening the set a := n u w , : z b|u|dx < o and the subsets n := n u w , : z \u0010 a|u| b|u|\u0011 dx o a and n := n u w , : z \u0010 a|u| b|u|\u0011 dx = o . lemma assume and a = . then n and n are non-empty sets and n is unbounded set. proof take u a. so, it follows from and lemma in , that z |u|dx t z dx min{t, t+} z dx and z |u|dx + t z dx max{t, t+} z dx hold for t > large enough. so, we obtain from and that as t and as t +. besides this, we have from again that > for all t > , where := j = z dx + t z b|u|dx, t > and so there exists a unique t= t such that = this shows that tu n . as another consequence of the above information, we have that for all t > large enough, that is, tu n for all t > large enough. in particular, n is unbounded as well. this ends the proof. by using similar ideas as done yijing for the homogeneous case, we are able to prove the below lemma in the context of non-local and non-homogeneous operator. lemma assume and a = . then: the set n is strong closed, is not an accumulation point of n . to complete our basics tools to prove theorem , let us prove the below lemma that is interesting itself. lemma assume that < b l, and hold. let g : r be a carath eodory function such that \u0000g g \u0001 for all s, t > then the problem m \u0012z dx \u0013 u = b u + g, in u > in , u = on has at most one solution in w , . proof first, we note that the fact of m being non-increasing implies that m is convex. with similar arguments together with the hypotheses , we show that convex as well. these facts and the hypotheses lead us to infer that the functional j := m \u0012z dx \u0013 , u w , is convex as well. let u, v w , be two dierent solutions of the problem . so, it follows from and the convexity of j, that j j , u v = z \u0012 b u b v \u0013 dx + z g)dx z \u0012 b u b v \u0013 dx < , where the last inequality follows from b, > this is impossible and so the proof of lemma is done. proof of theorem -conclusion. we begin proving the rst implication. proof of i) = ii). first, we note that the assumption i) implies that a = . so, it follows from lemmas and that n is a nonempty complete metric space. moreover, by lemmas , lemma and the fact that j min{u , u+ } we have that j is lower semicontinuous and bounded below. thus, by the ekeland variational principle there exists a minimizing sequence n to j constrained to n such that: i) j inf n j + n; ii) j j + n, w n . besides this, we may assume un > a.e in , because j = j and if we assume that un = in a measurable set , with || > , then we would have from un n , b > a.e in and reverse h older inequality that > + z dx z bu n \u0010 z b/dx \u0011\u0010 z |un|dx \u0011 = , which is an absurd. thus, un > a.e in . since j inf n j , we have min{un , un+ } z dx + inf n j for all n large enough, which implies that is bounded. as a consequence of this, we have that un uin w , ; un ustrongly in lg for all n-function g ; un ua.e in for some uw , . by standard arguments, we are able to show that j = inf n j, that is, z dx + z b|un|dx n z dx + z b|u|dx holds. so, as a consequence of , fatous lemma and lemma vi), we obtain lim n z dx = z dx. thus, it follows from the assumption , theorem . and lemma . in that w , is uniformly convex. this together with the weak convergence and , lead us to conclude that un uin w , . after this strong convergence, we are able to follow similar arguments as done in yijing in the homogeneous case to prove that z audx z bu dx holds for any w , given. hence, it follows from the same arguments as used to prove lemma that uis a w , -solution of such that ucd for some c > independent of u. proof of ii) = iii). by corollary , there exist three critical points to functional i, being two of them local minima and the other one a mountain pass point to energy functional i. so, by theorem we know that each one of these critical point is a solution for the problem that satisfy the qualitative properties claimed. proof of iii) = i). let < u w , be a solution of . then u dom, that is, r bu dx < . these ends the proof of the equivalences. below, let us prove the items iv) and . we are going to prove iv) rst. let u = ua be a solution of problem . assume by contradiction that u a a.e. in for any a > so, it follows from f = f for all < t < and a.e. x that ua w , is a solution of m \u0012z dx \u0013 u = b u + f in , u > in , u = on , that is, ua is constant in a > by lemma . on the other hand, by taking > > , we have that u a > can be used as a test function in and this yields the inequality m \u0012z dx \u0013 z a|ua|u a dx = z bu a dx + z fu a |b|a + c||( + e h h)a for any a > given. so, by doing a > small enough we get an absurd, because the rst term of the above inequality is a positive number that does not depends on a > this ends the proof of this item. finally, we are going to prove v). let ua be a solution of problem . assume by contradiction that u a a.e. in for any a > again. so, it follows that ua is a super solution to problem m \u0012z dx \u0013 u = b in , u > in , u = on , whenever a < on the other hand, we are able to show that the associated-energy functional to problem is coercive due the assumption > so, by following standard arguments, we show that there exists a non-trivial v w , solution for the problem . that is, we have m \u0012z dx \u0013 ua m \u0012z dx \u0013 v in , u = v = on . so, it follows from the hypotheses that m is such that a comparison principle holds, that ua u > for all < a this fact together with the contradiction assumption lead us to have u ua a for all < a , which is impossible for a > small enough, because u is non-trivial. this ends the proof of item v) and the proof of theorem . proof of corollary : by the implication (i = ii) in theorem , it suces to exhibit a u w , such that z bu dx < . let us construct a such one. first, we note that the regularity of the domain implies that there exists an > suciently small such that d c and |d| = in , where d := dist and = {x : d < }. with these, dene u = d if d < , + z d \u0010 t \u0011/ dt if d < , + z \u0010 t \u0011/ dt if d < for each > given, where < < will be chosen later. a simple calculation yields u = dd if d < , \u0010 d \u0011/ d if d < , if d < , which implies that u w , if z |d|)dx < . since |u| = in , we obtain from lemma in that z |d|)dx = z )dx < c z d+dx that lead us to show for such that + > , due well-known result in . that is, for such , we have that u w , . to complete the exhibition, if < < is such that q > q, we have z bddx \u0010 z bqdx \u0011/q\u0010 z dqdx < \u0011/q < , because b lq and the result in again. finally, to occur and simultaneously, we have to be able to choose a < < satisfying at same time + > and q > q. we can do these by controlling the range of . since + < q q if, and only if, < < q + q , we are able to pick a \u0010 + , min n , q q o\u0011 , whenever range as above. this proves that u, dened as above, satises the condition of item i) in theorem . this nishes the proof. references adams, r. a.: sobolev spaces, academic press [a subsidiary of harcourt brace jovanovich, publishers], pure and applied mathematics, new york-london, ambrosetti, a. and arcoya, d.: remarks on non homogeneous elliptic kirchhoequations, d. nonlinear dier. equ. appl. : https://doi.org//s--- cammaroto, f. and vilasi, l.: multiple solutions for a kirchho-type problem involving the p-laplacian operator, nonlinear analysis. theory, methods & applications , no. , carl, s., v. k., le and motreanu, d.: nonsmooth variational problems and their inequalities. comparison principles and applications, springer monographs in mathematics, springer, new york, cianchi, a. and mazya, v.: second-order l-regularity in nonlinear elliptic problems, available at arxiv:. chang, k. c.: variational methods for nondierentiable functionals and their applications to partial dierential equations, j. math. anal. appl. , no. , clarke, f.h.: optimization and nonsmooth analysis, siam, philadelphia, corr ea, f. j. s. a. and gon calves, j. v.: sublinear elliptic systems with discontinuous nonlinearities, applicable analysis , diening, l., harjulehto, p., h ast o, p. and ru zi cka, m.: lebesgue and sobolev spaces with variable exponents, lecture notes in mathematics, springer, heidelberg, fang, f. and tan, z.: existence of three solutions for quasilinear elliptic equations: an orlicz- sobolev space setting, acta math. appl. sin. engl. ser. , no. , faraci, f. and smyrlis, g.: three solutions for a class of higher dimensional singular problems, nodea nonlinear dierential equations appl. , no. , faraci, f. and smyrlis, g.: three solutions for a singular quasilinear elliptic problem. proceedings of the edinburgh mathematical society , - doi:/s goncalves, j. v., carvalho, m. l. and santos, c. a.: about positive w , loc -solutions to quasilinear elliptic problems with singular semilinear term. to appear in topological methods in nonlinear analysis, arxiv:. gon calves, j. v., carvalho, m. l. m. and santos, c. a.: quasilinear elliptic systems with convex- concave singular terms and -laplacian operator, dierential integral equations , no. , guedda, m. and v eron, l.: quasilinear elliptic equations involving critical sobolev exponents, nonlinear anal. , no. , krasnoselskii, m. a. and rutickii , j. b.: convex functions and orlicz spaces, p. noordho ltd., groningen, kufner, a., john, o., and f uc k, s.: function spaces, noordhointernational publishing, leyden; academia, prague, lazer, a. c. and mckenna, p. j.: on a singular nonlinear elliptic boundary-value problem, proc. amer. math. soc. , no. , le, v. k., motreanu, d. and motreanu, v. v.: on a non-smooth eigenvalue problem in orlicz- sobolev spaces, appl. anal. , no. , lou, h.: on singular sets of local solutions to p-laplace equations, chin. ann. math. b , no. , marano, s. a. and motreanu, d.: on a three critical points theorem for non-dierentiable functions and applications to nonlinear boundary value problems, nonlinear analysis , mohammed, a.: positive solutions of the p-laplace equation with singular nonlinearity, j. math. anal. appl. , no. , nguyen, t. c.: three solutions for a class of nonlocal problems in orlicz-sobolev spaces, j. korean math. soc. , no. , rao, m. m. and ren, z. d.: theory of orlicz spaces, , marcel dekker, inc., new york, ricceri, b.: on an elliptic kirchho-type problem depending on two parameters, j. global optim. , no. , ricceri, b.: sublevel sets and global minima of coercive functionals and local minima of their perturbations, j. nonlinear convex anal. , no. , ricceri, b.: a further three critical points theorem, nonlinear anal. , no. , sun, y.: compatibility phenomena in singular problems, proc. roy. soc. edinburgh sect. a , no. , szulkin, a.: minimax principles for lower semicontinuous functions and applications to nonlinear boundary value problems, ann. inst. h. poincar e anal. non lin eaire , no. , zhao, l., he, y. and zhao, p.: the existence of three positive solutions of a singular p-laplacian problem, nonlinear anal. , no. , zhao, l. and zhao, p.: the existence of three solutions for p-laplacian problems with critical and supercritical growth, rocky mountain j. math. , no. ,",
        "Subsections": [],
        "Groundtruth": "In this section, the text discusses proving equivalences among three conditions. To prove the equivalences, various lemmas and proofs are presented. The section concludes by ensuring the discontinuity of the nonlinearity function f. Key concepts discussed include homogeneous operators, Nehari sets, variational principles, and the proof of Theorem ??. The section also references related works in the field of non-homogeneous elliptic equations and variational problems."
    },
    {
        "Section_Num": "Abstract",
        "Section": "Abstract",
        "Text": "handwritten indic character recognition using capsule networks bodhisatwa mandal,suvam dubey, swarnendu ghosh, riteshsarkhel, nibaran das dept. of cse, jadavpur university, kolkata, , wb, india. {bodhisatwam,suvamdubey}@gmail.com, swarnendughosh.cse.rs,nibaran.das@jadavpuruniversity.in dept. of cse, ohio state university, columbus, oh , usa sarkhelritesh@gmail.com abstractconvolutional neural networks has become one of the primary algorithms for various computer vision tasks. handwritten character recognition is a typical example of such task that has also attracted attention. cnn architectures such as lenet and alexnet have become very prominent over the last two decades however the spatial invariance of the different kernels has been a prominent issue till now. with the introduction of capsule networks, kernels can work together in consensus with one another with the help of dynamic routing, that combines individual opinions of multiple groups of kernels called capsules to employ equivariance among kernels. in the current work, we have implemented capsule network on handwritten indic digits and character datasets to show its superiority over networks like lenet. furthermore, it has also been shown that they can boost the performance of other networks like lenet and alexnet. index termscapsule network, convolutional neural net- work, image classication, classier combination, deep learn- ing",
        "Subsections": [],
        "Groundtruth": "The text discusses the use of capsule networks for handwritten Indic character recognition as an improvement over traditional convolutional neural networks like LeNet and AlexNet. Capsule networks address the issue of spatial invariance among kernels by utilizing dynamic routing to combine opinions from multiple groups of kernels (capsules). The study demonstrates the superior performance of capsule networks on handwritten Indic digits and character datasets, showing potential to enhance the performance of existing networks. Key terms include capsule network, convolutional neural network, image classification, classifier combination, and deep learning."
    },
    {
        "Section_Num": "I",
        "Section": "I Introduction",
        "Text": "it has been two decades since the rst convolutional neural networks was introduced in for handwritten digit classication problem. since then computer vision has matured a lot in terms of both the complexity of the architectures as well as the difculty of the challenges they address. many works have been introduced in later years to address challenges like object recognition . however through all these years one principal issue was yet to be addressed. convolutional neural networks by its nature employ invariance of features against their spatial position. as the kernels that represent specic features are convolved throughout the entire image, the amount activation is position invariant. the activations across different kernels do not communicate with each other and hence their outputs are spatially invariant. we have intuitively developed the skill to analyze relative positions of various parts of an object. to learn these relations the capsule networks were proposed . in short capsule networks consist of a group of kernels that work together and pass information to next layers through a mutual agreement that is achieved by dynamic routing of information during the forward pass. in our experiments, our goal is to analyze the performance of these networks for some indic digit and character datasets. we have used three of the most popular indic digits dataset namely,devanagari digits, bangla digits, and telugu digits. while these have only classes, the two character datasets, namely, bangla basic character and bangla compound char- acters have and classes respectively. there have been many works in indic datasets using cnns before , . in our experiments, the performance of the capsule networks are compared with respect to lenet and alexnet. to show that the capsule network learns unique concepts, we have combined it with other networks to show a boost in performance. in the next section, a refresher is provided as to the basics of a simple cnn. in section it is shown how the capsule network evolves over the simple cnn along with explanations regarding its internal mechanisms. in section , the experimentations and results are discussed and nally concluding in section",
        "Subsections": [],
        "Groundtruth": "The text discusses the advancements in computer vision over the past two decades, particularly focusing on the evolution of convolutional neural networks (CNNs) and the introduction of capsule networks to address spatial invariance issues. Capsule networks consist of interconnected kernels that communicate and pass information to the next layers through dynamic routing during the forward pass. The text outlines experiments conducted on indic digit and character datasets, comparing the performance of capsule networks with traditional CNN models like LeNet and AlexNet. Results indicate that the capsule network can learn unique concepts and show improved performance when combined with other networks. Further details are provided on the evolution of capsule networks from simple CNN structures, along with experimental procedures and results."
    },
    {
        "Section_Num": "II",
        "Section": "II CNN Refresher",
        "Text": "convolutional neural networks are typically designed as a series of -d convolution and pooling operations along with non-linear activations in the middle followed by a fully connected network for classication. a -d convolution is performed by convolving a kernel k over an input of size cin hin win where hin, win and cin are height, width and number of input channels of the input . a kernel convo- lution on such an input should be of a shape cin kh kw. here cin, kh, kw are the depth, height and width of the kernel. note that the depth of the kernel is equal to the input number of channels. if we use cout number of such kernels, then the output tensor generated shall be of the shape cout hout wout. the height hout and width wout are dependent on factors like input height hin, input width win, stride of the kernel and the padding of the input. convolutions are typically followed by non-linear activations such as a sigmoid, tanh, or a rectied linear units. pooling operations normally take a small region of the input and compresses it to a single value by taking either maximum or average of the corresponding activations. this reduces the size of the activation maps. hence when kernels convolve over this tensor, it actually corresponds to a larger area in the original image. after a series of convolutions, activations and pooling, we obtain a tensor signifying the extracted features of the image. this tensor is attened to form a linear vector of shape c h w , which can be fed as an input to a fully connected network. here c, h and w are the depth, height, and width of the tensor to be attened. the total number of neurons in this layer nf c is arxiv:v jan fig. a schematic diagram of a sample convolutional network(pooling layers omitted to keep the architecture analogous with respect to g chw . at the end of the fully connected network we get a vector of size nclass that corresponds to the output layer. a loss such as mean-square error, or cross entropy, or negative log likelihood is computed which is then back-propagated to update the weights using optimizers like stochastic gradient descent or adaptive moments. a schematic diagram is shown in g., with a typical convolution operation followed by a fully connected layer to perform classication. layers such as pooling and non-linearities are not shown to keep simplicity and to keep the diagram analogous to g.",
        "Subsections": [],
        "Groundtruth": "Convolutional neural networks consist of 2D convolution and pooling operations with non-linear activations, followed by a fully connected network for classification. The convolution operation involves convolving a kernel over an input tensor, resulting in an output tensor based on factors like input dimensions, kernel size, stride, and padding. Non-linear activations like sigmoid, tanh, or ReLU are applied after convolutions. Pooling operations compress input regions to a single value, reducing activation map size. The network extracts image features through convolution, activation, and pooling, leading to a flattened tensor fed into the fully connected layer. The output layer produces a vector for classification. Loss functions like mean-square error or cross entropy are computed and back-propagated to update weights using optimizers like stochastic gradient descent. Pooling and non-linearities are omitted in diagrams for simplicity."
    },
    {
        "Section_Num": "III",
        "Section": "III The Capsule Network",
        "Text": "the primary concern with cnns are that the different ker- nels work independently. if two kernels are trained to activate for two specic parts of an object they will generate the same amount activations irrespective of the relative positions of the object. capsule networks brings a factor of agreement between kernels in the equation. subsequent layers receive higher activations when kernels corresponding to different parts of the object agree with the general consensus. the capsule network proposed in consist of two different capsule layers. a primary capsule layer that groups convolutions to work together as a capsule unit. this is followed by a digit capsule layer that is obtained by calculating agreement among different capsules through dynamic routing. a schematic diagram of capsule network is provided in g. the diagram does not represent the actual architecture proposed in the original work , rather it demonstrates a primary and digit capsule layer. the diagram drawn is kept analogous to a typical cnn shown in g. to highlight the major differences. a. primary capsules the capsule network starts with typical convolution layer that converts the input image into a block of activations. this tensor is fed as an input into the primary capsule layer. if the number of channels in this input is cin and the desired dimension of primary capsules is dp c then the shape of one kernel is dp c cin kh kw. kh and kw are the height and width of the kernel. with cout number of such kernels we shall get an output of shape coutdp c houtwout, where the height hout and width wout is dependent on factors like input height hin, input width win, stride of the kernel and the padding of the input. unlike normal convolutions, where each activation tensor had a depth of , the depth of the activations in primary capsules is dp c. the total number of primary capsules np c is cout h w . before passing to the next layer this tensor is reshaped into np c dp c. b. digit capsules normally output layers in a fully connected network is of the shape nclass , where nclass is the number of classes. the capsule network replaced the output layers with a digit capsule layer. each class is represented by a capsule of dimension ddc. hence we get a digit capsule block of shape nclass ddc. by calculating the l norm of each row we get our output layer of shape nclass the values of digit capsules are calculated by dynamic routing between primary capsules. c. dynamic routing the dynamic routing is computed to obtain the digit capsules from the primary capsules. two different types of weights are required to perform dynamic routing. firstly we need the weights to calculate individual opinions of every capsule. these weights, w dc are normally trained using back-propagation. if i is the index of the primary capsules of dp c dimension and j is the index of the digit capsules of ddc dimension. w dc ij is of shape dp c ddc. the individual opinion of i regarding the digit capsule j is given by, uj|i = uiw dc ij , where ui is the i th primary capsule. so for each capsule i we get an individual digit capsule block of shape nclass ddc. the second type of weight can be called the routing weights . the routing weights are used to combine these individual digit capsules to form the nal digit capsules. fig. a schematic diagram of capsule network demonstrating primary and digit capsule layers. these routing weights are updated on during the forward pass based on how much the individual digit capsules agree with the combined one. the routing weight matrix w dr is of the shape of np c nclass. during each forward pass the routing weights are rst initialized as zeros. the coupling coefcients cij is given by, cij = exp(w dr ij ) p k exp(w dr ik ). the coupling coefcients are used to combine the individual digit capsules and form the combined digit capsule. the jth combined digit capsule sj is given by, sj = x i cij uj|i a squashing function stretches the values of sj such that bigger values go close to one and lower values go close to zero. the squashed combined digit capsule vj is given by, vj = ||sj|| + ||sj|| sj ||sj||. the agreement between individual digit capsules uj|i and the squashed combined digit capsules vj can be calculated using a simple dot product. the more the value of the agreement the more preference is awarded to the corresponding capsule i in the next routing iteration. this is obtained by updating the w dr ij as, w dr ij = w dr ij + uj|i.vj. equations - are repeated for a specic number of routing iterations to perform iterative dynamic routing of opinions of primary capsules to form the digit capsule. d. loss function the loss function used for capsule networks is a marginal loss for the existence of a digit. the marginal loss for digit k is given by, lk = tk max+ max here, tk = iff a digit of class k is present. the upper and lower bounds m+ and mare set to and respectively. is set as e. regularization proper regularization of a network is essential to stop models from over-tting the data. in case of capsule networks a parallel decoder network is connected with the obtained digit capsules as its input. the decoder tries to reconstruct the input image. a reconstruction loss is also minimized along with margin loss so that the network does not over-t the training set. however the reconstruction loss is scaled down by a factor of so that the margin loss is not dominated.",
        "Subsections": [],
        "Groundtruth": "Capsule networks address the limitation of CNNs where kernels work independently. In capsule networks, kernels trained to detect object parts generate higher activations when agreeing with consensus. This network consists of primary and digit capsule layers, utilizing dynamic routing for agreement calculation. Primary capsules convert input into activations grouped as capsule units, while digit capsules represent classes. Dynamic routing involves individual and routing weights for forming digit capsules. A squashing function is applied to combine digit capsules. Loss function involves a marginal loss for the existence of a digit, with regularization using a decoder network for reconstruction to prevent overfitting."
    },
    {
        "Section_Num": "IV",
        "Section": "IV Experimentations and Results",
        "Text": "our experiments focus on the implementation of capsule networks for handwritten indic digits and character databases. the results have been compared with other famous cnn architectures like lenet and alexnet. while lenet was built for smaller problems like digit classication, alexnet was intended for much more complicated data like the imagenet. the input image is resized to the native size supported by the network that is for capsule networks, for lenet and for alexnet. in total models have been tested on datasets. firstly, the basic lenet, alexnet and capsule network was tested. second set of experiments involved an ensemble of two of the three networks using a probabilistic averaging. finally all the three networks were combined by averaging the output probability distribution. all the results are tabulated in table fig. test accuracy vs number of training epochs for character datasets. a. datasets we have used ve datasets for our experiments. firstly we have indic handwritten digit databases in three scripts that is bangla, de- vanagari and telugu. these are a typical class problems to primarily chal- lenge the performance of lenet. subsequently, the character databases namely, bangla basic characters and bangla compound characters give us a class and a class problem to deal with. the description of the datasets are given below. all the datasets were split into train and test set in the ratio : the accuracies provided are with respect to the best model in terms of training accuracy. b. architecture and hyperparameters the capsule network has been used as it has been proposed in . the performance is compared with respect to lenet and alexnet. the specics of the capsule network architecture is provided in table the lenet was primarily built for mnist digit classication with only around k trainable parameters. the alexnet has around million trainable parameters so that it can tackle harder problems. like lenet, the capsule network was also proposed for mnist digit clas- sication, however it is much more robust. it has around million parameters out of which around k parameters are trained on the runtime by dynamic routing. all the provided statistics is with respect to a single channel input of native input size and a class output. all networks are optimized with adam optimizer with an initial learning rate of , eps of e- and beta values as and . the experiments were carried out using a nvidia quadro p with cuda cores and gb of vram. https://code.google.com/archive/p/cmaterdb/ c. result and analysis the result of the experiments have been tabulated in table it can be clearly seen that capsule networks(written as cap- snet) surpasses lenet in case of every dataset used. alexnet being almost times larger network as compared to capsule networks performs better than capsule networks. however the difference in performance is much more visible in case of the character datasets with much higher number of classes as compared to digits. lenet fail poorly for the character datasets. capsule network proved to be much more robust against complex data with higher number of classes. upon combination we can see that combining lenet with alexnet is detrimental in nature with respect to alexnet alone for every dataset. however combining capsule networks have always shown a positive effect. this proves that capsule network are capable of extracting some information that even alexnet fails to obtain. for most datasets the best performance was achieved by combining alexnet with capsule networks except for telugu digits, where combination of all three networks proved to be the best. furthermore we have analyzed the rise of test accuracy with every epoch of training. it can be seen that the capsule networks have the steepest slope signifying that they have the fastest learning curve. finally in table we have compared the obtained result against some state of the art works performed on the datasets. in terms of computational complexity, the extra computa- tional overhead is during the dynamic routing phase. other than training w dc by backpropagation, for every sample the routing weights w dc must also be tuned for riter times, where riter is the number of routing iterations. during each iteration np c nclass number of coupling coefcients must be calculated. further an weighted sum over np c dimension is needed to compute the combined digit capsule. finally np c nclass number of routing weights must be tuned using agreement of individual and combined digit capsules. with all these, capsule networks generally have quite slow iterations, table i specifications for the capsule network with respect to a single channel input and ten class output network type of layer kernel size stride padding input channels output channels input capsule dimension output capsule dimension weights + biases routing weights convolution na na , na primary caps ,, capsule digit caps na na na , ,, , networks decoder fc na na na na na , na decoder fc na na na , na na , na decoder fc na na na , na na , na total number of parameters ,, table ii test accuracy for various networks and their ensemble architectures bengali digits devanagari digits telugu digits bengali basic characters bengali compound characters mean (standard deviation) lenet alexnet capsnet lenet+alexnet lenet+capsnet alexnet+capsnet all combined table iii comparitive study against other state of the art approaches dataset our approach accuracy other approaches accuracy bangla digits alexnet + capsnet basu et al . roy et al. roy et al. devanagari digits alexnet + capsnet das et al. roy et al. telugu digits alexnet + capsnet + lenet sarkhel et al. roy et al. bangla basic characters alexnet + capsnet sarkhel et al. bhattacharya et al. bangla compound characters alexnet + capsnet roy et al. pal et al. sarkhel et al. but as evident from fig. iv it also learns much faster as compared to lenet and alexnet.",
        "Subsections": [],
        "Groundtruth": "The experiments focused on implementing capsule networks for handwritten indic digits and character databases, comparing results with other famous CNN architectures like LeNet and AlexNet. Multiple models were tested on five datasets, including handwritten digit databases in three scripts and character databases in Bangla. Results showed that capsule networks outperformed LeNet for all datasets, while AlexNet, being a larger network, performed better than capsule networks, especially for character datasets. Combining capsule networks consistently showed positive effects, with the steepest learning curve and fastest learning rate among the networks tested. The computational complexity of capsule networks mainly arises during the dynamic routing phase. The experiments demonstrated the effectiveness and robustness of capsule networks, especially for datasets with more complex data and higher number of classes."
    },
    {
        "Section_Num": "V",
        "Section": "V Conclusion",
        "Text": "in our current work we have implemented the capsule networks on handwritten indic digits and character databases. we have shown that capsule networks are much superior and robust compared to the lenet architecture. we have also seen that capsule networks can act as a booster when combined with other networks like lenet and alexnet. the best per- formance was achieved by combining alexnet with capsule networks for most of the datasets. only in case of telugu dataset, combination of all three networks worked the best. from the results it can be concluded that even with times more parameters that capsule networks, the alexnet failed to capture some information that the capsule network learnt. thus it was able to improve the performance of alexnet. finally it has also been seen the capsule network converge much faster that lenet or alexnet. in terms of pros and cons, the use of capsule networks can be benecial for learning with much lesser number of features and also as improvement technique for other bigger networks. the problem with capsule network is its slow iterative process and limitation to single layer routing. that reveals many avenues of research. acknowledgment this work is partially supported by the project order no. sb/s/eece//, dated //, sponsored by serb and carried out at the centre for microprocessor application for training education and research, cse department, jadavpur university. references y. lecun, l. bottou, y. bengio, and p. haffner, gradient-based learning applied to document recognition, proceedings of the ieee, vol. , no. , pp. , a. krizhevsky, i. sutskever, and g. e. hinton, imagenet classication with deep convolutional neural networks, pp. , s. sabour, n. frosst, and g. e. hinton, dynamic routing between capsules, pp. , s. ukil, s. ghosh, s. m. obaidullah, k. santosh, k. roy, and n. das, deep learning for word-level handwritten indic script identication, arxiv preprint arxiv:, r. sarkhel, n. das, a. das, m. kundu, and m. nasipuri, a multi- scale deep quad tree based feature extraction method for the recognition of isolated handwritten characters of popular indic scripts, pattern recognition, vol. , pp. , s. basu, n. das, r. sarkar, m. kundu, m. nasipuri, and d. k. basu, an mlp based approach for recognition of handwritten bangla numerals, arxiv preprint arxiv:, a. roy, n. mazumder, n. das, r. sarkar, s. basu, and m. nasipuri, a new quad tree based feature set for recognition of handwritten bangla numerals, pp. , a. roy, n. das, r. sarkar, s. basu, m. kundu, and m. nasipuri, an axiomatic fuzzy set theory based feature selection methodology for handwritten numeral recognition, pp. , n. das, b. das, r. sarkar, s. basu, m. kundu, and m. nasipuri, handwritten bangla basic and compound character recognition using mlp and svm classier, arxiv preprint arxiv:, r. sarkhel, a. k. saha, and n. das, an enhanced harmony search method for bangla handwritten character recognition using region sam- pling, pp. , u. bhattacharya, m. shridhar, and s. k. parui, on recognition of handwritten bangla characters, pp. , s. roy, n. das, m. kundu, and m. nasipuri, handwritten isolated bangla compound character recognition: a new benchmark using a novel deep learning approach, pattern recognition letters, vol. , pp. , a. pal and j. pawar, recognition of online handwritten bangla charac- ters using hierarchical system with denoising autoencoders, pp. , r. sarkhel, n. das, a. k. saha, and m. nasipuri, a multi-objective approach towards cost effective isolated handwritten bangla character and digit recognition, pattern recognition, vol. , pp. ,",
        "Subsections": [],
        "Groundtruth": "Capsule networks were implemented on handwritten Indic digits and characters databases, demonstrating their superiority and robustness compared to the LeNet architecture. When combined with other networks like LeNet and AlexNet, capsule networks acted as a booster, with the best performance achieved by combining AlexNet with capsule networks for most datasets. The study showed that even with more parameters, AlexNet failed to capture some information that capsule networks could learn. Capsule networks also converged much faster than LeNet or AlexNet. While beneficial for learning with fewer features and as an improvement technique for larger networks, capsule networks have drawbacks such as slow iterative processes and limitations to single-layer routing, suggesting further avenues of research."
    },
    {
        "Section_Num": "References",
        "Section": "References",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "Abstract",
        "Section": "Abstract",
        "Text": "the peebles - vilenkin quintessential ination model revisited jaume haro,, jaume amor os,, and supriya pan, departament de matem` atiques, universitat polit` ecnica de catalunya, diagonal , barcelona, spain department of mathematics, presidency university, / college street, kolkata , india. we review the well-known peebles-vilenkin quintessential ination model and discuss its possible improvements in agreement with the recent observations. the improved pv model depends only on two parameters: the inaton mass m, and another smaller mass m; where the latter has to be chosen in order to undertake that, at present time, the dark energy density of the universe is approximately about % of the total energy budget of the universe. the value of the inaton mass m is calculated using the observational value of the power spectrum of the scalar pertur- bations, and the value of mass m, which depends on the reheating temperature, is calculated by solving the corresponding dynamical system whose initial conditions are taken at the matter-radiation equality and are obtained from three observational data: the red shift at the matter-radiation equality, the ratio of the matter energy density to the critical one at the present time and the current value of the hubble parameter. keywords: ination, quintessence, evolution of the universe",
        "Subsections": [],
        "Groundtruth": "The text reviews and improves the Peebles-Vilenkin quintessential inflation model, focusing on its alignment with recent observations. The enhanced model involves two parameters: the inaton mass (m) and a smaller mass (m) selected to ensure the dark energy density constitutes approximately 70% of the universe's total energy budget. The inaton mass is determined using the observed power spectrum of scalar perturbations, while the smaller mass depends on the reheating temperature and is derived by solving a dynamical system with initial conditions set at matter-radiation equality. This information is obtained from observational data on the redshift at matter-radiation equality, the ratio of matter energy density to critical density at present, and the current value of the Hubble parameter. Key topics include inflation, quintessence, and the universe's evolution."
    },
    {
        "Section_Num": "Contents",
        "Section": " Contents",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "1",
        "Section": "1 Introduction",
        "Text": "electronic address: jaime.haro@upc.edu electronic address: jaume.amoros@upc.edu electronic address: supriya.maths@presiuniv.ac.in arxiv:v jun",
        "Subsections": [],
        "Groundtruth": "The introduction section provides email addresses for contacting the authors and mentions a reference to arxiv."
    },
    {
        "Section_Num": "2",
        "Section": "2 The original model",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "3",
        "Section": "3 Evolution from kination to matter-radiation equality",
        "Text": "",
        "Subsections": [
            {
                "Section_Num": "3_1",
                "Section": "3.1 Decay before the end of kination",
                "Text": "",
                "Subsections": [],
                "Groundtruth": ""
            },
            {
                "Section_Num": "3_2",
                "Section": "3.2 Decay after the end of kination",
                "Text": "",
                "Subsections": [],
                "Groundtruth": ""
            }
        ],
        "Groundtruth": ""
    },
    {
        "Section_Num": "4",
        "Section": "4 Evolution from the matter-radiation equality",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "5",
        "Section": "5 Exponential quintessence potential",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "6",
        "Section": "6 Concluding remarks",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "References",
        "Section": " References",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "Contents",
        "Section": "Contents",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "Abstract",
        "Section": "Abstract",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "Optimal",
        "Section": "Optimal Object Placement using a Virtual Axis",
        "Text": "martin wei ostbayerische technische hochschule regensburg faculty of computer science and mathematics regensburg, germany martin.weiss@oth-regensburg.de abstract. a basic task in the design of a robotic production cell is the relative placement of robot and workpiece. the fundamental require- ment is that the robot can reach all process positions; only then one can think further optimization. therefore an algorithm that automatically places an object into the workspace is very desirable. however many it- erative optimzation algorithms cannot guarantee that all intermediate steps are reachable, resulting in complicated procedures. we present a novel approach which extends a robot by a virtual prismatic joint - which measures the distance to the workspace - such that any tcp frames are reachable. this allows higher order nonlinear programming algorithms to be used for placement of an object alone as well as the optimal placement under some dierentiable criterion. keywords: optimization, virtual joint, inverse kinematics, nonsmooth optimization, workspace, cartesian tasks problem statement we consider the following task: a robot should unload a storage box with a chess- board like structure containing bx by identical workpieces at positions pkl, k = , . . . , bx, l = , . . . , by, counted in the coordinate directions of the frame c i f (where i f denotes the set of all frames i r so) associated with the box at distances dx and dy. think of test-tubes in medicine or small parts in general production. the cell setup is considered xed, so only the placement of the box in the cell can be chosen, e.g. because a new order type. usually a pick-and-place operation is programmed at one corner only, the other position commands are computed from this corner position and the indices and distances. however it is dicult for the user to assess whether all positions are reachable because of nonlinearity and axis limits. testing the corners is a heuristic that works in many cases but there is no guarantee, so one has to run time-consuming tests. when the process needs to work on an object from dierent sides or with dierent orientations the situation is even more complicated. so the user would like to have an algorithm that determines a feasible object frame c near some initial guess c, maybe additionally optimizing one of the many known manipulability measures, see , . arxiv:v jan martin wei it is easy to check in a program whether a given frame c leads to reachable positions or not but it is dicult for a nonlinear optimizer to determine a di- rection that leads to a more feasible situation: feasibility is a binary decision; the backward transformation will usually issue an error only, and abort. our idea is to introduce a virtual joint as a slack variable in terms of nonlinear programming into the optimization problem that measures the distance of a position from feasibility. this variable therefore has an intuitive geometric interpretation. we can also interpret the virtual axis as a homotopy variable similar to which gives an easy solution for large values. our slack variable is not generated by the standard procedure replacing an inequality constraint g by the equality g + z = , with the sign constraing z our approach has some similarity to the introduction of virtual axes for singularity avoidance in or . however we do not introduce a rotational joint to reduce velocities near singularities but rather use a prismatic joint to enlargen the mathematical workspace in the optimization process. in combination with a smooting operation we can use standard optimization algorithms which require dierentiability of order like all algorithms based on gradient descent, or order like sequential quadratic programming , cf. . our approach is not related at all to voxelization of the workspace like in and other algorithms aiming at collision free planning. virtual axis approach for ease of exposition we choose a r robot resembling the well known puma but with more zeros in the parameters. we could extend all formulae to similar r real industrial robots. we use the dh convention rz tz tx rx =: rz bi =: ai to get the wrist centre point and tool centre point wcp = a a a a a a tcp = wcp tool expressed relative to the world coordinate system chosen as the axis coordinate system. note that a = d = in our case so the dh chain ends in the wcp as the essential point for the backward transform. figure shows the robot data and the reference position. we use l = , l = and a tool with tz, tz = , pointing in direction z from the ange. lengths are measured in , angles in . note that joint is pointing upward for q = , so the stretched position corresponds to q = . we assume joint limits qmin,i qi qmax,i , i = , . . . as usual, this type of robots has up to discrete solutions of the backward transform for non-singular positions. we identify these congurations with an integer s {, . . . , }. infeasibility of the backward transform for a given frame f and conguration s may arise from two reasons with dierent severity: first, the wcp may be to placement using virtual axis i i di ai i type q r q l r q r q l r q r q r fig. dh parameters and and reference position q = for original robots far from the robot such that the triangle construction for q fails. there is no remedy in this case. second, even if axis values q exist such that tcp = f, these might violate the joint limits: qi for some i. this is no obstacle during the optimization process, only for a solution. so the second problem can be xed by dropping the joint limits and allowing qi (, ], i = , . . . , we describe this dierence by a physical and a mathematical workspace wp and wm which are the wcp frames under the two joint restrictions under consideration: wp = wcp y i= ! , wm = wcp \u0000(, ]\u0001 we have wp wm and wp = wm in general but wm is still a bounded set, which is the rst problem. in order to use optimization algorithms which may leave the feasible set wm, our goal is to dene a virtual robot which has a solution for the backward transform for any frame f i f and any conguration s. so we associate to our original robot a virtual robot with an additional virtual prismatic joint between joints and , which has no joint limits. any wcp in i r is reachable then. the variable of the virtual joint will be denoted v, the other joints keep their names giving a combined joint variable q = i r dh parameters of the virtual robot are shown in figure sucient conditions for our approach are stated as two assumptions: assumption - reachability of i r: the mapping of the original joints and the virtual joint to the wcp position is surjective onto i r assumption - reachability of so joints ,, form a central wrist parametrizing all of so, i.e. the mapping , rz b rz b rz is surjective. in our case assumption is satised because for any xed q value the virtual joint generates an innite line with the wcp, the second joint rotates the line through a plane, the rst joint rotates the plane through all i r central wrists martin wei i i di ai i type q r q l r q r v p q l r q r q r fig. dh parameters and reference position q = for virtual robot satisfying assumption for unbounded joint variables are the most common choice in industry. assumption also guarantees an -solution kinematics. using the notation wcpv to distinguish the forward transform of the virtual robot we denote wv = wcpv \u0000(, ] i r (, ]] \u0001 . under our assumptions we get wv = i f. we call such a robot a dextrous robot because the dextrous workspace in i r (points reachable with all orientations, see ) and reachable workspace coincide, and are all of i r backward transform with redundancy resolution however we have introduced redundancy in our kinematics so we have to dene a backward transform giving unique results. the virtual robot backward transform sets the virtual joint to the smallest absolute value such that a solution exists. in our case this is the distance between the wcp position and wm which is a hollow sphere for our robot so calculations are simple. algorithm uses the backward transform of the original robot: note that both l > l and l < l lead to an empty interior, but l < l results in negative v values. also note that for our robot the stretched position for axis is q = , not as in most industrial robots. this stretched position is used for all wcp positions outside wm. for robots other than our simple one the computation of q, as well as the denition of the stretched position and the computation of v have to be adapted. smoothness properties the dexterity of the virtual robot makes all tcp frames feasible. also the joint values q depend continuously on the tcp frame, if we keep the conguration xed and avoid singularities or the original robot. inside wm continuity is clear placement using virtual axis algorithm virtual robot backward transform input: wcp frame f parametrized by p = i r, q so input: conguration coded as s {, . . . , } output: virtual robot axes q (, ] i r (, ]] with wcp( q) = f where q corresponds to conguration s if solution q (, ] of original backward transform exists then return essentially this solution as q = else compute q using the original backward transform: q = atan d := ||p||, distance of wcp from robot base if p is outside of the hollow sphere, d > l + l then v := d l l put joints and in stretched position q := atan, q := else p is in the empty interior of the hollow sphere, d < |l l| v := d l + l put joints and in stretched position q := atan, q := + end if end if from the usual backward transformation formulae. when the wcp reaches the boundary of wm the triangles for the elbow-up and elbow-down conguration degenerate to a line and coincide, hence q which is the same value as when approaching from outside. also q = atan depends continuously on the wrist centre point. however the dependence is not dierentiable: when the wrist centre point enters wm from outside the solution for q applies the cosine theorem to get c = cos near = . computing = atan( c, c) we get an innite slope at c = , see figure this non-dierentiability can aect all other axes. fig. backward transform and smoothing in order to get c behaviour of all angles we modify on some interval with a polynomial of degree with c transition at c = and rst and second derivative at c = , see the red graph in figure , right. martin wei this is a basic idea in nonsmooth optimization . of course this distorts the backward transform and one has to check whether smoothing aects the optimal solution but the hope is that the optimum is suciently inside the workspace, and smoothing is only a temporary help for the optimization algorithm. figure shows this behaviour for a motion of the virtual robot tcp from l along the x axis to l in height z = with constant orientation q with l = , l = , q = in the elbow-up conguration such that the tool is pointing downwards. the wcp crosses the boundary wm at x = p = p . axes q, q, q do not move. all joints are continuous indeed with innite slope of q, q, q at x. the left picture shows the axis values without smoothing, the right a zoom into the q behaviour without and with smoothing. the virtual axis remains at v = in wm, then v grows linearly. so the virtual joint v is also non-dierentiable at x but has essentially the same behaviour like x max{, x} at x = forming the square x creates a c function, we will use this trick in our objective function. fig. joint values for motion through workspace boundary formulation of the optimization problem we assume that poses must be reached with the same conguration; this is quite usual for cartesian task. we parametrize the corner frame as c = trans rz ry rx. these parameters or a subset thereof constitute our op- timization variables. denoting q = q = q the joint placement using virtual axis values obtained by the virtual robot backward transform for grid position in the box, q i and v the original and virtual joints, we may optimize min x,y,z,,, bx x k= by x l= ) under qmin,i q i qmax,i i = , . . . , , k = , . . . , bx, l = , . . . , bx we can also add constraints on the frame parameters . the square in the objective function makes the objective function c to make advantage of this however we must use the c smoothing of the backward transform at the workspace boundary so that the constraints are c as well. with |v| we could even obtain a c objective function. adding some manipulability criterion from or to the objective function can optimize feasibility and manipulability in combination. however one has to use appropriate weighting because in extreme cases the optimizer might tolerate some infeasible points in exchange for high manipulability at other points. numerical results we have tested our optimization procedure with the solvers implemented in the matlab fmincon command. we obtained optimal solutions both with the default interior point algorithm and the sqp algorithms. however, in many cases the sqp algorithm required only iterations, about half the iterations of the interior point algorithm. computation time was below sec on a standard laptop with bxby = = grid point in the box. figure shows some typical run where a box is drawn from far outside the workspace to the interior . the results were almost independent from dierentiability properties. the interpretation is: even when crossing the critical workspace boundary the majority of grid points is away from the vertical slope, dominating the numerical derivatives. when suciently inside the workspace the algorithm hardly ever enters the infeasible region again. we have also tested the algorithm with success for a more complicated ge- ometry: the robot makes the moves for the popular board game settlers of catan with real tokens on a hexagonal structure on a playeld simulated on a screen, see . however some manual intervention is necessary as the robot cannot reach all positions with the same conguration. conclusions the approach presented in this contribution opens a way to non-interior-point optimization algorithms for the most common class of industrial r robots. obviously the approach can also be used for the placement of a robot in a xed martin wei fig. optimization results and demonstrator setup work cell; we only need to consider the robots base coordinate system as the variable. furthermore it seems promising to use the idea for the optimization of redundant tasks like in or redundant robots and to compare the results. new algorithms for non-dierentiable problems should be investigated in comparision to higher order approaches, including estimates for the convergence of the solution of the smoothed problem to the original problems solution. references bertsekas, d.p.: nondierentiable optimization via approximation. math. pro- gramming study , pp. - . craig, j.: introduction to robotics: mechanics and control. addison wesley, curtis, f.e., overton, m.l.: a sequential quadratic programming algorithm for nonconvex, nonsmooth constrained optimization. siam j. optimization , pp. - garcia, c., zangwill, w.: pathways to solutions, xed points and equilibria. prentice-hall kuka robot group: robots play board games - students win big, https://www. youtube.com/watch?v=ocqpwv kyc, l eger, j., angeles, j.: o-line programming of six-axis robots for optimum ve- dimensional tasks. mechanism and machine theory , - leontjevs, v., flores, f.g., lopes, j., kecskemethy, a.: singularity avoidance by virtual redundant axis and its application to large base motion compensation of serial robots in: proceedings of the raad st international workshop on robotics in alpe-adria-danube region. naples merlet, j.p.: jacobian, manipulability, condition number, and accuracy of parallel robots. j. mech. des. , - nocedal, j., wright, s.j.: numerical optimization. springer, new york reiter, a.: ein beitrag zur singularit atsvermeidung bei industrierobotern durch einf uhrung virtueller achsen. master thesis, johannes kepler university linz vahrenkamp, n., asfour, t., dillmann, r.: robot placement based on reachability inversion. ieee international conference on robotics and automation yoshikawa, t.: manipulability of robotic mechanisms the international journal of robotics research , pp. - .",
        "Subsections": [],
        "Groundtruth": "The text presents a novel approach for optimal object placement in a robotic production cell using a virtual axis. The goal is to ensure that the robot can reach all process positions to enable further optimization. The approach extends the robot with a virtual prismatic joint, allowing any TCP frames to be reachable. By introducing this virtual joint as a slack variable in nonlinear programming, the algorithm ensures that all intermediate steps are reachable. This method simplifies the optimization process and allows for optimal object placement under different criteria. The algorithm has been tested successfully and offers potential for application in industrial robot tasks and redundant robot optimization."
    },
    {
        "Section_Num": "Abstract",
        "Section": "Abstract",
        "Text": "arxiv:v jan sums of certain fractional parts sums of certain fractional parts olivier bordells abstract. in this note, an upper bound for the sum of fractional parts of certain smooth functions is es- tablished. such sums arise naturally in numerous problems of analytic number theory. the main feature is here an improvement of the main term due to the use of weyls bound for exponential sums and a device used by popov.",
        "Subsections": [],
        "Groundtruth": "The abstract discusses an upper bound established for the sum of fractional parts of certain smooth functions in the field of analytic number theory. The main term improvement is achieved through the use of Weyl's bound for exponential sums and a specific method by Popov."
    },
    {
        "Section_Num": "1",
        "Section": "1. Introduction and main result",
        "Text": "le {x} be the fractional part of x r and := {x} be the rst bernoulli function. sums of the shape x n<nn ) where n is a large number and f is a smooth function, are of great importance in analytic number theory . a large amount of problems are reduced to obtaining a non-trivial bound for the sum , such as, among others, the dirichlet divisor problem, the gauss circle problem, the problem of the gaps between k-free numbers or the distribution of squarefull numbers. the general strategy is to use a truncated version of the expansion of in fourier series, providing the inequality x n<nn ) n l + x l x n<nn e) where e := eix and l is any integer parameter to be chosen optimally. the problem is hence- forth reduced to estimating exponential sums, for which several methods have been developed by weyl, van der corput and vinogradov. for instance, when f ck satises f k < , van der corputs estimate and the inequality yield the bound x n<nn ) k n k k + n k logn + n k+k k k when k , the term n k logn being removed in the case k = when k {,,}, this respectively gives x n<nn ) n/ +/ , x n<nn ) n/ + n /logn + n // , x n<nn ) n/ + n /logn + n // . the term n k k is usually called the main term, the other two terms being the secondary terms. for monomial functions, i.e. functions f c such that f t n k for some t and for any positive integer k, van der corputs method of exponent pairs provides better results (see or ). recently, using new bounds given in for the number js,k of integral solutions of the system x j ++ x j s = y j ++ y j s j k with xi , yi x , some improvements in exponential sums have appeared in the literature . if we use the main result in combined with , we obtain n x n<nn f ,k n kk+ k + n k + n k k k mathematics subject classication. primary l; secondary l, j key words and phrases. weyls and van der corputs exponential sums, fractional part. olivier bordells where k z and f ck is such that there exists k such that f k. this improves on the main term of as soon as k and gives the same exponent when k = : n x n<nn ) n/ + n / + n // , n x n<nn ) n/ + n / + n // . any improvement of or when k {,,} may lead to new results in the aforementioned prob- lems. the main purpose of this note is to improve the main term in the cases k for and k {,,,} for . to do this, we use weyls differencing method and add a device due to popov , also used in to estimates the sums x n<nn ) where p is a polynomial of degree or with small positive leading coefcient. the method was then generalized in to any polynomial of degree , and we use here the weyls schift to extend the results to smooth functions. unfortunately, as often in exponential sums estimates, the secondary terms remain too weak to be really efcients in practice. nevertheless, the result below seems to be new and we think that it may be of interest. theorem let k,n z, f ck+ such that there exist k,k+,sk > and ck,ck+ such that, for any x and any j {k,k +} j f cj j with k = sknk+ dene dk := k+k. then, for any > n x n<nn f k, nk k + n k k k n kk kdk + n n kk dk . note that, if n k+k k , then the nd term is absorbed by the rd one. to compare with and in the cases k {,,}, theorem respectively gives n x n<nn f n/ + n // + n // , n x n<nn f n/ + n // + n // , n x n<nn f n/ + n // + n // .",
        "Subsections": [],
        "Groundtruth": "The section introduces the importance of sums involving the fractional part of x in analytic number theory and mentions various problems related to obtaining non-trivial bounds for such sums. It discusses using Fourier series expansion and exponential sums to estimate these bounds, referencing methods by Weyl, van der Corput, and Vinogradov. The text highlights improvements in exponential sums using new bounds for integral solutions of certain systems. It focuses on improving the main term in cases where k is a positive integer, utilizing Weyl's differencing method and a device by Popov. A new theorem is presented that offers improved bounds for the sums, particularly in cases where k is small."
    },
    {
        "Section_Num": "2",
        "Section": "2. Technical lemmas",
        "Text": "lemma let m z, n z, h z and > then x m<nm+n min h, n hn+ n + logh + h. proof. this is . lemma . let n,n z such that n < n n and an+,,an c satisfying |an| then, for any h {,,n } x n<nn an h x n<nnh x hh an+h + h. proof. dene n := ( an, if n < n n , otherwise. sums of certain fractional parts then x n<nn an = h x hh x nz n = h x hh x nz n+h = h x hh x nh<nnh n+h = h x nn x hh n+h h x nn x hh n+h = h x nnh x hh n+h + h x nh<nn x hh n+h h x nn x hh n+h = h x n<nnh x hh n+h + h x nh<nn x hh n+h since, in the rst sum, n+h = an+h, we get x n<nn an h x n<nnh x hh an+h + h h as asserted.",
        "Subsections": [],
        "Groundtruth": "The text introduces technical lemmas related to mathematical proofs involving inequalities. In particular, the lemmas involve the comparison and manipulation of fractional parts and sums of certain values. These lemmas provide a foundation for proving inequalities using specific mathematical expressions and fractional relationships."
    },
    {
        "Section_Num": "3",
        "Section": "3. Proof of Theorem ??",
        "Text": "one may assume n k < k < , otherwise nk k + n n kk /dk > n. using and lemma , we get for any h,l z such that h < n x n<nn f n l + x l x n<nn e f n l + x l ( h x n<nnh x hh e f + h ) n l + h x n<nnh x l x hh e f + h logl n l + h x n<nn x l x hh e gn+rn,k + h logl with gn := k!hk f + +h f and rn,k := k! zn+h n k f dt. note that x hh rn,krn,k < ck+ ! hk+k+ = ck+ sk! hk+n k so that, by partial summation x n<nn f n l + h x n<nn ( x l + hk+n k max hh x hh e gn ) + h logl. now assume l hkn k so that x n<nn f n l + h x n<nn sh,l+ h logl where sh,l := x l max hh x hh e gn olivier bordells and set n,k := k! f . from weyls bound , we get for any > sh,l x l max hh hk + hkk+ x hk!hk min h, hn,k k x l hk + hkk+ x hk!hk min h, hn,k !k hk logl + hkk+ x l x hk!hk min h, hn,k !k and hlders inequality applied with exponent k k yields sh,l hk logl + hkk+ x l !k x l x hk!hk min h, hn,k !k hk logl + hkk+k x mk!lhk min h, mn,k x |m l m/k!hk k hk logl + hkk+k k!l x j= x j hk<mhk min h, mn,k x |m l m/k!hk k . following , notice that, in the innersum, we have k!hk m < k! j j so that sh,l hk + hkk x mhk min h, mn,k m !k +hkk k!l x j= j x j hk<mhk min h, mn,k !k and lemma and the crude bounds m loglogm and m imply that sh,l hk + hkk hkn,k + hk + n,k k hk n,k + hk + hkkk n,k . inserting in and using n,k k, we get x n<nn f n l + h x n<nn hk n,k + hk + hkkk n,k + h logl n l + nk k + nhk + nhkkk k + h logl. considering , the choice of l = hkn k gives x n<nn f hk+k + hn k nk k + nhk + nhkkk k + h logn. the asserted result follows by choosing h = n k k /dk , the extra term h logn being ab- sorbed by the term n + n kk /dk since n kk > note that this latter hypothesis also ensures that < l < n k+ and h < n, completing the proof. sums of certain fractional parts",
        "Subsections": [],
        "Groundtruth": "The text discusses a proof of a theorem involving various mathematical calculations and applications of lemmas. It involves manipulating equations and applying bounds to prove a certain result. The proof concludes by choosing specific values for variables to satisfy the conditions of the theorem."
    },
    {
        "Section_Num": "4",
        "Section": "4. Extension to integer points close to smooth curves",
        "Text": "in this section, let , , n z large, f : r be any function, and dene r := card n z : f < . since it is known that, for any integer l + , we have r n l + l l x = x nnn e f the proof of theorem may easily be adapted in a similar way to get a bound for r of the same kind. theorem let , , k,n z, f ck+ such that there exist k,k+,sk > and ck,ck+ such that, for any x and any j {k,k +} j f cj j with k = sknk+ dene dk := k+k. then, for any > r k, n+ n nk k + n k k k n kk kdk + n n kk dk ! . this must be compared to the existing results of the theory. for instance, under the hypothesis f , it is proved in that r n/ + n+ / + in the cases k = or k = , it is known from that r f ,n, n/ + n/ + n / + / + if f j for j {,} such that = n, and r f ,n, n/ + n/ + n / + / + if f j for j {,} such that = n references o. bordells,arithmetic tales, springer, universitext, o. bordells, f. luca and i. shparlinski, on the error term of a lattice counting problem, j. number theory , - o. bordells,on a lattice counting problem, ii, preprint, , https://arxiv.org/abs/. m. branton & p. sargos, points entiers au voisinage dune courbe trs faible courbure, bull. sci. math. , o. m. fomenko, on the distribution of fractional parts of polynomials, j. math. sci. , s. w. graham and g. kolesnik,van der corputs method of exponential sums, cambridge univ. press, d. r. heath-brown,a new k-th derivative estimate for exponential sums via vinogradovs mean value, tr. mat. inst. steklova , m. n. huxley & p. sargos, points entiers au voisinage dune courbe plane de classe cn, ii, functiones et approximatio , h. l. montgomery, ten lectures on the interface between analytic number theory and harmonic analysis, ams, cbms , v. n. popov, on the number of integral points under a parabola, mat. zametki , o. robert, on van der corputs k-th derivative test for exponential sums, indag. math. , t.d. wooley, the cubic case of the main conjecture in vinogradovs mean value theorem, adv. math. , value theorem alle de la combe, aiguilhe, france e-mail address: borde@wanadoo.fr",
        "Subsections": [],
        "Groundtruth": "The section extends the analysis to integer points near smooth curves by introducing a function f and defining r as the cardinality of integer points satisfying a certain condition. A theorem is presented that provides a bound for r based on specific parameters and functions. The results are compared to existing theories and references are provided for further study."
    },
    {
        "Section_Num": "References",
        "Section": "References",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "Abstract",
        "Section": "Abstract",
        "Text": "disparity-preserved deep cross-platform association for cross-platform video recommendation shengze yu , xin wang, wenwu zhu, peng cui and jingdong wang department of computer science and technology, tsinghua university, china microsoft research ysz@com, {xin wang, wwzhu, cuip}@tsinghua.edu.cn, jingdw@microsoft.com abstract cross-platform recommendation aims to improve recommendation accuracy through associating in- formation from different platforms. existing cross- platform recommendation approaches assume all cross-platform information to be consistent with each other and can be aligned. however, there re- main two unsolved challenges: i) there exist in- consistencies in cross-platform association due to platform-specic disparity, and ii) data from dis- tinct platforms may have different semantic granu- larities. in this paper, we propose a cross-platform association model for cross-platform video recom- mendation, i.e., disparity-preserved deep cross- platform association , taking platform- specic disparity and granularity difference into consideration. the proposed dca model em- ploys a partially-connected multi-modal autoen- coder, which is capable of explicitly capturing platform-specic information, as well as utiliz- ing nonlinear mapping functions to handle gran- ularity differences. we then present a cross- platform video recommendation approach based on the proposed dca model. extensive experiments for our cross-platform recommendation framework on real-world dataset demonstrate that the pro- posed dca model signicantly outperform ex- isting cross-platform recommendation methods in terms of various evaluation metrics.",
        "Subsections": [],
        "Groundtruth": "The text proposes a novel cross-platform association model for cross-platform video recommendations, called the Disparity-Preserved Deep Cross-Platform Association (DCA) model. This model addresses challenges in existing cross-platform recommendation approaches by considering platform-specific disparity and granularity differences. The DCA model utilizes a partially connected multi-modal autoencoder to capture platform-specific information and handle granularity differences through nonlinear mapping functions. Experimental results show that the DCA model outperforms existing methods in terms of various evaluation metrics on real-world datasets."
    },
    {
        "Section_Num": "1",
        "Section": "1 Introduction",
        "Text": "recommender systems are playing an important role in our life. with the emergence of various online services, peo- ple are now getting used to engaging on different platforms simultaneously in order to meet their increasing diverse in- formation needs . the complementary information from various platforms jointly reects user inter- ests and preferences, providing us with a great opportunity to tackle the data sparsity problem and improve the recom- mendation accuracy through associating information across corresponding author platforms. in order to improve user experience and in- crease user adoption, many online service providers release new features to encourage cross-platform data associations. for example, google+ encourages users to share their home- pages on other platforms, enabling different accounts of the same person to be linked together. to date, quite a number of various platforms tend to associate with each other, which provides chances for cross-platform information transfer and association analysis possible. existing cross-platform recommendation works try to im- prove their cross-platform performances mainly by investi- gating the ablity of cross-platform association. yan et al. propose a user-centric topic association framework to map cross-platform data in a common latent space . they then introduce a predened micro-level metric to adaptively weight data while doing the data integration . man et al. propose an embedding and mapping framework, em- cdr, to represent and associate users across different plat- forms . however, existing works on cross-platform video recommendation ignore the inconsistencies in cross- platform association and differences in semantic granulari- ties, two challenging phenomena discovered in this paper. the rst challenge, inconsistency in cross-platform associ- ation, results in a reconsideration for the current assumption that all cross-platform information of the same user is consis- tent and can be aligned, which will be empirically validated on a real-world cross-platform dataset later. our discovery demonstrates that the inconsistency is mainly caused by the platform-specic disparity, i.e., in addition to a users inher- ent personal preferences, its interests shown on different plat- forms also contain platform-specic factors due to different focuses of various platforms. the second challenge is that different platforms may have different semantic granularities. take twitter and youtube as an example, tags on twitter are mainly hot event topics, which are generally ner-grained than video categories on youtube. as such, a good cross- platform video recommendation model based on the corre- lated information across platforms should have the ability to reveal characteristics at different semantic granularities. to address these challenges, we propose the disparity- preserved deep cross-platform association model for cross-platform recommendation, which employs a partially- connected multi-modal autoencoder to explicitly capture and preserve platform-specic disparities in latent representa- arxiv:v nov tion. besides, we also introduce nonlinear mapping function, which is superior to their linear competitors in terms of han- dling granularity difference. to validate the advantages of our proposed dca model, we further design a cross-platform video recommendation approach based on the proposed dca model, which is capable of automatically targeting and trans- ferring useful cross-platform information from a comprehen- sive semantic level. the contributions of this paper can be summarized as follows: we recognize the inconsistency phenomenon, which is caused by platform-specic disparity, as well as the granularity difference problem in cross-platform associ- ation. we propose a novel disparity-preserved deep cross- platform association model for cross-platform recommendation through employing a modied multi- modal autoencoder, which is able to handle the cross- platform inconsistency issue by preserving the platform- specic disparities and solve the granularity difference problem via nonlinear functions. we present a cross-platform video recommendation ap- proach based on the proposed dca model to test its ef- fectiveness compared with other approaches. we conduct experiments on a real-world cross-platform dataset on twitter and youtube to demonstrate the supe- riority of our proposed dca model over several state-of- the-art methods in terms of various evaluation metrics.",
        "Subsections": [],
        "Groundtruth": "Recommender systems are crucial in today's diverse online landscape, where users engage on multiple platforms simultaneously. Cross-platform data associations are key to enhancing recommendation accuracy and user experience. Existing works focus on improving cross-platform performances by investigating the ability of cross-platform association. Challenges such as inconsistencies in cross-platform association and differences in semantic granularities exist. A novel Disparity-Preserved Deep Cross-Platform Association (DCA) model is proposed to address these challenges, utilizing a modified multi-modal autoencoder and nonlinear mapping functions. The DCA model is tested on a cross-platform video recommendation approach, showcasing superior performance over existing methods on a real-world cross-platform dataset involving Twitter and YouTube."
    },
    {
        "Section_Num": "2",
        "Section": "2 Related Work",
        "Text": "existing cross-platform recommendation works can be cate- gorized in two ways. categorization by what to associate. one option is to take advantage of different platform characteristics towards col- laborative applications. particularly, qian et al. propose a generic cross-domain collaborative learning framework based on nonparametric bayesian dictionary learning model for cross-domain data analysis . min et al. propose a cross-platform multi-modal topic model which is capable of differentiating topics and aligning modalities . an al- ternative option is to associate in a user-centric way, which focuses on integrating multiple sources of overlapped users activities. in particaular, xptrans opti- mally bridges different platforms through exploiting the in- formation from a small number of overlapped crowds. yan et al. propose an overlapped user-centric topic association framework based on latent attribute sparse coding, and prove that bridging information from different platforms in com- mon latent space outperforms explicit matrix-oriented trans- fer . man et al. propose an embedding and mapping framework, emcdr, where user representations on differ- ent platforms are rst learnt through matrix factorization and then mapped via multi-layer perceptron . although the user-centric works above are based on different premises, they share the same core idea that all cross-platform infor- mation is consistent and should be aligned. however, several works have pointed out the data inconsistency phenomenon in cross-platform data association, and attempted to solve this user groups clustered on twitter user groups clustered on youtube figure : visualization of users on twitter and youtube in the casia-crossosn subset. orange and purple dots represent two groups of aligned users across platforms. concentrated areas are circled with dashed lines. problem by data selection. to be concrete, lu et al. nd that selecting consistent auxiliary data is important for cross- domain collaborative ltering . they propose a novel criterion to assesses the degree of consistency, and embed it into a boosting framework to selectively transfer knowledge. yan et al. divide users into three groups and introduce a pre- dened micro-level user-specic metric to adaptively weight data while integrating information across platforms . our proposed dca model associates information from dif- ferent platforms in the user-centric way. categorization by entire model structure. one group of methods [chen et al., a; jiang et al., ; jing et al., ; qian et al., ] build the model in a unied frame- work, where the former two works adopt matrix factoriza- tion and the latter two employ the probabilistic models. an- other group of works [yan et al., ; yan et al., ; man et al., ] adopt a two-step framework. these works rst map users from different platforms into corresponding la- tent spaces for representations, and then associate these rep- resentations. our proposed model in this work utilizes the two-step structure in a topic-based way.",
        "Subsections": [],
        "Groundtruth": "Existing cross-platform recommendation works can be categorized based on what to associate. One approach focuses on leveraging platform characteristics for collaborative applications, with works like Qian et al. proposing a cross-domain collaborative learning framework and Min et al. introducing a multi-modal topic model. Another approach is user-centric, integrating overlapped user activities from multiple sources, such as XPTrans and Yan et al.'s user-centric topic association framework. Despite the focus on consistency, some works have noted data inconsistency in cross-platform associations and address this issue through data selection methods. The proposed DCA model in this work aligns cross-platform information in a user-centric manner. Methodologies in this field can also be categorized by entire model structure, with some adopting a unified framework and others utilizing a two-step approach. The model proposed in this work follows a two-step structure in a topic-based manner."
    },
    {
        "Section_Num": "3",
        "Section": "3 Measurement and Observation",
        "Text": "to validate the existence of the inconsistency, we rst vi- sualize users in casia-crossosn, a cross-platform dataset linking user accounts between youtube and twitter (whose detailed description will be given in section ), following three steps as follows. ) for users who engage in both twitter and youtube, we extract their interest representa- tions by latent dirichlet allocation . ) we adopt the interest representations as inputs to the visualization tool t- sne so that users are represented as two-dimensional vectors and visualized as dots the platform average euclidean distance from center of user groups random clustered on twitter clustered on youtube twitter youtube table : average euclidean distance from the center of user groups clustered on twitter vs. random. user groups clus- tered on twitter no longer concentrate on youtube , indicating cross-platform user interest disparity. closer two dots are in distance, the more similar their cor- responding users are in topic space. ) we cluster users into different groups by their twitter interest rep- resentations, select two groups, and mark them with orange and purple respectively in figure such that users who share a same color are regarded as similar. we observe from figure that only a part of users who are similar on twitter (resp. youtube) still stay similar on youtube and the rest actually becomes quite different. moreover, in fig- ure , orange user group clustered on youtube splits into two subset on twitter, indicating that two platforms may have different semantic granularities. in addition to visualizing the interest representations, ta- ble further conrms the existence of the inconsistency phenomenon through a measurement study on casia- crossosn. i) for each clustered user group on twitter and youtube, we calculate the average euclidean distance from the group center to group members. ii) we do the same thing for randomly sampled groups on both platforms and use the calculated average euclidean distances as normalizers to ob- tain the concentration ratio such that a larger concentration ratio indicates a less clustered pattern. it can be observed from table that when we cluster users by their interest representations on twitter , the concentration ratio is small on the origin platform while becomes quite large on the other platform, in- dicating that users having similar interests on one platform tend to behave quite similarly to randomly sampled users on the other they no longer share similar interests. we close this section by giving a conclusion on the exis- tence of inconsistency in cross-platform association: users interests on different platforms may be diverse and inconsis- tent. dca: disparity-preserved deep cross-platform association in this section, we employ multi-modal autoencoder to present our disparity-preserved deep cross-platform associ- ation model. problem formulation we rst introduce the problem of user-centric cross-platform association. given a set of aligned users u, for each user u u, his or her representations on different platforms (ut on twitter and uy on youtube in the experiment) are known. the goal is to nd the association among these representa- tions (direct mapping function or unied latent representa- tion), so that cross-platform applications can be carried out (specically inferring uy for video recommendation in the experiment). disparity-preserved deep cross-platform association multi-modal autoencoder is an extension to basic autoen- coder. it is trained to reconstruct multiple inputs from a uni- ed hidden layer when given multiple modalities of the same entity, thus is able to discover correlations across modali- ties. it has been widely applied to various multi-modal tasks since being proposed [ngiam et al., ; zhang et al., ; wang et al., ]. hong et al. extend the concept of multi- modal to multi-source . in this paper, we treat rep- resentations of the same user on different platforms as multi- ple modalities of his or her unied latent representation, and modify the structure of multi-modal autoencoder to conduct disparity-preserved cross-platform association. the detailed structure of multi-modal autoencoder is formulated as: h = g \" x i wi xi ! + b # , xi = g \u0010 wi h + bi \u0011 , where i {t, y } denotes different platforms (e.g., twitter and youtube). xi is the multi-modal input layer, specif- ically the representations ui of the user u on different plat- forms. h is the hidden layer, i.e., the derived unied user la- tent representation. xi is the output layer, i.e., the reconstruc- tions of inputs. weight matrices w and bias units b serve as parameters of the multi-modal autoencoder, denoted as together. g is the activation function. the loss function of multi-modal autoencoder is dened as follows: l = x i xi xi + x w w f + h, where the rst term is reconstruction error. the second term is regularizer to avoid overtting. the third term is a spar- sity constraint to encourage sparse latent representations. we minimize the loss function and train the parameters through backpropagation. multi-modal autoencoder takes representations of the same user on different platforms as inputs, mapping them to a uni- ed hidden layer h and expecting accurate reconstructions. it learns mapping functions from representations on different platforms to the unied representation and then vice versa, which automatically completes the process of cross-platform association. moreover, it is able to handle the granularity dif- ference through introduction of nonlinearity. we note that all multi-modal inputs should be given for the multi-modal autoencoder to work properly. however, in many cross-platform applications, we need to infer unknown data on other platforms, which is extremely difcult. one straightforward solution is to train several networks with a certain combination of several platforms as inputs and output the results for all platforms, setting every decoding weight figure : disparity-preserved deep cross-platform association model. ut and uy are representations of a same user on twitter and youtube respectively. in latent representations, ht and hy are platform-specic parts preserving the disparities, and hc is the common part associating different platforms. the estimated repre- sentations ut and uy are derived from both common and platform- specic parts. equally. however, such an approach does not scale well as we will need an exponential number of models. ngiam et al. propose to train bi-modal autoencoder using augmented but noisy data with additional examples that have only one single modality as inputs . in this paper, we augment data with average values of different platforms, and still ex- pect the network to reconstruct real representations for all modalities. to make the model become more robust to absent inputs and being capable of inferring unknown data in cross- platform association task, we split the training data as fol- lows: one-third of the training data includes user representa- tions on twitter and average user representation on youtube, one-third consists of average user representation on twitter and user representations on youtube, the remaining one-third contains groundtruth representations. as aforementioned, platform-specic disparity may cause inconsistencies in cross-platform association. we need to en- visage it and avoid messing up the association. to achieve this goal, we divide the hidden layer into three parts and cut off certain links, through which the shared and platform- specic parts are explicitly captured and preserved. specif- ically, the hidden layer is divided into h = , where ht and hy are twitter and youtube platform-specic parts and hc is the common part. as is shown in figure , we assume that twitter representations are derived from ht and hc without hy while youtube representations are derived from hy and hc without ht . the fully-connected structure is modied through cutting off links between units: from xt to hy , from xy to ht , from ht to xy and from hy to xt . the model is able to automatically map representations of the same user on different platforms to a unied representation while preserving the information in platform-specic parts at the same time. it does not require cross-platform data to be thoroughly consistent by allowing the existences of inconsis- tencies. thanks to the usage of shared common structure, cross-platform association becomes clearer and tighter. in addition, most traditional linear methods associate rep- resentations on different platforms by utilizing a linear trans- fer matrix, which is formulated in eq. and eq. . the linear combination limits the power in modeling complex re- lations across different semantic granularities. our modied multi-modal autoencoder naturally introduces nonlinear acti- vation function g, possessing great advantages in handling the challenging granularity differences over linear methods. we choose sigmoid as the activation function in this paper and other activation functions such as relu or tanh can also be used. the time complexity is o) for training and o) for testing. m is the number of training iterations, n is the number of users for training, k is the hid- den layer dimension, n and n are the input dimensions of two platforms. our proposed dca model, whose structure is presented in figure , automatically captures common infor- mation and preserves platform-specic disparities through its partially-connected structure, and meanwhile naturally adapts to granularity difference via nonlinear functions.",
        "Subsections": [],
        "Groundtruth": "The text discusses a validation study on cross-platform user interests and the existence of inconsistencies between Twitter and YouTube. Through visualization and measurement using a dataset called Casia-Crossosn, it is confirmed that user interests on different platforms may be diverse and inconsistent. To address this, a Disparity-Preserved Deep Cross-Platform Association model is proposed using a multi-modal autoencoder. This model is designed to map user representations from different platforms to a unified hidden layer, preserving platform-specific differences and handling granularity variations through nonlinearity. The model aims to improve cross-platform association by capturing common information while accommodating platform-specific disparities. The proposed model offers advantages over traditional linear methods in handling complex relations and granularity differences across platforms."
    },
    {
        "Section_Num": "4",
        "Section": "4 DCA: Disparity-preserved Deep Cross-platform Association",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "5",
        "Section": "5 Cross-platform Video Recommendation",
        "Text": "in this section, we present the cross-platform video recom- mendation approach based on the proposed dca model.",
        "Subsections": [
            {
                "Section_Num": "5_1",
                "Section": "5.1 Problem Formulation",
                "Text": "we rst formulate the cross-platform video recommendation problem. given a set of aligned users u, for each user u u, his or her behaviors on social platform are observed (i.e., tweet text t in collection tu on twitter). the goal of cross- platform video recommendation is to recommend videos for those aligned users on video platform through making use of the cross-platform information. dca-based cross-platform video recommendation as is shown in figure , our proposed cross-platform video recommendation method includes preprocessing and match- ing stages in addition to the dca model. the capability of dca model in tackling the cross-platform inconsistency is- sue through explicitly capturing shared information and pre- serving platform disparities enables the cross-platform rec- ommendation method to automatically concentrate on use- ful information and discard useless one. moreover, nonlin- ear mapping functions in our proposed approach enhance the methods ability to conduct association across platforms at a deeper and more comprehensive semantic level. in order to apply our proposed dca model for the asso- ciations between twitter and youtube, we rst obtain user representations by latent dirichlet allocation [blei et al., ]. on twitter, we regard each users tweeting data together as a document in latent dirichlet allocation . then each user u u can be represented by a topic distribu- tion ut . on youtube, as we focus on the relevance between videos and users in the recommendation problem, we extract representations for both users and videos by two steps: treat the surrounding textual information (title, category, tags and description) of each video together as a document and to get video representation v through online lda [hoffman et al., ], take the average v of the interacted videos con- sumed by each user to get uy . we remark that v and uy are in the same youtube topic space, which enables similarity matching for recommendation. figure : cross-platform video recommendation framework based on the proposed disparity-preserved deep cross-platform associa- tion model then the representation pairs of the same user as well as the augmented examples are fed to the dca model to learn cross-platform association. because of the input augment- ing, the trained model is able to predict user representations on youtube when only information from twitter is available. by setting the input from youtube to its average value, the corresponding output can be used to predict user representa- tions uy . we nally adopt euclidean similarities between the repre- sentations of the predicted user and candidate videos as the properness measure. the recommendation results can be ob- tained according to the rank of candidate videos.",
                "Subsections": [],
                "Groundtruth": "The section discusses the formulation of the cross-platform video recommendation problem, aiming to recommend videos on a video platform for aligned users based on their behaviors on social media platforms like Twitter. The proposed method includes preprocessing, matching stages, and a DCA model to address cross-platform inconsistencies by capturing shared information and preserving platform disparities. User representations are obtained using Latent Dirichlet Allocation for Twitter and YouTube, enabling similarity matching for recommendation. The DCA model learns cross-platform associations, allowing prediction of user representations on YouTube using only Twitter information. Recommendations are made based on Euclidean similarities between user representations and candidate videos."
            }
        ],
        "Groundtruth": "The text presents a cross-platform video recommendation approach utilizing the proposed dca model."
    },
    {
        "Section_Num": "6",
        "Section": "6 Experiments",
        "Text": "in this section, we carry out extensive experiments on a real-world cross-platform dataset and compare our proposed method with several state-of-the-art algorithms to show the advantages of the proposed approach.",
        "Subsections": [
            {
                "Section_Num": "6_1",
                "Section": "6.1 Dataset",
                "Text": "casia-crossosn is a cross-network user dataset with ac- count linkages between youtube and twitter, created by in- stitute of automation, chinese academy of sciences. it con- tains , aligned users across platforms and ,, youtube videos. on youtube, three kinds of user behav- iors as well as videos rich metadata are collected. on twitter, only users representations extracted from their tweets via stan- dard topic modeling process are provided because of privacy concerns. we lter aligned users and youtube videos by keeping users who interacted with at least videos and videos which were consumed by at least users. we nally obtain a subset containing , users and , videos, and randomly select % users for training, and the rest % for testing. casia-crossosn dataset: http://www.nlpr.ia.ac.cn/mmc/ homepage/myan/dataset.html",
                "Subsections": [],
                "Groundtruth": "The casia-crossosn dataset is a cross-network user dataset linking YouTube and Twitter accounts. It includes aligned users and YouTube videos, with user behaviors and rich video metadata collected on YouTube. Twitter data is limited to user representations extracted from tweets due to privacy concerns. The dataset contains 9,200 users and 30,000 videos, with a subset of 6,000 users and 20,000 videos selected for training and testing. Access the dataset at http://www.nlpr.ia.ac.cn/mmc/homepage/myan/dataset.html."
            },
            {
                "Section_Num": "6_2",
                "Section": "6.2 Experimental Settings",
                "Text": "on twitter, casia-crossosn offers a -dimensional top- ical distribution for each user. on youtube, we resort to perplexity to determine the number of topics. we select , which leads to the smallest perplexity on a % held-out youtube video subset, in our experiments. to evaluate the effectiveness of the proposed dca model, we implement the following baselines: linear regression-based association lr treats cross-platform association as a linear transfer prob- lem, and pursues an explicit transfer matrix based on regres- sion. the objective function is: min w wut uy f + w, latent attribute-based association instead of pursuing hard transfer, yan et al. introduce another method by discovering the shared latent structure behind two topic spaces , whose objective function is: min dt ,dt ,s ut dt s f + uy dy s f + s, s.t. dy i , dt j , i, j, the problem can be efciently solved by sparse coding al- gorithm after a few transformations. mlp-based nonlinear mapping man et al. employ mlp to cope with the latent space match- ing problem . mlp is a exible nonlinear transforma- tion. the optimization problem can be formulated as: min x uu fmlp uy , to train the above neural network-based model and our dca model, we adopt adam optimizer to minimize the loss functions. evaluation of cross-platform association as for the proposed model in this paper, multi-modal autoen- coder with l-norm but without disparity-preserved structure and the dca model are examined. we perform evaluations in two scenarios: infer youtube from twitter. given users with their twitter rep- resentation ut , to estimate their youtube representation uy . infer twitter from youtube in turn. as is discussed be- fore, by setting the unknown input to zeros, we can get the corresponding output as the predicted representation u. in both scenarios, we utilize mean absolute error and root mean square error as evaluation metric: maei = |utest| x utest ki ui ui , rmsei = |utest| x utest r ki ui ui f where i {t, y } denotes platform, and ki is the dimension of representation. ui and ui are the real and predicted user representation. figure : maet , maey and rmset , rmsey of the examined models in two testing scenarios. the results show that both dispar- ity preserving and nonlinearity contribute to better cross-platform association performance. for model parameters, in lr model as eq. , is selected by line search. in la model as eq. , and the number of latent attributes m are selected by two-dimensional grid search. in the dca model as eq. , parameters consist of , and m, which further divides into . we rst tune the ma model (equivalent to set mt = my = , m = mc in the dca model) by multi-dimensional grid search, and get an optimal setting for , and m. we then carefully ne-tune by a combined line-search strategy. specically for all randomly initialized autoencoder model, we conduct experiments and take the average result. performances are shown in figure . several observa- tions can be made: la and mlp achieve better result than lr, indicating that the shared latent structure and non- linear mapping function both benet the association. ma outperforms la and mlp through combining the two strate- gies. compared with ma, dca further improves the performance, demonstrating the contribution of the disparity- preserved structure. we also notice that compared with non- linear models , the optimal parameters of the linear models vary greatly between two scenarios. we think it is the granularity differences that cause the param- eter gaps. non-linear mappings can naturally tackle this is- sue, and have more advantages over linear ones. in total, the proposed dca model outperforms la by reducing rel- atively % on maey , % on rmsey , % on maet and % on rmset , and outperforms mlp by reduc- ing relatively % on maey , % on rmsey , % on maet and % on rmset . it greatly improves the perfor- mance by preserving the disparities and introducing nonlin- earity, validating that envisaging the inconsistency caused by platform-specic disparity and granularity difference jointly contributes to better cross-platform association performance. evaluation of cross-platform video recommendation then we evaluate the proposed dca-based cross-platform video recommendation method for new youtube users, fol- lowing the above infer youtube from twitter scenario. for each test user u, we randomly select as many as groundtruth interacted videos as additional candidates. we perform a top-k recommendation task: to recommend the top k youtube videos with the highest topic-based euclidean similarity, and adopt top-k precision, recall and f-score as figure : precision@k vs. recall@k, k from to of examined methods. model precision recall f-score lr la mlp ma dca table : top- precision, recall and f-score of examined methods. the results show that the proposed dca-based cross-platform video recommendation method outperforms other approaches. evaluation metrics . the evalua- tion metrics are calculated by examining whether the recom- mended videos are included in us groundtruth video set vu. final results are averaged over all test users. as is shown in figure and table , the proposed dca- based cross-platform video recommendation method outper- forms la-based method by enhancing relatively % on f-score@, and outperforms mlp-based method by en- hancing relatively % on f-score@ the proposed dca-based method improves over existing cross-platform association-based recommendation approaches.",
                "Subsections": [],
                "Groundtruth": "The experimental settings encompass various approaches to evaluating cross-platform association models. Baselines include linear regression, latent attribute-based association, and MLP-based nonlinear mapping. The models are trained using an Adam optimizer. Evaluation metrics such as mean absolute error and root mean square error are utilized to assess model performance. Results indicate that disparity preservation and nonlinearity enhance cross-platform association. The proposed DCA model shows significant improvements over other methods, showcasing the benefits of addressing platform-specific disparities and granularity differences. Additionally, the DCA-based cross-platform video recommendation method outperforms other approaches in terms of precision, recall, and F-score."
            }
        ],
        "Groundtruth": "The section presents the results of experiments conducted on a real-world cross-platform dataset. The proposed method is compared with various state-of-the-art algorithms, demonstrating the advantages of the proposed approach."
    },
    {
        "Section_Num": "7",
        "Section": "7 Conclusion",
        "Text": "in this paper, we discover the existence of inconsistency in cross-platform recommendation. we propose the dca model, which tackles the inconsistency issue, as well as the granularity difference problem. we further present a cross- platform video recommendation method based on the pro- posed dca model. extensive experiments demonstrate the superiority of the dca model and the dca-based cross- platform recommendation approach over several state-of-the- art methods. acknowledgments this research is supported by national program on key ba- sic research project no. cb, national natural science foundation of china major project no. u, china postdoctoral science foundation no. bx and shenzhen nanshan district ling-hang team grant un- der no.lhtd references david m blei, andrew y ng, and michael i jordan. latent dirichlet allocation. journal of machine learning research, :, leihui chen, jianbing zheng, ming gao, aoying zhou, wei zeng, and hui chen. tl- rec:transfer learning for cross-domain recommendation. computer science, pages , terence chen, mohamed ali kaafar, arik friedman, and roksana boreli. is more always mer- rier?: a deep dive into online social footprints. in proceed- ings of the acm workshop on workshop on online social networks, pages acm, jonathan l herlocker, joseph a konstan, loren g terveen, and john t riedl. evaluating collaborative ltering recommender systems. acm trans- actions on information systems , :, matthew hoffman, francis r bach, and david m blei. online learning for latent dirichlet al- location. in advances in neural information processing systems, pages , chaoqun hong, jun yu, jian wan, dacheng tao, and meng wang. multimodal deep autoen- coder for human pose recovery. ieee transactions on im- age processing, :, meng jiang, peng cui, nicholas jing yuan, xing xie, and shiqiang yang. little is much: bridg- ing cross-platform behaviors through overlapped crowds. in aaai, pages , how jing, an-chun liang, shou-de lin, and yu tsao. a transfer probabilistic collective factoriza- tion model to handle sparse data in collaborative ltering. in data mining , ieee international confer- ence on, pages ieee, honglak lee, alexis battle, rajat raina, and andrew y ng. efcient sparse coding algorithms. in advances in neural information processing systems, pages , zhongqi lu, erheng zhong, lili zhao, evan wei xiang, weike pan, and qiang yang. selective transfer learning for cross domain recommendation. in proceedings of the siam international conference on data mining, pages siam, laurens van der maaten and ge- offrey hinton. visualizing data using t-sne. journal of machine learning research, :, tong man, huawei shen, xiaolong jin, and xueqi cheng. cross-domain recommendation: an embedding and mapping approach. in twenty-sixth inter- national joint conference on articial intelligence, pages , weiqing min, bing kun bao, changsheng xu, and m. shamim hossain. cross-platform multi-modal topic modeling for personalized inter-platform recommen- dation. ieee transactions on multimedia, : , jiquan ngiam, aditya khosla, mingyu kim, juhan nam, honglak lee, and andrew y ng. mul- timodal deep learning. in proceedings of the th interna- tional conference on machine learning , pages , shengsheng qian, tianzhu zhang, richang hong, and changsheng xu. cross-domain col- laborative learning in social multimedia. in proceedings of the rd acm international conference on multimedia, pages acm, daixin wang, peng cui, mingdong ou, and wenwu zhu. deep multimodal hashing with orthogo- nal regularization. in ijcai, pages , ming yan, jitao sang, and changsheng xu. mining cross-network association for youtube video promotion. in proceedings of the nd acm international conference on multimedia, pages acm, ming yan, jitao sang, and changsheng xu. unied youtube video recommendation via cross- network collaboration. in proceedings of the th acm on international conference on multimedia retrieval, pages acm, hanwang zhang, yang yang, huanbo luan, shuicheng yang, and tat-seng chua. start from scratch: towards automatically identifying, modeling, and naming visual attributes. in proceedings of the nd acm international conference on multimedia, pages acm,",
        "Subsections": [],
        "Groundtruth": "The paper discusses the discovery of inconsistency in cross-platform recommendation and proposes the DCA model to address this issue as well as the granularity difference problem. A method for cross-platform video recommendation based on the DCA model is presented. Extensive experiments show the superiority of the DCA model and the DCA-based recommendation approach over other state-of-the-art methods. The research is supported by various funding sources. Additionally, references to related works in the field of recommendation systems are provided at the end of the paper."
    }
]