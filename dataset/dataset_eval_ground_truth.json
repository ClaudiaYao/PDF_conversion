[
    {
        "Section_Num": "I",
        "Section": "I Introduction",
        "Text": "due to its simplicity and exibility in handling a diver- se set of conguration spaces without requiring an expli- cit representation, sampling based motion planning is the mainstream approach to global motion planning for high- dimensional, highly nonlinear robotic systems, such as robot manipulators . however, the performance of such ran- domized motion planners strongly depends on the choice of distance measure, sampling method, and local steering; and is known to degrade signicantly around complicated regions of conguration spaces, such as narrow passages , . this performance degrade is usually considered as an issue of sampling, because uniform sampling has a voronoi bias towards yet unexplored larger regions of conguration spaces; and accordingly many heuristic rejection sampling approaches and retraction methods are suggested to mitigate this issue, but retraction methods often require a distance- to collision measure , . on the contrary, assuming that this performance decay is due to the lack of effective local steering, in a geometric local steering policy that can feel the local geometry of conguration spaces is proposed for efcient planning around narrow passages; these authors contributed equally to this work. j. huh is with the general robotics, automation, sensing, and perception (grasp) laboratory, university of pennsylvania, philadelphia, pa e mail: jinwookh@seas.upenn.edu om ur arslan is with the autonomous motion department at max planck institute for intelligent systems, t ubingen, germany. e mail: omur.arslan@tuebingen.mpg.de daniel d. lee is at cornell tech, new york, ny e mail: ddl@cornell.edu fig. : (left) probabilistically safe corridor in d space constructed around a sample conguration (red) by using tangent hyperplanes (gray) of condence ellipsoids of a learned gaussian mixture model of conguration space obstacles. (right) local steering via probabilistically safe corridor in d space: an rrt is extended along the safe direction (red dotted line) towards the projection of a sample goal (red) onto the associated probabilistically safe corridor (red polygon), instead of the standard straight line extension (blue dotted line) towards the sample goal. however, its computation also requires a distance to collision measure. since the exact computation of distance to collision in complex high dimensional conguration spaces is hard , gaussian mixture learning and locally weighted regression are applied to construct approximate proba- bilistic models of collision and collision free subspaces of conguration spaces for fast collision checking and biased sampling over free space and difcult regions of congurati- on spaces. in particular, simultaneous modeling of collision and free subspaces is shown to be critical for local planning around narrow passages . in this paper, by combining the strengths of and , we introduce a new notion of probabilistically safe corridors for probabilistically safe guided local steering for sampling based planning without requiring an explicit computation of distance to collision. more precisely, we construct a probabilistically safe cor- ridor around a conguration using tangent hyperplanes of condence regions of learned gaussian mixtures that sepa- rate the input conguration from the condence ellipsoids, as illustrated in fig. (left). accordingly, we propose a pro- babilistically safe local steering primitive towards a sample goal conguration via its projection onto the probabilistically safe corridor, as shown in fig. (right). since the proposed steering method exploits the local geometry of congura- tion spaces via learned gaussian mixture models (gmms) and generates steering motion within probabilistically safe corridors, in our numerical simulation and experiments, we observe that it yields a better exploration of conguration spaces while minimizing collision likelihood. in summary, the main contributions of the paper include: i) a novel geometric approximation of conguration space obstacles by condence ellipsoids of learned gmms, arxiv:v jan ii) a new construction of probabilistically safe corridors using tangent hyperplanes of condence ellipsoids, iii) an effective probabilistically safe local steering primitive that can minimize collision likelihood. using numerical simulations and real experiments, we de- monstrate that the proposed probabilistically safe local stee- ring approach can dramatically improve the performance of randomized motion planners around narrow passages and signicantly outperforms the straight line local planner in high dimensional conguration spaces by decreasing the number of collisions. ",
        "Subsections": [],
        "Groundtruth": "Sampling-based motion planning is widely used in global motion planning for complex robotic systems. However, the performance of random planners can degrade in challenging areas of configuration space. This degradation is often attributed to issues with sampling or lack of effective local steering. A new approach is proposed, using probabilistically safe corridors and geometric approximations of configuration space obstacles. This method improves planning around narrow passages, minimizing collision likelihood and outperforming traditional straight-line planners. Numerical simulations and experiments demonstrate the effectiveness of this approach in improving the performance of randomized motion planners."
    },
    {
        "Section_Num": "II",
        "Section": "II Related Work",
        "Text": "sampling based planning approaches suffer from heavy computational time in complex environments since they typically require a considerable number of sample congu- rations and their collision checks. therefore, several biased sampling methods , and rejection sampling methods are proposed to reduce the number of sample nodes and so to improve computational efciency. however, these approaches have many heuristic parameters and require explicit conguration space information, such as visibility or collision boundaries, which usually limits their application to low dimensional settings. another alternative approach to increase the computation efciency is to reduce the number of collision checks, using either lazy collision checking or fast probabilistic collision checks , . exact safety certicates are also utilized for minimizing the computational cost of collision checks . however, these methods are still not able to address the narrow passage problem of sampling based motion planning. in order to resolve the narrow passage problem, zhang and manocha present a steering approach that retracts sample congurations to become more likely to be connected to nearby nodes . however, it requires a signicant number of iterations to nd a new collision free conguration that is around the collision boundary, and also requires an ap- propriate distance to collision measure. in practice, since the exact distance to collision measurement in high dimensional conguration spaces is very hard, its applicability is also li- mited to low dimensional motion planning problems. moreo- ver, workspace topology is utilized in biasing conguration space exploration for planning around difcult regions , , but the topology of high dimensional conguration space (e.g., robot manipulators) is signicantly different and more complex than the corresponding workspace topology. local safe corridors recently nd signicant applications in collision free motion planning by using se- quential composition of simple local planners . such safe corridors are usually constructed based on a convex decomposition of the environment, which requires an ex- plicit representation of the environment. in , a sensory steering algorithm is proposed for sampling based motion planning that increases the connectivity of randomized mo- tion planning graphs, especially around narrow passages, by exploiting local geometry of conguration spaces via convex local safe corridors. this construction is further extended to integrate local system dynamics and local workspace geometry in kinodynamic motion planning . however, the original construction of sensory steering requires an explicit representation of conguration space obstacles or an explicit distance to collision metric, and so its direct application to high dimensional motion planning is limited. in this paper, we enhance this sensory steering algorithm to adapt it to high dimensional settings, such as robotic manipulation, by dening probabilistically safe corridors that are constructed using a learned approximate probabilistic model of a conguration space. iii. safety guided rrt via probabilistically safe corridors in this section, we rst present a brief overview of how learning of gaussian mixtures can be used for approximate probabilistic modeling of conguration spaces, and then introduce a new notion of a probabilistically safe corridor around a conguration that identies a safe neighborhood of the conguration with minimal collision risk. accordingly, we propose a practical extension of the standard rrt planner, called safety guided rrt (sg rrt), where tree extension is guided to ensure safety constraints dened by probabilistically safe corridors. a. gaussian mixture modeling of conguration spaces let c denote the conguration space of a robotic system embedded in an n dimensional euclidean space rn, and denote by f c and o c, respectively, the free subspace and the collision subspace (i.e., obstacles) of the congura- tion space c, which, by denition, satisfy f = c \\ o. in general, an explicit representation of the free space f or the collision space o in terms of simple geometric shapes is known to be very hard to obtain, especially for high- dimensional complex systems such as robotic manipulators. hence, as in , we consider approximate probabilistic representations of the free space f and the collision space o in terms of gaussian mixtures models, respectively, denoted by gm(f, f, f) and gm(o, o, o), that are constructed using collision and collision free sample congurations as described below. here, a gaussian mixture distribution gm(, , ), consisting of k n mixture components, is parametrized by a list of mixture means := (, , . . . , k) (rn)k, a list of positive denite cova- riance matrices := (, , . . . , k) (rnn)k and a list of normalized mixture weights := (, , . . . , k) (r)k, satisfying pk k= k = , and its value at a point x rn is given by gm(x; , , ) := k x k= in(x; k, k), () although other probabilistic (mixture) models can be used for approxi- mating f and o, we nd it convenient to use gaussian mixtures since their condence regions can be accurately and efciently approximated using condence regions of individual gaussians which have an ellipsoidal form. safety guided steering via probabilistically safe corridors can be integra- ted with any (sampling based) motion planning algorithm (e.g., probabilistic roadmapsprms) as a local steering primitive, especially for uncertainty- aware belief space planning, which we plan to explore in a future paper. fig. : examples of learned gaussian mixture models. ellipsoids show the condence regions associated with the condence level of = . (left) gaussian mixtures in the d workspace shown in fig. , (right) gaussian mixtures in the conguration space of a dof planar manipulator. where n(x; , ) is the multivariate gaussian distribution with mean and covariance matrix , n(x; , ):= det() exp \u0012 (x)t(x) \u0013 .() note that the numbers of mixtures, kf and ko, used for modeling the free space f and the collision space o can be different, especially the meanshift clustering algorithm used in this paper automatically determines the number of mixture components using sample congurations based on a geometric bandwidth parameter as described below. it is also important to highlight that one can simply use gm(x, f, f, f) and gm(x, o, o, o) to estimate how likely a conguration is in collision, which is leveraged in for fast collision checking and biased sampling. in addition to such demonstrated potential improvements, we shall show below that condence regions of these gaussian mixture models can be utilized for understanding the local geometry of the conguration space c and for increasing the quality of the local steering heuristic (which is the euclidean distance in our case) to better approximate the true geodesic (cost to go) metric of the conguration space c. ) learning gaussian mixtures: one can use a num- ber of expectation maximization (em) variant methods for gaussian mixture learning for modeling the free space f and the collision space o using collision and collision- free sample congurations in an ofine or online manner, as in our previous work . in this paper, we apply the meanshift clustering method with a gaussian kernel for learning gaussian mixtures using collision information of sample congurations obtained during previous attempts of a randomized motion planner, which is a convenient way of learning from past experiences and exploiting the collision history. in addition, this approach resolves the problem that general mixture modeling approaches have no explicit way of determining the required number of mixtures, because the meanshift clustering requires a kernel bandwidth b instead of the number of clusters k. the kernel bandwidth b can be set based on the desired level of spatial resolution. with the bandwidth b, we initialize the clusters and then perform a single step em update to estimate cluster statistics. we set the membership weight value as zi k = if the ith point in n samples is included in the kth cluster, and zi k = otherwise. then, the cluster statistics (mass mk, mean k, covariance matrix k, and weight k) for the kth cluster are given by mk = n x i= zi k, k = mk n x i= zi kxi, k = mk pk j= mj , k = mk n x i= zi k(xik)(xik)t, for k {, , k}. in fig. , we present some examples of constructed probabilistic models of different conguration space and workspace by the suggested approach. fig. (left) shows a probabilistic model to dene the collision space from d point clouds obtained by a depth sensor. fig. (right) shows the generated probabilistic models using collision information of samples in the conguration space of a dof planar manipulator. such probabilistic representations of conguration spaces can be utilized for collision likelihood estimation, as a computationally efcient alternative to the exact distance to collision measurement . ) condence regions of gaussian mixtures: while a gaussian mixture model gm(f, f, f) of the free space f can be used to bias sampling over the free space, in addition to its use in fast collision checking , we propose a new novel use of condence regions of a gaussian mixture model gm(o, o, o) of the collision space o for understanding the local geometry of the conguration space c, which is the main contribution of the present paper. denition : the condence region cp() of a continuous probability distribution p : rn r associated with a condence level is dened to be the super level set lp() := {x rn| p(x) } of p, for some r, over which the cumulative mass distribution of p is , i.e, cp() = lp() such that z lp() p(x)dx = . () hence, it is convenient to have lp() denote the level function of p that returns the corresponding level of p dening the condence region cp(), i.e., cp() = lp(lp()). () although condence regions of an arbitrary probability distribution cannot be expressed explicitly in terms of simple geometric shapes and so are needed to be computed numeri- cally , condence regions of gaussian distributions have an analytical ellipsoidal form. remark : for any condence level , the ellip- soidal condence region cn (() and the level function ln (,)() of the gaussian distribution n(x; , ) are, respectively, given by cn (,)() = n xrn (x)t(x) f n () o ,() ln (,)() = det() exp \u0012 f n () \u0013 , () where f n : r denotes the cumulative probability distribution of n distribution with n degrees of freedom. hence, for any r, the condence level of the super x (x) gaussian mixture mixture component mixture component confidence region x (x) gaussian mixture mixture component mixture component confidence region (a) (b) (c) (d) fig. : gmm condence regions. (a) super level sets of individual gaussians at condence level k = . (b) super level sets of gaussians at the condence levels corresponding to a shared probability level. (c) an example conguration space (collisions are in blue and free space is in red) and (d) the associated condence ellipsoids of learned gmm distributions from collision samples (black in (c)). level set ln (,)() of the gaussian distribution n(, ) is explicitly given by = l n (,)() = f n \u0000log \u0000 det() \u0001 \u0001 . () accordingly, since it lacks an exact closed form expres- sion, we suggest approximating the condence region of a gaussian mixture distribution gm(, , ) associated with a condence level as a union of ellipsoi- dal condence regions of individual gaussians, associated with condence levels := (, , . . . , k) that satisfy pk k= kk = , as cgm(,,)() := [k k= cn (k,k)(k), () = k [ k= n x rn|(x k)t k (x k) f n (k) o , . () observe that, by construction, we have z cgm(,,)() gm(x; , , )dx . () a standard choice of the condence levels of individual gaussians is k = for all k as shown in fig. (a); however, this usually yields a poor approximation of the actual condence region of the mixture model because less accurate gaussians with high variances become more inuential in determining the condence region. a more accurate analytical choice for the individual condence levels is k = l n (k,k) \u0010 k \u0011 based on a shared probability level = pk k= kln (k,k)() . alternatively, in this paper, we use an iterative search algorithm to nd a more accurate shared probability level as described in and set k = l n (k,k) \u0010 k \u0011 for all k, as shown in fig. (b). with this approach, we obtain condence regions of gaussian mixture models that approximately represents conguration space obstacles, as illustrated in fig. (c)-(d). b. probabilistically safe corridors suppose gm(o, o, o) be a gaussian mixture mo- del constructed as described above for modeling the col- lision subspace o of a conguration space in rn and let cgm(o,o,o)(o) be the corresponding approximate condence region associated with a desired condence level = pko k= okok. accordingly, we dene the probabili- stically safe corridor around a conguration p rn to be sco(p):= ( x (pok) t ok(xok) ok(pok) min r f n (ok) ok(pok) , ! , k ) , () = xrn (ok p) t ok(xp) ok(ok p) max r f n (ok) ok(ok p) , , k , () which is constructed using tangent hyperplanes of con- dence ellipsoids of gaussians and is a closed convex polytope, as depicted fig. here, r is a scalar safety tolerance parameter, and .denotes the standard euclidean norm, and for any positive denite covariance matrix rnn, a positive denite choice of is = v \u0010 diag \u0010 , , . . . , n \u0011\u0011 vt where = v diag(, , . . . , n)vt is the singular value decompo- sition of . it is also useful to observe from () that f n (ok) = ok(okp) for any condence region boundary point p cn(ok ,ok)(ok). hence, the safety constraints encoded by sco are relaxed with increasing . proposition : for , the probabilistically safe corri- dor sco(p) of a conguration p rn is a nonempty convex neighborhood of p; and for > , sco(p) strictly contains p in its interior sco(p), i.e., for any p rn p sco(p) , and p sco(p) > () proof: by denition (), the probabilistically safe corridor sco(p) is constructed as an intersection of half- spaces and so is a convex polytope. moreover, for any (resp. > ), these half spaces are guaranteed to contain p (resp. strictly in their interiors). thus, the result follows. proposition : for , the probabilistically safe cor- ridor sco(p) of a probabilistically safe state p rn \\ cgm(o,o,o)(o) contains p in its interior sco(p) and is also probabilistically safe, i.e., p rn \\ cgm(o,o,o)(o) = p sco(p) rn \\ cgm(o,o,o)(o).() proof: for any p rn \\ cgm(o,o,o)(o), we have from () that r f n (ok) ok(pok)< for all k. hence, the result directly follows from () and the fact that for any safe conguration p rn \\ cgm(o,o,o)(o) fig. : local steering via probabilistically safe corridors. (left) example tree extension using a probabilistically safe corridor in d space, (right) probabilistically safe corridor in d space. the probabilistically safe corridor sc(p; o, o, o) is bounded by tangent hyperplanes of condence regions of individual gaussians that strictly separates the point p from the gaussian condence ellipsoids. note that the safe corridor sco(p) around a probabili- stically unsafe conguration p cgm(o,o,o)(o) can be empty for < , especially for gaussian mixture models with signicant overlap. fortunately, many gaussian mixture learning algorithms yield proper mixture models with mini- mal overlap. moreover, in order to resolve this issue, one can consider using a nonnegative , which adaptively relaxes the safety constraints of sco(p) depending on the safety level of the conguration p and yields a nonempty relatively safe corridor sco(p). thus, an optimal selection of is = , which ensures nonempty safe corridors for all congurations (proposition ) and exact probabilistically safe corridors for probabilistically safe congurations (proposition ). c. guided steering via safe corridors we now describe a novel use of probabilistically safe corridors for guided local steering of sampling based plan- ning, in particular, rrts. in the original rrts, a sample conguration qrand is randomly drawn in the conguration space, and then its nearest node qnear in the tree is found based on a distance measure, which is set to be the standard euclidean distance in this paper. then, a new conguration qnew is slightly extended from qnear towards qrand, say using the standard straight line steering. if qnew is collision free, it is added to the tree as a new node, which is connected to the nearest node. if qnew collides with an obstacle, then tree construction repeats with another qrand. in this paper, we propose a new approach for tree expan- sion where qnew is adjusted to head towards collision free space using probabilistically safe corridors sco, as shown in fig. , by projecting qrand onto sco(qnear) as follows: qproj = sco(qnear)(qrand) () where a(x) := arg minaax ais the metric projection of a point x rn onto a closed convex set a rn; that is to say, a(x) returns the closest point of set a to the input point x. hence, the tree is extended towards qproj instead of qrand, as shown in fig. proposition : if a sampling based motion planning algo- rithm is probabilistically complete for the standard straight- line steering, then the straight line steering towards the pro- algorithm tree extension in conguration space require: : o, o : t .init(qinit); : while distance(qgoal, qnew) > dmin do : qrand getrandomsampling(), iter = ; : while iter < max iter do : qnear getnearestneighbor(t , qrand); : qproj steeringguide(o, o, qnear, qrand); : qadj straightlinesteering(qnear, qproj, ); : if straightline(qnear, qadj) is collision free then : t .addtree(qadj), iter = iter + ; : else : break; : end if : end while : end while jected goal onto probabilistically safe corridors, as described in (), preserves its probabilistic completeness for > proof: the result simply follows from proposition because the probabilistically safe corridor sco(p) of a conguration p rn strictly contains p in its interior for > and the metric projection onto a probabilistically safe corridor locally behaves as the identity map. in other words, for > , the straight line steering toward the projected goal onto probabilistically safe corridors is locally equivalent to the standard unconstrained straight line steering. one computational challenge of our guided steering ap- proach is that it requires to recompute the metric projection of qrand onto sco(qnear) for each new selection of qrand and so qnear. metric projection onto a convex polytope can be solved using any state of the art quadratic optimization solver. for efciency, we apply the active set method for qua- dratic optimization, which is an iterative solver that ensures a feasible solution and a decrement on the objective function at each iteration. this enables us to inherit some useful information from prior computation and stop its computation after some desired number of iterations. in order to reduce to computational cost, we keep qrand the same until a maximum number of iteration max iter is reached. this enables us to warm start the active set method with the active constraints of the previous computation. if active constraints at the optimal solution are given, then a quadratic optimization problem with inequality constraints can be converted into a quadratic problem with equality constraints, which requires signicantly less computational time to solve the optimizati- on problem. for example, previous active constraints could be still active for slightly changed qnear if the sample goal qrand is kept the same. therefore, to increase computational efciency, we always check rst if the quadratic optimization is feasible with previously active hyperplane constraints of probabilistically safe corridors. ) tree extension in the conguration space: algorithm presents the pseudocode for the proposed tree extension methods in the conguration space. here, the nearest node algorithm tree extension in task space require: : o, o : t .init(einit, qinit); : while distance(qgoal, qnew) > dmin do : qrand getrandomsampling(); : qnear getnearestneighbor(t , qrand); : qnew straightlinesteering(qnear, qrand, ); : xrand, xnear, xnew fwdkin(qrand, qnear, qnew); : xproj steeringguide(o, o, xnear, xrand); : xadj xprojxnear ||xprojxnear|| ||xnew xnear||; : qadj qnear + j(qnear)xadj ; : if straightline(qnear,qadj) is collision free then : t .addtree(qadj); : end if : end while qnear of a random goal qrand in tree t is extended by a new node qadj towards the projected goal qproj through the probabilistically safe corridor sco of qnear. if the random goal qrand satises the safety corridor constraints, then the tree is directly extended to the random goal, just like the standard straight line extension method. in our implementa- tion, we set the maximum number of iterations, max iter (line ), for using the same random goal qrand to be , and we select the maximum stepsize of the straight line planner, (line ), manually depending on the desired accuracy level of collision checks. ) tree extension in the task space: for task space planning, we also use probabilistically safe corridors for guiding the end effector of a manipulator as described in algorithm using forward kinematics, we dene xrand to be the end effector position of the random goal qrand and xnear to be the end effector position of the nearest node qnear of qrand in tree t . here, our objective is to steer the end effector position xnear towards xrand via the projection xproj of xrand onto the sco(xnear) along the safe corridor sco(xnear) in d space, as shown in fig. accordingly, we select a steering step that is proportional with the stepsize of the standard straight line steering of the end effector as xadj = xproj xnear ||xproj xnear|| ||xnew xnear||, () and determine the corresponding conguration as: qadj = qnear + j(qnear)xadj, () where j is the pseudoinverse of manipulator jacobian j, satisfying j = jt (jjt ) in fig. , we illustrate the guided steering of a manipulator using probabilistically safe corridors in task space: the new conguration (magenta), suggested by the standard straight line planner, collides with obstacles, whereas the adjusted conguration (green), consistent with probabilistically safe corridors, moves in the tangent direction of obstacles. ) gmm based biased sampling: in our experiments, we also compute the mixtures of gaussian gm(x, f, f, f) fig. : examples of task space steering of a robotic manipulator. here, the new conguration (magenta), suggested by the straight line planner from the nearest conguration (black), is adjusted to a better conguration (green) based on the associated probabilistically safe corridor. for modeling the free space, which is used for biased samp- ling over the free space as described in . for the settings where biased sampling is used, instead of uniform sampling in line in algorithms and , we randomly sample a conguration from the collision free gaussian mixture distribution gm(x, f, f, f). this sampling method increases the likelihood of a new sample being collision free, and so can increase the computational efciency of planning as discussed below. ",
        "Subsections": [],
        "Groundtruth": "The section discusses various methods to address the computational challenges faced by sampling-based planning approaches in complex environments. Biased sampling methods and rejection sampling methods are proposed to reduce the number of sample nodes and improve computational efficiency, but they have limitations in high-dimensional settings. Another approach to enhance computational efficiency is to reduce the number of collision checks, such as lazy collision checking or fast probabilistic collision checks. Various methods are explored, including utilizing safety certificates, steering approaches, and biasing configuration space exploration. A new approach called Safety Guided RRT via Probabilistically Safe Corridors is introduced, which uses Gaussian Mixture Models to approximate configuration spaces and identify safe neighborhoods with minimal collision risk. The method integrates probabilistically safe corridors into motion planning algorithms to guide tree expansion towards safe spaces, improving computational efficiency. Various algorithms and techniques are proposed and described in detail to implement this approach efficiently."
    },
    {
        "Section_Num": "III",
        "Section": "III Safety-Guided RRT  via Probabilistically Safe Corridors",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "IV",
        "Section": "IV Results",
        "Text": "we evaluate sg rrt in various environments using both a simulator and a real robot. we analyze the performance of sg rrt by comparison with several existing rrt ap- proaches. in addition, we demonstrate sg rrt on a real humanoid robot and provide results under real settings. all experiments are performed on a ghz pc, and all planners are implemented in matlab. a. learning gaussian mixture models in all our experiments, we learn gaussian mixture models ofine by using the samples generated during the standard rrt planning (which was rich enough for accurate modeling, see fig. (b)) and by manually selecting the kernel band- width for the meanshift clustering so that the desired level of representation resolution is guaranteed. in particular, we select the gaussian kernel sizes for the meanshift clustering as degrees for dof manipulator planning, degrees for dof manipulator planning, and cm for task space planning. gmm learning takes seconds for clusters from , collision samples for dof manipulator, seconds for , clusters from , collision samples for dof manipulator, and seconds for clusters from a d point cloud (including , data points) for task space planning. for probabilistically safe corridors, we set the desired condence level = and the safety tolerance = for all cases. in future work, we plan to consider online gmm learning for adaptive motion planning in dynamic environments. b. dof planar manipulator for ease of visual presentation, we rst consider motion planning of a dof planar manipulator whose rst link is units long and second link is units long as illustrated (a) workspace (b) rrt (c) sg rrt execution time time (sec) rrt srrrt biasedrrt biasedsrrrt birrt bisrrrt corridortime (d) execution time collision check number (x) rrt srrrt biasedrrt biasedsrrrt birrt bisrrrt collision number (e) number of collision checks fig. : rrt planning performance for a dof planar manipulator in fig. (a). in fig. , we compare the computational perfor- mance of several variants of rrt planners (the standard rrt, the biased rrt with % goal bias, and the bidirectional rrt) with and without our proposed safety guided steering. here, gmms are learned ofine along the collision space boundary (as shown in fig. (d)) using collision samples obtained during the standard rrt planning (green points in fig. (b)) and they are used online for constructing probabilistically safe corridors. in our quantitative evaluation, we consider the total execution time and the total number of collision checks as a performance measure, and we obtain the statistics (average and standard deviation) of these performance measures by running each planning algorithm the number of samples for gmm learning iteration for a path to goal number of collision checks collision noncollision total fig. : safety guided rrt planning performance with respect to the number of collision samples used for gmm learning fig. : (left) prm with the standard straight line planner, (right) prm with our safety guided local planner for times for different start and goal pairs. in overall, we observe that our safety guided steering increases compu- tation performance signicantly over the standard straight- line steering by dramatically reducing the required number of planning iterations (i.e., collision checks) to nd a path between any given start and goal pair, as shown in fig. (e). because safety guided steering via probabilistically safe corridors minimizes collision risk by adaptively adjusting steering direction and stepsize. as a result, our safety guided local planner yields steering action that are signicantly less likely to be in collision; whereas the standard straight line planner ends up being in collision with more than % chance, as seen in fig. (e). finally, we nd it useful to emphasize that the construction of and the projection onto a probabilistically safety corridor takes around msec in average for each new sample (denoted by corridortime in figure (d)), which is in the same order of magnitude as the computation cost of a collision check that takes around msec. in fig. , we demonstrate how the average number of rrt iterations (i.e., collision checks), required for nding a path between any given start and goal pair, changes with the number of sample collision congurations (i.e., training data) used for gaussian mixture learning. as expected, the performance of rrt planning with safety guided steering increases with the increasing size of training data as a result of increasing accuracy of the gaussian mixture model. in fig. , we present an application of our safety guided steering to the probabilistic roadmap (prm) planning of the dof planar manipulator. as seen in fig. , our safety guided steering noticeably increases the connectivity of a prm as compared to the standard straight line planner. here, two vertices of a prm is said to be connected if safety guided steering can joining them in at most table i: gmm and prm computation times gmm construction time (sec) prm construction time (sec) num. of sampling gmm total num. of prm collision connected samples time time time vertices time checks prm , no , no , , no , , no , , yes , , yes , , yes , , yes execution time case number time (sec) rrt wssgrrt sgrrt gmmrrt gmmwssgrrt gmmsgrrt total collision check case number number (x) rrt wssgrrt sgrrt gmmrrt gmmwssgrrt gmmsgrrt total , fig. : rrt planning performance for a dof manipulator: (top) sequential planning tasks, (middle) average execution time, (bottom) average number of collision checks steps. finally, to briey compare the computation cost of the learning phases of the gmm and prm methods, we provide in table i the average computation time for the gmm and prm constructions for the dof planar manipulator planning. as expected, for the same number of samples, gmm learning is around two orders of magnitude faster then the prm construction because the connectivity test of prms is signicantly computationally costly than the nearest neighbor search and the statistics computation of gmm. c. dof manipulator in d space in order to validate the performance of sg rrt quan- titatively in high dimensional space, we compare it with traditional approaches with a dof manipulator in d space using the webots simulator of the cyberbotics ltd. company. fig. (top) shows the simulation scenario that is composed of seven sequential planning tasks. this scenario includes a difcult task, where the robot must remove its arm from the lower shelf and then insert it into the upper shelf. the simulation trials are repeated times for accurate evaluation, and we use the average execution time and the number of collision checks as the evaluation criteria. for the comparison, we evaluate the standard rrt, safe- guided rrt (sg rrt), and safe guided rrt in the task space (wssg rrt). in addition, since we can apply gmm- based sampling as described in section iii c., we also evaluate gmm based rrt (gmm rrt), gmm based safe- guided rrt (gmmsg rrt), and gmm based safe guided rrt in the task space (gmmwssg rrt). note that we apply a bidirectional method (rrt connect) in all approaches. the gmm rrt can be faster than the standard rrt, and the gmmsg rrt is the fastest among all approa- ches. the wssg rrt and the gmmwssg rrt are faster than the rrt and gmm rrt. this demonstrates that the end effector of the manipulator is effectively guided by the safe corridor in the high dimensional space, and it can reduce the computational time and the number of collision checks compared to traditional approaches. we also observe in fig. that sgrrt planning is faster and requires less collision checks in conguration spaces than in task spaces, because probabilistically safe corridors are geometrically more infor- mative when constructed in conguration spaces than in task spaces. therefore, the tree extension with the safe corridor is signicantly more efcient than the traditional methods. d. physical robot experiments we demonstrate the performance of sg rrt on a dof manipulator (length: cm) of an actual humanoid robot and an rgbd camera (asus xtion live pro) with the scenario shown in fig. (top). the robot is positioned cm from the shelf (cm cm) on the table. figure presents the comparison results of gmmsg rrt and the standard rrt in terms of the execution time and the number of collision checks. note that we apply a bidirectional method (rrt connect) and give % goal biased samples. since the gmmsg rrt adjusts a new node in the direction that avoids obstacles using probabilistically safe corridors and also utilizes biased sampling over collision free space, the sample connectivity increases around narrow spaces, and tree expansion efciently avoids obstacles. gmmsg rrt is signicantly efcient even when the robot needs to insert its arm onto the shelf. on the other hand, the computational time and the number of collision checks for the standard rrt planner dramatically increases in such complicated tasks. execution time case number time (sec) rrt gmmsgrrt total collision check case number number (x) rrt gmmsgrrt total , , fig. : rrt planning performance with an actual physical robot: (top) experiment with a physical robot, (middle) average execution time, (bottom) average number of collision checks ",
        "Subsections": [],
        "Groundtruth": "The section evaluates sg rrt in various environments using a simulator and a real robot. Performance analysis compared sg rrt with existing rrt approaches, demonstrating effectiveness on a humanoid robot. Gaussian mixture models are learned offline for accurate modeling. Results show significant computational performance improvement with safety guided steering, reducing collision risk and required planning iterations. sg rrt is compared with traditional methods in high dimensional space, showing faster execution and fewer collision checks. Physical robot experiments further validate sg rrt's efficiency and effectiveness in navigating obstacles."
    },
    {
        "Section_Num": "V",
        "Section": "V Discussion",
        "Text": "in this paper, we present an effective local steering ap- proach for sampling based motion planning using probabi- listically safe corridors of learned gaussian mixture models of conguration spaces. we construct a probabilistically safe corridor around a conguration using tangent hyperplanes of condence ellipsoids of gaussian mixture models that are learned using collision history to approximate conguration space obstacles. accordingly, we propose a probabilistically safe local steering primitive that extends a random motion planning graph towards a sample goal using its projection onto the associated probabilistically safe corridor, which heu- ristically minimizes collision likelihood. we observe that the proposed local steering approach improves the performance of sampling based planning in challenging regions, especi- ally narrow passages, by adjusting steering direction and stepsize. in our simulations and experiments with a real robot manipulator, we demonstrate that our proposed safety guided local planner shows signicant performance improvement over the standard straight line planner for randomized motion planning of dof and dof manipulators. in a future paper, we plan to extend our work using online gmm learning for uncertainty aware adaptive planning. ",
        "Subsections": [],
        "Groundtruth": "The paper introduces an effective local steering method for sampling-based motion planning using probabilistically safe corridors based on learned Gaussian mixture models. The method constructs safe corridors around configurations using confidence ellipsoids of Gaussian mixture models learned from collision history. A probabilistically safe local steering primitive is proposed to guide a random motion planning graph towards a goal by projecting it onto the safe corridor, minimizing collision likelihood. The approach enhances planning performance in challenging regions, such as narrow passages, by adjusting steering direction and step size. The method outperforms standard straight-line planners in simulations with a real robot manipulator, showing significant performance improvements. Future work includes extending the approach using online Gaussian mixture model learning for uncertainty-aware adaptive planning."
    },
    {
        "Section_Num": "References",
        "Section": "References",
        "Text": "] l. e. kavraki, p. svestka, j.-c. latombe, and m. h. overmars, proba- bilistic roadmaps for path planning in high dimensional conguration spaces, ieee trans. robot. autom., vol. , no. , pp. , s. m. lavalle and j. j. kuffner, randomized kinodynamic planning, int. j. robot. res., vol. , no. , pp. , d. hsu, j.-c. latombe, and r. motwani, path planning in expansive conguration spaces, in proc. ieee int. conf. robot. autom., , pp. s. karaman, m. r. walter, a. perez, e. frazzoli, and s. teller, anytime motion planning using the rrt*, in proc. ieee int. conf. robot. autom., , pp. d. hsu, l. e. kavraki, j.-c. latombe, r. motwani, and s. sorkin, on nding narrow passages with probabilistic roadmap planners, in int. work. on algorithmic foundations of robotics, , pp. s. r. lindemann and s. m. lavalle, current issues in sampling based motion planning, in int. symp. robotics research, , pp. m. saha, j.-c. latombe, y.-c. chang, and f. prinz, finding nar- row passages with probabilistic roadmaps: the small step retraction method, autonomous robots, vol. , no. , pp. , l. zhang and d. manocha, an efcient retraction based rrt plan- ner, in ieee int. conf. robot. autom., , pp. o. arslan, v. pacelli, and d. e. koditschek, sensory steering for sampling based motion planning, in proc. ieee/rsj int. conf. intell. robots syst., , pp. j. denny, m. morales, s. rodriguez, and n. m. amato, adapting rrt growth for heterogeneous environments, in proc. ieee/rsj int. conf. intell. robots syst., , pp. j. huh and d. d. lee, learning high dimensional mixture models for fast collision detection in rapidly exploring random trees, in proc. ieee int. conf. robot. autom., , pp. b. burns and o. brock, sampling based motion planning using pre- dictive models, in ieee int conf robot autom, , pp. j. denny and n. m. amato, toggle prm: a coordinated mapping of c free and c obstacle in arbitrary dimension, in int. work. on algorithmic foundations of robotics. springer, , pp. v. boor, m. h. overmars et al., the gaussian sampling strategy for probabilistic roadmap planners, in proc. ieee int. conf. robot. autom., , pp. a. shkolnik and r. tedrake, sample based planning with volumes in conguration space, arxiv preprint arxiv:, a. shkolnik, m. walter, and r. tedrake, reachability guided samp- ling for planning under differential constraints, in proc. ieee int. conf. robot. autom., , pp. a. yershova, l. jaillet, t. sim eon, and s. m. lavalle, dynamic- domain rrts: efcient exploration by controlling the sampling do- main, in ieee int. conf. robot. autom., , pp. v. hwang, m. phillips, s. srinivasa, and m. likhachev, lazy valida- tion of experience graphs, in proc. ieee int. conf. robot. autom., , pp. g. s anchez and j.-c. latombe, on delaying collision checking in prm planning: application to multi robot coordination, int. j. robotics res., vol. , no. , pp. , r. bohlin and l. e. kavraki, path planning using lazy prm, in ieee int. conf. robot. autom., , pp. j. huh, b. lee, and d. d. lee, adaptive motion planning with high- dimensional mixture models, in proc. ieee int. conf. robot. autom., , pp. j. pan and d. manocha, fast and robust motion planning with noisy data using machine learning, in int. conf. on machine learning, g. s. aoude, b. d. luders, j. m. joseph, n. roy, and j. p. how, probabilistically safe motion planning to avoid dynamic obstacles with uncertain motion patterns, autonomous robots, vol. , no. , pp. , j. bialkowski, m. otte, s. karaman, and e. frazzoli, efcient collisi- on checking in sampling based motion planning via safety certicates, int. j. robot. res., vol. , no. , pp. , e. plaku, l. e. kavraki, and m. y. vardi, motion planning with dynamics by a synergistic combination of layers of planning, ieee trans. robot. autom., vol. , no. , pp. , j. denny, r. sandstr om, a. bregger, and n. m. amato, dynamic region biased rapidly exploring random trees, in int. work. on algo- rithmic foundations of robotics, r. wein, j. van den berg, and d. halperin, planning high quality paths and corridors amidst obstacles, int. j. robot. res., vol. , no. -, pp. , r. geraerts, planning short paths with clearance using explicit corridors, in ieee int. conf. robot. autom., , pp. j. chen, t. liu, and s. shen, online generation of collision free trajectories for quadrotor ight in unknown cluttered environments, in ieee int. conf. robot. autom., , pp. s. liu, m. watterson, k. mohta, k. sun, s. bhattacharya, c. j. taylor, and v. kumar, planning dynamically feasible trajectories for quadrotors using safe ight corridors in -d complex environments, ieee robot. autom. letters, vol. , no. , pp. , d. c. conner, a. rizzi, and h. choset, composition of local potential functions for global robot control and navigation, in proc. ieee/rsj int. conf. intell. robots syst., , pp. v. pacelli, o. arslan, and d. e. koditschek, integration of local geometry and metric information in sampling based motion planning, in ieee int. conf. robot. autom., , pp. y. cheng, mean shift, mode seeking, and clustering, ieee trans pattern anal mach intell, vol. , no. , pp. , r. j. hyndman, computing and graphing highest density regions, the american statistician, vol. , no. , pp. , o. arslan, approximating condence regions of gaussian mixtures by unions of ellipsoids, in preparation. j. j. kuffner and s. m. lavalle, rrt connect: an efcient approach to single query path planning, in ieee int. conf. robot. autom., vol. , , pp. ",
        "Subsections": [],
        "Groundtruth": "The References section lists various research papers on motion planning algorithms and strategies for path planning in high dimensional configuration spaces. It includes works on probabilistic roadmaps, randomized kinodynamic planning, sampling-based motion planning, and methods for finding narrow passages in complex environments. Research topics cover a range of techniques such as reachability-guided sampling, adaptive planning with mixture models, collision checking, and integration of local geometry into motion planning algorithms."
    },
    {
        "Section_Num": "NA",
        "Section": "NA",
        "Text": "probabilistically safe corridors to guide sampling based motion planning jinwook huh, om ur arslan, and daniel d. lee abstract in this paper, we introduce a new probabilistically safe local steering primitive for sampling based motion planning in complex high dimensional conguration spaces. our local steering procedure is based on a new notion of a convex probabilistically safe corridor that is constructed around a conguration using tangent hyperplanes of condence ellipso- ids of gaussian mixture models learned from prior collision history. accordingly, we propose to expand a random motion planning graph towards a sample goal using its projection onto probabilistically safe corridors, which efciently exploits the local geometry of conguration spaces for selecting proper steering direction and adapting steering stepsize. we observe that the proposed local steering procedure generates effective steering motion around difcult regions of conguration spaces, such as narrow passages, while minimizing collision likelihood. we evaluate the proposed steering method with randomized motion planners in a number of planning scenarios, both in simulation and on a physical dof robot arm, demonstrating the effectiveness of our safety guided local planner over the standard straight line planner. i. introduction ",
        "Subsections": [],
        "Groundtruth": "The paper introduces a new probabilistically safe local steering primitive for sampling-based motion planning in complex high-dimensional configuration spaces. The proposed method utilizes convex probabilistically safe corridors constructed around configurations based on the confidence ellipsoids of Gaussian mixture models learned from collision history. By expanding a random motion planning graph toward a sample goal using its projection onto these corridors, the method efficiently leverages local geometry for proper steering direction and step size adaptation, effectively navigating challenging regions like narrow passages while reducing collision likelihood. Evaluation on both simulation and physical robot arm scenarios shows the effectiveness of the safety-guided local planner over standard straight line planners."
    },
    {
        "Section_Num": "1",
        "Section": "1 Introduction",
        "Text": "when electrons scatter oelectric eld of proton or nucleus, they can emit real pho- tons. this is bremsstrahlung (braking radiation). bremsstrahlung appears in nearly all branches of physics. bethe and heitler rst gave a quantum mechanical description of the bremsstrahlung emission at the coulomb potential of an innite heavy atom . the bethe heitler formula is an elementary and important equation in quantum electromag- netic dynamics (qed) and astrophysics. the bremsstrahlung process of electron in the coulomb eld is a second order process, which involves the photon emission of electron and the coulomb scattering. however, these two sub processes have divergences. the former is infrared divergence, while the later origins from the long range /r potential. for a neutral atom, the nuclear coulomb potential is completely screened by the elec- tron cloud, which reduces a signicant contribution from scattering distance larger than the atomic radius. therefore, the coloumb potential in the s matrix element should be replaced by a phenomenological screening potential. however, the complex correlations between the above mentioned two sub processes hinder us to obtained an exact analytical solution for the integrated cross section. bethe and heitler take an extra model to intro- duce the screening radius r in the solution and the result predicts a slower ln r dependent cross section. the bethe heitler formula was broadly applied in astrophysics. recently, a puzzled dierence of the energy spectra of electron/positron at gev tev energy band in cosmic ray raises our doubts to the validity of the bethe heitler formula . we found that the bremsstrahlung cross section at the soft photon limit (the photon energy = ) contradicts with the predictions of the bethe heitler formula. it means that the bethe heitler formula should be improved at a general case of = for this sake, we re derive the cross section formula of the bremsstrahlung emission with the screening potential in this work, where the time ordered perturbative theory (topt) will be used to separate the scattering and radiation processes at the equivalent photon (or weizs acker williams) approximation. this method was successfully used to decompose a complex feynman diagram to a several simpler sub processes in our previous works . we consider the scattering of electron on a light nucleus, where the recoil eect is not negligible, since most targets in the interstellar medium are light nuclei. besides, we will show that the contributions of interference terms between scattering and radiation sub processes can be neglected due to the recoil eect, thus we can further decompose the process. the new bremsstrahlung formula () predicts a strong r dependent cross section. the result will inspire us to review the traditional electromagnetic shower theory at the extreme conditions. the paper is organized as follows. in sec. we detail the derivation of the bremsstrahlung formula using the topt. then we compare the results with the bethe heitler formula in sec. a short summary is given in sec. the bremsstrahlung cross section with screening potential the bethe heitler formula assumes that the target atom is innitely heavy. for using the topt in the following derivation, we consider a more general case: electron scattering oa nite heavy atom. the dierential cross section of the bremsstrahlung emission (fig. ) in the covariant perturbation theory at the leading order approximation is d = mem q (pipi) m em |mpipipfpf k|()(pi + pi pf pf k) d k () med pf ()ef md pf ()ep f , () where the screening photon propagator in the matrix takes ig q + i. () the screening parameter has the dimension of mass and / r, r is the atom radius for a neutral atom. according to the topt, a covariant feynman propagator in mpipipfpf k s = z dl i l + me l m e + i, () may decompose to a forward and a backward components: sf = i e l l + me + ef e l , forward () and sb = i e l l + me + ef e l . backward () figure : two elemental bremsstrahlung amplitudes. figure : the topt decomposition of fig. dashed lines indicate the time ordered of the process. note that l(el, lt, ll) is o mass shell l = m e, while l = (e l, lt, ll) or l = (e l, lt, ll) are on mass shell, i.e., l = m e. it seems that the topt decomposition complicates the calculation with increasing the propagators (fig. ). however, the backward component will be suppressed at higher energy and small emitted angle. for example, we take l along the z direction, and dene v as the momentum fraction of l carried by the longitudinal momentum kl of photon, v = kl l e l , () where we neglect the electron mass me at high energy and note that l is on mass shell. at high energy and small emitted angle we denote l = (e l, lt, ll) = (e l, , e l), () k = (, kt, kl) = ve l + k t ve l , kt, ve l , () and pf = (ef, pf,t, pf,l) = ( v)e l + k t ( v)e l , kt, ( v)e l . () if v = and one can nd that sf e l + ef e l v( v) k t , () which is much larger than sb e l ef + e l ve l . () therefore, the contributions of the backward propagator are negligible. this not only reduces the number of diagrams, but also allows us to factorize the complex feynman graph due to the on mass shell of the forward propagator. this is the theoretical basic of the equivalent photon approximation. we use the processes in fig. to show this approximation. we take the laboratory frame, where the target atom is at rest, but the incident elec- tron has a high energy. this is an innite momentum frame for the electron. note that the physical picture of the same process has dierent appearances in the dierent coor- dinate frames, even in the dierent innite momentum frames. in the above mentioned frame, both the longitudinal and vertical momenta of the virtual photon generally does figure : four topt diagrams after neglecting the contributions of the backward com- ponents at high energy and small scattering angle. not disappear. thus, the contributions of figs. b and c are not negligible due to the coherence between fig. a and c. now we consider the recoil eect. the electron atom interaction time is , () is the energy loss of the incident electron. the radiation time is t t ef + e l = e lv( v) k t . () we set = ei. one can nd that at high energy ei and small scattering angle (ei kt) a small energy loss ( ) may suciently lead to < t,, () i.e., the scattering time can not cover two time periods t and t in a same bremsstrahlung event. in this case, the contributions of the interferant processes in figs. b and c are figure : the factorized bremsstrahlung processes with the recoil eect at high energy and small scattering angle. inhibited. after removing these coherent diagrams, using the on mass shell of the mo- mentum l, the process can further decompose to two sub processes (fig. ). we discuss the process involving (a) in fig. , eq. () becomes da = mem q (pipi) m em |mpipi lpf| md pf ()ep f med pf ()ef ()(pi + pi pf pf k) e l ! ef + e l ! |m lpfk| d k () d adpa, () where d a = mem q (pipi) m em |mpipi lpf| md pf ()ep f med pf ()ef ()(pi+pipf pf k), () and dpa = e l ! ef + e l ! |m lpfk|d k . () we calculate d a using |mpipi lpf| = e(ze)() m em (q ) h lp i + p i l g(l pi m e) i h p f p i + p i p f g(pf pi m ) i , () where we use l to replace l in the matrix since el e l for the small emitted angle. the result is d a = z e i (sin + /ei(ef + )) cos q m sin + ei m sin d = z e i (( + eim) sin + /e i ) cos q m sin + ei m sin d z e i (sin + /e i ) cos q m sin + ei m sin d. () where we used the -transfer momentum q = ei(ef + ) sin , () the energy momentum conservation = ei ef = ei(ef + ) m sin , () and ef + = ei + ei m sin ! . () note that a q dependent term in eq. () is absent when the target is a spin- particle, however, it is does not change the following results. on the other hand, through a simple calculation, we obtain dpa padvdk t = ( + ( v))( v) v dvd lnk t. (), where v = /(ef +) = /(ei ). in the calculation, we turn the z axis direction from pi to l. combining eqs. () and (), we have da = z e i (sin + /e i ) cos q m sin + ei m sin ( + ( v))( v) v ddvd lnk t. () using z dv v = ln max min = ln ei min , () and k t (q) () with since k t origins from q, we have z d ln k t = ln k t,max k t,mim = ln q , () where we introduce a cut o to regularize the collinear divergence. usually, the value relates to the measurement resolution. thus, we have the bremsstrahlung cross section in the dierential form da = z e i ln q (sin + /ei(ef + )) cos q m sin + ei m sin ( + ( v))( v) v dvdd padvd, () where we reorganize d a = z e i ln q (sin + /ei(ef + )) cos q m sin + ei m sin d, () and pa = ( + ( v))( v) v . () we calculate the integrated bremsstrahlung cross section at a given initial energy through the angle integral. note that q implies sin(min/) = /(e i ) . () we decompose ln q = ln e i ln \u0012 + ei m sin / \u0013 + ln(sin /). () the contribution of the second term on the right side is negligible comparing with that of the rst term. the rst term can be integrated and it contributes z/( + ) ln(e i /) at the leading order approximation. however, the third integral is not so lucky. through the numeric computations we nd that the contribution of the third term is almost z/( + ) ln(e i /) with . thus we have da z( + ( v))( v) v dv ln e i , > ln e i . () if keeping a leading term /v in pa, we obtain the total cross section a z ln ei min ln e i , > ln e i . () one can understand the physical sense of as follows. the parameters and appear in a same factor /( + ) of the cross section. since / is a space scale of the screening coulomb potential, / should also be related to a space character about the process. in fact, min in eq. () relates to a maximum impact parameter bmax for the scattering of electron in a central coulomb potential. therefore, / bmax and eq. () has two space scales. the results () and () show that as long as < , the bremsstrahlung cross section is almost proportional to the geometric area of the atomic coulomb eld r, rather than a weaker ln r dependence that the bethe heitler formula predicted. now we calculate the process involving (b) in fig. its dierence from da is not only the interpretations of pi l and l pf, but also they have the dierent phase spaces. corresponding to eq. () we have db = mem q (pipi) m em |m lpipfpf| md pf ()ep f med pf ()ef ()(pi + pi pf pf k) e l ! e l + ei ! |mpi lk| d k () d b pdvd, () where d b = z e i ln q (sin + /ef(ei )) cos q m sin + (ei) m sin d = z e i ln q (( ef m) sin + /e f) cos q m sin + (ei) m sin d z e i ln q (sin + e f ) cos q m sin + (ei) m sin d. () note that ei = ef ef m sin ! , () q = ef(ei ) sin , () and the energy momentum conservation = ei ef = ef(ei ) m sin () are used. after integral, we have b = a \u0012ef ei \u0013 < a. () ",
        "Subsections": [],
        "Groundtruth": "The text discusses the bremsstrahlung process, which involves electron scattering in the Coulomb field of a proton or nucleus and emitting photons. Bethe and Heitler first described this process quantum mechanically and introduced the Bethe Heitler formula, an essential equation in quantum electromagnetic dynamics and astrophysics. The formula accounts for divergences in the photon emission and Coulomb scattering sub processes by introducing a screening potential for neutral atoms. The paper aims to derive a new bremsstrahlung formula with screening potential using time-ordered perturbative theory and equivalent photon approximation. The new formula predicts a strong radius-dependent cross section, challenging the traditional Bethe Heitler formula's validity, particularly at soft photon limits. The recoil effect and interference terms between scattering and radiation processes are considered, leading to a more accurate cross section prediction. The new formula's results suggest a different geometric interpretation of the bremsstrahlung process compared to the Bethe Heitler formula, highlighting the importance of screening coulomb potential in determining the cross section."
    },
    {
        "Section_Num": "2",
        "Section": "2 The bremsstrahlung cross section with screening potential",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "3",
        "Section": "3 Comparing with the Bethe-Heitler formula",
        "Text": "the dierential cross section of the bethe heitler formula for the bremsstrahlung is dbh = z | pf| | pi| d dcdk | q| \" pf sin f (ef | pf| cos f)(e i q) + pi sin i (ei | pi| cos i)(e f q) + p i sin i + p f sin f (ei | pi| cos i)(ef | pf| cos f) | pi|| pf| sin i sin f cos (ei | pi| cos f)(ef | pf| cos f)(e i + e f q) # , () where i, f are the angles between k and pi, pf respectively; the angle between ( pi k) plane and ( pf k) plane. the bethe heitler formula does not consider the recoil eect, therefore the scattering time of the electron with an innite heavy target is = . thus, we can not distinguish whether the photon is emitted from the incoming or outgoing electrons. in opposite to the bethe heitler formula, eq. () presents two factorized sub processes, because of the recoil eect suppresses the contributions of figs. b and c. after integral over angles, the dierential cross section () at high energy ei, ef me can be simplied as dbh z m e d e i (e i + e f eief) \u0012 log eief me \u0013 . () unfortunately, eq. () does not present the screening eect since it used a pure coulomb potential. bethe and heitler dene a term eief/m e in eq. () as a screening radius r, i.e., dbh = z m e d e i (e f + e i efei) \u0012 log(mer) \u0013 , () or dbh = z m e d e i (e f + e i efei) \u0012 log(z/) \u0013 , () where the thomas fermi model is used. however, this method freezes unreason- ably the variables eief/ and has following uncertainty: if setting any factor = ((eief)/(me))n/((me)/(eief))n into eq. (), one can get the dierent r dependent results. for further comparison, we refer eq. () to rewrite eq. () as dbh = bh pbhdv, () where bh = z m e \u0012 log(mer) \u0013 , () and pbhdv = d e i \u0012 e i + e f eief \u0013 . () using v = /ei (note that = in the bethe heitler formula) and d/ = dv/v, we have pbh(v) = \" + ( v) v ( v) v # . () thus, the bethe heitler formula () becomes dbh z m e \u0012 ln(z/) \u0013 \" + ( v) v ( v) v # dv. () figure : the bremsstrahlung process on a coulomb potential at the double lines are the eikonal form of the electron propagators. figure : the factorized bremsstrahlung process for a soft photon version. this result has a similar leading behavior /v as eq. (). however, there is a completely dierent r dependence in bh. it is well known that at limit any process leading to photon emission can be factorized . the bremsstrahlung cross section has its soft version. two electron propagators in fig. , which are indicated by the double lines, are the eikonal form at the soft photon limit. a corresponding factorized dierential cross section (see fig. ) is dsoft d = druth. d ln ei min v sin , nr ln q m e er () using the screening potential, we have nr soft z e i ln ei min \" ln + e i # , () at the nonrelativistic (nr) limit, which has the ln r dependent cross section similar to eq. () but with a /e i -suppletion. we emphasize that eq. () at the nr limit is valid only at a very narrow kinematics range near ei me, where the contributions of order o((v/c)) are almost vanished. any term without scattering angle in eq. () will appear a strongly r dependent bremsstrahlung cross section, if they can not be completely canceled at ei me. we consider the integral of eq. () at the er limit. because of m e in ln(q/m e) is not introduced as a cut oparameter , the theory itself does not have any restrictions on the value of q therefore, q < m e is allowed. it implies a negative cross section. taking a step back, if we regard this me as a cut oparameter, the result shows that the scattering will be restricted inside a small range r /| q| < /me fm, which is smaller much than atomic radius fm, and the cross section becomes irrelevant to the screening parameter . according to eq. (), m e in ln(q/m e) should be replaced by a general cut oparameter using the following substitution ln(q/m e) ln[(q/m e)/(q min/m e)] ln q , () is the process dependent and we request q > thus, the cross section at the er limit reads er soft = z e i z min d sin (sin + e i ) \" ln q ! # ln ei min , () where sin(min/) /(e i ). we get er soft z lne i ln ei min, > z lne i ln ei min. () it is compatible with eq. (). the coecients of eq. () are smaller than that of eq. (). the reason is that the contributions of db are neglected in eq. (). besides, the contribution of the third term in eq. () is more negative due to the recoil eect. therefore, our formula is consistent with the soft version of bremsstrahlung, but both contradict with the bethe- heitler formula. according to the qed results either eq. () or eq. (), we conclude that the screening parameter or /r is dened in a wrong location in the bethe heitler formula () (or ()) for high energy bremsstrahlung. now we try to answer why a strong screening scale dependence of the bremsstrahlung cross section has not been discovered in a long time? if , for example, me, eq. () has structure m e ln e i m e ln ei min , () which is similar to eq. () with = me. the result is irrelevant to the screening param- eter and without a strong r dependent eect. we think that the measurements of the dierential bremsstrahlung cross sections belong to this example. they detect the angular or energy distributions for the photons. the separation of the detected photon from the electron is restricted by the instrument resolution, which has a larger parameter . on the other hand, the r dependent eect may obviously appear in the total bremsstrahlung cross section, where the angle and energy of the projected particles are integrated over all possible phase space and they have a minimum value of . the measurement of the ra- diation length a /a is such an example. in practical applications, there are several uncertainties: a quantitative relationship r , the value of min and the corrections of the approximations. we have suggested to measure the high energy electron spectra when they pass through the completely ionized and extremely thin atmosphere . where the atomic coulomb potential may expand to a macroscopic spatial scale cm, which is much larger than an atomic radius cm and can provide a big r dependent eect. besides, the above mentioned uncertainties can be eectively canceled though the comparison with a normal radiation length. this application can be simplied as a(ei, , min) = (ei, , min) ! , () or for the radiation length a(ei, , min) = (ei, , min) ! , () where / is the radius of a referring neutral atom and or are xed by the corre- sponding data. we should mention the classical bremsstrahlung theory. it is the radiation of acceler- ated charged particle during its collisions with atomic electric eld. analogy to the soft photon limit of quantum theory, under low frequency limit the intensity of radiation is written as a factorized form d ddq di d druth dq , () where is radiation frequency and ruth the classical rutherford cross section. using lim di d = m e q () and druth dq = z ! q (q + ), () we have d d = z m e z qmax qmin qdq (q + ) = z m e \" ln qmax ! # , () where qmin is used. the result is ln r. this is not surprising, since eq. () corresponds really to the nonrelativistic limit in eq. (). therefore, the description of bremsstrahlung in the classical electrodynamics using () at high energy is not sucient. ",
        "Subsections": [],
        "Groundtruth": "The Bethe-Heitler formula for bremsstrahlung does not consider recoil effects, making it difficult to distinguish the source of emitted photons. A modified formula factors in recoil effects, leading to a more complex but accurate representation. At high energies, the Bethe-Heitler formula can be simplified but lacks screening effects. Introducing a screening radius improves the formula's accuracy, although uncertainties arise due to parameter choices. The significance of factorization in describing photon emission processes is emphasized. Comparisons between different formula versions highlight the importance of proper parameter definitions for accurate results. Classical bremsstrahlung theory, which considers radiation during charged particle collisions with atoms, provides insights into the behavior of radiation under low-frequency limits. Practical applications aim to measure high-energy electron spectra accurately, considering various uncertainties and corrections for precise results."
    },
    {
        "Section_Num": "4",
        "Section": "4 Summary",
        "Text": "the bethe heitler formula describes bremsstrahlung of high energy electrons in a pure coulomb potential, which may lead to an innite total cross section since the coulomb scattering is a long range interaction. a natural method is to use a screening potential to replace the coulomb potential. however, the complex interference eect between scatter- ing and radiation sub processes makes a diculty for us to get an analytical solution if considering the screening potential. it brings the uncertainty in the bethe heitler formula. for this sake, we re derive the formula for the bremsstrahlung cross section of elec- tron in the atomic eld using the topt framework. we prove that the recoil correc- tions of a nite mass atom at high energy may further decompose the bremsstrahlung cross section to two sub processes at the equivalent photon approximation. the im- proved bremsstrahlung formula contains the screening potential and predicts a strong r dependent bremsstrahlung cross section. the results remind us to review the tradi- tional electromagnetic shower theory at the extreme conditions. acknowledgments author thanks l. feng, p. liu, j.h. ruan and f. wang for useful discussions. this work is supported by the national natural science of china (no.). references h. bethe and w. heitler, proc.roy. soc., , (). ams collab. (m. aguilar et al, phys. rev. lett. , (). . fermi lat collab. (s. abdollahi et al, phys. rev. d, (). dampe collab. (g. ambrosi et al, nature, , ().. s. torii (for the calet collaboration), the calorimetric electron telescope (calet): high energy astroparticle physics observatory on the international space station, the th international cosmic ray conference, july- august, the hague, the netherlands. https://www.lsu.edu/physics/les/icrc torii.pdf. w. zhu, j.h. ruan, p. liu, l. feng and f. wang, arxiv:. m.d. scadron, advaced quantum theory and its applications through feynman dia- grams, springer verlag, w. zhu, nucl. phys. b, (); w. zhu and j.h. ruan, nucl. phys. b, (); w. zhu, z.q. shen and j.h. ruan, nucl. phys. b, (); w. zhu, z.q. shen and j.h. ruan, b, (). w. greiner and j reinhardt, quantum electrogynamics, springer verlag f.e. low, phys. rev. , (). j.d. jackson, classical elelctodynamics, wiley, new york, ",
        "Subsections": [],
        "Groundtruth": "The Bethe Heitler formula describes bremsstrahlung of high-energy electrons in a pure Coulomb potential, which can result in an infinite total cross section due to the long-range nature of Coulomb scattering. Introducing a screening potential to replace the Coulomb potential presents challenges due to complex interference effects between scattering and radiation subprocesses, hindering the attainment of an analytical solution. To address this, the formula for the bremsstrahlung cross section is re-derived using the topt framework, identifying that recoil corrections of a finite mass atom at high energies can decompose the cross section into two subprocesses in the equivalent photon approximation. The improved formula incorporates the screening potential and predicts a significant r-dependent bremsstrahlung cross section. These findings prompt a reevaluation of the traditional electromagnetic shower theory under extreme conditions. The work is acknowledged thanks to various contributors and is supported by the National Natural Science of China. Several references are provided for further study."
    },
    {
        "Section_Num": "NA",
        "Section": "NA",
        "Text": "arxiv:v jan improved bethe heitler formula wei zhu department of physics, east china normal university, shanghai , china abstract the bremsstrahlung cross section of electron in the atomic electric eld is re- derived using the time ordered perturbative theory. the results are compared with the bethe heitler formula. we indicate that both the topt description and a soft version for the bremsstrahlung process predict a strong screening parameter- dependent cross section, which is missed by previous bremsstrahlung theory. keywords: bremsstrahlung; qed; screening eect pacs numbers: .-m; .-t; .jx introduction ",
        "Subsections": [],
        "Groundtruth": "The text presents an improved Bethe Heitler formula for the bremsstrahlung cross section of an electron in an atomic electric field. The formula is derived using time-ordered perturbative theory and compared with the previous Bethe Heitler formula. Both the topt description and a soft version of the bremsstrahlung process indicate a strong screening parameter-dependent cross section, which was not accounted for in previous theories."
    },
    {
        "Section_Num": "1",
        "Section": "1. Introduction",
        "Text": "let g be a complex simple lie algebra. the kirillov reshetikhin (kr) modules constitutes an important family of nite dimensional irreducible representations of the quantum ane algebra uq(b g). an interesting problem is to understand how a kr module or their tensor product decomposes into irreducible uq(g)-modules. the fermionic formula by kirillov and reshetikhin , proven through a series of works , gives an answer to this question by expressing the multiplicity of each irreducible summand as a certain combinatorial rule. however, it is dicult to use in practice, and it is often desirable to have a more explicit and computationally cheaper way of decomposing a single kr module. if g is of classical type, there is a well known explicit formula called the domino removal rule. it is a polyhedral formula in the sense that the highest weight of an irreducible summand with non zero multiplicity is characterized as a lattice point in a suitable bounded polyhedron. even when g is of exceptional type, a polyhedral formula still seems to exist, but an irreducible summand with multiplicity greater than one may appear. such a formula with multiplicity remains largely conjectural , and furthermore, even a conjectural formula has not been written completely (for example, in type e, e, or f). in fact, the only known polyhedral formula with multiplicity is when g is of type g by chari and moura. since their method is rather specic to type g, it seems dicult to adapt it to other cases in general. in this paper, we propose a method to prove a polyhedral formula. the key objects in our approach are the coecients that appear when the characters of kr modules date: january , chul hee lee are written in some exponential form. they are essentially the residues at the poles of the generating function of the characters of kr modules. it turns out that it is possible to decompose a polyhedral formula into a nite list of identities involving these coecients; see (). our method seems quite appropriate for a computer aided mechanical approach. the diculty of the actual implementation, of course, varies according to the type of g. as our main objective in mind is of exceptional type, such a mechanical approach could be justied. after presenting the general strategy, we consider a special case when g is of type f for each node a of the dynkin diagram of g and m z, let us denote the corresponding kirillov reshetikhin module, as a uq(g)-module, by res w (a) m . we obtain a computer aided proof of the following polyhedral formula conjectured in : theorem . let g be of type f for every m z, the following holds : () res w () m = m j+j+j+jm j,j,j,jz p(j, j, j, j)l (j + j + j + j) , where p(j, j, j, j) = min ( + j, + m j j j j) (j + ), and (, , , ) = (, , , ). here we have used the same convention for enumerating the nodes of the dynkin diagram as in . this paper is organized as follows. in section we review the necessary background for our approach such as the q system, and linear recurrence relations satised by the characters of kr modules. in section , we explain the steps for proving a polyhedral formula for kr modules. in section , we follow the procedures described in section to give a proof of theorem (). ",
        "Subsections": [],
        "Groundtruth": "The Kirillov Reshetikhin (KR) modules, representing irreducible representations of the quantum affine algebra, are a key focus in understanding how they decompose into irreducible modules. Existing methods like the fermionic formula offer combinatorial rules but are challenging to apply practically. For classical Lie algebras, the Domino Removal Rule provides explicit decomposition, while for exceptional types, a polyhedral formula is conjectured. A new approach proposed in this paper involves coefficients from character expressions of KR modules to derive a polyhedral formula. The method is suitable for computer-aided implementation, particularly for exceptional types. A specific computer-aided proof of a polyhedral formula for type F Lie algebras is presented, showing how KR modules decompose into irreducible summands. The paper also details the necessary background, steps for proving polyhedral formulas, and provides a proof of the formulated theorem."
    },
    {
        "Section_Num": "2",
        "Section": "2. Background",
        "Text": "",
        "Subsections": [
            {
                "Section_Num": "notation",
                "Section": "notation",
                "Text": "e will use the following notation throughout the paper. g : simple lie algebra over c of rank r h : cartan subalgebra of g i = {, . . . , r} : index set for the dynkin diagram of g a, a i : simple root ha, a i : simple coroot a, a i : fundamental weight c = (cab)a,bi : cartan matrix with cab = b(ha) p = aiza : weight lattice p + p : set of dominant integral weights q : root lattice on polyhedral formula for kirillov reshetikhin modules = p ai a : weyl vector q : highest root + : set of positive roots h r := aira (, ) : h r h r r : r bilinear form induced from the killing form with (, ) = z : integral group ring of p (which is the same as the ring zji of laurent polynomials in ej) k := c(ej)ji : eld of rational functions in ej with coecients in c ta := (, )/(a, a) {, , } []a z ( q, a i) : coecients in the expansion = p ai[]aa sa : simple reection acting on h r by sa() = (ha)a w : weyl group generated by {sa : a i} w, p : isotropy subgroup of w xing wj, j i : standard parabolic subgroup of w generated by {sa : a j} l(), p + : irreducible highest weight representation of uq(g) (v ) z : character of a nite dimensional uq(g)-module v = v with weight spaces v, i.e. (v ) = p p(dim v)e o(), p : w orbit of ",
                "Subsections": [],
                "Groundtruth": "Throughout the paper, the following notation is used:  \n- g is a simple Lie algebra over C of rank r  \n- h is the Cartan subalgebra of g  \n- i = {1, ..., r} is the index set for the Dynkin diagram of g  \n- a, a_i represent simple roots  \n- h_a, a_i represent simple coroots  \n- a, a_i signify fundamental weights  \n- c is the Cartan matrix with (c_ab) representing the products of coroots  \n- p is the weight lattice, with p + p being the set of dominant integral weights  \n- q is the root lattice with a polyhedral formula for Kirillov-Reshetikhin modules  \n- ai a is the Weyl vector  \n- q+ denotes the set of positive roots  \n- h_r := (ai, ai) is a bilinear form induced from the killing form  \n- z is the integral group ring of p  \n- k is the field of rational functions with coefficients in C  \n- t_a represents a normalization factor  \n- []_a z (q, a_i) are coefficients in the expansion  \n- s_a is a simple reflection  \n- w is the Weyl group  \n- w, p is the isotropy subgroup of w  \n- fixing w_j, j_i signifies the standard parabolic subgroup  \n- l(), p_+ is an irreducible highest weight representation  \n- z denotes the character of a finite-dimensional uq(g)-module  \n- v = v with weight spaces denoted by v_i  \n- o(), p represents the w orbit of..."
            },
            {
                "Section_Num": "2_1",
                "Section": "2.1. Some properties of characters of KR modules",
                "Text": " kr modules form a family of irreducible nite dimensional representations of the quantum ane algebra uq(b g), where q c is not a root of unity. for every (a, m, u) i z c, there exists a corresponding kr module w (a) m (u). by restriction, we obtain a nite dimensional uq(g)-module res w (a) m (u), which can be denoted by res w (a) m since its isomorphism class does not depend on u, the spectral parameter, as a uq(g)-module. let q(a) m := (res w (a) m ). the q system () (q(a) m ) = q(a) m+q(a) m + y b:cab< cab y k= q(b) \u0004 cbamk cab \u0005, a i, m is a dierence equation that the characters of the kr modules satisfy. nakajima and hernandez proved the q characters of kr modules satisfy the t system from which we obtain the q system () by ignoring the spectral parameter. in , we studied a linear recurrence relation with constant coecients that the sequence (q(a) m ) m= satises. we can summarize its main properties in terms of its generating function q(a)(t) := p m= q(a) m tm as follows : theorem (). let g be a simple lie algebra which is not of type e or e for each a i, there exist w invariant nite subsets a and a of p with the following properties : chul hee lee (i) if we set d(a)(t) := q a( et) q a( etta), then () n(a)(t) := q(a)(t)d(a)(t) is a polynomial in t with coecients in z and deg n(a) < deg d(a). (ii) taa a = , where taa = {ta | a}. (iii) a a. let us x a and a as in the appendix of . when g is simply laced, a is simply the set of weights of the fundamental representation l(a). note that (ii) shows that d(a)(t) has only simple roots. from the partial fraction decomposition of q(a)(t) = n(a)(t)/d(a)(t), we deduce that for each (, , l) p c z> there exists c(q(a), , , l) k(ej/ta)ji such that () q(a) m = x (,,l) c(q(a), , , l)mem/l, m z, and it vanishes unless either (, , l) = (, , ) with a; or, (, , l) = (, , ta) with a and ta = due to the w symmetry of q(a) m , we have () w \u0000c(q(a), , , l) \u0001 = c(q(a), w(), , l), w w. let c(a) := c(q(a), , , ) for a and c(a) , := c(q(a), , , ta) for a and ta = we can rewrite () as () q(a) m = x a c(a) em + x a x :ta= c(a) ,mem/ta. for c(a) a we have an explicit product formula. theorem . for each a i, () c(a) a = q +( e)[]a . here, []a z denotes the coecient in the expansion = p ai[]aa. we call () the mukhin young formula, which is originally conjectured in . we note that, in general, coecients other than c(q(a), , , ) do not seem to admit an expression as compact as (). on polyhedral formula for kirillov reshetikhin modules ",
                "Subsections": [],
                "Groundtruth": "KR modules are irreducible finite dimensional representations of the quantum affine algebra. These modules correspond to a difference equation satisfied by their characters, known as the q system. The q characters also satisfy the t system. The text presents a linear recurrence relation for the sequence of q characters and describes their generating function. It introduces the Mukhin-Young formula, providing a product formula and explicit expression for certain coefficients. The properties of characters of KR modules are studied with respect to the quantum affine algebra."
            },
            {
                "Section_Num": "2_2",
                "Section": "2.2. fermionic formula",
                "Text": " the fermionic formula, proposed by kirillov and reshetikhin , concerns the decomposition of a tensor product of kirillov reshetikhin modules into irreducible uq(g)-modules. let ((a) m )ai,m be a family of non negative integers such that (a) m is zero for all but nitely many (a, m). consider () w = o (a,m) \u0000res w (a) m \u0001(a) m , a tensor product of kirillov reshetikhin modules, and its decomposition into irreducible uq(g)-representations () w = m p + m(w, )l(), m(w, ) z, where l() denotes an irreducible uq(g)-representation with highest weight . the fermionic formula provides an explicit combinatorial description of the multiplicity m(w, ) in terms of ((a) m )ai,m and . since this formula is somewhat complicated and not essentially used in this paper, we refer the reader, for example, to for its precise statement. ",
                "Subsections": [],
                "Groundtruth": "The fermionic formula, developed by Kirillov and Reshetikhin, deals with decomposing a tensor product of Kirillov Reshetikhin modules into irreducible uq(g)-modules. It involves a family of non-negative integers and provides a detailed description of the multiplicity in the decomposition process. The formula is rather complex and is not a central focus in the given text, so readers are directed to other sources for a precise statement of the formula."
            },
            {
                "Section_Num": "2_3",
                "Section": "2.3. polyhedral formula",
                "Text": " let = p ai caa, ca z> be the highest root of g. fix a i such that ca it is shown in (see also ) that there exist positive integers (bj)jja, and dominant integral weights (j)jja for some nite set ja such that res w (a) m = m xf (a) m l(x) where f (a) m = {(xj)jja | p jja bjxj = m, xj z}, and x = p jja xjj for each x f (a) m . when ca > , we still expect to have a similar formula, but now with multiplicity, of the form () res w (a) m ? = m xf (a) m p(x)l(x), where p is a piecewise step polynomial. a polyhedral formula for the decomposition of kr modules will mean a formula of the form (). the fermionic formula can be used to decompose res w (a) m for small individual ms, from which we can observe patterns and guess the form of (). and then, we need a separate argument to prove () since it is now a formula which is supposed to be true for all m z in the next section, we explain an approach for a proof of (). chul hee lee ",
                "Subsections": [],
                "Groundtruth": "\nThe section introduces the concept of a polyhedral formula for the decomposition of KR modules. It discusses the use of a fermionic formula to decompose certain components and infer the structure of the polyhedral formula. The polyhedral formula provides a decomposition formula for modules and is expected to hold true for all relevant values. Further discussion on the proof of the formula is outlined in the following section."
            }
        ],
        "Groundtruth": ""
    },
    {
        "Section_Num": "3",
        "Section": "3. framework for proving polyhedral formula",
        "Text": "let p (a) m denote the character of the right hand side of (), i.e., p (a) m = x xf (a) m p(x)(l(x)). here the letter p is chosen from the word polyhedron. when there is such a polyhedral formula, consider its generating function p(a)(t) := x m= p (a) m tm, which is expected to be a rational function in t in general. then, () is equivalent to () p(a)(t) = q(a)(t), an identity between two rational functions. we can state the steps necessary to prove () as follows : (i) prove that p(a) has at most simple poles, and the set of poles of p(a) is a subset of the set of poles of q(a). also make sure that the degree of the denominator of p(a) is greater than that of its numerator. (ii) prove the equality of the coecients () c(p(a), , , l) = c(q(a), , , l) when (, , l) belongs to one of the following cases : (, , l) = (, , ) with a p +; (, , l) = (, , ta) with a p + and ta = recall that we already know explicitly where the poles of q(a) are located, which are always simple. we can deduce from (i) that p (a) m tm can be written in the form () p (a) m = x (,,l) c(p(a), , , l)mem/l with c(p(a), , , l) k(ej/ta)ji, which vanishes unless the non vanishing conditions for c(q(a), , , l), stated after (), are satised. step (ii) is equivalent to showing that p(a) and q(a) have the same residues at their poles, which are simple at most. since both p(a) and q(a) are w invariant, the coecients follow the same w symmetry in (). hence, it is enough to consider weights in p + because every element of p has a unique element in p + in its w orbit. once p (a) m is explicitly given as in (), it is more or less straightforward to compute p(a) and c(p(a), , , l). for computing c(q(a), , , l), we can use the q system () along with previously known c(q(b), , , l) with b i such that cab < ; when there is a known polyhedral formula for b i, we can explicitly compute c(q(b), , , l). on polyhedral formula for kirillov reshetikhin modules suppose that we already have proved (i). then another way to nish the proof of p(a)(t) = q(a)(t) is by showing q(a) m = p (a) m for m = , , . . . , |a| + ta| a| since now we know that both sequences satisfy the same linear recurrence relation of order |a| + ta| a| although it is a purely mechanical task to check q(a) m = p (a) m for given m using the fermionic formula, we have found that it is still computationally challenging for m large. by considering c(p(a), , , l) and c(q(a), , , l), we localize the problem in the sense that we are looking at a single pole at a time, and thus obtain further simplications. in a nutshell, it is possible to check both (i) and (ii) algorithmically. in the next section, we follow this strategy to prove a conjectural polyhedral formula in type f, where we discuss some practical issues in our method in detail. remark . we know that c(q(a), , , l) is invariant under w from (). when we explicitly compute c(p(a), , , l), it is given as a sum over w; see () for an example. thus we can regard () as a summation formula over w for c(q(a), , , l). ",
        "Subsections": [],
        "Groundtruth": "A framework for proving polyhedral formulas involves defining the character p(a) as a rational function, expected to be a rational function in t in general. Steps to prove the equality between two rational functions involve verifying simple poles, comparing pole sets, and ensuring the degree of the denominator of p(a) is greater than its numerator. The process includes proving equal coefficients for specific cases and confirming residue equality at poles. Computational challenges arise in verifying equality for large values of m, but a systematic approach can simplify the process algorithmically."
    },
    {
        "Section_Num": "4",
        "Section": "4. Proof of Theorem ??",
        "Text": "",
        "Subsections": [
            {
                "Section_Num": "NA",
                "Section": "NA",
                "Text": " let g be a simple lie algebra of type f when a = or , there is a known polyhedral formula for res w (a) m . the formula for a = is given in () and will be used later. the main goal of this section is to prove theorem , namely, the polyhedral formula for a = for m z, let p () m be the character of the right hand side of () and p()(t) = p m= p () m tm. by following the strategy outlined in section , we will show that () q() m = p () m , that is, q()(t) = p()(t) as rational functions in t. before turning to proofs, we present a table for a and a from . since they are w invariant, they are given as a disjoint union of w orbits of elements of p + : a a p + a p + , , , , , , , , , + , . throughout the section, = p ww()(w)ew() denotes the weyl denominator and (, , , ) = (, , , ). we often need to explicitly deal with w or its subgroups. one may refer to for an algorithm to nd the weyl orbit of a weight or a minimal coset representative for a coset of a standard parabolic subgroup chul hee lee of a weyl group. the accompanying mathematica notebook le for some computer calculations is available at https://github.com/chlee-/kr polyhedral formula. . step ",
                "Subsections": [],
                "Groundtruth": "The text discusses a polyhedral formula for a simple Lie algebra of type f. The main goal is to prove a theorem regarding the formula for a specific case. The character of the formula is presented as a rational function. A table for specific cases is provided, and the section uses the Weyl denominator and Weyl orbits in its explanations. Algorithms for finding Weyl orbits and subgroup representatives are referred to. Additionally, a Mathematica notebook for computer calculations related to the polyhedral formula is made available."
            },
            {
                "Section_Num": "4_1",
                "Section": "4.1. Step ??",
                "Text": "(i). from (), it is clear that q()(t) can have only poles of order at most and they can only be found at t = e, let us nd the poles of p()(t). to write p()(t) explicitly, dene a sequence {am} m= by am = x j+j+j+jm j,j,j,jz p(j, j, j, j) xj xj xj xj , whose generating function is x m= amtm = ( t) ( tx) ( tx) ( tx) ( tx) ( tx) by combining this with the weyl character formula, p()(t) can be written as () ( t) x ww ()(w)ew() ( ew()t) ( ew()t) ( ew()t) ( ew()t) ( ew()t) at this point, it is not entirely clear whether p()(t) has only simple poles at t = e, or not. for example, p()(t) may have a double pole at t = e consider the partial fraction decomposition of a summand in () : () ( t) ( ew()t) ( ew()t) ( ew()t) ( ew()t) ( ew()t) = d() w; t + d() w; ew()t + d() w; ew()t + d() w; ew()t + e() w; ew()t + e() w; ew()t + e() w; ( ew()t), where d() w;, e() w; k, and e() w; and e() w; k are polynomials of degree at most they are uniquely determined by this form of decomposition. for example, () d() w; = ( ew()) ( ew()) ( ew()) ( ew()) because these expressions are long but easy to nd, we do not write them here; one can refer to the accompanying le for an explicit description. proposition . we have p()(t) = x ww ()(w)ew() d() w; t + d() w; ew()t + d() w; ew()t + d() w; ew()t ! . on polyhedral formula for kirillov reshetikhin modules proof. it is sucient to show that for {, , }, () x ww ()(w)ew()e() w; = we may use computers to verify this directly. below, we will explain how to reduce the amount of calculation to check (). while this reduction is not essential as long as we focus on type f whose weyl group is manageable in size, it might be useful for treating a similar vanishing sum over bigger groups in other types. for = = , w = w{,,}. the parabolic subgroup w{,} of w{,,} satises x ww{,} ()(w)ew()e() w; = which implies the vanishing of the sum over w{,,} since () can be written as x ww ()(w)w x ww{,} ()(w)ew()e() w; where w is the set of minimal coset representatives for cosets in w{,,}/w{,}. similarly, when = = , we have w = w{,,} and the following sum over the parabolic subgroup w{,} vanishes : x ww{,} ()(w)ew()e() w; = when = , we have not found any proper parabolic subgroup of w = w{,,}, over which the sum vanishes. however, if we let e() := x ww{,} ()(w)ew()e() w;, then the left hand side of () becomes x ww ()(w)w(e() ) where w is the set of minimal coset representatives for cosets in w{,,}/w{,}. the size of w is , and it is possible to partition this set into pairs of distinct elements so that the contribution from each pair to the above sum is zero. in other words, for each w w , there exists w w , w = w such that ()(w)w(e() ) + ()(w)w(e() ) = this proposition immediately implies the following : chul hee lee proposition . the rational function p()(t) has only simple poles, possibly at t = e, and no other poles. now we know that the poles of q()(t) and p()(t) can only appear at t = e, ",
                "Subsections": [],
                "Groundtruth": "The text discusses finding the poles of the function p()(t) and determining if they are only simple poles at t = e. Through calculations and decomposition, it is shown that p()(t) has only simple poles, potentially at t = e, and no other poles. The analysis involves mathematical expressions and proofs using the Weyl character formula, partial fraction decomposition, and the vanishing sum over parabolic subgroups. The proposition concludes that both q()(t) and p()(t) can only have poles at t = e."
            },
            {
                "Section_Num": "4_2",
                "Section": "4.2. Step ??",
                "Text": "(ii). it remains to carry out the second step of our strategy in section to prove (). as is empty, we have to show c(p(), , , ) = c(q(), , , ) for {, , , } p +. we rst explain how to compute both sides, and then check their equality. let us write c() = c(q(), , , ) and d() = c(p(), , , ). how to calculate c() . let us explain how to calculate c() . recall the q system rela- tion () q() m = (q() m ) q() mq() m+ for a = by rewriting this relation using (), we obtain an expression for c() in terms of c() , : () c() = x (,)s c() c() ( e), where s := {(, ) : = , + = }. to handle () explicitly, we need a way to compute c() and c() . and these are all we need to nd c() , because c() with non zero o() is given by c() w() = w(c() ). lemma . we have () c() = p ww()(w)ew()/( ew()) , and c() = / y + ( e)[] proof. note that c() is given by theorem , the mukhin young formula. to nd c() , we can exploit the known polyhedral formula from () q() m = m x k= (l(k)) . on polyhedral formula for kirillov reshetikhin modules by the weyl character formula, q()(t) = x ww ()(w)ew() ( t)( tew()) = x ww ()(w)ew() d() w; t + d() w; tew() ! , where d() w; = ew(), and d() w; = ew(). therefore, c() = p ww()(w)ew()d() w; . how to calculate d() . recall that we have p () m = x d() em. by proposition , we can write d() as () d() = p ww()(w)ew()d() w; . for a dominant weight , w is a standard parabolic subgroup of w. once we enumer- ate the elements of w, it is straightforward to compute d() . of course, it becomes computationally easier to manipulate () when w is a proper subgroup of w. in this sense, the most dicult case arises when = now we can compute both () and () and thus, are ready to check c() = d() for {, , , }. to use () we need s = {(, ) : = , + = }, as described below. the cases of {, , } do not bring much diculty, and a computer can easily simplify c() d() and return zero. we give further comments on the = case, which is the most dicult one. . = case. s = n (, ), (, ), (, ), (, ), (+, ), ( , + ), ( , + ), ( + , ), ( + , + ), ( + , + ) o . chul hee lee . = case. in this case, s = {(, ), ( , )}. thus () gives c() = c() c() ( e e). in fact, this identity is a special case of . . = case. s = n (, ), (, ), (, +), (+, ), ( , + ), ( + , ) o . . = case. our goal is to check whether c() d() is actually zero, but this calculation is not quite straightforward as before, since they are quite huge rational functions. () can be rewritten as c() = x o() c() c() ( e), and |o()| = and d() involves an alternating sum of orbits of () over the entire weyl group w and hence, it is obtained by adding |w| = rational functions in e, . . . , e it is slightly better to work with c() and d() to simplify their denominators. let us consider d() = x ww ()(w)ew()d() w; we can rewrite the above as () d() = x ww {,,} ()(w)w x ww{,,} ()(w)ew()d() w; where w {,,} denotes the set of minimal coset representatives of cosets in w/w{,,}. note that the size of w {,,} is let fw = ()(w)w x ww{,,} ()(w)ew()d() w; , w w {,,}. in our computer calculation, we further considered a partition of w {,,} into disjoint subsets w {,,} i , i = , . . . , , say, w {,,} = f i= w {,,} i . we can write () as d() = x i= x ww {,,} i fw . on polyhedral formula for kirillov reshetikhin modules finally, we start with c() , subtract (p ww {,,} i fw), and simplify the expression at each step i = , . . . , once we subtract every summand, the result becomes be zero, as we wanted. on our desktop computer with a ghz cpu and gb of ram, it took about seconds to complete this calculation. we note that the partition for w {,,} we used is simply found through many computer experiments to reduce the time required to complete the calculation, and may be hardly optimal. ",
                "Subsections": [],
                "Groundtruth": "The text discusses the computational steps involved in proving the equality between two complex functions, c() and d(). It involves calculating c() and d() based on known formulas and using them to verify the equality condition. The process includes manipulating rational functions, partitioning sets of coset representatives, and performing computations that require significant computational resources. The most challenging case involves checking the equality for a specific set {, , } and necessitates careful consideration and efficient computational strategies."
            }
        ],
        "Groundtruth": ""
    },
    {
        "Section_Num": "References",
        "Section": "References",
        "Text": "cha] v. chari, on the fermionic formula and the kirillov reshetikhin conjecture, internat. math. res. notices (), no. , v. chari and a. moura, the restricted kirillov reshetikhin modules for the current and twisted current algebras, comm. math. phys. (), no. , , kirillov reshetikhin modules associated to g, lie algebras, vertex operator alge- bras and their applications, contemp. math., vol. , amer. math. soc., providence, ri, , pp. p. di francesco and r. kedem, proof of the combinatorial kirillov reshetikhin conjecture, internat. math. res. notices (), no. , art. id rnn, d. hernandez, the kirillov reshetikhin conjecture and solutions of t -systems, j. reine angew. math. (), g. hatayama, a. kuniba, m. okado, t. takagi, and y. yamada, remarks on fermionic formula, recent developments in quantum ane algebras and related topics (raleigh, nc, ), contemp. math., vol. , amer. math. soc., providence, ri, , pp. a. n. kirillov and n. yu. reshetikhin, representations of yangians and multiplicities of the inclusion of the irreducible components of the tensor product of representations of simple lie algebras, zap. nauchn. sem. leningrad. otdel. mat. inst. steklov. (lomi) (), no. anal. teor. chisel i teor. funktsi . , , c.-h. lee, linear recurrence relations in q systems via lattice points in polyhedra, trans- form. groups (), (to appear). , product formula for the limits of normalized characters of kirillov- reshetikhin modules, arxiv e prints (), arxiv:. e. mukhin and c. a. s. young, anization of category o for quantum groups, trans. amer. math. soc. (), no. , h. nakajima, t analogs of q characters of kirillov reshetikhin modules of quantum ane algebras, represent. theory (), (electronic). d. m. snow, weyl group orbits, acm trans. math. software (), no. , school of mathematics, korea institute for advanced study, seoul -, korea e mail address: chlee@kias.re.kr ",
        "Subsections": [],
        "Groundtruth": "The text discusses various research papers related to the fermionic formula and the Kirillov-Reshetikhin conjecture in mathematics, specifically focusing on modules associated to Lie algebras, vertex operator algebras, and quantum affine algebras. It includes papers that provide proofs and solutions related to the conjecture, as well as remarks on the fermionic formula and the organization of category O for quantum groups."
    },
    {
        "Section_Num": "NA",
        "Section": "NA",
        "Text": "arxiv:v jan on polyhedral formula for kirillov reshetikhin modules chul hee lee abstract. we propose a method to prove a polyhedral branching formula for kirillov- reshetikhin modules over a quantum ane algebra. when the underlying simple lie algebra is of exceptional type, such a formula remains mostly conjectural. we con- vert a polyhedral formula into an identity between two rational functions of a single variable with only simple poles at known locations. it is then sucient to check the equalities of the residues at those poles, which are explicitly computable quanti- ties. by following this strategy, we obtain a computer assisted proof of a conjectural polyhedral formula in type f introduction ",
        "Subsections": [],
        "Groundtruth": "The text proposes a method to prove a polyhedral branching formula for Kirillov-Reshetikhin modules over a quantum affine algebra. This formula, which is mostly conjectural for exceptional type simple Lie algebras, is converted into an identity between two rational functions of a single variable with simple poles at known locations. By checking the residues at these poles, which are explicitly computable, the text presents a computer-assisted proof of the conjectural polyhedral formula in Type F."
    },
    {
        "Section_Num": "Top-Assisted",
        "Section": "Top-Assisted Di-Higgs boson Production Motivated by Baryogenesis",
        "Text": "",
        "Subsections": [
            {
                "Section_Num": "NA",
                "Section": "NA",
                "Text": "wei shu hou, masaya kohda and tanmoy modak department of physics, national taiwan university, taipei , taiwan we study top assisted di higgs production via cg th thh, where h is the gev scalar boson, and h is the cp even heavy higgs. the context is the two higgs doublet model without a z symmetry, where the extra yukawa coupling tc generates th production, with the extra top yukawa tt to avoid gg h constraints. we nd that discovery is possible for mh around gev or so at the lhc, but would need nite h h mixing angle cos to allow for nite hhh coupling, and tc also needs to be not too small. a sizable tc could drive electroweak baryogenesis, which further motivates the search. i. introduction ",
                "Subsections": [],
                "Groundtruth": "The text discusses a study on top-assisted di Higgs production in the context of the Two Higgs Doublet Model without a Z symmetry. The researchers investigate the production of th thH, where H is a CP-even heavy Higgs and h is a GeV scalar boson. The study shows that discovery of this process is feasible at the LHC for a certain range of Higgs masses, provided that certain conditions are met, such as a non-negligible mixing angle cos and a sizable extra Yukawa coupling. The presence of a significant coupling could potentially drive electroweak baryogenesis, providing further motivation for this research."
            },
            {
                "Section_Num": "I",
                "Section": "I Introduction",
                "Text": "the highlight at the large hadron collider (lhc) so far is the discovery of the gev scalar boson h in , which resembles rather closely the higgs boson of the standard model (sm). to improve our understand- ing of the higgs potential, a key goal at the lhc is to search for di higgs, or pp hh, production. the pro- gram is rather challenging, as hh production in sm is rather suppressed. the atlas and cms experiments have already conducted searches for reso- nant and non resonant di higgs production, but there is little expectation that the sm process can be observed even at the high luminosity lhc (hl lhc). in this paper we explore a novel possibility with new physics, that of resonant hh production in association with a top quark. the context is a two higgs doublet model (hdm) with extra yukawa couplings, i.e. without a discrete z symmetry to impose the glashow weinberg nfc (natural flavor conservation) condition to forbid avor- changing neutral higgs (fcnh) couplings. note that the usual z symmetry eliminates all extra yukawa cou- plings. two processes can be operative that feed di higgs production. the existence of extra diagonal yukawa cou- pling tt of the exotic cp even neutral higgs boson h means that one could have gg h hh production through triangle top quark loop. a second process de- pends on the fcnh tch coupling, tc, whereby one can have cg th thh (conjugate process implied). in a previous paper , we considered turning tt o, so the rst process is subdued, and one is left with the second process: di higgs and top associated production. in this paper we focus on this process, exploiting the extra top quark to investigate possible prospects at the lhc. we stress that hdm without extra z symmetry could account for baryon asymmetry of the universe (bau), via electroweak baryogenesis (ewbg). the lead- ing mechanism is via tt and is rather robust. however, in exploring the prospect for a lighter pseudoscalar a boson around gev, in face of direct search bounds, we opted to turn ott, noting that tc would still oer an alternative mechanism for ewbg, hence is interesting in itself. thus, the tc driven cg th thh process studied here is a companion to the cg ta tt c process that bears a rather intriguing signature. we nd that thh discovery is possible at the hl lhc for relatively light h, where the associated top quark gives extra handle on background reduction. however, a relatively large hhh coupling would be needed, hence the prospect cannot be said as very likely, but it is not negligible. in the following, we start with the formalism in sec. ii, then the collider signatures in sec. iii and end with some discussions in sec. iv. ",
                "Subsections": [],
                "Groundtruth": "The Large Hadron Collider (LHC) has made significant discoveries, including the GeV scalar boson H, similar to the Higgs boson in the Standard Model. One key goal at the LHC is to explore di-Higgs production, despite its rarity in the Standard Model. In this study, a new approach involving resonant di-Higgs production in association with a top quark is explored within the context of a Two Higgs Doublet Model. The experiment focuses on the potential of this process at the LHC, considering its implications for baryon asymmetry and electroweak baryogenesis. Results suggest that discovery is possible at the High Luminosity LHC, albeit with a need for a relatively large HHH coupling. The study delves into formalism, collider signatures, and discussions surrounding this novel approach to exploring physics beyond the Standard Model."
            },
            {
                "Section_Num": "II",
                "Section": "II Formalism",
                "Text": "the cp even scalars h, h and cp odd scalar a couple to fermions by x f =u,d,l fil \u0014\u0000f ijs + f ijc \u0001 h + \u0000f ijc + f ijs \u0001 h i sgn(qf )f ija \u0015 fjr + h.c., () where i, j = , , are generation indices that are summed over, f ij = ( mf i /v) ij (with v gev) and f are real diagonal and complex matrices, respectively. with shorthand c = cos , s = sin , the mixing angle is usually written as in type ii hdm notation. however, as we advocate no z sym- metry and there exists a second set of yukawa couplings f ij, we prefer the notation of ref. , since tan is ill dened. the fcnh couplings of interest for tch are u ct and u tc. b physics sets stringent limits on ct , while tc is only mildly constrained , de- pending on mh+. in our study, we set ct = and take |tc| < the most general cp conserving two higgs doublet potential is given in higgs basis as v (, ) = || + || ( + h.c + || + || + |||| + || + \u0014 () + \u0000|| + ||\u0001 + h.c. \u0015 , () where v arises from the doublet via = v, while = (hence > ), is are quartic couplings, arxiv:v apr again in the notation of ref. . a second minimization condition, = v, removes and reduces the to- tal number of parameters to nine . the mixing angle between the cp even bosons satises the relations c = v m h m h m h , sin = v m h m h , () which, for c small but not innitesimal, one has c ||v/(m h m h). this is approximate alignment , i.e. small c values can be attained without requiring to be small. but in the alignment limit, c , either has to vanish (and m h v), or else one has decoupling , i.e. m h/v we are interested in the hhh coupling, which is the coecient of the hhhhh term derivable from eq. (), hhh = v \u0014 cs + c(c ) + s( c ) + sc \u0015 , () with = + + it reduces further to hhh c v \u0014 m h v + sgn(s)c + o(c ) \u0015 , () for small c, so hhh as c to enhance cg th thh, sizable hhh is needed, and < may be preferred so the rst two terms add up. however, hhh could still be sizable if m h/v > either way, a large || with proper sign for cs would help. the quartic couplings , can be expressed in terms of mh, ma, mh, mh, , all normalized to v, as well as the mixing angle : = m hs + m hc v , () = (m h ) v , () = m hc + m hs m h + m a v , () = m hs + m hc m a v , () = (m h m h)(s)c v , () but and are not related to masses, nor the mix- ing angle . thus, we take v, , mh, ma, mh, mh, , and as the phenomenological parameters. to save computation time, we randomly generate these pa- rameters in the following ranges: gev, mh gev, mh gev, , , and values that satisfy c , with mh = gev. we choose two dierent scenarios for ma. in the main scenario, we generate ma gev, with mh < ma, mh. in the second scenario we take |ma mh| < , where the choice of is discussed later in the section. we explore up to mh = gev because, while hhh and b(h hh) increases with mh, the discovery potential for cg th thh suers the drop in parton luminosities for heavier mh. the dynamical parameters in the higgs potential, eq. (), need to satisfy perturbativity, tree level uni- tarity and positivity conditions, for which we utilize hdmc . hdmc uses the input parameters mh and in higgs basis, and with v implicit. we identify with further, we conservatively de- mand all |i| , while > is required by the poten- tial positivity, in addition to more involved conditions for other couplings. to match the convention of hdmc, we take / / we also need to impose the stringent oblique t param- eter constraint, which constrains the scalar masses mh, ma and mh , and hence is. we apply the t parameter constraint on the points that passed hdmc, using the expression given in ref. . the - nal scanned points within error of t parameter are plotted in fig. the upper panel is for mh < mh, ma, such that h az, hw decays are disallowed, which in turn enhances h hh branching ratio. as ex- pected, the upper range for hhh mildly increases as mh becomes heavier, but vanishes with c the fcnh coupling tc also receives constraint from cms four top search through the cg th tt c process, which is proportional to |stc| if we take ct = however, if a and h are mass and width degen- erate, the processes cg ta tt c and cg th tt c cancel each other exactly , resulting in potentially much weaker constraint on tc, which can in principle give rise to larger thh production. the lower panels of fig. are for this scenario of nearly degenerate a and h, where we assume |ma mh| < , with = gev for illustration. the dependence of hhh on mh and c is similar as in mh < mh, ma case. we note that in the left panels, i.e. hhh vs mh, we have drawn a line at hhh gev, to illustrate that hhh can be sizable over a nite parameter region. ",
                "Subsections": [],
                "Groundtruth": "The formalism section discusses the coupling of scalar particles to fermions using complex matrices and introduces the notation for Yukawa couplings. It presents the general two Higgs doublet potential and minimization conditions to reduce the number of parameters. The alignment of CP-even bosons is discussed, along with the Higgs self-coupling terms. The importance of the HHH coupling term is highlighted, as well as the quartic couplings expressed in terms of masses and mixing angles. Random parameter generation is explained to explore different scenarios, with considerations for perturbativity, unitarity, and positivity conditions. The impact of scalar masses on potential discovery potential is analyzed, and constraints from the t-parameter and four-top searches are discussed. The section illustrates the dependence of the HHH coupling on masses and mixing angles and explores scenarios of nearly degenerate scalar masses with potential implications for HHH production."
            },
            {
                "Section_Num": "III",
                "Section": "III Collider signature",
                "Text": "the discovery potential of the cg th thh (con- jugate process implied) depends on the hh decay nal states. in this paper, we primarily focus on t b+ (= e, ) with both h bosons decaying via h b b, giv- ing rise to ve b jets, one lepton and missing transverse energy (emiss t ) signature. we do not look for hadronic decay of t due to qcd multi jet backgrounds, as also discussed in . in general, hh b b and hh b b modes are suppressed. but hh ww b b decay could provide some sensitivity, which we discuss towards the end of this section. we set all ij = except tc for simplicity throughout this section. due to the presence of non zero c, the fig. the hhh vs mh and c plots for the scan points that pass perturbativity, tree level unitarity and positivity through hdmc, where |i| < is maintained. the t parameter constraint is also imposed. upper panels are for mh < ma, mh, and lower panels are for |ma mh| < gev. see text for detailed explanation. bp mh ma mh c s |hhh| v (gev) (gev) (gev) (gev) a b c table i. parameter values for the six benchmark points of table i. see text for details. branching ratios of h will be modied compared with sm, albeit in minor way. since we set all ij = except tc and assume |c| < , the branching ratios remain practically the same. in the following, we assume all branching ratios of h are sm like for simplicity. to illustrate the discovery potential of cg th thh, we choose six benchmark points (bp) from fig. with large |hhh| values, which are summarized in ta- ble i. the rst three, bp, bp and bp, are for the mh < mh, ma scenario, while the other three, bpa, bpb and bpc, are for |ma mh| < gev scenario. the values of mh are chosen for mh < mh < gev, mh gev, and mh > mt (above the t t threshold), respectively. all six benchmark points are for < , in accordance with the discussions in the preceding section to achieve large |hhh|. the cg th thh process depends also on tc. for sizable c, the available parameter space for tc is con- strained by the b(t ch) measurement. the latest at- las % cl upper limit (with tev fb data) is b(t ch) < . using this limit and our c value, we nd the upper limit on tc = , applicable to all six benchmark points. we nd the upper limit on tc for bp, bp and bp to be , , respec- tively. the bpa, bpb and bpc benchmark points were chosen such that the constraint from ref. becomes much weaker due to cancellation between cg ta tt c and cg th tt c . however, besides the afore- bp tc tc hh ww zz a b c table ii. h decay branching ratios for the benchmark points. mentioned t ch constraint, tc can still be constrained by bs,d mixing and b(b xs), where tc enters via charm loop through h+ coupling . a reinter- pretation of the result from ref. , nds |tc| for mh = gev . in our analysis we choose tc = for all six benchmark points, where the h decay branching ratios are given in table ii we remark that the fcnh tuh coupling tu can also induce top assisted di higgs via ug th thh, and our analysis can be extended to the case where all ij = ex- cept tu (see also ref. ). while the atlas % cl upper limit b(t uh) < is not much dif- ferent from the t ch case, the cms four top search would give a stronger limit on tu than the tc case. the latter is because the relevant process qg th tt q (q = u, c) is enhanced by the parton distribution func- tion (pdf) of up quark while the signal region does not dierentiate u and c. similarly and more eciently, the t channel scalar exchange process qq tt via tq is en- hanced by up pdf; hence, the atlas same sign top search may provide a signicant constraint in con- trast to the tc case . despite stronger constraints on tu, the discovery potential of ug th thh would be balanced to some extent by the similar up pdf en- hancement in comparison with the tc case; but, we do not expect improvement in the signal signicance for bp, bp and bp by contrast, for an equivalent of bpa, bpb or bpc, ug ta tt u eectively cancels ug th tt u, relaxing the four top search limit. the same is true for the atlas qq tt limit. in such a case, under the t uh constraint, tu may be as large as the tc case; hence, we expect a better discovery potential of ug th thh, boosted by up pdf. to investigate the discovery potential of top assisted di higgs production at the lhc, we study pp th + x thh + x with both h decaying to b b, while t b+. the dominant backgrounds are t t+jets, single- top, t th, t, t tw and t tz, while tzj, dy+jets, w+jets and twh are subdominant. we do not include back- grounds from non prompt and fake sources, as these are not properly modeled in monte carlo simulations and require data to estimate. we generate signal and back- ground event samples at lo, utilizing monte carlo event generator madgraph amc@nlo with default pdf set nnlo for pp collisions at s = tev, inter- faced with pythia for showering and hadroniza- tion, and adopt mlm matching scheme for matrix element and parton shower merging. the event sam- ples are then fed into delphes . for detector ef- fects (atlas based). the eective lagrangian is imple- mented using feynrules . the t t+jets background cross section is normalized to note that for bp and bpc the coupling tc induces h t t decay. however, for both of the benchmark points b(h t t) and not displayed in table ii. ut is tightly constrained by bd mixing and b d . bp signal total bkg. signicance (fb) (fb) () fb () () () a () b () c () table iii. signal and total background cross sections after selection cuts for the bprocess for the benchmark points of table i, where the last column gives the signicance for () fb integrated luminosity. bp t t single t th t t tw t tz others top (fb) (fb) (fb) (fb) (fb) (fb) (fb) a b c table iv. cross sections for dierent background contribu- tions after selection cuts at s = tev. the nnlo ones by a factor . the lo wt com- ponent of the single top cross section is normalized to nlo by a factor , while t- and s channels by factors and , respectively . the t, t th, t tw, t tz cross sections at lo are adjusted to the nlo ones by factors , , , . the dy+jets background is normalized to nnlo cross sec- tions by factor . the twh and w+jets back- ground are kept at lo. the correction factors for conju- gate processes are assumed to be the same for simplicity. note that we do not include correction factor for the lo signal cross sections. to distinguish signal from background, we apply the event selection criteria as follows. each event should con- tain one lepton, at least ve jets, out of which at least four are b tagged (denoted as b). this reduction in the required number of b jets, from ve (one from top and four from the h decays) to four , is in consid- eration of the nite b tagging eciency. the transverse momentum (pt ) of the lepton should be > gev, while pt > gev for all ve jets. the pseudo rapidity () of lepton and all jets should be || < . we reconstruct jets by anti kt algorithm with radius parameter r = . the minimum separation (r) between any pair of jets, or between the lepton and any jet, should be > . the emiss t is required to be > gev. in order to reduce backgrounds further, we construct all possible mbb combinations from the four leading b- jets, and demand the two mbb pairs that are closest to mh should lie within gev mbb gev. finally, we demand the invariant mass of the four leading b jets (mb) to be within |mh mb| < gev. note that in our exploratory study, we have not optimized the mb cut for each of the benchmark points out of simplicity. we adopt the pt and dependent b tagging eciency and c- and light jet misidentication eciencies of delphes. the signal and total background cross sections after se- lection cuts are summarized in table iii, while individual components of backgrounds are given in in table iv. we estimate the statistical signicance given in ta- ble iii by use of z = p , where s and b are the number of signal and background events after selection cuts. we nd that, with fb data, the signicance can reach above for bp and bpa, for bp and bpb, but only for bp and bpc. with fb at the hl lhc, the sig- nicance can reach beyond for bp and bpa, about for bp and bpb, and just below for bp and bpc. the signicance depend heavily on the choice of hhh and tc. to get a feeling, we rescaled the signif- icance of the bps by hhh = gev (denoted by red dashed line in fig. ) with c and tc xed as in table i and table ii, respectively. we nd is possible for bp and bpa, while for bp and bpb. the signicance is below for both bp and bpc. note that hhh = gev is possible even for lower values of c. a lower c allows larger tc for bpa, bpb and bpc. take c = , for example, tc = is allowed, where one can achieve , and respectively for bpa, bpb and bpc with fb though this is not as good as those shown in table iii, it illustrates the chance for nding some signal for lower hhh values, but compensated by gains in tc. bp, bp, bp do not have this feature as discussed earlier. in general, discovery is possible for gevmh gev with hhh = gev for |ma mh| < gev, while signicance drops for mh < mh, ma scenario. as the parton luminosities falter away, the signicance drops rapidly if mh gev for both scenarios. before closing, let us mention briey the prospect for pp th thh where t b+, but one h decays to w +w and the other to b b (conjugate process im- plied). assuming the w and w decay leptonically, one has bplus emiss t (denoted as b) signature. we nd that discovery cannot be attained for any of the six benchmarks at the hl lhc, but and are possible for bp and bp, reaching and signicance for bpa and bpb, respectively. the signi- cance for bp is , while for bpc. here we fol- low the same cut based analysis as described in ref. for the bprocess, with the additional requirement of gev < mbb < gev. sensitivity is poor above mh gev, but if one has non zero tt, the sensi- tivity to cg thh is lost for mh > mt. this, how- ever, opens up the cg th tt t triple top process, which also has bsignature but without the gev < mbb < gev cut, which hl lhc can actually cover . indeed, non zero tt motivates the conven- tional gg h t t search or gg ht t t tt t i.e. the four top search. the former process suers from large interference with the overwhelming gg t t background, however a recent search by atlas found some sensitivity . it should be clear, however, that pp th + x thh in bcan provide a supporting role in the top assisted di higgs program at the hl lhc. ",
                "Subsections": [],
                "Groundtruth": "The text discusses the discovery potential of the cg th thh process, focusing on t b+ signature with both h bosons decaying via h b b, resulting in ve b jets, one lepton, and missing transverse energy signature. The hh b b and hh b b decay modes are generally suppressed, with sensitivity potentially found in the hh ww b b decay. Benchmark points are chosen with large |hhh| values for analysis, considering constraints on tc from b(t ch) measurement and exploring the impact of coupling parameters on the discovery potential. Event selection criteria are applied to distinguish signal from dominant backgrounds such as t t+jets, single-top, and others. The statistical significance of the process is evaluated for different benchmark points, showing potential for discovery with varying values of hhh and tc. Exploration of the pp th thh process with one h decaying to w+ w and the other to b b is also discussed, indicating different levels of significance for various benchmark points at the HL-LHC."
            },
            {
                "Section_Num": "IV",
                "Section": "IV Discussion and summary",
                "Text": "the hdm without nfc allows resonant di higgs pro- duction via cg th thh process. the process can be searched for at the lhc via pp th + x thh + x, followed by both h decays to b b and t b+. if all other ij = , this process can be discovered at hl lhc in the mass range gev mh gev if tc and hhh gev. the other decay modes such as hh b b, hh b b are suppressed. furthermore, pp th + x thh + x with hh w +w b b with t b+could be sensitive. the signicances can be as large as as depending on the masses of h, a and h. however, both processes could be preceded by pp th + x tt c, unless h, a are degenerate in mass and width. in such scenarios, non zero tt helps via cg th tt t . in general, presence of other yukawas reduce the h hh branching ratios, making discovery of top assisted di higgs less likely. the cross section for cg th thh vanishes as c approaches zero, and the signature requires c . if larger |i| values are allowed beyond , hhh can be enhanced even for smaller c. non zero c would also induce cg th tw +w and cg th tzz. we nd the signicances of the former process lie just below for all the bench- mark points with full hl lhc dataset. however, for xed value of c and tc a smaller hhh enhances the signature for cg th tw +w through enhanced b(h w +w ). due to smaller b(z ), we do not nd cg th tzz to be promising for any of the benchmark points. in summary, we have explored associated th thh production at the lhc via cg th thh, where pro- duction involves the extra yukawa coupling tc, and h hh decay needs a nite hh mixing angle cos = as well as o() extra higgs quartic couplings. we nd non- negligible discovery potential at hl lhc for mh gev. considering that hh production within sm is not quite hopeful at the hl lhc, this is an interesting result. furthermore, a discovery might shed light on strongly rst order electroweak phase transition. if evidence is found, not only one would have discovered new physics induced di higgs production, but together with the com- panion same sign top signal from cg th tt c, one would be probing the tc driven electroweak baryogene- sis scenario provided by this two higgs doublet model, as well as starting to probe the associated higgs potential. ",
                "Subsections": [],
                "Groundtruth": "The text discusses resonant di-Higgs production without NFC, highlighting the process of cg th -> thh. The process can be searched for at the LHC via pp th + x thh + x, with the possibility of discovery at HL-LHC in the mass range of around 500 GeV, under specific conditions. Other decay modes are suppressed, and sensitivity can be achieved through processes like pp th + x thh + x with hh -> w+w and bb, along with tb+. Non-zero tt processes help in certain scenarios, while the cross-section for cg th -> thh is dependent on specific parameters. The study suggests non-negligible discovery potential at HL-LHC for certain mass ranges, which could shed light on new physics and electroweak phase transitions."
            },
            {
                "Section_Num": "Acknowledgments",
                "Section": " Acknowledgments",
                "Text": "cknowledgments we thank k.-f. chen and y. chao for fruitful discussions. this research is supported by grants most --m---my, --m- -, and --m-- g. aad et al. , phys. lett. b , (); s. chatrchyan et al. , ibid. b , (). g. aad et al. , jhep , (). m. grazzini, g. heinrich, s. jones, s. kallweit, m. kerner, j.m. lindert and j. mazzitelli, jhep , (). lhc higgs cross section hh sub group, https: //twiki.cern.ch/twiki/bin/view/lhcphysics/ lhchxswghh#lhc_higgs_cross_section_hh_sub_g. m. aaboud et al. [atlas collaboration], arxiv: . m. aaboud et al. , jhep , (). m. aaboud et al. , eur. phys. j. c , (). a.m. sirunyan et al. , phys. lett. b , (). a.m. sirunyan et al. , jhep , (). a.m. sirunyan et al. , phys. lett. b , (). cms collaboration, cms pas hig-- s.l. glashow, s. weinberg, phys. rev. d , (). w.-s. hou, m. kohda, t. modak, phys. lett. b , (). k. fuyuto, w.-s. hou, e. senaha, phys. lett. b , (). see, e.g., s. davidson and h.e. haber, phys. rev. d , (). b. altunkaynak, w.-s. hou, c. kao, m. kohda and b. mccoy, phys. lett. b , (). w.-s. hou and m. kikuchi, eur. phys. lett. , (). a. crivellin, a. kokulu and c. greub, phys. rev. d , (). j.f. gunion and h.e. haber, phys. rev. d , (). d. eriksson, j. rathsman and o. stal, comput. phys. commun. , (). m.e. peskin and t. takeuchi, phys. rev. d , (). c.d. froggatt, r.g. moorhouse and i.g. knowles, phys. rev. d , (). h.e. haber and o. st al, eur. phys. j. c , (). m. baak and r. kogler, arxiv: . a.m. sirunyan et al. , eur. phys. j. c , (). m. kohda, t. modak and w.-s. hou, phys. lett. b , (). m. buschmann, j. kopp, j. liu and x.-p. wang, jhep , (). m. aaboud et al. [atlas collaboration], arxiv: . g. aad et al. , jhep , (). j. alwall et al., jhep , (). r.d. ball et al. , nucl. phys. b , (). t. sj ostrand, s. mrenna and p. skands, jhep , (). j. alwall et al., eur. phys. j. c , (). j. de favereau et al. , jhep , (). a. alloul, n.d. christensen, c. degrande, c. duhr and b. fuks, comput. phys. commun. , (). atlas cms recommended t t cross section predictions: https://twiki.cern.ch/twiki/bin/view/lhcphysics/ ttbarnnlo. n. kidonakis, phys. rev. d , (). atlas cms recommended predictions for single- top cross sections using the hathor v program https://twiki.cern.ch/twiki/bin/view/lhcphysics/ singletoprefxsec. sm higgs production cross sections at s = tev: https://twiki.cern.ch/twiki/bin/view/lhcphysics/ cernyellowreportpageattev j.m. campbell and r.k. ellis, jhep , (). j. campbell, r.k. ellis and r. r ontsch, phys. rev. d , (). y. li and f. petriello, phys. rev. d , (). w.-s. hou, m. kohda and t. modak, phys. rev. d , (). g. cowan, k. cranmer, e. gross and o. vitells, eur. phys. j. c , (). n. craig, j. hajer, y.-y. li, t. liu and h. zhang, jhep , (). for a recent reference, see m. carena and z. liu, jhep , (), and ",
                "Subsections": [],
                "Groundtruth": "The Acknowledgments section extends thanks to K.-F. Chen and Y. Chao for valuable discussions. The research is backed by multiple grants and references various publications and collaborations in the field of high-energy physics. Additionally, recommendations and predictions for specific cross sections and production rates are highlighted in the text."
            },
            {
                "Section_Num": "References",
                "Section": " References",
                "Text": "herein. m. aaboud et al. , phys. rev. lett. , (). ",
                "Subsections": [],
                "Groundtruth": "The text refers to a paper by M. Aaboud and colleagues published in the journal Physical Review Letters."
            }
        ],
        "Groundtruth": ""
    },
    {
        "Section_Num": "1",
        "Section": "1 Introduction",
        "Text": "numerical integration that preserves at least one of geometric properties of a given dynamical system has attracted much attention in these years . as suggested by kang feng , it is natural to look forward to those discrete systems which preserve as much as possible the intrinsic properties of the continuous system this is a truly ingenious idea for devising good integrators to properly simulate the evolution of various dynamical systems with geometric features. it is evidenced that numerical methods with such a special purpose can not only perform a more accurate long time integration than those traditional methods without any geometric feature preservation, but also produce an improved qualitative behavior . such type of methods, generally associated with the terminology geometric integration, are distinguished by the geometric properties they inherit, including symplectic methods for hamiltonian systems, symmetric methods for reversible systems, volume preserving methods for divergence free systems, invariant preserving methods for conservative systems, multi symplectic methods for hamiltonian partial dierential equations etc. for more details, we refer the interested readers to and references therein. reversible systems and reversible maps are of interest in both aspects of theoretical study and numerical simulation for many dierential equations . let be an invertible linear transfor- corresponding author. email addresses: tangws@lsec.cc.ac.cn (wensheng tang), jjzhang@outlook.com (jingjing zhang) preprint submitted to elsevier june , arxiv:v jun mation in the phase space of a rst order system given by z = f(z), then the system is called -reversible if f(z) = f(z), for z, and a map (z) is called -reversible if = . particularly, it is shown in that all second order systems with the form z = f(z) are reversible as they can be transformed into reversible rst order systems. in addition, notice that the exact ow of a reversible system is a reversible map, it is therefore natural to nd a numerical method h, which is better referred to as a reversibility preserving integrator, such that it is also a reversible map (i.e., h = h ). it is known that a number of symmetric integrators automatically possess this property, e.g., all symmetric runge kutta (rk) methods, some partitioned runge- kutta (prk) methods for special partitioned systems, some composition and splitting methods, and standard projection methods for dierential equations on special manifolds (see , page ). to be specic, we quote the following result from . theorem . a runge kutta method or a runge kutta nystr om (rkn) method is reversible iit is symmetric. thanks to the property of reversibility preservation, symmetric integrators often have an ex- cellent long time numerical behavior than those non symmetric integrators for reversible systems . so far, a wide variety of eective symmetric integrators have been proposed (see [, , , , , , ] and references therein). in the context of geometric integration, the greatest interest has been given to the develop- ment of symplectic integrators for solving hamiltonian systems over the last decades . however, if the hamiltonian h(p, q) satises h(p, q) = h(p, q), then the system is reversible with respect to the linear transformation : (p, q) (p, q). particularly, a well known class of separable hamiltonian systems determined by the hamiltonian h(p, q) = pt mp + u(q) happens to be such type of reversible systems. therefore, it makes sense for devising a numerical method that preserves symplecticity and reversibility at the same time, and fortunately, this has been shown to be an attainable goal (see and references therein). besides, a numerical method which is energy preserving and reversibility preserving can also be of interest . in recent years, numerical methods with innitely many stages including continuous stage runge kutta (csrk) methods, continuous stage partitioned runge kutta (csprk) methods and continuous stage runge kutta nystr om (csrkn) methods are presented and discussed by several authors, see . they can be viewed as the natural generalizations of numerical methods with nite stages (e.g., classical rk methods). it is shown in that by using continuous stage methods many classical rk, prk and rkn methods of arbitrary order can be derived, without resort to solving the tedious nonlin- ear algebraic equations (associated with order conditions) in terms of many unknown coecients. if the system is non autonomous, we can introduce an extra equation namely t = to rewrite the original system as an autonomous system. the construction of continuous stage methods seems much easier than that of those traditional methods with nite stages, as the associated butcher coecients are continuous or smooth functions and hence they can be treated by using some analytical tools . moreover, as presented in , numerical methods serving some special purpose including symplecticity preserving methods for hamiltonian systems, sym- metric methods for reversible systems, energy preserving methods for conservative systems can also be established within this new framework. besides, a well known negative result we have to mention here is that no rk methods is energy preserving for general non polynomial hamilto- nian systems , in contrast to this, energy preserving csrk methods can be easily constructed . in addition, as presented in , some galerkin variational methods can be interpreted as continuous stage (p)rk methods, but they can not be completely understood in the classical (p)rk framework. therefore, continuous stage methods have granted us a new insight for numerical integration of dierential equations and some subjects in this new area need to be investigated. since symmetric integrators possess important theoretical and real values in numerical ordinary dierential equations , we are concerned with the development of new symmetric integrators for solving second order ordinary dierential equations (odes). the construction of such methods in this paper is on the basis of the notion of csrkn methods and heavily relies on the legendre polynomial expansion technique. furthermore, by using gaussian and lobatto quadrature formulas we show that new families of symmetric rkn type schemes can be easily devised. moreover, by theorem , these methods are also reversibility preserving and therefore very suitable for solving reversible systems. this paper will be organized as follows. in section , we introduce the exact denition of csrkn methods for solving second order odes and the corresponding order theory previously developed in will be briey revisited. in section , by using legendre expansion technique, we present some useful results for devising symmetric integrators which is then followed by giving some illustrative examples for deriving new symmetric integrators in section some numerical experiments are reported in section at last, we give some concluding remarks in section to end this paper. ",
        "Subsections": [],
        "Groundtruth": "The text discusses the importance of numerical integration methods that preserve geometric properties of dynamical systems. Such methods, known as geometric integration, include symplectic methods for Hamiltonian systems, reversible methods, and volume preserving methods. The focus is on the development of integrators that preserve symplecticity, reversibility, and energy conservation. The text also introduces continuous stage methods, such as continuous stage Runge-Kutta (CSRK) methods, as a generalization of traditional numerical methods. The development of new symmetric integrators for second order ordinary differential equations (ODEs) using Legendre polynomial expansion techniques is highlighted. These integrators are shown to be reversibility preserving and suitable for solving reversible systems. The paper presents the organization of the content, including the definition of CSRKN methods, derivation of symmetric integrators, illustrative examples, numerical experiments, and concluding remarks."
    },
    {
        "Section_Num": "2",
        "Section": "2 Continuous-stage RKN method and its order theory",
        "Text": "",
        "Subsections": [
            {
                "Section_Num": "NA",
                "Section": "NA",
                "Text": "in this section, we will recall the notion of the so called continuous stage runge kutta nystr om (csrkn) methods and review some known results which are useful for constructing such methods of arbitrarily high order. for more details, see . . continuous stage rkn method ",
                "Subsections": [],
                "Groundtruth": "The text discusses the continuous stage Runge-Kutta-Nyström (CSRKN) methods, focusing on recalling the concept and reviewing known results that aid in constructing these methods of high order. These methods are essential for precise numerical computations and illustrate a detailed approach to enhancing their efficiency."
            },
            {
                "Section_Num": "2_1",
                "Section": "2.1 Continuous-stage RKN method",
                "Text": "consider the following initial value problem governed by a second order system q = f(t, q), q(t) = q, q(t) = q , () where f : r rd rd is a smooth vector valued function. a well known numerical method for solving () is the so called rkn method with s stages, which can be depicted as qi = q + hciq + h s x j= aijf(t + cjh, qj), i = , , s, (a) q = q + hq + h s x i= bif(t + cih, qi), (b) q = q + h s x i= bif(t + cih, qi), (c) and it can be characterized by the following butcher tableau c a b b where a = ( aij)ss, b = ( b, , bs)t , b = (b, , bs)t , c = (c, , cs)t . compared with an s stage rk method applied to the corresponding rst order system deduced from (), the rkn method is preferable since about half of the storage can be saved and the computational work can be reduced a lot . as a counterpart of the classical rkn method, the csrkn method can be formally dened. denition . let a, be a function of variables , and b, b, c be functions of . for solving (), the continuous stage runge kutta nystr om (csrkn) method as a one step method mapping (q, q ) to (q, q ) is given by q = q + hcq + h z a,f(t + ch, q)d, , (a) q = q + hq + h z bf(t + ch, q)d, (b) q = q + h z bf(t + ch, q)d, (c) which can be characterized by the following butcher tableau c a, b b ",
                "Subsections": [],
                "Groundtruth": "The Continuous-stage RKN method is a numerical technique used to solve initial value problems governed by a second-order system. The RKN method with s stages is a popular approach characterized by a Butcher tableau. It offers advantages over traditional methods by saving storage and reducing computational work. A variant, the Continuous Stage Runge-Kutta Nyström (CSRKN) method, is also defined as a one-step mapping for solving the same problem, offering further computational efficiency."
            },
            {
                "Section_Num": "2_2",
                "Section": "2.2 Order theory for RKN-type method",
                "Text": "denition . a rkn type method is of order p, if for all regular problem (), the following two formulas hold, as h , q(t + h) q = o(hp+), q(t + h) q = o(hp+). we introduce the following classical simplifying assumptions for rkn methods b() : s x i= bic i = , , cn() : s x j= aijc j = c+ i ( + ), i s, , dn() : s x i= bic i aij = bjc+ j ( + ) bjcj + bj + , j s, () theorem . if the coecients of the rkn method (a)-(c) satisfy the simplifying assumptions b(p), cn(), dn(), and if bi = bi( ci) holds for all i = , . . . , s, then the method is of order at least min{p, + , + }. analogously to the classical case, we have the following simplifying assumptions for csrkn methods b() : z bc d = , , cn() : z a, c d = c+ ( + ), , , dn() : z bc a, d = bc+ ( + ) bc + b + , , theorem . if the coecients of the csrkn method (a)-(c) satisfy the simplifying assumptions b(p), cn(), dn(), and if b = b( c) holds for , then the method is of order at least min{p, + , + }. let us introduce the normalized shifted legendre polynomial pk(x) of degree k by the following rodrigues formula p(x) = , pk(x) = k + k! dk dxk , k = , , , . a well known property of legendre polynomials is that they are orthogonal to each other with respect to the l inner product in z pj(x)pk(x) dx = jk, j, k = , , , , where jk is the kronecker delta. for convenience, we list some of them as follows p(x) = , p(x) = (x ), p(x) = (x x + ), . theorem . for the csrkn method (a)-(c) denoted by ( a,, b, b, c) with the assumption b = , c = , the following two statements are equivalent to each other: (i) both cn() and dn() hold true; (ii) a, possesses the following form in terms of legendre polynomials a, = p() + p() + n x = +p()p+() n x = \u0000 + + \u0001 p()p() + n x = +p+()p() + x i j (i, j)pi()pj(). () where = , n = max{ , }, n = max{ , }, n = max{ , } and (i, j) are arbitrary real numbers. recall that we have b() by using b = , c = , thus theorem implies that we can easily construct a csrkn method with order min{, +, +} = min{+, +} (by theorem ). however, for the sake of deriving a practical csrkn method, we need to dene a nite form for the coecient a, , which can be easily realized by truncating the series (). in such a case, we get a, which is a bivariate polynomial. consequently, by applying a quadrature formula denoted by (bi, ci)s i= to (a)-(c), it leads to an s stage rkn method qi = q + hcciq + h s x j= bj aci,cjf(t + ccjh, qj), i = , , s, (a) q = q + hq + h s x i= bi bcif(t + ccih, qi), (b) q = q + h s x i= bibcif(t + ccih, qi), (c) whose butcher tableau is cc b ac,c bs ac,cs . . . . . . . . . ccs b acs,c bs acs,cs b bc bs bcs bbc bsbcs () if we additionally assume b = b( c), b = , c = , then it gives an s stage rkn method with tableau c b ac,c bs ac,cs . . . . . . . . . cs b acs,c bs acs,cs b bs b bs () where bi = bi( ci), i = , , s. in view of theorem , we have the following result for analyzing the order of the rkn method with tableau (). theorem . assume a, is a bivariate polynomial of degree a in and degree a in , and the quadrature formula (bi, ci)s i= is of order p. if the coecients of the underlying csrkn method (a)-(c) satisfy b = b( c), b = , c = , and both cn(), dn() hold true, then the rkn method with tableau () is of order at least min(p, + , + ), where = min(, p a + ) and = min(, p a + ). ",
                "Subsections": [],
                "Groundtruth": "The text discusses the mathematical framework for RKN-type methods, particularly focusing on the order theory and simplifying assumptions for such methods. It introduces conditions for determining the order of RKN methods and CSRKN methods, with references to legendre polynomials and quadrature formulas. The text also presents equations and the Butcher tableau for these methods, along with the implications of certain assumptions on the method's order. The discussions provide insights into constructing practical RKN methods with specific orders based on the coefficients and assumptions involved."
            }
        ],
        "Groundtruth": ""
    },
    {
        "Section_Num": "3",
        "Section": "3 Conditions for the symmetry of csRKN methods",
        "Text": "now let us introduce the denition of symmetric methods and then show the conditions for a csrkn method to be symmetric. denition . a numerical one step method h is called symmetric if it satises h = h, where h = h is referred to as the adjoint method of h. symmetry implies that the original method and the adjoint method give identical numerical results. an attractive property of symmetric integrators is that they possess an even order . by denition, a one step method z = h(z; t, t) is symmetric if exchanging h h, z z and t t leaves the original method unaltered. theorem . if the coecients of the csrkn method (a)-(c) satisfy c = c, a, = b( c) b + a,, b = b b, b = b, () for , , then the method is symmetric. proof. firstly, let us establish the adjoint method. from (a)-(c), by interchanging t, q, q , h with t, q, q , h respectively, we have q = q hcq + h z a,f(t ch, q)d, , (a) q = q hq + h z bf(t ch, q)d, (b) q = q h z bf(t ch, q)d. (c) notice that t ch = t + ( c)h, thus (c) becomes q = q + h z bf(t + ( c)h, q)d. () substituting it into (b) yields q = q + hq + h z (b b)f(t + ( c)h, q)d. () next, by inserting () and () into (a), it follows that q = q + h( c)q + h z (b( c) b + a,)f(t + ( c)h, q)d. () by replacing and with and respectively, we can recast (), () and () as q = q + hc q + h z a ,f(t + c h, q )d, , q = q + hq + h z b f(t + c h, q )d, q = q + h z b f(t + c h, q )d, () where q = q, and c = c, a , = b( c) b + a,, b = b b, b = b, () for , . therefore, we have get the adjoint method dened by () and (). given that a csrkn method can be uniquely determined by its coecients, hence if we require the following condition c = c , a, = a ,, b = b , b = b , namely the condition (), then the original method is symmetric. in the following we present a preferable result for ease of devising symmetric csrkn methods. theorem . suppose that b = b( c), b = , c = , then the csrkn method de- noted by ( a,, b, b, c) is symmetric, if a, possesses the following form in terms of legendre polynomials a, = (,) p() + p() + x i+j is even i+j> (i,j)pi()pj(), , , () where = and (i,j) are arbitrary real numbers. proof. by noticing b = b( c), b = , c = , it suces for us to consider the second condition given in (). by using a simple identity = p() + p(), it implies a, a, = = (p() p()). () next, let us consider the following expansion of a, in terms of the legendre orthogonal basis {pi()pj() : i, j }, a, = x i,j (i,j)pi()pj(), (i,j) r, and then by replacing and with and respectively, with the help of p( t) = ()p(t) ( ), we have a, = x i,j ()i+j(i,j)pi()pj(). substituting the above two expressions into () and collecting the like basis, follows (,) = , (,) = , (i,j) = , when i + j is odd and i + j > , which completes the proof. by putting theorem and theorem together, we can devise symmetric integrators of arbitrarily high order. besides, as an alternative way, we can use the same technique as presented in to construct symmetric integrators for arbitrary order, that is, substituting () into the order conditions (see , page ) one by one and determining the corresponding parameters (i,j). as symmetric methods possess an even order, it is sucient to consider those order conditions for odd orders, so we can increase two orders per step. we present the the following result without a proof (please see for a similar proof). theorem . suppose that a, is in the form () and b = b( c), b = , c = . then the corresponding csrkn method is symmetric and of order at least. if we additionally require (,) = , then the method is of order at least. moreover, if we further require that (,) = , (,) = , (,) = (,) = , (i,) = , for even i > , () then the method is of order at least. ",
        "Subsections": [],
        "Groundtruth": "Symmetric methods in csRKN involve conditions where the coefficients satisfy specific relationships. The adjoint method of csRKN is defined by interchanging certain parameters within the equations. If the coefficients meet the condition (), the csRKN method is defined to be symmetric. Additionally, a preferable condition for symmetric csRKN methods involves Legendre polynomials in the form of equation (). By combining two theorems, symmetric integrators of high order can be developed. An alternative method involves substituting specific expressions into order conditions to increase the method's order. Ultimately, by satisfying certain criteria, csRKN methods can achieve high-order symmetry."
    },
    {
        "Section_Num": "4",
        "Section": "4 Symmetric RKN method",
        "Text": "in this section, we show that symmetric rkn methods can be easily derived from symmetric csrkn methods by using quadrature formulas. theorem . if the coecients of the underlying symmetric csrkn method satisfy (), then the associated rkn method () is symmetric, provided that the weights and abscissae of the quadrature formula satisfy bs+i = bi and cs+i = ci for all i. proof. the symmetric condition for an s stage classical rkn method denoted by ( aij, bi, bi, ci) is known as (see, e.g., ) ci = cs+i, aij = bs+j( cs+i) bs+j + as+i,s+j, bi = bs+i bs+i, bi = bs+i, + table : two families of symmetric and symplectic rkn methods of order , by using gaussian (on the left) and lobatto (on the right) quadrature formulas respectively. for all i, j = , , s. by using (), we have cci = cci, aci,cj = bcj( cci) bcj + aci,cj, bci = bci bci, bci = bci, for all i, j = , , s. in view of bs+i = bi and cs+i = ci for all i, the coecients (bj aci,cj, bi bci, bibci, ci) of the associated rkn method satisfy cci = ccs+i, bj aci,cj = bs+jbcs+j( ccs+i) bs+j bcs+j + bs+j acs+i,cs+j, bi bci = bs+ibcs+i bs+i bcs+i, bibci = bs+ibcs+i, for all i, j = , , s, which completes the proof by the classical result. corollary . if a, takes the form () and b = b( c), b = , c = , then by using a quadrature formula (bi, ci)s i= with bs+i = bi and cs+i = ci for all i, the resulting rkn method () is symmetric. since the weights and abscissae of gaussian type and lobatto type quadrature formulas satisfy bs+i = bi and cs+i = ci for all i, they can be used for devising symmetric rkn methods. example . if we take the coecients ( a,, b, b, c) as a, = p() + p(), b = , b = , c = , () with one parameter being introduced, then we get a family of symmetric csrkn methods with order by theorem presented in (see also theorem in ), such methods are also symplectic and thus suitable for solving general second order hamiltonian systems. by using suitable quadrature formulas with order p we can get symmetric rkn methods of order the resulting symmetric rkn methods are shown in table . this can be easily checked by the classical order conditions that listed in (see also ). + + + + + ++ (+) () + (+) + () (+) + () + (+) () ++ (+) table : two families of symmetric rkn methods of order , by using gaussian ( nodes) and lobatto ( nodes) quadrature formulae. example . if we take the coecients ( a,, b, b, c) as a, = p() + p() + p()p() + p()p() + p()p(), b = , b = , c = , () then we get a family of symmetric csrkn methods with order by using suitable quadrature formulas with order p we get symmetric rkn methods of order , which are shown in table . remark . we point out that: () the left family of rkn methods in table are always symmetric and symplectic, while the right family of rkn methods of table are symmetric and symplectic when = . () the classical -stage lobatto iiia method induces the following rkn method, () which can be retrieved by taking = , = , = in table . () the classical -stage lobatto iiib method induces the following rkn method, () which can be retrieved by taking = , = , = in table . + + + + + + + + + + + + + + + + + + + + + + + + + + + table : two families of symmetric and symplectic rkn methods of order , by using gaussian (on the top) and lobatto (on the bottom) quadrature formulas respectively. example . if we take the coecients ( a,, b, b, c) as a, = x i+j (i,j)pi()pj() + p()p(), b = , b = , c = , () where (,) = , (,) = , and the remaining (i,j) satisfy (), then we get a family of -order symmetric and symplectic csrkn methods. by using suitable quadrature formulas with order p we get symmetric and symplectic rkn methods of order , which are shown in table . ",
        "Subsections": [],
        "Groundtruth": "Symmetric RKN methods can be derived from symmetric CSRKN methods using quadrature formulas. If the coefficients of the symmetric CSRKN method satisfy certain conditions, the associated RKN method will be symmetric when the weights and abscissae of the quadrature formula also fulfill specific relationships. By following these conditions, symmetric RKN methods can be devised using Gaussian and Lobatto quadrature formulas, resulting in symmetric and symplectic methods suitable for solving general second-order Hamiltonian systems. The resulting symmetric RKN methods can be determined by classical order conditions and are showcased in tables."
    },
    {
        "Section_Num": "5",
        "Section": "5 Numerical experiments",
        "Text": "in this section, we perform some numerical results for comparing the numerical behaviors of the presented methods. for this aim, we consider the -order method () and the following three -order methods: by taking = , = = in table it leads to a diagonally implicit symplectic and symmetric rkn method () by taking = , = , = in table it gives the following symmetric rkn method () by taking = , = , = in table it gives the following symmetric rkn method () for convenience, we denote four symmetric rkn methods (), (), () and () by rkn- iiib, rkn diagsymp, rkn a and rkn b methods respectively. these methods are applied to the following perturbed pendulum equation q = sin q cos(q), q(t) = , q(t) = , () where the initial values are taken the same as that given in . the system () is reversible with respect to the reection p p (here p = q) and the corresponding hamiltonian function (energy) is given by h(p, q) = p cos q + sin(q). global errors of the numerical solutions by the above four methods with six small step sizes are shown in fig. with log log scales, which veries the order of all the methods. from fig. , it is seen that rkn iiib method and rkn b method produce obvious energy drifts, though these methods are symmetric. this shows that not all symmetric rkn methods nearly preserve the energy over long times even if the system is reversible this observation has been shown for symmetric runge kutta methods in . it is observed that the energy error keeps bounded for the rkn diagsymp method. besides, it seems that the non symplectic rkn a method gives a better behavior. however, when we integrate the system on a much longer time interval , it gives a worse result (energy drift) compared with the rkn diagsymp method (see fig. ). from these numerical tests we may conclude that symplectic structure preservation is more essential than the reversibility preservation of the reversible hamiltonian systems in long term numerical simulation. nevertheless, for general reversible non hamiltonian systems, symmetric methods are also preferable. ",
        "Subsections": [],
        "Groundtruth": "In this section, numerical experiments were conducted to compare the numerical behaviors of different methods. Four symmetric rkn methods were applied to a perturbed pendulum equation, with global errors of the numerical solutions shown in a figure. The results indicated that not all symmetric rkn methods preserved energy over long times, with the rkn iiib and rkn b methods showing obvious energy drifts. The rkn diagsymp method had bounded energy errors, and the non-symplectic rkn a method exhibited better behavior initially but performed worse over longer time intervals compared to the rkn diagsymp method. The experiments suggested that symplectic structure preservation is more crucial than reversibility preservation in long-term numerical simulations of reversible Hamiltonian systems. "
    },
    {
        "Section_Num": "6",
        "Section": "6 Concluding remarks",
        "Text": "we develop symmetric integrators by means of continuous stage runge kutta nystr om (csrkn) methods in this paper. the crucial technique based on legendre polynomial expansion combining with the symmetric conditions and order conditions is fully utilized. as illustrative examples, new - - - - - - - - - - - - - - - - - - - - figure : global errors of the numerical solutions by rkn iiib method(black line), rkn diagsymp method (blue line), rkn a method (red line) and rkn b method (green line) for the perturbed pendululm equation (). the reference line has slope in every subplots. - - - - - - figure : energy errors of the numerical solutions by rkn iiib method(black line), rkn diagsymp method (blue line), rkn a method (red line) and rkn b method (green line) for the perturbed pendululm equation (): step size h = , integration interval . - - - - - figure : energy errors of the numerical solutions by rkn diagsymp method (blue line), and rkn a method (red line) for the perturbed pendululm equation (): step size h = , integration interval . families of symmetric integrators (most of them are also symplectic) are derived in use of gaussian- type and lobatto type quadrature formulas. it is worth observing that other quadrature formulas can also be considered for devising symmetric integrators and more free parameters can be led into the formalism of the butcher coecients. acknowledgements the rst author was supported by the national natural science foundation of china (), china scholarship council (no.) and scientic research fund of hunan provincial education department (c). the second author was supported by the foundation of nsfc (no. , ) and phd scientic research foundation of east china jiaotong univer- sity. references l. brugnano, f. iavernaro, d. trigiante, hamiltonian boundary value methods: energy pre- serving discrete line integral methods, j. numer. anal., indust. appl. math., () (), l. brugnano, f. iavernaro, d. trigiante, analysis of hamiltonian boundary value methods (hbvms): a class of energy preserving rungeckutta methods for the numerical solution of polynomial hamiltonian systems, commun. nonlinear. sci. numer. simulat., () (), l. brugnano, f. iavernaro, line integral methods for conservative problems, monographs and research notes in mathematics, crc press, boca raton, fl, l. brugnano, f. iavernaro, d. trigiante, a simple framework for the derivation and analysis of eective one step methods for odes, appl. math. comput., (), l. brugnano, f. iavernaro, line integral solution of dierential problems, axioms, () () , https://doi.org//axioms c. burnton, r. scherer, gauss runge kutta nystr om methods, bit, (), j. r. cash, a variable step runge kutta nystr om integrator for reversible systems of second order initial value problems, siam j. sci. comput., (), e. celledoni, r. i. mclachlan, d. mclaren, b. owren, g. r. w. quispel, w. m. wright., energy preserving runge kutta methods, man (), r. p. k. chan, on symmetric rungekutta methods of high order, computing, (), e. faou, e. hairer, t. l. pham, energy conservation with non symplectic methods: examples and counter examples, bit numerical mathematics, (), k. feng, on dierence schemes and symplectic geometry, proceedings of the -th inter., sym- posium of dierential geometry and dierential equations, beijing, , k. feng, k. fengs collection of works, vol. , beijing: national defence industry press, k. feng, m. qin, symplectic geometric algorithms for hamiltonian systems, spriger and zhejiang science and technology publishing house, heidelberg, hangzhou, first edition, e. hairer, s. p. nrsett, g. wanner, solving ordiary dierential equations i: nonstiprob- lems, springer series in computational mathematics, , springer verlag, berlin, e. hairer, c. lubich, g. wanner, geometric numerical integration: structure preserving algorithms for ordinary dierential equations, second edition. springer series in computa- tional mathematics, , springer verlag, berlin, e. hairer, energy preserving variant of collocation methods, jnaiam j. numer. anal. indust. appl. math., (), j. hong, a survey of multi symplectic runge kutta type methods for hamiltonian partial dier- ential equations, frontiers and prospects of contemporary applied mathematics, , y. li, x. wu, functionally tted energy preserving methods for solving oscillatory nonlinear hamiltonian systems, siam j. numer. anal., ()(), r. i. mclachlan, g. r. w. quispel, g. s. turner, numerical integrators that preserve symme- tries and reversing symmetries, siam j. numer. anal., (), y. miyatake, an energy preserving exponentially tted continuous stage runge kutta methods for hamiltonian systems, bit numer. math., (), y. miyatake, j. c. butcher, a characterization of energy preserving methods and the con- struction of parallel integrators for hamiltonian systems, siam j. numer. anal., ()(), d. okunbor, rd. skeel, explicit canonical methods for hamiltonian systems, math. comput., (), g. r. w. quispel, d. i. mclaren, a new class of energy preserving numerical integration methods, j. phys. a: math. theor., () j. m. sanz serna, m. p. calvo, numerical hamiltonian problems, chapman & hall, d. stoer, variable steps for reversible integration methods, computing, () (), w. tang, y. sun, a new approach to construct runge kutta type methods and geometric numerical integrators, aip. conf. proc., (), - w. tang, y. sun, time nite element methods: a unied framework for numerical discretiza- tions of odes, appl. math. comput. (), w. tang, y. sun, construction of runge kutta type methods for solving ordinary dierential equations, appl. math. comput., (), w. tang, g. lang, x. luo, construction of symplectic (partitioned) runge kutta methods with continuous stage, appl. math. comput. (), w. tang, y. sun, w. cai, discontinuous galerkin methods for hamiltonian odes and pdes, j. comput. phys., (), w. tang, j. zhang, symplecticity preserving continuous stage runge kutta nystr om methods, appl. math. comput., (), w. tang, y. sun, j. zhang, high order symplectic integrators based on continuous stage runge- kutta nystr om methods, arxiv: , w. tang, a note on continuous stage runge kutta methods, appl. math. comput., (), w. tang, continuous stage runge kutta methods based on weighted orthogonal polynomials, arxiv: , w. tang, an extended framework of continuous stage runge kutta methods, arxiv: , ",
        "Subsections": [],
        "Groundtruth": "The text presents the development of symmetric integrators using continuous stage Runge-Kutta Nyström (CSRKN) methods. Key techniques, such as Legendre polynomial expansion and symmetric conditions, are applied for deriving these integrators. Various numerical examples are provided to demonstrate the performance of the integrators, including comparisons of global and energy errors. The study showcases families of symmetric integrators, some of which are also symplectic, derived using Gaussian-type and Lobatto-type quadrature formulas. The authors acknowledge support from various research grants and provide a list of references that further expand on related topics in numerical integration methods."
    },
    {
        "Section_Num": "NA",
        "Section": "NA",
        "Text": "symmetric integrators based on continuous stage runge kutta nystr om methods for reversible systems wensheng tanga,b, jingjing zhangc,,, acollege of mathematics and statistics, changsha university of science and technology, changsha , china bhunan provincial key laboratory of mathematical modeling and analysis in engineering, changsha , china cschool of science, east china jiaotong university, nanchang , china abstract in this paper, we study symmetric integrators for solving second order ordinary dierential equations on the basis of the notion of continuous stage runge kutta nystr om methods. the construction of such methods heavily relies on the legendre expansion technique in conjunction with the symmetric conditions and simplifying assumptions for order conditions. new families of symmetric integrators as illustrative examples are presented. for comparing the numerical behaviors of the presented methods, some numerical experiments are also reported. keywords: continuous stage runge kutta nystr om methods; reversible systems; symmetric integrators; simplifying assumptions; legendre polynomials. introduction ",
        "Subsections": [],
        "Groundtruth": "The text discusses the development of symmetric integrators using continuous stage Runge-Kutta-Nyström methods for reversible systems. The integrators are designed for solving second-order ordinary differential equations and are constructed based on Legendre expansion techniques, symmetric conditions, and simplifying assumptions for order conditions. New families of such integrators are presented as illustrative examples, and numerical experiments are conducted to compare their numerical behaviors. Key terms include continuous stage Runge-Kutta-Nyström methods, reversible systems, symmetric integrators, simplifying assumptions, and Legendre polynomials."
    },
    {
        "Section_Num": "1",
        "Section": "1 Introduction",
        "Text": "quantifying the similarity of time series has always been a very useful primitives for time series analysis, with ap- plications to many elds (hu et al., ; silva et al., ; gomes and batista, ; mueen et al., ; mcgovern et al., ; chiu et al., ; mueen and keogh, ; tataw et al., ). the key point of measuring similarity is to de- ne a suitable and effective distance between two time series (hu et al., ; tarango et al., ; mi skiewicz and ausloos, ). the widely adopted denitions of distance include the euclidean distance and correlation measures (mi skiewicz and ausloos, ). however, in terms of measuring the similarity of time series, the euclidean distance is often average, some- times bad (wang et al., ). for most time series analysis problems, the dynamic time warping (dtw) provides a highly competitive distance metric (silva et al., ; wang et al., ). to get the best performance of dtw, we need to regulate its unique parameter to optimize the dynamic time warpings window width (dau et al., b). the complexity of the dtw method is relatively high, so many researchers provide some improved methods to have bet- ter performance (petitjean et al., ; dau et al., ; mueen and keogh, ). moreover, practitioners generalize the dtw to some multi dimensional time series classication experiments (shokoohi yekta et al., ). similar subsequences in time series can be dened as time series motifs, which characterize the temporal proper- ties and dynamics of the corresponding long time series (mcgovern et al., ; chiu et al., ; mueen and keogh, ). it is useful for exploratory data mining and often used as inputs for classication of time series, clustering, segmentation (bagnall et al., ; mori et al., ; dau et al., ; petitjean et al., ). time series motif anal- ysis has been widely used in diverse elds (yeh et al., ; zhu et al., ; linardi et al., a,b; yeh et al., ; zakaria et al., ). gomes and batista presented a sax based motif discovery method to classify the urban sound (gomes and batista, ). wang et al. proposed a method to automatically detect repeating segments in music and two time series data sets (wang et al., ). son and anh introduced two novel methods to discover approximate corresponding to: meilong road, p.o. box , school of business, east china university of science and technology, shanghai , china. email address: wxzhou@ecust.edu.cn (wei xing zhou) preprint submitted to elsevier january , k motifs in time series data (son and anh, ) and their methods play an important role in several time series data mining tasks by using motif discovery. lots of researchers have used time series motifs analysis for applications in many different domains (mueen et al., ; mcgovern et al., ; chiu et al., ; mueen and keogh, ). triadic time series motifs (xie et al., b) are inspired by the network motifs in visibility graph (lacasa et al., , ; ni et al., ; yang et al., ; elsner et al., ; qian et al., ) and horizontal visibility graphs (hvg) mapping from time series (lacasa et al., ; elsner et al., ; lacasa and toral, ; shao, ; dong and li, ; ahmadlou et al., ; tang et al., ; xie et al., , a). the six triadic time series motifs are similar in some features with sequential hvg motifs (iacovacci and lacasa, b,a) and ordinal patterns (keller and sinn, ; mccullough et al., , ; zhang et al., ). the permutation entropy based on ordinal patterns (bandt and pompe, ; amig o, ) is a natural complexity measure and useful in the presence of dynamical or observational noise. similarly, the triadic time series motif analysis can also mine the dynamical characteristics of time series from com- plex system. xie et al. (b) used the triadic time series motif analysis to uncover the different dynamics in the heartbeat rates of healthy subjects, congestive heart failure subjects, and atrial brillation subjects and identify the bullish and bearish markets from the price uctuations of nancial markets. in this work, we identify six triadic time series motifs and investigate their occurrence proles in time series from logistic maps with different control paratemers and chaotic time series generated from chaotic logistic map, chaotic henon map, chaotic ikeda map, hyperchaotic generalized henon map and hyperchaotic folded tower map. it is of great signicance to be able to discover the characteristics of time series from different types of chaotic maps. we also apply the triadic time series motif analysis to classify the time series in data sets from ucr time series classication archive (dau et al., a). ",
        "Subsections": [],
        "Groundtruth": "The text discusses the importance of quantifying the similarity of time series for time series analysis. It mentions different distance metrics such as Euclidean distance and correlation measures, with Dynamic Time Warping (DTW) being highlighted as a highly competitive distance metric. Researchers have also proposed improved methods to enhance DTW's performance. Time series motifs are defined as similar subsequences in time series data, useful for tasks like classification, clustering, and segmentation. Triadic time series motifs are inspired by network motifs and are used to analyze the dynamics of time series data from complex systems. The text also explores the application of triadic time series motifs in classifying chaotic time series data and data sets from the UCR Time Series Classification Archive."
    },
    {
        "Section_Num": "2",
        "Section": "2 Triadic time series motifs",
        "Text": "triadic time series motifs are determined by the relative magnitude and ordinal order of three data points that are randomly chosen from the time series xie et al. (b). for three arbitrary data {xi, x j, xk} with i < j < k in the time series {xi}i=,...,l, a time series motif forms if the following conditions is fullled (xie et al., b): ( xi > xn and x j > xn, n (i, j) x j > xm and xk > xm, m (j, k) . () we obtain six triadic time series motifs, which are denoted as m, m, m, m, m, m in fig. this denition does not consider situations where two or three data points of {xi, x j, xk} are equal. when two data points are identical, we treat it as if the latter data point is larger than the former one. figure : illustrative example showing the six types of triadic motifs in time series. the time series motifs are different from the conventional motifs of horizontal visibility graphs (lacasa and toral, ; lacasa et al., ; shao, ; dong and li, ; ahmadlou et al., ; elsner et al., ; tang et al., ; xie and zhou, ; xie et al., , a). considering the triadic hvg motif, there are only two admissible motifs in undirected hvgs, one being a chain and the other being a triangle. as shown in fig. , the open triadic motif can be mapped from the time series (, , ), (, , ), (, , ) and (, , ) and the close triadic motif can be mapped from the time series (, , ) and (, , ). time series motifs consider not only the visibility between data points, as hvg motifs, but also the order and relative magnitudes of the points. hence, time series motifs explore ner structures of hvg motifs (xie et al., b). ",
        "Subsections": [],
        "Groundtruth": "Triadic time series motifs are based on the relative magnitude and order of three data points randomly selected from a time series. Six triadic motifs are defined based on conditions related to the values of the chosen data points. The motifs differ from traditional horizontal visibility graph motifs by considering the order and magnitudes of the data points, leading to more detailed structures. The motifs are depicted in a figure, illustrating how each motif is formed. By analyzing time series motifs, researchers can explore more intricate patterns compared to conventional horizontal visibility graph motifs."
    },
    {
        "Section_Num": "3",
        "Section": "3 Triadic time series motif analysis of chaotic maps",
        "Text": "",
        "Subsections": [
            {
                "Section_Num": "3_1",
                "Section": "3.1 Chaotic maps",
                "Text": "we perform triadic time series motif analysis numerically for different time series in continuous and discrete dynamic systems. through extensive numerical experiments, we investigate the motif distribution extracted from the logistic map, the chaotic logistic map, the chaotic henon map, the chaotic ikeda map, the hyperchaotic generalized henon map, and the hyperchaotic folded tower map. the logistic map is a representative example of how complex, chaotic behaviour can arise from very simple nonlinear dynamical equation. mathematically, the logistic map is written as xn+ = rxn( xn). () to distinguish chaotic maps and hyperchaotic maps, we generate four types of time series from chaotic henon map, chaotic ikeda map, hyperchaotic generalized henon map and hyperchaotic folded tower map. the specic equations for these four types of dynamic systems are given below (xu et al., ). mathematically, the chaotic henon map is written as xn+ = yn + ax n, yn+ = bxn. () where a = and b = . the chaotic ikeda map is written as xn+ = + (xn cos tn yn sin tn), yn+ = (xn sin tn + yn cos tn). () where tn = /( + x n + y n) and = . the hyperchaotic generalized henon map is written as xn+ = a y n bzn, yn+ = xn, zn+ = yn. () where a = and b = . the hyperchaotic folded tower map is written as xn+ = axn( xn) (yn + )( zn), yn+ = ((yn + )( + zn) )( xn), zn+ = zn( zn) + byn. () where a = and b = . ",
                "Subsections": [],
                "Groundtruth": "The text explores triadic time series motif analysis on various continuous and discrete dynamic systems such as the logistic map, chaotic logistic map, chaotic Henon map, chaotic Ikeda map, hyperchaotic generalized Henon map, and hyperchaotic folded tower map. The logistic map exemplifies how intricate chaotic behavior can emerge from simple nonlinear equations. To differentiate between chaotic and hyperchaotic maps, four types of time series are generated from the chaotic Henon map, chaotic Ikeda map, hyperchaotic generalized Henon map, and hyperchaotic folded tower map, each with specific mathematical expressions defining their dynamics."
            },
            {
                "Section_Num": "3_2",
                "Section": "3.2 Occurrence frequency distributions of triadic motifs",
                "Text": "we rst generate time series by using the logistic map with control parameter r. the parameter r ranges in the interval of (, ]. when r = , x will approach permanent oscillations between two values from almost all initial conditions. when r = , x will approach permanent oscillations among four values from almost all initial conditions. when r = , , or , x exhibits chaotic behaviour. for each parameter r, we generate time series with length , determine the occurrence frequencies fi of the six triadic time series motifs, and obtain the occurrence frequency distribution of each motif. fig. shows the distributions of the occurrence frequency fi of the motif i in the logistic time series with r = , , , , and we nd that the ve classes of time series have very different occurrence frequency distributions of time series motifs. f p (f) f p (f) r = (oscillations between values) r = (oscillations among values) r = (chaotic) r = (chaotic) r = (chaotic) f p (f) f p (f) f p (f) f p (f) a b c d e f figure : the probability distribution of occurrence frequency fi of motifs mi for different types of time series of the logistic map with parameter r = , , , , when the parameter r = , the time series is {xi}i=,...,l = {a, b, a, b, a, b, }. without loss of generality, we assume that a > b. the set of motif m is the union of {xi, xi+, xi+} and {xi, xi, xi+} with i = , , ..., , so that the occurrence count of m is o(m) = () the sets of motifs m, m and m are respectively the union of {xi, xi+, xi+}, of {xi, xi+, xi+}, and of {xi, xi, xi+}, where i = , , ..., it follows that o(m) = o(m) = o(m) = () by denition, motifs m and m cannot appear. therefore, the occurrence frequencies fi := f(mi) = lim l o(mi) p i= o(mi) () are obtained as follows f := = . () this analytical result is veried by the numerical simulations, as shown in fig. (red bars). when the parameter r = , the time series is slightly more complicated. as shown in fig. , for the same motif, the distribution of occurrence frequency fi of the motif mi is concentrated and the variance is small, even when r > , the oscillation period becomes longer and longer, until about r = , the period tends to innity, and the system becomes a chaotic system. when r > , the result of the iterative run will switch between the period type and the chaotic type. until r = , the system is complete chaos. although they are all chaos, the distribution of occurrence frequency fi for r = , , in fig. has a big difference. we further perform triadic time series motif analysis of the four types of discrete chaotic time series: chaotic henon map, chaotic ikeda map, hyperchaotic generalized henon map and hyperchaotic folded tower map. in fig. , the length of time series is and there are big difference in the occurrence frequency of motif m and motif m between the four types of discrete chaotic time series. it can be imagined that the longer the time series is, the larger the difference of the occurrence frequency of the individual motifs will be, and the easier it is to distinguish the the four types of discrete chaotic time series. in fig. , we cannot use one motifs occurrence frequency to distinguish different types of chaotic time series, but in fig. , we can use a single indicator f to distinguish the ve types of logistic time series. this method can be understood as a dimension reduction method, which reduces the time series with length n to -dimensional space for different time series, because p i= fi = f p (f) chaotic henon map chaotic ikeda map hyperchaotic generalized henon map hyperchaotic folded tower map f p (f) f p (f) f p (f) f p (f) f p (f) a b c d e f figure : the probability distribution of occurrence frequency fi of motifs mi for different types of time series: chaotic henon map, chaotic ikeda map, hyperchaotic generalized henon map and hyperchaotic folded tower map . the length of time series is . classication of time series the triadic motif analysis is applied to the classication of time series to investigate the effectiveness of the similarity measure of time series. in order to classify different time series, we need to extract the features of time series. the triadic time series motifs are used as the features of time series, and then the time series are classied based on the motif occurrence frequency distributions. from a common sense, the longer the time series, the more information obtained by the method for extracting features of time series, the more accurately the time series can be classied. therefore, we consider the inuence of time series lengths on the accuracy of classication. we compare two classical methods for measuring the similarity of time series: one is the simple euclidean distance method, and the other is the dynamic time warping method (dtw). we select the nearest neighbor method (nn) to classify the time series based on the three similarity measures. we analyze respectively the time series generated from the logistic map with parameter r = , , , and and from the four chaotic maps. for each type of time series, we generate time series as the training set and time series as the test set. to analyze the accuracy of classication of time series with different lengths l, the length of time series is changed from l = to l = , and then the data sets are classied by the nearest neighbor method based on three similarity measures. the three colors (red, green, blue) in fig. (a) and (c) correspond to the three similarity measures: the motif occurrence prole, the dtw and the euclidean distance. the red dot indicates the accuracy of classication of the nn method based on the motif occurrence prole. the green dot indicates the accuracy of classication of the nn method based on the dtw. the blue dot indicates the accuracy based on the euclidean distance. the ordinate represents the discriminant correctness rate based on the training set and the test set. the abscissa represents the time series length l. in general, the dtw based discriminant accuracy is the best and the motif prole method performs slightly worse, especially when the time series length is less than the euclidean distance method is the worst. usually, the longer is the time series, the more information is extracted by the methods. however, the accuracy of the classication method based on euclidean distance decreases with the increase of the time series length. this is mainly because that the euclidean distance calculation is simple. when the time series is longer, there is more noise, which is not l accuracy e accuracy l accuracy e accuracy a b c d figure : the results of classication based on similarity of occurrence frequency of motifs for different types of time series. (a) the average classication accuracy rate , and for the ve types of logistic time series with parameter r = , , , and the time series contains data points. for each type of logistic time series, we generates time series training sets and test sets. the ordinate represents the average classication accuracy rate. the abscissa represents the length of time series. the three colors (red, green, blue) correspond to three similarity measurements: the motif distribution, the dtw and euclidean distance. (b) the relationship between the data deletion rate e and the average classication accuracy rate for the ve types of logistic time series. (c) the average classication accuracy rate , and for the chaotic henon map, chaotic ikeda map, hyperchaotic generalized henon map and hyperchaotic folded tower map, respectively. (d) the relationship between the data deletion rate e and the average classication accuracy rate for the four chaotic maps. conducive to depicting the similarity between time series. the euclidean distance method has a relatively good effect when the time series length is less than when the time series length is greater than , we nd that = and = , indicating that the dtw based method and the motif prole method provided in this paper are able to distinguish completely different time series. the euclidean distance method is not good and it is the same as the random classication, from which the accuracy is /c, where c is the number of categories in the data set. the logistic map series has categories, we have /c = / the discrete chaotic time series has categories, we have /c = / in order to analyze the accuracy of classication in the case of data loss, we perform the same analysis on the time series after data deletion. the length of the original time series is l = we randomly delete a proportion (e) of the data from each time series. we then classify the remaining data and calculate the classication accuracy rates. this process is repeated times and the average classication accuracy rates , and are obtained. fig. (b) and (d) show the relationship between the data deletion rate e and the classication accuracy rates , and . overall, , and decrease with increasing e. we observe that the euclidean distance method performs as the random classication, with = / for the logistic maps and = / for the chaotic time series. for the logistic maps, the motif prole method is more robust to data deletion than the dtw method. when the data deletion rate is close to %, the motif based classication accuracy can still reach %, while the dtw based classication accuracy rate drops to about %. in contrast, for the chaotic time series, the dtw method outperforms the motif prole method. the dtw classication method is very robust to data deletion and its accuracy rate is close to % even when the data deletion rate e is as high as %. not surprisingly, each method (dtw or motif prole) has its own advantages and disadvantages. different methods usually have different performances when they are applied to different time series. triadic time series motif analysis of the ucr time series classication archive to test the effectiveness of this method on similarity measures of time series, we use this method to classify real time series. the data source is from the ucr time series classication archive (dau et al., a). the ucr time series classication archive contains data sets, each of which is divided into a training set and a test set. the dataset website also presents some results about classication accuracy of three methods. the rst method uses the nearest neighbor method to classify based on the euclidean distance. the accuracy rate is expressed by , where the highest rate is % (coffee) and the lowest correct rate is % (pigairwaypressure). the second uses the nearest neighbor method to classify based on the dtw method. the correct rate is represented by , where the highest is % (trace, two patterns, plane, coffee) and the lowest correct rate is % (pigairwaypressure). the third method is based on the improvement of dtw. from previous research results, we found that the dtw method can describe the similarity of time series very well, which is more effective than the euclidean distance, but the complexity of the dtw method is too high. we apply our method to the data sets and compared the results with those obtained from the rst and second methods. a b c d e f figure : motif occurrence proles f for different categories of time series in six data sets: pigairwaypressure (a), pigcvp (b), phoneme (c), wafer (d), motestrain (e), and toesegmentation (f). each radar chart corresponds to a data set. each solid line in the radar map represents the average motif occurrence prole of a class of time series in the data set. fig. shows the radar charts of the motif occurrence proles averaged within different classes of time series in six representative data sets. each radar chart corresponds to a data set. each solid line in the radar map represents the average of the motif occurrence prole f of one category of time series in the data set. the time series belonging to the same class in the training set and the test set are included in the averaging process. it can be seen that the six radar charts are very different, implying that the motif prole method can classify different data sets effectively. for each data set, the difference between the prole lines in the corresponding radar chart represents the difference between different time series. the classication will be more accurate if the difference is larger. the three radar charts on the top panel of fig. have many prole lines that are not sufciently separated, which indicates that it would be hard to distinguish those categories. in contrast, each of the three radar charts on the bottom panel of fig. have only two prole lines that are well separated, which indicates that the two categories can be well distinguished. indeed, the classication accuracy is low for the former data sets and high for the later data sets (see also fig. ). cincecgtorso ethanollevel forda ham motestrain pigairwaypressure pigartpressure rock screentype wafer wine birdchicken computers ethanollevel forda housetwenty inlineskate insectepgregulartrain phoneme pigairwaypressure pigartpressure pigcvp refrigerationdevices screentype smallkitchenappliances toesegmentation wafer wine worms , figure : results of classication based on the occurrence frequency fi of motifs. (a) each data point in the gure corresponds to a data set. in the gure, the abscissa indicates the correct discriminant rate based on the motif distribution, , and the ordinate indicates the discriminant correctness rate, , based on the dtw distance. (c) the abscissa in the gure indicates the correct discrimination rate based on the distribution of the motifs, , and the ordinate indicates the correctness rate based on the euclidean distance, . euclidean distance, and dtw based. we use the triadic motif occurrence prole as the characteristic time series feature to classify the data sets. the classication accuracy is shown in fig. for data sets, our method is better than the dtw method, since . the data sets is shown in the lower right triangle of fig. fig. also compares the classication accuracy of the motif prole method and the euclidean distance method. there are data sets satisfying , indicating that our method performs better than the euclidean distance method for these data sets. for instance, for the data set smallkitchenappliances, our method is % more accurate than the euclidean distance method. in general, the dtw method does a very good job in the measurement of time series similarity. our method is superior to the dtw method for some data sets. ",
                "Subsections": [],
                "Groundtruth": "The text discusses the occurrence frequency distributions of triadic motifs in time series generated using the logistic map with different control parameters. Parameters are varied to observe how the time series behavior changes, ranging from permanent oscillations to chaotic behavior. Triadic motifs are analyzed for each parameter, and the occurrence frequency distributions of motifs are compared. The study extends to analyzing different types of chaotic time series for classification. The effectiveness of similarity measures for time series classification is evaluated using triadic motifs. Comparison is made with traditional methods like Euclidean distance and dynamic time warping. Results show that the motif profile method outperforms the Euclidean distance method in some cases, while dynamic time warping remains the most effective. The method is further applied to real time series datasets, and the classification accuracy is compared with existing methods, showing promising results in differentiating time series categories."
            }
        ],
        "Groundtruth": ""
    },
    {
        "Section_Num": "4",
        "Section": "4 Triadic time series motif analysis of the UCR Time Series Classification Archive",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "5",
        "Section": "5 Conclusions",
        "Text": "it is of great signicance to be able to discover the characteristics of time series from a unique perspective through novel methods. here, we studied the characteristics of time series through triadic time series motifs. we dened six different network motif. the simulation analysis nds that the distributions of the motif occurrence frequencies cor- responding to logistic maps and chaotic time series (chaotic henon map, chaotic ikeda map, hyperchaotic generalized henon map and hyperchaotic folded tower map) all have their own characteristics. the motif occurrence proles can quantify the time series characteristics in different dynamical systems and show comparative classication power as the dtw method. we apply the motif analysis to the ucr data sets. the advantage of the euclidean distance method is that the calculation is simple and fast. the dtw method performs best, but in some data sets, the performance is not as good as the motif prole method. our method has better accuracy than the dtw method for data sets. the starting point of our method is completely different from the euclidean distance method and the dtw method. this study is based on the complex networks, and mines the features in the time series. it is expected to be effectively improved in future research and provide a more effective method for measuring the similarity of time series. indeed, there are many methods for extracting motifs from time series. different motif extraction methods can describe different time series features. in order to improve the practicality of our method, we will develop different motif recognition methods to measure time series similarity. acknowledgements this work was supported by national natural science foundation of china (, , u) and fundamental research funds for the central universities (). references ahmadlou, m., adeli, h., adeli, a., new diagnostic eeg markers of the alzheimers disease using visibility graph. j. neural transm. , amig o, j., permutation complexity in dynamical systems. springer verlag berlin heidelberg. bagnall, a. j., lines, j., bostrom, a., large, j., keogh, e. j., the great time series classication bake off: a review and experimental evaluation of recent algorithmic advances. data min. knowl. discov. (), bandt, c., pompe, b., permutation entropy: a natural complexity measure for time series. phys. rev. lett. (), chiu, b., keogh, e., lonardi, s., probabilistic discovery of time series motifs. in: acm sigkdd international conference on knowledge discovery and data mining. acm, pp. dau, h. a., begum, n., keogh, e. j., semi supervision dramatically improves time series clustering under dynamic time warping. in: proceedings of the th acm international conference on information and knowledge management, cikm , indianapolis, in, usa, october -, pp. dau, h. a., keogh, e., kamgar, k., yeh, c.-c. m., zhu, y., gharghabi, s., ratanamahatana, c. a., yanping, hu, b., begum, n., bagnall, a., mueen, a., batista, g., october a. the ucr time series classication archive. https://www.cs.ucr.edu/eamonn/time_series_data_/. dau, h. a., silva, d. f., petitjean, f., forestier, g., bagnall, a. j., keogh, e. j., judicious setting of dynamic time warpings window width allows more accurate classication of time series. in: ieee international conference on big data, bigdata , boston,ma, usa, december -, pp. dau, h. a., silva, d. f., petitjean, f., forestier, g., bagnall, a. j., mueen, a., keogh, e. j., b. optimizing dynamic time warpings window width for time series data mining applications. data min. knowl. discov. (), dong, z., li, x., comment on network analysis of human heartbeat dynamics. appl. phys. lett. (), elsner, j. b., jagger, t. h., fogarty, e. a., visibility network of united states hurricanes. geophys. res. lett. , l gomes, e., batista, b., classifying urban sounds using time series motifs. advanced science and technology letters , hu, b., chen, y., keogh, e. j., classication of streaming time series under more realistic assumptions. data min. knowl. discov. (), hu, b., rakthanmanon, t., campana, b. j. l., mueen, a., keogh, e. j., establishing the provenance of historical manuscripts with a novel distance measure. pattern anal. appl. (), iacovacci, j., lacasa, l., a. sequential motif prole of natural visibility graphs. phys. rev. e , iacovacci, j., lacasa, l., b. sequential visibility graph motifs. phys. rev. e , keller, k., sinn, m., ordinal analysis of time series. physica a (), lacasa, l., luque, b., ballesteros, f., luque, j., nu no, j. c., from time series to complex networks: the visibility graph. proc. natl. acad. sci. u.s.a. (), lacasa, l., luque, b., luque, j., nu no, j. c., the visibility graph: a new method for estimating the hurst exponent of fractional brownian motion. epl (europhys. lett , lacasa, l., toral, r., description of stochastic and chaotic series using visibility graphs. phys. rev. e , linardi, m., zhu, y., palpanas, t., keogh, e. j., a. valmod - scalable discovery of variable length motifs in data series. in: proceedings of the international conference on management of data, sigmod conference , houston, tx, usa, june -, pp. linardi, m., zhu, y., palpanas, t., keogh, e. j., b. valmod: a suite for easy and exact detection of variable length motifs in data series. in: proceedings of the international conference on management of data, sigmod conference , houston, tx, usa, june -, pp. mccullough, m., small, m., iu, h. h. c., stemler, t., multiscale ordinal network analysis of human cardiac dynamics. philos. trans. a (), mccullough, m., small, m., stemler, t., iu, h. h. c., time lagged ordinal partition networks for capturing dynamics of continuous dynamical systems. chaos (), mcgovern, a., rosendahl, d. h., brown, r. a., droegemeier, k. k., identifying predictive multi dimensional time series motifs: an application to severe weather prediction. data min knowl disc (-), mi skiewicz, j., ausloos, m., correlation measure to detect time series distances, whence economy globalization. physica a , mori, u., mendiburu, a., keogh, e. j., lozano, j. a., reliable early classication of time series based on discriminating the classes over time. data min. knowl. discov. (), mueen, a., keogh, e., online discovery and maintenance of time series motifs. in: acm sigkdd international conference on knowledge discovery and data mining, washington, dc, usa, july. pp. mueen, a., keogh, e., zhu, q., cash, s., westover, b., exact discovery of time series motifs. in: proceedings of the siam international conference on data mining. siam, pp. mueen, a., keogh, e. j., extracting optimal performance from dynamic time warping. in: proceedings of the nd acm sigkdd interna- tional conference on knowledge discovery and data mining, san francisco, ca, usa, august -, pp. ni, x.-h., jiang, z.-q., zhou, w.-x., degree distributions of the visibility graphs mapped from fractional brownian motions and multifractal random walks. phys. lett. a , petitjean, f., forestier, g., webb, g. i., nicholson, a. e., chen, y., keogh, e. j., faster and more accurate classication of time series by exploiting a novel dynamic time warping averaging algorithm. knowl. inf. syst. (), petitjean, f., forestier, g., webb, g. i., nicholson, a. e., chen, y.-p., keogh, e. j., dynamic time warping averaging of time series allows faster and more accurate classication. in: ieee international conference on data mining, icdm , shenzhen, china, december -, pp. qian, m.-c., jiang, z.-q., zhou, w.-x., universal and nonuniversal allometric scaling behaviors in the visibility graphs of world stock market indices. j. phys. a , shao, z.-g., network analysis of human heartbeat dynamics. appl. phys. lett. (), shokoohi yekta, m., hu, b., jin, h.-x., wang, j., keogh, e. j., generalizing dtw to the multi dimensional case requires an adaptive approach. data min. knowl. discov. (), silva, d. f., de souza, v. m. a., ellis, d. p. w., keogh, e. j., batista, g. e. a. p. a., exploring low cost laser sensors to identify ying insect species - evaluation of machine learning and signal processing methods. journal of intelligent and robotic systems (supplement-), silva, d. f., giusti, r., keogh, e. j., batista, g. e. a. p. a., speeding up similarity search under dynamic time warping by pruning unpromising alignments. data min. knowl. discov. (), son, n. t., anh, d. t., discovery of time series k motifs based on multidimensional index. knowledge and information systems (), tang, q., liu, j., liu, h.-l., comparison of different daily streamow series in us and china, under a viewpoint of complex networks. mod. phys. lett. b , tarango, j., keogh, e. j., brisk, p., accelerating the dynamic time warping distance measure using logarithmetic arithmetic. in: th asilomar conference on signals, systems and computers, acssc , pacic grove, ca, usa, november -, pp. tataw, o. m., reddy, g. v., keogh, e. j., roy chowdhury, a. k., quantitative analysis of live cell growth at the shoot apex of arabidopsis thaliana: algorithms for feature measurement and temporal alignment. ieee/acm trans. comput. biology bioinform. (), wang, l., chng, e. s., li, h., a tree construction search approach for multivariate time series motifs discovery. pattern recognition letters (), wang, x.-y., mueen, a., ding, h., trajcevski, g., scheuermann, p., keogh, e. j., experimental comparison of representation methods and distance measures for time series data. data min. knowl. discov. (), xie, w.-j., han, r.-q., jiang, z.-q., wei, l.-j., zhou, w.-x., analytic degree distributions of horizontal visibility graphs mapped from unrelated random series and multifractal binomial measures. epl (europhys. lett (), xie, w.-j., han, r.-q., zhou, w.-x., a. tetradic motif proles of horizontal visibility graphs. commun. nonlinear sci. numer. simul. xie, w.-j., han, r.-q., zhou, w.-x., b. triadic time series motifs. epl (europhys. lett. xie, w.-j., zhou, w.-x., horizontal visibility graphs transformed from fractional brownian motions: topological properties versus the hurst index. physica a , xu, x.-k., zhang, j., small, m., superfamily phenomena and motifs of networks induced from time series. proc. natl. acad. sci. u.s.a. (), yang, y., wang, j.-b., yang, h.-j., mang, j.-s., visibility graph approach to exchange rate series. physica a , yeh, c.-c. m., kavantzas, n., keogh, e. j., meaningful multidimensional motif discovery. in: ieee international conference on data mining, icdm , new orleans, la, usa, november -, pp. yeh, c.-c. m., zhu, y., ulanova, l., begum, n., ding, y.-f., dau, h. a., zimmerman, z., silva, d. f., mueen, a., keogh, e. j., time series joins, motifs, discords and shapelets: a unifying view that exploits the matrix prole. data min. knowl. discov. (), zakaria, j., mueen, a., keogh, e. j., young, n. e., accelerating the discovery of unsupervised shapelets. data min. knowl. discov. (), zhang, j.-y., zhou, j., tang, m., guo, h., small, m., zou, y., constructing ordinal partition transition networks from multivariate time series. sci. rep. (), zhu, y., zimmerman, z., senobari, n. s., yeh, c.-c. m., funning, g., mueen, a., brisk, p., keogh, e. j., exploiting a novel algorithm and gpus to break the ten quadrillion pairwise comparisons barrier for time series motifs and joins. knowl. inf. syst. (), ",
        "Subsections": [],
        "Groundtruth": "The study explored time series characteristics using triadic time series motifs, defining six network motifs. Simulation analysis revealed distinct motif occurrence distributions for various dynamical systems. The motif occurrence profiles demonstrated comparative classification power to the dynamic time warping (DTW) method. Applying the motif analysis to UCR datasets showed improved accuracy over DTW. The study, based on complex networks, aims to enhance future research for more effective time series similarity measurement methods. Various motif extraction methods can capture different time series features. Future work will focus on developing different motif recognition methods to enhance the practicality of the proposed approach."
    },
    {
        "Section_Num": "NA",
        "Section": "NA",
        "Text": "arxiv:v jan time series classication based on triadic time series motifs wen jie xiea,b, rui qi hanc, wei xing zhoua,b,c, adepartment of finance, east china university of science and technology, shanghai , china bresearch center for econophysics, east china university of science and technology, shanghai , china cdepartment of mathematics, east china university of science and technology, shanghai , china abstract it is of great signicance to identify the characteristics of time series to qualify their similarity. we dene six types of triadic time series motifs and investigate the motif occurrence proles extracted from logistic map, chaotic logistic map, chaotic henon map, chaotic ikeda map, hyperchaotic generalized henon map and hyperchaotic folded tower map. based on the similarity of motif proles, we further propose to estimate the similarity coefcients between different time series and classify these time series with high accuracy. we further apply the motif analysis method to the ucr time series classication archive and provide evidence of good classication ability for some data sets. our analysis shows that the proposed triadic time series motif analysis performs better than the classic dynamic time wrapping method in classifying time series for certain data sets investigated in this work. keywords: time series analysis, classication, time series motifs, motif proles, dynamic time wrapping jel: c, p, z introduction ",
        "Subsections": [],
        "Groundtruth": "The text presents a method for time series classification based on triadic time series motifs. Six types of motifs are defined and investigated in various chaotic maps. By analyzing motif occurrence profiles and estimating similarity coefficients, the method achieves high accuracy in classifying time series. The approach is applied to the UCR time series classification archive, showing better performance compared to the classic dynamic time wrapping method for certain datasets."
    },
    {
        "Section_Num": "1",
        "Section": "1 Introduction",
        "Text": "with the development of network science, the mathematical modeling of epidemic spread- ing has involved in a research area across many disciplines including mathematical biol- ogy, physics, social science, computer and information science, and so on. on the basis of classical epidemic spreading models, such as, susceptible infected susceptible (sis) corresponding author: tomlk@hqu.edu.cn arxiv:v jan model, susceptible infected recovered (sir) model, and susceptible infected recovered- susceptible (sirs) model, a variety of epidemic spreading models in networks were developed. investigations on these models have important signicance in public health domain, especially in infectious disease epidemiology, by providing a number of interesting and unexpected behaviors. the theoretical studies of epidemic spreading models in complex networks rely mostly on the mean eld theory approaches, especially on degree based mean eld (dbmf) the- ory which was the rst theoretical approach presented for the analysis of general dynam- ical processes on complex networks . this approach assumes that all nodes of degree k are statistically equivalent, and any given vertex of degree k is connected with the same probability to any node of degree k . therefore, the epidemic spreading model based on dbmf theory depends in general on the statistical topological properties of the under- lying networks instead of the whole network structure, resulting into the loss of detailed features of network topologies such that it is dicult to deeply understand the eect of network structures on the disease (or information) propagation. to the best of our knowledge, in , mieghem et al. rstly proposed the continuous time node based sis epidemic spreading model for understanding the inuence of network characteristics on epidemic spreading. youssef and scoglio established a new individual based sir model with the whole description of network structures. very recently, yang et al. suggested a node based susceptible latent exploding susceptible (slbs) model, and in the same year they presented a heterogeneous node based sirs model where each node has the dierent infected and recovered rates . the above models assume that disease transmission takes place between individuals in networks. however, diseases are propagated not only by the contact between individuals in the same population, but also by the contact between individuals and infective media. for instance, many human diseases, such as dengue fever, malaria, chagas disease, and so on, can be transmitted by the infective mosquito. for this case, shi et al. established a new sis epidemic model with an infective medium, which describes epidemics transmitted by infective media on various complex networks. by dierentiating the infective medium from individuals, yang et al. proposed a modied sis model. wang et al. pre- sented a modied sis with an infective vector by incorporating some infectious diseases. it is noteworthy that these existing models with infective media are degree based instead of node based. the motivation of this paper is to build a node based sirs epidemic model with infective media on various complex networks by integrating the node based approach and the infective medium, and investigate the stability of the equilibrium as well as the inuence of network structures, the infective medium and the eective recovered rate on the network infected steady state. the rest of this paper is organized as follows. some denitions and lemmas are introduced in sec. in sec. , a node based sirs epidemic network model with infective media is built and then its equilibrium is given. the global asymptotical stability analysis with respect to the equilibrium is performed in sec. in sec. , numerical simulations of three typical network topologies are provided for further verifying the theoretical results. the correlation between the infected percents of nodes and its degree, as well as the impact of some critical parameters on network average infected percents, are studied theoretically and numerically. finally, some conclusions and discussions are given in sec. ",
        "Subsections": [],
        "Groundtruth": "The text discusses the development of epidemic spreading models in complex networks within the context of network science. Various classical epidemic models have been adapted for network analysis, with a focus on understanding disease propagation and the impact of network structures. Theoretical studies often rely on mean field theory approaches, specifically degree-based mean field theory, to analyze dynamic processes on complex networks. Several node-based epidemic models with infective media have been proposed to better represent real-world disease transmission scenarios. The paper aims to create a node-based SIRS epidemic model with infective media on complex networks and examines the stability of the equilibrium and the influence of network structures and parameters on infected steady state. The paper includes definitions, model development, equilibrium analysis, numerical simulations on network topologies, and discussions on the findings."
    },
    {
        "Section_Num": "2",
        "Section": "2 Preliminaries",
        "Text": "first, some requisite denitions and lemmas are given as follows. denition : a matrix is metzler if its all o diagonal entries are non negative. denition : a matrix a is hurwitz stable if there exists a positive matrix d such that atd + da is negative denite. denition : a matrix a is diagonally stable if there exists a positive denite diagonal matrix d such that atd + da is negative denite. obviously, the diagonally stable matrix is hurwitz stable, and the opposite is also true for metzler matrices. lemma : a hurwitz and metzler matrix is diagonally stable. lemma : let a be a hurwitz and metzler matrix, d be a positive denite diagonal matrix, d and d be negative denite diagonal matrices. then, \u0014 a d d d \u0015 is diagonally stable. lemma : consider a smooth dynamical system x = g(x) dened at least in a compact set c. then, c is positively invariant if g(x) is pointing into c for any smooth table : description of parameters. parameters description si(t) the percents that node i is susceptible at time t. ii(t) the percents that node i is infected at time t. ri(t) the percents that node i is recovered at time t. sm(t) the percents that media is susceptible at time t. im(t) the percents that media is susceptible at time t. m the probability that a susceptible node is infected by an infective media. the probability that a susceptible node is infected by an infected neighbor. the probability of infective node turns into an immunized one. the probability of infective node turns into a susceptible one. the probability that an immunized node loses immunity into a susceptible one. the birth (death) rate of the medium. m the probability of a susceptible medium transforming into an infected one. point xon the boundary of c. ",
        "Subsections": [],
        "Groundtruth": "A metzler matrix has non-negative off-diagonal entries, a Hurwitz stable matrix has a positive matrix making certain operations negative definite, and a diagonally stable matrix has a positive definite diagonal matrix for negative definiteness. Hurwitz and metzler matrices are diagonally stable. A system is positively invariant if it points into a compact set for any smooth table. Parameters include percentages of susceptibility, infection, and recovery for nodes and media, probabilities of infection transmission and state transitions, birth/death rates, and boundary points."
    },
    {
        "Section_Num": "3",
        "Section": "3 Model formulation",
        "Text": "to begin with, we consider an underlying network( or a simple graph) denoted g = (v, e) where v is the set of nodes and e is the set of edges. the nodes labeled from number to number n represent the individuals in propagation networks, and the edges stand for network links through which disease can propagate. in a simpler way, we denote a = (aij)nn the adjacent matrix of graph g describing network topological structures, where aij = if there is an edge between node i and node j, otherwise aij = assume that (h) each node in the network has three possible states: susceptible(s), infected (i), and recovered (r), whereas the media has two possible states: susceptible (sm) and infected (im); (h) both states s and i convert each other with certain proba- bility, and the state sm is infected with the probability of m into the state im, but not vice versa; (h) the state i is recovered with the probability of into the state r , and the state r is converted with the probability of into the state s after the immunity is out of work; (h) the state s in the underlying network is infected with the probability of m by the infective media, and the media is with the birth (death) rate of . for simplicity, the variables and parameters in this node based sirs model with infec- tive media are summarized in table , and the schematic diagram of the model is shown in fig. let xi(t) = , , and represent three states of node i at time t: the susceptible figure : the schematic diagram of node based sirs network model with media. (a) the sirs network part, (b) the media part. (si(t)), the infected (ii(t)) and the recovered (ri(t)), respectively. xm(t) = , , and represent three states of media at time t: the susceptible (sm(t)), the infected (im(t)) and the dead, respectively. the state of individuals at time t can be expressed as by the vector x(t) = . then si(t) = p{xi(t) = }, ii(t) = p{xi(t) = }, ri(t) = p{xi(t) = }, sm(t) = p{xm(t) = }, im(t) = p{xm(t) = } according to the assumptions, it implies the following probability of state transition: p{xi(t + t) = |xi(t) = } = t[mim(t) + n x j= aijij(t)] + o(t) p{xi(t + t) = |xi(t) = } = t + o(t) p{xi(t + t) = |xi(t) = } = t + o(t) p{xi(t + t) = |xi(t) = } = t + o(t) p{xm(t + t) = |xm(t) = } = mt + o(t) p{xm(t + t) = |xm(t) = } = t + o(t) p{xm(t + t) = |xm(t) = } = t + o(t) by using the total probability law, one can obtain ii(t + t) =p{xi(t + t) = } =p{xi(t) = }p{xi(t + t) = |xi(t) = } + p{xi(t) = }p{xi(t + t) = |xi(t) = } + p{xi(t) = }p{xi(t + t) = |xi(t) = } =si(t)( t[mim(t) + n x j= aijij(t)]) + ii(t)t + ri(t)t + o(t). let t , we get dsi(t) dt = [msi(t)im(t) + si(t) n x j= aijij(t)] + ri(t) + ii(t). similarly, it is easy to get the equations dominating ii(t), ri(t), rm(t) and im(t). col- lecting them together, we have the following n + dimensional dynamical system: dsi(t) dt = [msi(t)im(t) + si(t) n p j= aijij(t)] + ri(t) + ii(t), dii(t) dt = msi(t)im(t) + si(t) n p j= aijij(t) ( + )ii(t), dri(t) dt = ii(t) ri(t), dsm(t) dt = sm(t) msm(t), dim(t) dt = msm(t) im(t), () with initial condition (s(), , sn(), ii(), , in(), ri(), , rn(), sm(), im())t e , where e = {(s(t), , sn(t), i(t), , in(t), r(t), , rn(t), sm, im)t rn+ + | si(t) + ii(t) + ri(t) = , sm + im = , i = , , n}. remark : from the view point of continuous time markov chain , model () is an approximation one on account of the linear transition rate instead of exact one from state s to state i. the performance examined in appendix c shows that model () is able to well forecast the epidemic dynamics of model () built by means of markov chain technique. furthermore, the dynamical behaviors of approximation models is more easily studied by applying the stability theory and method, and thus the similar approximation model is directly built and studied in a large number of related literatures. since si(t) + ii(t) + ri(t) , sm + im , i n, system () can be reduced into the following system: dii(t) dt = m( ii(t) ri(t))im(t) + ( ii(t) ri(t)) n p j= aijij(t) ( + )ii(t), dri(t) dt = ii(t) ri(t), dim(t) dt = m( im(t)) im(t). () with initial condition (i(), , in(), r(), , rn(), im())t , i = , , n, where = {(i(t), , in(t), r(t), , rn(t), im)t rn+ + |ii(t) + ri(t) , im , i = , , n}. let the right hand terms in () equal to zero, one gets an equilibrium e= (i i , r i , im) which is only one proven in appendix a, here im= m m + , r i = i i , i i = mm m+ + n p j= aiji j + + m( + ) m m+ + ( + ) n p j= aiji j . from the above equality, it is easy to get that i i < +/, implying that the percent with the infected state for any node is less than +/. remark : obviously, the equilibrium is not virus free, implying that the virus exists persistently in each individual. the phenomenon can be understood by the fact that the endemic disease remains safely under cover in each individual in some local areas. although the equilibrium is given in the implicit form, it can be calculated out by the numerical iterative method. remark : when the infected rate of media m = , the model is reduced to the node based sirs epidemic model with a virus free equilibrium, to some extent implying that our extended model is rational and practical. the infected medium terms not only increase the dimension of sirs models, but more importantly make stability analysis more complicated, especially in the part of global attractivity. ",
        "Subsections": [],
        "Groundtruth": "The text discusses a model formulation for assessing disease propagation on a network. The model considers nodes representing individuals and edges as network links for disease spread. Each node can be in susceptible, infected, or recovered states, while the media can be susceptible or infected. State transitions occur with specific probabilities, leading to a dynamic system of equations describing the spread of infection. The model utilizes linear transition rates and approximations from continuous-time Markov chains. Stability analysis is employed to study the system's behaviors, with equilibrium points indicating the persistence of the virus within individuals. The model's complexity increases with the inclusion of infected media, challenging global attractivity analysis."
    },
    {
        "Section_Num": "4",
        "Section": "4 Stability analysis with respect to equilibria",
        "Text": "",
        "Subsections": [
            {
                "Section_Num": "4_1",
                "Section": "4.1 Local stability",
                "Text": "to analyze the local stability of system () at the equilibrium e, we start with its jacobian je= \u0012 b c m \u0013 , where c = m(s , , s n, , , )t, b = \u0012 d mimen diag(ai) en en \u0013 , i= (i , i , , i n)t, en is the identity matrix of order n, diag() represents the diagonal matrix, and d = diag(s , s , , s n)a ( + )diag(s + i s , s + i s , , s n + i n s n ). () theorem : system () is asymptotically stable at the equilibrium e proof: obviously, m is a negative eigenvalue of je, and other eigenvalues are determined by matrix b. next, we show that all the eigenvalues of b have negative real part. for convenience, dene three matrices as follows k =a + diag \u0012 s , , s n \u0013 , () k =k + + max i \u0012 s i \u0013 en, () k =a + diag \u0012 s , , s n \u0013 diag \u0012s + i s , , s n + i n s n \u0013 . () here we consider an undirected and connected graph, so the adjacent matrix a is an irreducible one, indicating that k is also an irreducible matrix. according to perron- frobenius theorem , k has a positive eigenvector v corresponding to the largest eigenvalue max(k), i.e., kv = max(k)v. then kv = \u0012 max(k) + max i s i \u0013 v, and vtki= \u0012 max(k) + max i s i \u0013 vti. () on the other hand, it is easy from the second equality of system () to get that mims+ diag(s)ai( + )i= , where s= (s , , s n)t. thus, (diag(s))ki= mims implies ki when s s s n = since v and iare positive vectors, it holds that max(k) + max i s i , and max(k) = max(k) + max i s i from () and (), it follows that max(k) < max \u0012 a + diag( s , , s n ) \u0013 = max(k) thus, k is a negative denite matrix, and then d = diag(s)k is also a negative denite one. that is to say, d is a hurwitz and metzler matrix. according to lemma , matrix b is diagonally stable. therefore, the equilibrium eof system () is asymptoti- cally stable. ",
                "Subsections": [],
                "Groundtruth": "The text discusses the local stability analysis of a system at equilibrium using Jacobian matrices and eigenvalues. The system is proven to be asymptotically stable at the equilibrium point by showing that all eigenvalues have negative real parts. The analysis involves defining matrices and leveraging Perron-Frobenius theorem and irreducibility properties to establish stability criteria. The system stability is ultimately confirmed through the characterization of matrices as negative definite and diagonally stable."
            },
            {
                "Section_Num": "4_2",
                "Section": "4.2 Global attractivity",
                "Text": "to proof the global attractivity, it needs to determine the positively invariant set (in brief, once a trajectory of the system enters the set, it will never leave it again). next, it is not dicult to proof that = {(i(t), , in(t), r(t), , rn(t), im)t|ii(t) + ri(t) , im , i = , , n} is an invariant set. theorem : is a positively invariant set for system (). proof: denote the boundary of , and then it consists of the following n + hyperplanes: i = {(i(t), , in(t), r(t), , rn(t), im(t))t |ii(t) = }, i = , , n, n+i = {(i(t), , in(t), r(t), , rn(t), im(t))t |ri(t) = }, i = , , n, n+i = {(i(t), , in(t), r(t), , rn(t), im(t))t |ii(t) + ri(t) = }, i = , , n, n+ = {(i(t), , in(t), r(t), , rn(t), im(t))t |im(t) = }, n+ = {(i(t), , in(t), r(t), , rn(t), im(t))t |im(t) = }, for simplicity and convenience, system () is rewritten as: dz(t) dt = g(z(t)) with initial condition z() . take the outer normal vectors corresponding to n + hyperplanes as follows: pi = (, , | {z } i , , , , , )t, pn+i = (, , , , , | {z } n+i , , )t, pn+i = (, , | {z } i , , , , , | {z } n+i , , )t, pn+ = (, , , , , , | {z } n+ )t, pn+ = (, , , , , , | {z } n+ )t, and let z= (i , , i n, r , , r n, im)t be a smooth point of . on the basis of these hyperplanes, ve dierent cases of zare discussed respectively. case : for i i = , ( dz dt |zi, pi) = m( r i )im( r i ) n p j= aiji j < case : for r i = , ( dz dt |zn+i, pn+i) = i i < case : for i i + r i = , ( dz dt |zn+i, pn+i) = i i r i < case : for im= , ( dz dt |zn+, pn+) = m < case : for im= , ( dz dt |zn+, pn+) = < therefore, g(z) is pointing to , and is positively invariant according to lemma theorem : the equilibrium eof system () is globally attractive on {}. proof: denote y(t) = (y(t), , yn(t), yn+(t))t where yi(t) = ii(t), yn+i = ri(t), yn+ = im(t) (i = , , , n), and then the virus equilibrium y= (y , , y n+), here yi= (i = , , n). to explore the asymptotic behavior of solutions of eq. (), we dene two functions as follows: f(y(t)) = max i yi(t) y i : r, f(y(t)) = min i yi(t) y i : r. both functions are continuous and exist right hand derivatives along solutions of eq.(). let y(t) is the solution of eq.(), and suppose that f(y(t)) = yi(t) y i , t , for some t and suciently small > then we have f (y(t))|() = y i(t) y i , where f (y(t))|() lim h+sup f(y(t+h))f(y(t)) h . next, we proof that the derivative of f(y(t)) at t is non negative. according to the denition of f(y(t)), it follows that yi(t) y i yi(t) y i , i = , , , n + for f(y(t)) > , three cases as below are discussed (here t is ignored for conciseness). case : i n. y i y i(t) yi(t) =y i yi {m( yi yn+i)yn+ + ( yi yn+i) n x j= aijyj ( + )yi} <m( y i y n+i)yn+ y i yi + ( y i y n+i)y i yi n x j= aijyj ( + )y i <m( y i y n+i)y n+ + ( y i y n+i) n x j= aijy j ( + )y i = case : n + i n. y i y i(t) yi(t) = (yin yi) y i yi < y in y i = case : i = n + y i y i(t) yi(t) = (yin yi) y i yi < y in y i = since y i > and yt(t) > , one get y i(t) < , implying f (y(t)) < similarly, f |()(y(t)) if f(y(t)) = , f |()(y(t)) > if f(y(t)) < , and f |()(y(t) if f(y(t)) = denote u(y) = max{f(y) , }, y , v (y) = min{ f(y), }, y . obviously, u(y) and v (y) are non negative and continuous in , and u (y)|() and v (y)|() let hu = {y |u |()(y(t)) = } and hv = {y |v |()(y(t)) = }, then we have hu = {y : yj(t) y j} {} and hv = {y : y j yj(t) } {}. it follows from the lasalle invariance principle that any solution of system () staring in approaches hu hv = {y} {}. therefore, any solution y(t) with initial value y() satises limty(t) = y, i.e., yis globally attractive in . remark : together with local asymptotical stability, it is easily obtained that the equilibrium ein the sirs model with media is globally asymptotically stable. by the way, without the medium propagation, i.e., m = , system () has a virus free equilibrium e which is globally asymptotically stable if max(a) < ( + )/, otherwise unstable. here max(a) is the largest eigenvalue of the topological matrix a, and ( + )/ represents the actual eective recovered rate. ",
                "Subsections": [],
                "Groundtruth": "The section discusses the global attractivity of a system by proving the existence of a positively invariant set where trajectories do not leave once they enter. An invariant set is defined, and outer normal vectors are calculated for hyperplanes. Different cases are analyzed to show that the system points towards the invariant set. The equilibrium of the system is proven to be globally attractive through the exploration of the asymptotic behavior of solutions. The final conclusion is that the equilibrium in the model is globally asymptotically stable even with media propagation."
            }
        ],
        "Groundtruth": ""
    },
    {
        "Section_Num": "5",
        "Section": "5 Numerical simulations",
        "Text": "",
        "Subsections": [
            {
                "Section_Num": "5_1",
                "Section": "5.1 Three typical network models",
                "Text": "in order to verify the above theoretical results, we choose three typical network struc- tures (fully connected network, small world network and scale free network), and solve numerically system () with m = , = , = , = , = , = , m = . here, the fully connected network means that all nodes are connected with each other, the small world network is generated from nearest neighbor network with the probability p = of random adding edges (called nw small world network), and the scale free network is a ba scale free one with m = and m = each type of network is with nodes, respectively. figures - show the evolution of ii(t), ri(t) and im(t) over time for the fully connected network, nw small world network and ba scale free network, respectively. obviously, the infected (i) and susceptible (s) states of each node in each network tends to their equilibrium states over time, namely, ii(t) i i and si(t) s i (t ), implying that it further veries our theoretical results, and the infected percents of node is very low for sparsely connected networks, such as nw and ba networks. interestingly, for the fully connected network, all nodes tend to the same equilibrium state as time goes, namely, ii(t) iand si(t) s(t ). but for the small world network and scale free network, all nodes approach their equilibrium but nonidentical states as time goes, namely, ii(t) i i and si(t) s i (t ). in fact, the steady state of each node is closely related with its degree according to the formula of equilibria. for time t p(t) ii(t) ri(t) im(t) figure : the states ii(t), ri(t) and im(t) of time for fully connected networks and random initial conditions. time t p(t) ii(t) ri(t) im(t) figure : the states ii(t), ri(t) and im(t) of time for nw small world networks and random initial conditions. time t p(t) ii(t) ri(t) im(t) figure : the states ii(t), ri(t) and im(t) of time for ba scale free networks and random initial conditions. the fully connected network, each node has the same degree, resulting into that each node tends to the same equilibrium state, and the equilibrium states isatises + ( + )(n ) < i + < + ( + )(n ), and thus i +/ for enough large size networks. please refer to appendix b for the detailed derivation. for the nw small world network, the node with large (small) degree has large (small) infected equilibrium state, as shown in fig. (a). however, the correlation is a little weaker for the ba scale free network which is a heterogeneous one, please see fig. (b). in a word, the infected (susceptible) equilibrium state i i (s i ) is positively (negatively) correlated with the degree of nodes, and the node with higher degree is easier to be infected, further verifying the result obtained by the article . i* i node i degree i* i degree i* i node i degree i* i degree (b) (a) figure : the correlation between the node degree (red solid line) and its infected equilibrium state (blue solid line) for nw small world (a) and ba scale free networks (b). ",
                "Subsections": [],
                "Groundtruth": "The text discusses three typical network models - fully connected network, small world network, and scale free network. The fully connected network has all nodes connected to each other, the small world network is generated from a nearest neighbor network by randomly adding edges, and the scale free network follows a specific scale free model. The text explores the evolution of infected and susceptible states of nodes in each network over time, showing that nodes tend towards equilibrium states. It also highlights how the equilibrium states are related to the nodes' degrees, with higher degree nodes being more susceptible to infection. The correlation between node degree and infected equilibrium state is stronger in small world networks compared to scale free networks."
            },
            {
                "Section_Num": "5_2",
                "Section": "5.2 Impact of system parameters",
                "Text": "to learn more about this proposed model, we now analyze the inuence of several impor- tant parameters, namely, m, , , and , on network average infected state. let us look back these parameters, m means the infected rate resulted from the medium, represents the infected rate from the nodes neighbour, and can be considered as the potential infected rate due to the fact that the state of nodes never transforms back once it changes to the susceptible state from the recovered state, and the susceptible state is the potential part transferred into the infected state. represents the recovered rate from the infected state to the recovered state. here we call / and / the actual and potential eective recovered rate, respectively. we rstly analyze the inuence of m, , and on network average infected state for the general topologies. according to the implicit dierentiation theorem, it is easy to obtain the following theoretical results through the formula of equilibria. theorem : under the assumptions (h)-(h), it follows that (a) i m > , (b) i > , (c) i > , (d) i < , where i= n pn i= i i called network average infected state. (please see appendix d for the proof) the above theorem implies that the network average infected state rises with the increase of m, and decreases with the increase of / or /. furthermore, we numerically verify the theoretical implications for three typical net- work topologies, respectively. fig. (a) shows that with xed = , = , = , = , m = , = , as m increases, the network average infected state rises gradually for ba scale free network, but almost unchanged for the fully connected network and nw small world network, implying that the heterogenous network is sensitive to the infected rate of the medium, but the homogeneous network is the opposite. it is shown from fig. (c)(b)(d) that the average infected state of three typical net- works decreases exponentially as the potential recovered rate / increases with xed = , and it can be located at low level when / is enough large. it implies that the epidemic spreading on networks can be signicantly suppressed by the even small increase of the potential eective recovered rate /. similarly, the average infected state goes down with the increase of /, and the virus decreases exponentially for nw small world network, faster than those for the fully- connected network and ba scale free network, as shown in fig. a possible reason is that the small world network has the properties of the short average path length and small average degree. ",
                "Subsections": [],
                "Groundtruth": "The impact of system parameters such as infected rate from the medium (m), infected rate from node neighbors (), recovered rate from infected state to recovered state (/), and potential effective recovered rate (/) on network average infected state is analyzed. Theoretical results show that network average infected state increases with an increase in m and decreases with an increase in / or /. Numerical verification on different network topologies confirms these results. Heterogeneous networks are sensitive to the medium's infected rate, while homogeneous networks are not. Additionally, increasing the potential effective recovered rate leads to a significant suppression of epidemic spreading. The small world network exhibits faster virus reduction compared to fully connected and scale-free networks due to its unique properties."
            }
        ],
        "Groundtruth": ""
    },
    {
        "Section_Num": "6",
        "Section": "6 Conclusions and discussions",
        "Text": "in summary, this paper has presented a node based sirs epidemic model with media for understanding the disease spreading of networks with media propagation, where there is m i fcn ba nw / i = = = = / i = = = = / i = = = = (b) (c) (d) (a) figure : (a) the inuence of the infected rate m of media on network average infected states, and the curves from top to bottom correspond to the fully connected network, nw small world network and ba scale free network, respectively. the inuence of the potential spreading rate / on network average virus at dierent values of for the fully connect network (b), nw small world network (c) and ba scale free networks (d). the curves from bottom to top in each subplot correspond to the value of = , , , , respectively. i = = = = i = = = = / i = = = = (a) (b) (c) figure : the inuence of the actual spreading rate / on network average infected steady state at dierent values of for the fully connect network (a), nw small world network (b) and ba scale free networks (c). the curves from bottom to top in each subplot correspond to the value of = , , , , respectively. only an equilibrium yet not virus free one that is always globally asymptotically stable through the stability analysis. without the medium propagation, the model has a virus- free equilibrium which is globally asymptotically stable when the maximum eigenvalue of topological matrices is less than the eective recovered rate ( + )/. three typical networks, i.e., the fully connected, small world, and scale free networks, are applied to numerical investigations for further verifying the theoretical results. numer- ical simulations also show that the sparse network has less infected percents. in addition, it shows that the infected percents of network nodes have the positive correlation with the degree of the node, in particular for the homogenous network, such as the fully connected network and small world network. finally, theoretical and numerical studies on the inuence of the eective recovered rate and medium propagation rate on network average infected percents imply that network average infected percents go up (down) with the increase of the medium propagation rate (the eective recovered rate). numerical investigations further show that the medium propagation rate does nothing with network average infected percents for homogenous networks, and the infected percents decease exponentially with the increase of the eective recovered rate. moreover, the percents can be controlled at low level only if the eective recovered rate is enough large, in other words, only if the eective infected rate is enough small. conicts of interest the authors declare that they have no conicts of interest regarding the publication of this paper. acknowledgements this work is supported in part by the national natural science foundation of china (grants no. , and ), in part by the promotion program for young and middle aged teacher in science and technology research of huaqiao univer- sity (zqn yx), in part by the program for new century excellent talents in fujian province university in , and in part the project of education and scientic research for middle and young teachers in fujian province(jat, ja). appendix a: proof of uniqueness of equilibria here we now prove the equilibrium eis one and only equilibrium xed point. first of all, dene a continuous mapping h = (h, , hn) : (, )n (, )n as below: hi(y) = mm m+ + n p j= aijyj + + m( + ) m m+ + ( + ) n p j= aijyj , i = , , n. we can assert that the equilibrium is one and only if h is monotonic and exist a unique xed point. claim : h is monotonic. proof: let x, z (, )n, x z, (xi zi, i = , , n). then, hi(x) = mm m+ + n p j= aijxj + + m( + ) m m+ + ( + ) n p j= aijxj mm m+ + n p j= aijzj + + m( + ) m m+ + ( + ) n p j= aijzj = hi(z), which implies h(x) h(z), the proof of claim is completed. claim : h admits a unique xed point in (, )n. proof: () existence. since h(y) is monotonic and continue for y [, )n, it follows that hi() < hi() < hi(), (, )n. on the other hand, hi() > and hi() < , implying (, hi())n and (hi(), )n, such that n (, )n where (, , n) and (, , n). we conclude that i hi() i, i < i, i = , , n, so the restriction h on the compact convex set = . maps into . it follows from brouwer fixed point theorem that h exists a xed point u . () uniqueness. suppose h exists the other xed point v = (v , , v n)t (, )n. let = max i u i v i and i = argmax i u i v i , without loss of generality, we may assume > , it follows that u i = hi(u ) hi(v ) = mm m+ + n p j= aijv j + + m( + ) m m+ + ( + ) n p j= aijv j < mm m+ + n p j= aijv j + + m( + ) m m+ + ( + ) n p j= aijv j = hi(v ) = v i, which contradicts the assumption that u i = v i hence, the xed point is unique. this completes proof. appendix b: computation of the equilibrium in this appendix, we give the computation process of the equilibrium for the fully- connected network with n nodes. assume that i i = i, i = , , n, then i= mm m+ + n p j= aiji + + m( + ) m m+ + ( + ) n p j= aiji . denote a = ( + )(n ), b = ( + + m( + ) (n )), c = m, and m = mm m+. then, the above equality can be rewritten as ai + bi+ c = () as c a < , it follows from the hurwitz criterion that eq. () has two opposite sign roots. it is easy to verify that the positive root i + satises < i + < , so i + is the equilibrium due to the uniqueness of solutions. since < b ac < , it follows that + + ( + )(n ) < i + = b + b ac a < + + ( + )(n ). therefore, the equilibrium i +/ for the large size fully connected network. p(t) approximationi markovi approximationr markovr p(t) approximationi markovi approximationr markovr time t p(t) approximationi markovi approximationr markovr figure : comparison of results between two models for fully connected networks (top), nw small world networks (middle) and ba scale free networks (bottom). appendix c: comparison with exact markov models for the purpose of showing the performance of our model, the following exact markov model is established by means of continuous time markov chain technique . dsi(t) dt = si(t) n p j= aijp{xi(t) = , xj(t) = } msi(t)im(t) + ri(t) + ii(t), dii(t) dt = msi(t)im(t) + si(t) n p j= aijp{xi(t) = , xj(t) = } ( + )ii(t), dri(t) dt = ii(t) ri(t), dsm(t) dt = sm(t) msm(t), dim(t) dt = msm(t) im(t), () where p{xi(t) = , xj(t) = } represents the probability of node i being state r and node j being state i. as matter of fact, model () turns into model () when p{xi(t) = , xj(t) = } will replaced with ij(t), namely, the transition rate from state r to state i is linear and equals to mim(t) + pn i= aijij(t), as shown in fig. the relation between exact markov model and approximation model please refers to the reference . next, we select three typical networks with nodes, and use the gillespite algorithm to simulate the solution of the markov model () where model parameters and initial conditions are the same as those in sec. . in the experiment, we select randomly initial nodes including susceptible, infected and recovered nodes, and make realizations for fully connected networks, and realizations for nw small world and ba scale free networks. figure shows the comparison of network average states i(t) = n pn i= ii(t) and s(t) = n pn i= si(t) between the markov model and the approximation model. as time goes, both states i(t) and s(t) of approximation models are able to describe those of markov models, although there is a little overestimation. furthermore, the estimation for small world and scale free networks is better than that for the fully connected networks. on the whole, the performance of this new model is good for describing the real markov model. appendix d: proof of theorem proof of theorem : denote i(i, m, , , ) = [++(+ )(m+ n x j= aiji j )]i i m n x j= aiji j , i = , , , n where m = mm m+, i= (i , ..., i n)t. then, it follows from the formula of equilibria in model () that i(i, m, , , ) = , i = , , , n () taking the partial derivatives of i with respect to i j and , respectively, one gets j ( i (i j ))nn = diag( + i i ( + )) + diag(( + )i i )a, = [men + diag(i i )a]i. here diag(i) = diag{, , , n}, () = ( (), , n ())t, i () = ( i (), , i n ())t and the same below. according to the implicit dierentiation formula, it follows that j i = . obviously, < , namely, i < (i = , , , n). next, we prove that j is invertible and all the elements of (j) are negative. it is easy to obtain that ( + )i i < (i = , ..., n) due to i = ( + )i i + [( + )i i ](m + n x j= aiji j ) = denote m = a diag( + (+ )i i ) + diag( m i i ), and m = m + maxi{ + (+ )i i }en. obviously, m is non negative matrix, and it is irreducible due to the fact that a is irreducible on account of the connectedness of the graph g. according to the perron frobenius theorem , m has a simple positive eigenvalue (m) and a positive eigenvector u, such that mu = (m)u. so mu = [(m) maxi{ + (+ )i i }]u, implying that u is also eigenvector of m. on the other hand, it follows from eq.() that mi= , indicating that iis a positive eigenvector of m belonging to eigenvalue as uti> (= ), combining the simplicity of (m) resulted from the simplicity of (m), one gets (m) = , indicating that m has a zero eigenvalue and the other eigenvalues are negative. on the other hand, j = m diag(( + )i i )a diag( m i i ). it follows that all the eigenvalues of matrix j are negative, and j is metzler and irreducible. according to the result in , all the elements of matrix (j) are negative. thus i = (j) > , and consequently i = n pn i= i i > similarly, we get i = (j) > , i m = (j) m > , and i = (j) < where = diag((+ )i i )ai, i m = m m+[(+ )i i ], and = [(+ m )en + diag(i i )a]i. therefore, i > , i m > , and i < the proof of theorem is completed. references pastor satorras r, vespignani a. epidemic spreading in scale free networks. phys- ical review letters, , (): - mishra b k, saini d k. seirs epidemic model with delay for transmission of mali- cious objects in computer network. applied mathematics and computation, , (): - yuan h, chen g. network virus epidemic model with the point to group information propagation. applied mathematics and computation, , (): - newman m e j. spread of epidemic disease on networks. physical review e, , (): s elley f, besenyei a, kiss i z, et al. dynamic control of modern network based epidemic models. siam journal on applied dynamical systems, , (): - liu q, sun m, li t. analysis of an sirs epidemic model with time delay on hetero- geneous network . advances in dierence equations, , (): wei x, liu l, zhou w. global stability and attractivity of a network based sis epidemic model with nonmonotone incidence rate. physica a: statistical mechanics and its applications, , : - xiang wei, xiaoqun wu, shihua chen, jun an lu, guanrong chen, cooperative epidemic spreading on a two layered interconnected network, siam journal applied dynamical systems, , (): - pastor satorras r, castellano c,mieghem p, vespignani a. epidemic processes in complex networks. reviews of modern physics , ():- mieghem pv, omic j, kooij r. virus spread in networks. ieee/acm transactions on networking , ():- youssef m, scoglio c. an individual based approach to sir epidemics in contact networks. journal of theoretical biology, , (): - yang l x, draief m, yang x. the impact of the network topology on the viral prevalence: a node based approach. plos one, , (): yang l, draief m, yang x. heterogeneous virus propagation in networks: a theoreti- cal study. mathematical methods in the applied sciences, , (): - shi h, duan z, chen g. an sis model with infective medium on complex networks. physica a: statistical mechanics and its applications, , (-): - yang m, chen g, fu x. a modied sis model with an infective medium on com- plex networks and its global stability. physica a: statistical mechanics and its applications, , (): - wang y, jin z, yang z, et al. global analysis of an sis model with an infective vector on complex networks. nonlinear analysis: real world applications, , (): - hom r a, johnson c r. topics in matrix analysis. cambridge up, new york, narendra k s, shorten r. hurwitz stability of metzler matrices. ieee transac- tions on automatic control, , (): - yorke j a. invariance for ordinary dierential equations. mathematical systems theory, , (): - shamash e r. fixed point theory: banach, brouwer and schauder theorems. california state university, northridge, robinson r c. an introduction to dynamical systems: continuous and discrete. american mathematical soc., yan g, zhou t, wang j, fu z q, wang b h. epidemic spread in weighted scale free networks. chinese phys. lett. , :- yang l x, yang x, tang y y. a bi virus competing spreading model with generic infection rates. ieee transactions on network science and engineering, , ():- gillespie d t. exact stochastic simulation of coupled chemical reactions. the journal of physical chemistry, , (): - yang l x, yang x, wu y. the impact of patch forwarding on the prevalence of com- puter virus: a theoretical assessment approach. applied mathematical modelling, , : - stewart w j , probability, markov chains, queues, and simulation: the mathemat- ical basis of performance mdeling , princeton university press, bhatia r, matrix analysis, springer verlag, new york, usa, . ",
        "Subsections": [],
        "Groundtruth": "The paper presents a node-based SIRS epidemic model with media to analyze disease spreading in networks with media propagation. It explores the influence of factors such as infected rate of media on network average infected states and the effect of the potential spreading rate on virus prevalence in different network types. The model shows that the medium propagation rate affects infected percentages, especially in sparse networks. The infected percentages correlate positively with node degree, particularly in homogenous networks. Theoretical and numerical studies suggest that infected percentages increase with medium propagation rate but decrease with the effective recovered rate. The model is globally asymptotically stable, and numerical investigations support the theoretical findings. The authors declare no conflicts of interest, and the work is supported by various research grants. The study provides insights into controlling infected percentages based on effective recovered rates."
    },
    {
        "Section_Num": "NA",
        "Section": "NA",
        "Text": "a node based sirs epidemic model with infective media on complex networks leyi zheng a and longkun tang , a, b afujian province university key laboratory of computation science, school of mathematical sciences, huaqiao university, quanzhou , china. bdepartment of mathematics & statistics, georgia state university, atlanta , usa. abstract in this paper, we focus on the node based epidemic modeling for networks, introduce the propagation medium and propose a node based susceptible infected recovered- susceptible (sirs) epidemic model with infective media. theoretical investigations show that the endemic equilibrium is globally asymptotically stable. numerical examples of three typical network structures also verify the theoretical results. fur- thermore, comparison between network node degree and its infected percents im- plies that there is a strong positive correlation between both, namely, the node with bigger degree is infected with more percents. finally, we discuss the impact of the epidemic spreading rate of media as well as the eective recovered rate on the network average infected state. theoretical and numerical results show that () network average infected percents go up (down) with the increase of the infected rate of media (the eective recovered rate); () the infected rate of media has almost no inuence on network average infected percents for the fully connected network and nw small world network; () network average infected percents decrease expo- nentially with the increase of the eective recovered rate, implying that the percents can be controlled at low level by an appropriate large eective recovered rate. keywords: sirs; node based; propagation medium; complex network; epidemic spreading; stability. introduction ",
        "Subsections": [],
        "Groundtruth": "The text introduces a node-based epidemic model on complex networks with infective media. The model focuses on Susceptible-Infected-Recovered-Susceptible (SIRS) dynamics and explores the global stability of the endemic equilibrium. The research includes theoretical analysis and numerical examples on various network structures, highlighting a positive correlation between node degree and infection percentage. The impact of epidemic spreading rate and recovery rate on network infection levels is discussed, showing that higher media infection rates lead to increased infections while effective recovery rates can reduce infections. The findings suggest that controlling the effective recovery rate can help maintain low infection levels in networks."
    },
    {
        "Section_Num": "1",
        "Section": "1 Introduction",
        "Text": "choreographic motion of n bodies is a periodic motion on a closed orbit, n identical bodies chase each other on the orbit with equal time spacing. moore found a remarkable gure eight choreographic solution for n = under homogeneous potential /ra by numerical calculations, where r is a distance between bodies. chenciner and montgomery gave a mathematical proof of its existence for a = by variational method. the detailed initial conditions for three bodies are found in . arxiv:v apr morse index and bifurcation for gure eight choreographies sbano , and sbano and southall , studied n body choreographic solutions under an inhomogeneous potential ulj(r) = r r, () a model potential between atoms called lennard jones type (hereafter lj) potential. sbano and southall proved that there exist at least two n body choreographic solutions for suciently large period, and there exists no solution for small period. we conrmed their theorem numerically for n = and unexpectedly found a multitude of three body gure eight choreographic solutions under lj type potential () . following shibayamas preliminary calculation for a = , we did accurate numerical calculation of the morse index, for the three body gure eight choreography in the domain of periodic function . here the morse index is a number of independent variational functions giving negative second variation of action functional. in our paper , a strong relationship between the morse index and h solution found by sim o , which is a periodic solution close to the gure eight choreography but made up of three distinct orbits, was suggested. on the other hand gal an et al showed that the h solution bifurcated from gure eight choreography by changing the masses of three bodies. they also found many dierent periodic orbits on gure eight . there are several researches on the morse index for periodic solution of three- body problem. barutello et al calculated the morse index mathematically for the lagrangian circular solution, and hu and sun for elliptic lagrangian solutions, to discuss the linear stability. in this paper, we show a relationship between the morse index of the gure- eight choreographies and periodic solutions bifurcating for a system of three identical bodies interacting through a homogeneous potential or through lj type potential (). in section , we show the morse index changes at a bifurcation point and solutions bifurcating are approximated by variational functions responsible for change of the morse index. in section , we discuss the bifurcation from the gure eight choreography under homogeneous potential, /ra, by changing a. in section , we show the h solution bifurcates at a = where the morse index changes. in section , another bifurcation at a = is shown. these bifurcations at a = and are rst found in by mu noz almaraz et al using auto . there is no other point where the morse index changes for a in section , we discuss the bifurcation of the solution for the system under lj type potential, where the solution is a gure eight choreography tending to that under /r for innitely large period. there are seven points where the morse index changes for the solution. in section we show four points bifurcate periodic but non choreographic solutions, which are the same type of the bifurcations discussed in section in section , we show the rest three points yield choreographic solutions less symmetric than gure eight. alain chenciner, in icm , asked a question about morse index and bifurcation for gure eight choreographies the existence of less symmetric gure eight . we can say yes, they exist under lj type potential. section is a summary and discussions. our numerical results in this paper were calculated by mathematica . ",
        "Subsections": [],
        "Groundtruth": "The text discusses choreographic motion of multiple bodies, focusing on the gure eight choreographic solution. Moore and Chenciner-Montgomery confirmed its existence under specific conditions. Sbano and Southall then explored n-body choreographic solutions under an inhomogeneous potential, finding multiple gure eight solutions. The Morse index and bifurcation of these solutions were analyzed numerically, revealing relationships with other periodic solutions. Bifurcation points and changes in Morse index were identified for different potential types, particularly under lj type potential. The text concludes by discussing less symmetric gure eight choreographies and summarizing the numerical results obtained using Mathematica."
    },
    {
        "Section_Num": "2",
        "Section": "2 Morse index and bifurcation",
        "Text": "",
        "Subsections": [
            {
                "Section_Num": "2_1",
                "Section": "2.1 Morse index and eigenvalue problem",
                "Text": "for a system of three identical bodies in classical mechanics, we consider periodic solutions to equations of motion, d dt l qi = l qi , i = , , . . . , , () where dot represents a dierentiation in t. l is the lagrangian with the potential energy u(q), l = x i= q i u(q), () and q(t) = (q(t), q(t), . . . , q(t)) () a six component vector composed of position vectors rb(t) = (xb(t), yb(t))= (qb(t), qb(t)) () for body b = , , in a plane, where represents transpose. the subscript of six component vector is assumed to be in the range between and for a periodic solution q(t+t) = q(t) and variation function q(t+t) = q(t) with period t, the morse index n is dened as a number of independent variation function q(t) which make the second variation s() with s(k) = z t dt x i (qi qi + qi qi ) !k l () of the action functional s(q) = z t l(q, q)dt () negative. since the s() is written as s() = (q, hq) () by matrix operator h, ( h)ij = ij d dt u qiqj , () the morse index is a number of negative eigenvalues of the eigenvalue problem, h = , () morse index and bifurcation for gure eight choreographies (t + t) = (t), () (, ) = , () with the second variation s() = () and the variation function q = , () where (f, g) is the inner product dened by (f, g) = z t dtf g () and ij the kronecker delta. ",
                "Subsections": [],
                "Groundtruth": "The section discusses periodic solutions for a system of three identical bodies in classical mechanics. The Morse index is defined as the number of independent variation functions that make the second variation of the action functional negative. It is determined by the number of negative eigenvalues in the eigenvalue problem. The text also explores the Morse index and bifurcation for figure-eight choreographies, emphasizing the importance of eigenvalues in analyzing the dynamics of the system."
            },
            {
                "Section_Num": "2_2",
                "Section": "2.2 Bifurcation and eigenvalue problem",
                "Text": "for a periodic solution of equation of motion () with some parameter , q(t; ), suppose the other periodic solution qb(t; ) bifurcates at = , that is, lim qb(t; ) = q(t; ). () since for the lagrangian () q = u qi () and qb = u qb i , () q = qb q satises d dtqi = u qi q=qb + u qi = x j u qjqi qj + o(|q|), () that is, hq for () thus equation () is a necessary condition for to be a bifurcation point. when , equation () shows that q goes to the eigenfunction of h whose eigenvalue is zero. in other words, at a bifurcation point some eigenvalue of h has to go to zero. thus, using the normalized eigenfunctions (k) of the eigenvalue we have an approximate expression by variated orbit q = q + h g x k ck(k) () for the bifurcating solution qb = q + q q for , () where g is a degeneracy of the , ck and h are real coecients with pg k c k = and h since the variated orbit q has its own symmetry independent of and h , bifurcating solution qb will be found within the symmetry. morse index and bifurcation for gure eight choreographies - - h - - h (a) (b) figure an action functional s(q + h()) for n = bifurcation is supposed to be one side in > (a) < (b) > ",
                "Subsections": [],
                "Groundtruth": "A periodic solution of an equation of motion can bifurcate at a certain parameter value, leading to a necessary condition for a bifurcation point. At the bifurcation point, an eigenvalue of the system's Hamiltonian must reach zero. This results in a variated orbit solution that can be expressed as a sum of the original solution and a component related to the eigenvectors of the Hamiltonian. The bifurcating solution is found within the symmetry of the system, and considerations are made for the morse index and bifurcation in the context of figure eight choreographies."
            },
            {
                "Section_Num": "2_3",
                "Section": "2.3 Morse index and bifurcation",
                "Text": "we dene change of the morse index n() by n() = lim + n() lim n(). () thus g = |n()| () and an eigenvalue () changes the sign at = , n()( )() () around then the sign of the action, s() = s(qb(t; )) s(q(t; )), () has the same sign as () s()() () and n()( )s() () around since s is expanded in h with coecients () as s(q + hq) = s() + hs() + h ! s() + , () and qb tends to q = q + hq with q = pg k ck(k) for , s() s(q) s(q) = h s() + h ! s() + = h + o(h). () thus () then () are derived by (). on the basis of n, () and (), we can picture bifurcation through manifold of action functional s in the subspace of corresponding . for example, suppose n = and bifurcation is one side in > thus, in the one dimensional subspace of , top of a local maximum in s where q locates for < , shown in gure (a), will slightly cave in for > , which yield critical points for qb in the both sides of the cave, shown morse index and bifurcation for gure eight choreographies table n(a) and symmetry of q for a a n(a) q dy d in gure (b). we conrmed that the inequalities () and () corresponding to the picture of bifurcation as shown in gure hold in our numerical calculations. we dene an equivalent class, congruent class of bifurcated solutions b, by regarding the bifurcated solutions with congruent orbits as equivalent, and denote the number of elements by #b. then we dene number of incongruent bifurcated solutions as nb = lim #b() + lim + #b(). () in the following sections we will show nb = |n| () holds for = in our numerical calculations. note that at a bifurcation point the morse index may not change, n = , if = but d()/d = however for gure eight choreographies in this paper, such point never contribute to any bifurcation and belongs to = in the whole region of corresponding to the conservation laws. in other words, () holds not only for = but also for all . we call the region < the left side of the bifurcation point and < the right side. ",
                "Subsections": [],
                "Groundtruth": "The text introduces the concept of Morse index and bifurcation in the context of an action functional. The Morse index, denoted as n(), represents a change that occurs as an eigenvalue () changes sign. The action function, s(), undergoes changes in sign around the point of bifurcation. Bifurcation is visualized through the manifold of the action functional in the corresponding subspace. The text depicts an example where bifurcation occurs on one side of the subspace, leading to the formation of critical points for qb. The concept of congruent classes of bifurcated solutions is introduced, with the number of incongruent solutions denoted as nb. The region before and after the bifurcation point is defined as the left and right sides, respectively. The relationship between the Morse index and the number of incongruent bifurcated solutions is explored numerically."
            }
        ],
        "Groundtruth": ""
    },
    {
        "Section_Num": "3",
        "Section": "3 Morse index and bifurcation for homogeneous system",
        "Text": "in this section we investigate the bifurcation of the gure eight choreography by using a as the parameter for a system under homogeneous potential u(q) = x b>c ua(|(qb qc, qb qc)|), ua(r) = ra, () for a in table , a, n(a) = and symmetry of the variated orbit q are tabulated. the symbol d means that the q is not choreographic, and the subscript y indicates that the orbits are symmetric in the y axis. there is no other point with n(a) = for a than tabulated in table , a = and . . bifurcation at a = at a = , the morse index n(a) changes by n = as shown in table for g = |n| = , the variated orbit q is written as q = q + h(cos () + sin ()) () morse index and bifurcation for gure eight choreographies - - - - - - (a) (b) figure variated orbit q in () for a = close to a = ; t = , h = , = . (a) s(q) of . (b) q with dxy symmetry for a local maximum at = in s(q). note that the value of does not have universal meaning since it depends on the choice of orthonormal basis, () and (). filled circles are isosceles triangle conguration at t = and open circles euler conguration at t = t/ with c = cos and c = sin . the coecients are, thus, found as critical points in by s(q) = () as shown in gure (a) at h = , there are six critical points in s(q) of , three local maximums and three local minimums, which are independent of h. at all six critical points in , the variated orbits qs have the same symmetry higher than the dy indicated in table they consist of three distinct orbits symmetric in the y axis and in the x axis with exchange of two bodies; one orbit is symmetric itself but two collectively. thus we denote orbits with this symmetry by dxy. in gure (b), the variated orbit q at a local maximum is shown. black orbit is symmetric itself but two gray orbits collectively. consequently all bifurcating solutions from a = will be searched within dxy symmetry. note that since it is numerically dicult to calculate the q just at the bifurcation point a = and its symmetry does not depend on a, in gure calculation for a = is shown. conditions for q(t) to be dxy, derived in appendix a., are that q(t) takes an isosceles triangle conguration at t = shown by lled circles in gure (b), q() = (x, y, x, , x, y), () q() = (cos , sin , , sin , cos , sin )v, () with = tan y x, () by parameters (x, y, v), and an euler conguration at t = t/ shown by open circles in gure (b), (q, q, q q q q) = () for given period t, the three conditions () determine the three parameters (x, y, v). morse index and bifurcation for gure eight choreographies - - - - - - - - - - - - - - - - (a) (b) (c) figure dxy solutions bifurcated from a = ; (a) a = , (b) a = , and (c) a = , for t = parameters (x, y, v; d) for (a)(c) are (, , ; ), (, , ; ) and (, , ; ), respectively. in order to distinguish dxy from gure eight choreography which are very close around a bifurcation point, we use the y component of body on the x axis, d = q(t), () for t t/ with q(t) = since d is zero if and only if dxy is choreographic. mu noz almaraz et al rst found the bifurcation at a = using auto , and recently we re found it using the equations ()() with newtons method: three dxy solutions corresponding to the three local maximums in s(q) of bifurcate in the right side of the bifurcation point, and three to the local minimums in the left side. the six critical points in gure (a) are written by a position of local maximum as +j/+k, j = , , , k = , thus the q for the six solutions bifurcating in the both sides are represented by = + j/ with a smooth increasing function h of a. using the choreographic operator c dened by cfi(t) = fi+(t t/) () since for the doubly degenerate eigenvalue c(cos () + sin ()) = cos( + /)() + sin( + /)(), () the q for dxy is written as qdxy = q + h cj(cos () + sin ()), j = , , () since c = and cq = q, representation () for j = , , dier only in cyclic permutation of bodies with time shift. thus their orbits are congruent and the number of incongruent solutions nb is counted as nb = + = in gure (a) and (c), one of three congruent dxy solutions in the both sides of bifurcation point, a = , are shown with parameters (x, y, v) for initial condition and the index d. as we showed in , the sim os h solution is in good agreement with the variated orbit q because it is the solution dxy bifurcating from a = very close to a = actually the solution dxy at a = shown in gure (b) coincides with the sim os h solution. morse index and bifurcation for gure eight choreographies - - - - - - - - - - (a) (b) (c) figure variated orbit q for a = close to a = ; t = , = , h = . (a) s(q) of . (b) q with dx symmetry for a local maximum at = . note that the does not have universal meaning since it depends on the choice of orthonormal basis, () and (). filled circles are isosceles triangle conguration at t = and open circles t = t/ (c) q with d for a local minimum at = + / filled circles are euler conguration at t = and open circles at t = t/ . bifurcation at a = for a , there is one more point changing the morse index at a = as shown in table in this section we investigate this point in similar manner as in section . the eigenvalue which goes to zero at a = is doubly degenerate, g = |n| = , and the variated orbit q in () are expected as bifurcating solutions. however its symmetry d shown in table is lower than for dy at a = , and its action s(q) as a function of exhibits twelve critical points; six local maximums and six local minimums as shown in gure (a). in gure , s(q) and q for a = are shown as a close point to a = because of numerical convenience. at local maximums, the variated orbit q consists of three distinct orbits: one orbit is symmetric itself in the x axis and the other two are collectively, shown in gure (b). on the other hand, at local minimums: one orbit is symmetric itself at origin and the other two collectively, shown in gure (c). we denote former by dx and latter d here the orbits of dx can have non zero total angular momentum l since sum of signed area of the three orbits can be non zero whereas it is zero for solutions d, dxy and the gure eight choreography because of two fold symmetry at origin. conditions for q(t) to be dx, derived in appendix a., are that q(t) takes an isosceles triangle conguration shown by lled circles in gure (b), () and () with = tan y x tan l v p x + y, () at t = by parameters (x, y, v, l), and opposite isosceles triangle conguration at t = t/ shown by open circles in gure (b), (q, q, q q, q q) = () for given period t, four conditions () determine four parameters (x, y, v, l). an index to distinguish dx from gure eight choreography is (d, l) = , () morse index and bifurcation for gure eight choreographies - - - - - - - - - - (a) (b) figure (a) dx and (b) d solutions at a = with t = bifurcated from a = . d is rotated by in (). parame- ters: (a) (x, y, v, l) = (, , , ), (b) (x, u, v; i) = (, , ; ). since dx with l = is the dxy solution. conditions for q(t) to be d, derived in appendix a., are that q(t) takes an euler conguration at t = shown by lled circles in gure (c), q() = (x, , x, , , ), () q() = (u, v, u, v, u, v) () by parameters (x, u, v), and another euler conguration at t = t/ shown by open circles in gure (c), (q, q, q q q q) = () for given period t, three conditions () determine three parameters (x, u, v). an index to distinguish d from gure eight choreography is (d, i) = , () where i = i(t/) i() and i(t) = |q(t)| = p i |qi(t)|, since d with i = is the dxy solution. using auto , mu noz almaraz et al found the solution dx bifurcated at a = in together with dxy at a = . knowing the existence of dx, we recently re found dx and d using the above equations with newtons method: six dx and six d solutions bifurcate in the right side of bifurcation point. the six dx solutions are written by the q with at local maximums, and the six d at local minimums. the s at local maximums are written by a position of local maximum as + j/ with integer j, and local minimums as + (j + )/ thus, by (), the q for dx and d solutions are represented by qdx = q h cj(cos () + sin ()), () and qd = q h cj(cos( + /)() + sin( + /)()), () respectively, with j = , , and a > . in () and (), the sign in front of h eects inversion of orbits in the y axis. then the orbits of six dx solutions are all congruent, in direct isometry or mirror inversion, and the orbits of six d solutions are so. thus the number of incongruent bifurcating solutions nb is counted as nb = + = morse index and bifurcation for gure eight choreographies table n(t) and symmetry of q for lj . symbols, cx, cy, cxy and c mean that the q is symmetric in the x axis, in the y axis, in both the x and the y axes and at origin, respectively. + t n(t) q t n(t) q cxy d cx dy dy cy d c in gure , one of six congruent dx and d solutions at a = are shown with parameters for initial conditions and with i for d, where d is rotated by = tan q(t/) q(t/), () to make the x axis bisector of two euler congurations. ",
        "Subsections": [],
        "Groundtruth": "The section investigates the Morse index and bifurcation of the figure-eight choreography by varying parameter a in a homogeneous system. The potential energy function and symmetry of the variated orbit q are analyzed. Bifurcation occurs at specific values of a, causing changes in the Morse index. The text details the symmetry properties of the variated orbits and the conditions for different symmetries like dxy and dx. Numerical methods are employed to analyze the bifurcation points and the resulting solutions. The section also discusses conditions for different symmetries, such as dxy, dx, and d, with corresponding parameters and critical points. The presentation includes figures illustrating the variated orbits and critical points for different symmetric solutions."
    },
    {
        "Section_Num": "4",
        "Section": "4 Morse index and bifurcation for LJ system",
        "Text": "",
        "Subsections": [
            {
                "Section_Num": "NA",
                "Section": "NA",
                "Text": "in this section, we discuss the bifurcation of the solution by using t as the parameter for the system under lj type potential u(q) = x b>c ulj(|(qb qc, qb qc)|). () the solution bifurcates at t = tmin = , thus there exists no solution for t < tmin and two solutions for t > tmin. one branch from t = tmin of solution tends to the gure eight choreography under homogeneous potential with a = for t , and the other branch + gourd shaped for t . in table , t, n(t) = and symmetry of the variated orbit q for + and are tabulated. symbols, cx, cy, cxy and c mean that the variated orbit q is choreographic and is symmetric in the x axis, in the y axis, in both the x and the y axes, and at origin, respectively. note that though the symbols cx and cxy were written as c and ce in , respectively, we redened them since c and ce have the same symmetry as dx and dxy, dened in section , respectively. there is no other point with n(t) = for solution than seven points tabulated in table . bifurcation yielding dxy, dx and d solutions the points indicated by dy in table , t = for + and t = for , yield dxy solutions represented by () as in the section . the bifurcations are the both sides and the number of incongruent bifurcating solutions nbs are both two. in gure (a), s(t) for bifurcation from + at t = is plotted. though bifurcation is both sides, the bifurcating solution also bifurcate soon at t = . thus there exist two bifurcated solutions for t > . for < t < morse index and bifurcation for gure eight choreographies t -- -- - - - - - - - - - - - - - - (a) (b) (c) figure bifurcation of + at t = yielding dxy solutions. (a) s(t), (b) dxy for t = from the right side of bifurcation point, and (c) from the left side. parameters (x, y, v; d); (b) (, , ; ), (c) (, , ; ). t - -- - - - - - - - - - (a) (b) (c) figure bifurcation of at t = yielding dxy solutions. (a) s(t), (b) dxy for t = from the right side of bifurcation point, and (c) from the left side. parameters (x, y, v; d): (b) (, , ; ), (c) (, , ; ). both solutions are bifurcated from left side of bifurcation point but for t > one from right side and the other from left side. in gure (b) and (c), the two bifurcated solutions for t = from both sides are shown with parameters (x, y, v) and d. in gure , bifurcation of dxy solution from at t = is shown as gure for +. the points indicated by d in table , t = for + and t = for , yield dx and d solutions represented by () and (), respectively, as in the section . the bifurcations are one side in the right side and the number of incongruent bifurcating solutions nbs are both two. in gures and , s(t) and the bifurcated solutions for t = are shown with parameters. . choreographic bifurcation ",
                "Subsections": [],
                "Groundtruth": "The section discusses the bifurcation of solutions using the parameter t for a system under an LJ type potential. The solution bifurcates at t = tmin, with no solution for t < tmin and two solutions for t > tmin. One branch tends to the figure-eight choreography under a homogeneous potential for t < tmin and the other branch is gourd-shaped for t > tmin. The table lists t, n(t), and the symmetry of the variated orbit q for different scenarios. Bifurcations yield dxy, dx, and d solutions, with two incongruent bifurcating solutions on both sides. The bifurcated solutions are shown in figures with their parameters."
            },
            {
                "Section_Num": "4_2",
                "Section": "4.2 Choreographic bifurcation",
                "Text": "the points with |n| = in table , t = and t = for +, and t = for , bifurcate choreographic solutions since the variated orbit q for g = |n| = , q = q + h(), () is choreographic, cq = q . at t = for , the variated orbit q is symmetric in the x axis as shown in gure (a). we denote this orbit by cx. conditions for q(t) to be cx, derived morse index and bifurcation for gure eight choreographies t - - - -- - - - - - - - - - - (a) (b) (c) figure bifurcation of + at t = yielding dx and d solutions. (a) s(t) for dx solution (full curve) and for d (dashed curve). (b) dx solution and (c) d for t = d is rotated by in (). parameters; (b) (x, y, v, l) = (, , , ), (c) (x, u, v; i) = (, , , ). t - - - - - - - - - - - - - (a) (b) (c) figure bifurcation from at t = yielding dx and d solutions. (a) s(t) for dx solution (full curve) and for d (dashed curve). (b) dx solution and (c) d for t = d is rotated by in (). parameters; (b) (x, y, v, l) = (, , , ), (c) (x, u, v; i) = (, , ; ) in appendix a., are that q(t) takes an isosceles triangle conguration shown by lled circles in gure (a), () and () with () at t = by parameters (x, y, v, l), and opposite isosceles triangle conguration at t = t/ shown by open circles in gure (a), (q, q, q q, q q) = () for given period t, four conditions () determine four parameters (x, y, v, l). an index to distinguish solution cx from gure eight choreography is l = in gure (a), the orbit of the solution cx for t = bifurcated at t = from is shown with parameters (x, y, v, l). at t = for +, the variated orbit q is symmetric at origin as shown in gure (b). we denote this orbit by c conditions for q(t) to be c, derived in appendix a., are that q(t) takes an euler conguration shown by lled circles in gure (b), () and () at t = by parameters (x, u, v), and another euler conguration at t = t/ shown by open circles in gure (b), (q, q, q q q q) = () for given period t, three conditions () determine three parameters (x, u, v). an index morse index and bifurcation for gure eight choreographies - - - - - - - - - - - - - - - - (a) (b) (c) figure variated orbit q for lj solution. (a) cx in ; t = , h = and = . filled circles are isosceles triangle conguration at t = and open circles at t = t/ (b) c in +; t = , h = and = . filled circles are euler congurations at t = and open circles at t = t/ (c) cy in +; t = , h = and = . filled circles are conguration where a body on the y axis at t = and open circles at t = t/ - - - - - - - - - - - - - - - - (a) (b) (c) figure choreographic solutions for t = ; (a) cx bifurcating from at t = , (b) c from + at t = , (c) cy from + at t = . c is rotated by in (). parameters; (a) (x, y, v, l) = (, , , ), (b) (x, u, v; i) = (, , ; ), (c) (x, y, u, v, w, y) = (, , , , , ). to distinguish solution c from gure eight choreography is i = in gure (b), the orbit of the solution c for t = bifurcated at t = from + is shown with parameters (x, y, v) and i. at t = for +, the variated orbit q is symmetric in the y axis as shown in gure (c). we denote this orbit by cy. conditions for q(t) to be cy, derived in appendix a., are that a body is on the y axis at t = shown by lled circles in gure (c), q() = (x, y, x, y y, , y), () q() = (u, v, u s, v w, s, w), () where s = (xv yu) + xw yu y + y () by parameters (x, y, u, v, w, y), and its inversion in the y axis at t = t/ shown by open circles in gure (c), (q, q + x, q y, q + u, q v, q y) = () for given period t, the six conditions () determine six parameters (x, y, u, v, w, y). an index to distinguish solution cy from gure eight choreography is y = in gure (c), morse index and bifurcation for gure eight choreographies the orbit of the solution cy for t = bifurcating at t = from + is shown with (x, y, u, v, w, y). for these three choreographies c = cx, c, cy, the variated orbit q in () is written as qc = q h(). () the sign in front of h eects inversion of the orbit in the y axis for cx, and in the x axis for c and cy. these couples of congruent orbits bifurcate in the right side of the bifurcation point and nb = ",
                "Subsections": [],
                "Groundtruth": "The section discusses choreographic bifurcation, where solutions exhibit symmetric behavior in different axes. The text describes the conditions and parameters that lead to choreographic solutions bifurcating from gure eight choreographies. Various configurations and parameters are detailed for different solutions, such as isosceles triangle and Euler configurations. The solutions bifurcate at specific time points, with the variated orbits showing symmetry in the x and y axes. Index morse index and bifurcation are used to distinguish between the choreographic solutions."
            }
        ],
        "Groundtruth": ""
    },
    {
        "Section_Num": "5",
        "Section": "5 Summary and discussions",
        "Text": "in this paper, we showed that the morse index changes at a bifurcation point for periodic solution and inversely all points where the morse index changes are bifurcation points for gure eight choreography under homogeneous potential with a and for the solutions under lj type potential. thus, for these choreographies, change of the morse index, n = , is not only necessary but also sucient condition for bifurcation point. further we observed that n determines the number of incongruent bifurcating solutions nb as (). the bifurcations are conrmed numerically by newtons method. if the number of parameters is three as in the dxy, d and c cases, the parameters of the solution are represented graphically like . at t satisfying one of the three conditions under some restriction f = const where f is a function of the three parameters, the rest conditions are given by two curves in the plane of two parameters. in gure , for solution dxy determined by three parameters (x, y, v) in a = homogeneous system, at t satisfying q(t) = , the two conditions q(t) = and q(t) q(t) q(t) q(t) = are shown in (x, y) plane with total energy e = x i= q i + u(q) () as a restriction. the parameter (x, y) for the dxy solution and the gure eight choreography are observed as crossing points of two curves in gure newtons method sometimes does not converge unless initial parameters are good enough. we introduced another graphically assisted method: draw one condition as a function of one parameter by newtons method in one lower dimension. in gure , for solution dx determined by four parameters (x, y, v, l) in a = homogeneous system, q in () are shown as a function of v which is obtained by dimensional newtons method for the rest of three parameters (x, y, l) with the rest of three conditions (q, q q, q q) = zero about v = corresponds to gure eight choreography and two other zeros dx solutions in gure it is useful to evaluate the euler characteristics = x q ()n(q) () morse index and bifurcation for gure eight choreographies - - - - figure map to search dxy solution. full and dashed curves are q(t) = and q(t) q(t) q(t) q(t) = , respectively, at t satisfying q(t) = for q(t) starting from the initial conditions ()() with e = for a = homogeneous potential. horizontal and vertical axes are x x and y y, respectively, where (x, y) is (x, y) for gure eight choreography with t = crossing points of two curves show the parameters for the dxy solutions and for the gure eight choreography. v - - figure q in () as a function of v. zero about v = corresponds to gure- eight choreography and two other zeros bifurcating solution dx. for the manifold of action functional in the domain of periodic functions, where n(q) is the morse index at a periodic solution q. since the will conserve, sometimes comparison of at the both sides of the bifurcation point helps to nd bifurcating solution. we assume the euler characteristics conserves at the both sides of bifurcation point, and () holds. then we denote a number of congruent solutions which belong to the ith incongruent class by ni, and their morse index by ni, i = , , . . . , nb. for a bifurcation point with |n| = , bifurcation is one side since nb = from the conservation of at the both sides of bifurcation point, ()n = ()n + n()n () we obtain n = and ()n = ()n. for a bifurcation point with |n| = , bifurcation can be one side or both sides since nb = for one side bifurcation, conservation of , ()n = ()n + n()n + n()n () morse index and bifurcation for gure eight choreographies table restrictions of bifurcation by conservation of the euler characteristics and (). symbols ni and ni are a number of congruent solutions which belongs to the ith incongruent class, and their morse index, respectively, i = , . . . , nb, nb = |n|. |n| bifurcation restriction for ni restriction for ni one side n = ()n = ()n one side n = n ()n = ()n both sides n = n ()n = ()n leads n = n and ()n = ()n for both sides bifurcation, ()n + n()n = ()n + n()n () leads n = n and ()n = ()n in table , these restrictions on bifurcation are tabulated. according to the restriction for one side bifurcation at |n| = we found d solution after nding dx. the procedure to nd bifurcation numerically by the morse index is summarized as follows: ) find a point the morse index changes, n = ) investigate the corresponding variated orbit q which is an approximation of bifurcating solution. indeed, symmetry of q is useful for numerical calculation. ) if |n| > , variated orbit is chosen to make action critical. ) check conservation of the euler characteristics at both sides of bifurcation point by table we leave the followings for future works: calculation of the morse index for bifurcating solution; calculation of the morse index for gure eight choreography under homogeneous potential with negative a, that is, ra for a < ; tracking solutions bifurcated as much as possible; calculation of linear stability for gure eight choreography and bifurcated solutions; conditions for the point the morse index changes to be bifurcation point; and conditions for observation () on the number of bifurcating solutions to hold. acknowledgments we thank kazuyuki yagasaki for his valuable comment on variated orbits at symposium on celestial mechanics and n body dynamics (). this work was supported by jsps grant in aid for scientic research k (hf) and k (ho). appendix a. conditions for solutions we derive conditions for q(t) to be dxy, dx, d, cx, c and cy solutions. we assume an inertia frame that total linear momentum p is zero, p = p b rb = , and center of mass g is at origin, g = p b rb = the subscript and index for body is assumed to be in the range and for dxy, d, c and cy, total angular momentum l = p b rb rb is zero since sum of signed area of three orbits is zero from the symmetry of orbits. morse index and bifurcation for gure eight choreographies a conguration that a body b is in the x axis and the other two bodies b have the same x coordinate is called isosceles triangle conguration. for q(t) symmetric in the x axis conditions for isosceles triangle conguration is written as (qb, qb, qb+ qb, qb+ qb) = (a.) by p = for b = with x = q, y = q, v = p q + q and total angular momentum l, (a.) is written as (), () and (). the motion beginning from (a.) is time reversal motion from (a.) with inversion in the x axis and exchange of bodies b a conguration that a body b is at origin is called as euler conguration. for q(t) symmetric at origin a condition for euler conguration is written as (qb, qb, qb+ qb qb+ qb) = (a.) by g = for b = with x = q, u = q, v = q (a.) is written as () and (). the motion beginning from (a.) is time reversal motion from (a.) with rotation and exchange of bodies b appendix a. dxy solution the dxy solution takes initial conditions () and () with () and takes euler conguration when body reaches at origin at t = t. thus (a.) with b = , () holds at t = t. thus at t = t, three bodies take initial conditions with rotation and exchange of bodies and then at t = t, three bodies take initial conditions again and the motion is periodic with period t = t. since the orbits were constructed by those from t = to t = t/ and their rotation, the orbits are symmetric in the x and the y axes. appendix a. dx solution the dx solution takes initial conditions () and () with () and takes isosceles triangle conguration when body reaches in the x axis again at t = t. then (a.) with b = , () holds at t = t. then at t = t, three bodies take initial conditions again and the motion is periodic with period t = t. since the orbits were constructed by those from t = to t = t/ and their inversion in the x axis, the orbits are symmetric in the x axis. appendix a. cx solution the cx solution takes initial conditions () and () with () and takes isosceles triangle conguration when body reaches in the x axis again at t = t. then (a.) with b = , () holds at t = t. then at t = t, three bodies take initial conditions again but with cyclic permutation of bodies, and the motion is choreographic with period t = t = t. since the orbits were constructed by those from t = to t = t/ and their inversion in the x axis, the orbits are symmetric in the x axis. morse index and bifurcation for gure eight choreographies appendix a. d solution the d solution takes initial conditions () and (), and takes euler conguration when body reaches at origin at t = t. then (a.) with b = , () holds at t = t. thus at t = t, three bodies take initial conditions again and the motion is periodic with period t = t. since the orbits were constructed by those from t = to t = t/ and their rotation, the orbits are symmetric at origin. appendix a. c solution the c solution takes initial conditions () and (), and euler conguration when body reaches in the x axis again at t = t. then (a.) with b = , () holds at t = t. thus at t = t, three bodies take initial conditions again with cyclic permutation of bodies, and the motion is choreographic with period t = t = t. since the orbits were constructed by those from t = to t = t/ and their rotation, the orbits are symmetric at origin. appendix a. cy solution the cy solution takes initial conditions () and () by g = p = with () by l = where body is on the y axis, which are represented by six parameters. at t = t when body reaches in the y axis, the relation between the positions and velocities at t = t have to be inversion in the y axis with exchange of bodies and thus we have twelve relations but six conservation quantities, g, p, l and total energy, reduce it to six as (). thus at t = t if () is satised, three bodies take initial conditions again with cyclic permutation of bodies, and the motion is choreographic with period t = t = t. since the orbits were constructed by those from t = to t = t/ and their inversion in the y axis, the orbits are symmetric in the y axis. references moore c braids in classical gravity phys. rev. lett. chenciner a and montgomery r a remarkable periodic solution of the three body problem in the case of equal masses annals of mathematics sim o c dynamical properties of the gure eight solution of the three body problem contemporary mathematics sbano l symmetric solutions in molecular potentials proceedings of the international conference spt, symmetry and perturbation theory (world scientic publishing, singapore) sbano l and southall j periodic solutions of the n body problem with lennard jones type potentials dynamical systems fukuda h, fujiwara t, ozaki h figure eight choreographies of the equal mass three body problem with lennard jones type potentials j. phys. a: math. theor. shibayama m numerical calculation of the second variation for the choreographic solution (in japanese) proceedings of symposium on celestial mechanics and n body dynamics ed. saito m, shibayama m and sekiguchi m morse index and bifurcation for gure eight choreographies fukuda h, fujiwara t, ozaki h morse index for gure eight choreographies of the planar equal mass three body problem j. phys. a: math. theor. gal an j, mu noz almaraz f j, freire e, doedel e and vanderbauwhede a stability and bifurcations of the figure- solution of the three body problem phys. rev. lett. mu noz almaraz f j, gal an j and freire e families of symmetric periodic orbits in the three body problem and the gure eight monografas de la real academia de ciencias de zaragoza barutello v, jadanza riccardo d and portaluri a morse index and linear stability of the lagrangian circular orbit in a three body type problem via index theory a. arch rational mech anal hu x and sun s morse index and stability of elliptic lagrangian solutions in the planar three body problem advances in mathematics hu x and sun s index and stability of symmetric periodic orbits in hamiltonian systems with application to figure eight orbit commun. math. phys. mu noz almaraz f j and vanderbauwhede a private communication mu noz almaraz f j, gal an j, freire e and vanderbauwhede a numerical explorations in a modied potential of the tbp (version v) zenodo. http://doi.org//zenodo. doedel e, keller b h and kernevez j p numerical analysis and control of bifurcation problems (i): bifurcation in nite dimensions international journal of bifurcation and chaos no. doedel e, keller b h and kernevez j p numerical analysis and control of bifurcation problems (ii): bifurcation in innite dimensions international journal of bifurcation and chaos no. chenciner a some facts and more questions about the eight topological methods, variational methods and their applications, proceedings of the icm satellite conference on nonlinear functional analysis (world scientic) shibayama m private communication fujiwara t, fukuda h and ozaki h decomposition of the hessian matrix for action at choreographic three body solutions with gure eight symmetry arxiv: ",
        "Subsections": [],
        "Groundtruth": "The text discusses the changes in Morse index at bifurcation points for periodic solutions, specifically for figure eight choreography under homogeneous potential with a and LJ type potential. The change in Morse index, denoted as n, is a necessary and sufficient condition for bifurcation points for these choreographies. Numerical confirmation of bifurcations is done using Newton's method, particularly for cases with three parameters. The paper also proposes a graphically assisted method for determining bifurcations numerically. Future work includes calculating Morse index for bifurcating solutions and exploring linear stability for figure eight choreography. Additionally, conditions for the point where the Morse index changes to be a bifurcation point are considered."
    },
    {
        "Section_Num": "Appendix",
        "Section": "Appendix A Conditions for solutions",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "NA",
        "Section": "NA",
        "Text": "morse index and bifurcation for gure eight choreographies of the equal mass three body problem hiroshi fukuda, toshiaki fujiwara and hiroshi ozaki college of liberal arts and sciences, kitasato university, -- kitasato, sagamihara, kanagawa -, japan laboratory of general education for science and technology, faculty of science, tokai university, -- kita kaname, hiratsuka, kanagawa, -, japan e mail: fukuda@kitasato u.ac.jp, fujiwara@kitasato u.ac.jp and ozaki@tokai u.jp april abstract. we report on the morse index and periodic solutions bifurcating from the gure eight choreography for the equal mass three body problem under homogeneous potential /ra for a , and under lennard jones (lj) type potential /r /r, where r is a distance between bodies. it is shown that the morse index changes at a bifurcation point and all solutions bifurcating are approximated by variational functions responsible for the change of the morse index. inversely we observed bifurcation occurs at every point where the morse index changes for the gure eight choreography under /ra, and for solution under lj type potential, where solution is a gure eight choreography tending to that under /r for innitely large period. thus, to our numerical studies, change of the morse index is not only necessary but also sucient condition for bifurcation for these choreographies. further we observed that the change of the morse index is equal to the number of bifurcated solutions regarding solutions with congruent orbits as the same solution. submitted to: j. phys. a: math. theor. introduction ",
        "Subsections": [],
        "Groundtruth": "The text reports on the morse index and periodic solutions bifurcating from the figure eight choreography in the equal mass three body problem. The study considers different potentials and shows that the morse index changes at a bifurcation point, with solutions approximated by variational functions. Bifurcation occurs at points where the morse index changes, indicating its significance in these choreographies. The change of the morse index is found to be a sufficient condition for bifurcation, with the number of bifurcated solutions equal to the change in morse index."
    },
    {
        "Section_Num": "Abstract",
        "Section": "Abstract",
        "Text": "obust policy search is the problem of learning policies that do not degrade in performance when subject to unseen environment model parameters. it is particularly relevant for transferring poli- cies learned in a simulation environment to the real world. several existing approaches involve sampling large batches of trajectories which reflect the differences in various possible environments, and then selecting some subset of these to learn robust policies, such as the ones that result in the worst performance. we propose an active learning based framework, effacts, to selectively choose model parameters for this purpose so as to collect only as much data as nec- essary to select such a subset. we apply this framework using linear bandits, and experimentally validate the gains in sample efficiency and the performance of our approach on standard continuous con- trol tasks. we also present a multi task learning perspective to the problem of robust policy search, and draw connections from our proposed framework to existing work on multi task learning. keywords deep reinforcement learning, robust learning, active learning, ro- botics ",
        "Subsections": [],
        "Groundtruth": "Robust policy search aims to learn policies that maintain performance across varying environments, crucial for transferring simulation-based policies to real-world settings. Current methods involve sampling diverse trajectories to identify robust policies. An active learning framework called effacts is introduced to efficiently select model parameters for data collection, leveraging linear bandits. Experimental results on continuous control tasks demonstrate improved sample efficiency and performance. A multi-task learning perspective is also applied to robust policy search, drawing connections to existing work. Key terms include deep reinforcement learning, robust learning, active learning, and robotics."
    },
    {
        "Section_Num": "1",
        "Section": "1 Introduction",
        "Text": "recent advances in deep reinforcement learning (drl) algorithms have achieved remarkable performance on continuous control tasks . traditionally, these algorithms are used to learn policies to perform a given task in simulation. however, it has been found that policies learned in simulation often do not perform well in, or transfer to, a real world system that the simulation models . indeed, the prospect of being able to deploy policies learned in simulation on real world systems such as physical robots is one of the major drivers for research in reinforcement learning. one class of approaches towards this goal that has gained trac- tion is to learn from multiple simulated domains that approximate the real target domain. these usually correspond to an ensemble of environment models with various parameters such as the mass of a part of a robot or the coefficient of friction between the robots foot and the ground. given such an ensemble, the problem of robust policy search is to learn policies that perform well across this ensemble. one prominent group of approaches in this class involves sam- pling model parameters from the ensemble and collecting batches work performed when author was at the robert bosch centre for data science and ai, iit madras. preprint: this is the authors own version of their work that is published in the th joint international conference on data science and management of data (th acm ikdd cods and th comad) (cods comad ). doi:/ of trajectories simulated using these parameters , which are then used for training a policy, typically by a model free rl algorithm. these approaches differ mainly in the way in which they choose subsets of these trajectories to focus on for policy learning. although robust policy search is inevitably a harder learning prob- lem than standard policy search, the amount of data collected by these methods is still quite large, up to almost orders of magnitude more than is typically required by the usual policy optimization al- gorithms, regardless of the method used to choose a subset of these trajectories for learning. therefore, although these approaches are shown to be effective for learning robust policies, and offer other advantages such as reduced modeling burden, the requirement for an abundance of data makes them computationally expensive. in this work, we demonstrate a novel way to improve their sam- ple complexity while maintaining the performance and robustness of the learned policy through the use of active learning for intelli- gently selecting model parameters for which to sample trajectories for learning. active learning is used to directly acquire some de- sired subset of the trajectories (such as the subset resulting in the worst performance), while collecting as little additional data as pos- sible. in contrast, existing methods sample parameters directly from the ensemble, and possibly discard large portions of the collected trajectories . the resulting framework, effacts, offers greatly improved scalability, thus broadening its applicability to real world problems. the structure of the framework and the use of active learning for trajectory sampling results in some connections between robust policy search and multi task learning. we discuss the relation between the two problems, as well as the differences in their solution approaches. thus, the contributions of this paper are as follows: () we in- troduce a novel active learning framework that performs more judicious collection of trajectories for training robust policies, re- sulting in low sample complexity; () we present an instantiation of the framework using linear bandits, and perform experimental validation on environment ensembles from standard continuous control benchmarks to empirically demonstrate significant reduc- tions in sample complexity while still being able to learn robust policies; () we explore connections to multi task learning that are revealed upon casting robust policy search as a multi task learning problem and discuss its relation to existing work in the area. ",
        "Subsections": [],
        "Groundtruth": "Recent advances in deep reinforcement learning have led to impressive performance on continuous control tasks. However, policies learned solely in simulation often struggle to perform well in real-world systems. To address this, researchers are focusing on learning from multiple simulated domains that closely resemble the target real-world environment. One approach involves using an ensemble of environment models with varying parameters, such as mass or friction. The challenge lies in robust policy search, where policies must perform well across this ensemble. A new framework, effacts, introduces active learning to select model parameters for trajectory sampling, reducing sample complexity while maintaining policy performance and robustness. The framework improves scalability and draws connections between robust policy search and multi-task learning, offering significant reductions in sample complexity for learning robust policies. Experimental validation confirms the framework's effectiveness on standard continuous control benchmarks."
    },
    {
        "Section_Num": "2",
        "Section": "2 Related Work",
        "Text": " learn controllers with a specific functional form using trajecto- ries sampled for parameters drawn from an ensemble, and optimize for the average case performance. propose epopt, which learns arxiv:v nov narayanaswami et. al. a neural network (nn) policy using a model free drl algorithm, but on simulated domains sampled from an ensemble of models. an adversarial approach to training is taken that involves selectively exposing to the model free learner only data from those sampled models on which the learner exhibits the least performance. epopt optimizes the conditional value at risk, which has also been used for learning robust options. even though this is a more so- phisticated approach than the former and is demonstrated to have greater performance and robustness, the number of trajectories collected is still very large. a form of adversarial training is also employed in and , but in these works, external (adversarial) disturbances are applied to the agent, rather than the model itself changing. propose an approach that optimizes the average case perfor- mance, but additionally performs explicit system identification, and the estimated model parameters are fed to a nn policy as additional context information alongside the original observations. also uses system identification on data from the real world to decide the parameters on which to train. also perform system identifica- tion, but operate in a belief space over the model parameters. again, the data requirements are quite large, both for policy learning as well as system identification. a recent work that learns from an ensemble of models is , but the ensemble here consists of learned dnn models of the dynamics for use in model based rl, rather than being induced by changing physical properties of the environment. a similar ensemble gener- ated by perturbing an already learned model is used for planning through in . this work also does not deal with model uncertain- ties with physical meaning. approaches related to learning from an ensemble of models have also been studied under dynamics randomization and domain randomization . although uses only an appropriate subset of models to train on, none of the above approaches consider ways to sample trajec- tories only as necessary. our proposed framework employs active learning to decide with data from only a few model parameters the models for which the agent requires more training. active sampling approaches have also been explored for task selection in multi task learning by , a viewpoint we discuss in more detail in section ",
        "Subsections": [],
        "Groundtruth": "The section discusses various approaches to learning controllers with specific functional forms using trajectories sampled from an ensemble of models. The proposed method, Epopt, utilizes a neural network policy through a model-free deep reinforcement learning algorithm on simulated domains. Epopt optimizes for the conditional value at risk and employs adversarial training to improve performance and robustness. Other approaches involve explicit system identification and additional context information fed to the neural network policy. Different works focus on system identification in various ways, with some using an ensemble of learned models for model-based reinforcement learning. Active learning is proposed to determine which models need further training, aiming to optimize trajectory sampling."
    },
    {
        "Section_Num": "3",
        "Section": "3 Background",
        "Text": "",
        "Subsections": [
            {
                "Section_Num": "3_1",
                "Section": "3.1 RL on an Ensemble of Models",
                "Text": "we work with the same setting described in where the model ensemble is represented as a family of parametrized mdps on a fixed state and action space. following the same notation, this is the set m () = s, a, t , r,, s, for each parameter in the space of parameters p, whose elements are respectively the state and action spaces, transition functions, reward functions, discount factor and the initial state distribution. those items that are subscripted with depend on, i.e., different parameters induce different dynamics and rewards. we note here that we say parameter even if it is a vector rather than a real number. further, there is a source distribution p that indicates the likelihood of any particular p in the model ensemble. we denote the typical trajectory from any of these mdps by = {,,} =, where is the time horizon, and the discounted return from the start state () = = . these trajectories are generated by following a policy which are parameterized by a vector , which we denote by . we define the performance at parameter as the expected discounted return from the start state in m () (, ) = e \" = # () robust policy learning via cvar optimization robust policy search seeks policies that perform well across all parameters in p, and do so without knowing the parameter for the mdp on which they are being tested. this translates to being able to perform well on some unknown target domain, and also potentially handle variations not accounted for in p. the intuitive objective for this is to consider the average performance of the policy over the source distribution () = ep . however, this objec- tive could be close to the maximum even if there are sharp drops in performance in some regions of p. a different objective used by approaches such as is the conditional value at risk (cvar) formulation from , which considers the performance across only the subset of p that corresponds to the bottom percentile of returns from p, for a given (, ]. this has the effect that policies which have such sharp drops in performance (i.e bad worst case performance) are no longer considered good solutions. ",
                "Subsections": [],
                "Groundtruth": "In this section, the text outlines a method for robust policy learning on an ensemble of models. The models in the ensemble are represented as parametrized Markov Decision Processes (MDPs) with varying dynamics and rewards based on different parameters. The goal is to find policies that perform well across all parameters without knowing the specific parameter of the MDP being tested, allowing for handling variations not accounted for in the parameter space. The approach involves optimizing the Conditional Value at Risk (CVaR) to ensure good performance even in worst-case scenarios, rather than only considering average performance over the source distribution. This technique aims to mitigate sharp drops in performance in certain regions of the parameter space."
            },
            {
                "Section_Num": "3_3",
                "Section": "3.3 Linear Stochastic Bandits",
                "Text": "here, we provide a quick overview of linear stochastic bandits (lsb) since they play an important role as solutions to the active learning problem in section the lsb problem is one of finding the optimal arm from a given set of arms x similar to the standard multi armed bandit (mab) problem, but with the average reward from each arm being an unknown linear function of the features associated with that arm. that is, if x is an arm, and we also denote its features by , the reward is given by () = + , where is some zero mean noise, and gives the parameters for said linear function. thus, finding the optimal arm amounts to estimating . although it may seem restrictive to assume a linear dependence on the arms, more expressiveness can be achieved by using a feature transformer, in a manner similar to the practice for linear regression. the feature transformer is a function : x rthat maps each arm x to a feature vector (), and the lsb learner would be estimating so that () = ()+ . there have been several approaches to solving the lsb problem under various objectives. one group of works are based on the principles of the upper confidence bound algorithm for mab problems. the other popular class of approaches is based on thompson sampling. active learning for efficient trajectory sampling existing approaches for robust policy search are based on col- lecting trajectories at various parameters from p, and using some subset of these trajectories to optimize the policy being learned. an active learning framework for efficient robust policy search use active learner to learn about policy performance by collecting trajectories as necessary sample parameters according to given objective based on learned performance prole use active learner to learn about policy performance by collecting trajectories as necessary perform policy update using trajectories collected at each of these parameters. figure : main loop of the effacts framework. as mentioned before, the number of trajectories required can be very large, up to orders of magnitude more than required for a standard rl problem. although robust policy search is expected to require more data than standard rl, improvements to its sample efficiency are still necessary in order for it to be viable in complex real world systems. to motivate our developments to improve on the sample effi- ciency, we start with the observation that there is some functional dependence of the performance of a given policy on the model parameter corresponding to a task under consideration. existing approaches all disregard this dependence when evaluating their objective, leading to increased sample complexity. the increase in sample complexity is more severe when using the cvar objective such as in , due to having to discard most of the trajectories collected in order to estimate the cvar. in fact, for a standard value of = , % of the collected trajectories need to be discarded (more generally, a fraction). here, we wish to devise a strat- egy to utilize the information from the aforementioned functional dependence effectively so as to minimize such wastage. active learning and the effacts framework active learning is a paradigm where the agent chooses data to learn from based on its previous experience (see for a comprehensive survey). it has been used to speedup learning tasks, especially in situations with limited data. an active learner not only needs to work with as few samples as possible, it also needs to account for the uncertainty in whatever data it has collected. these are exactly the desiderata of the required strategy, as it must be able to fit the performance function across p by collecting as few trajectories as possible, which come with noisy evaluations of the performance. thus, quite clearly, the problem of efficiently performing such sam- pling is connected to active learning. the case of active learning that is of interest to us is when the agent is allowed to sample output for arbitrary points in the input space (instead of having to choose points from a finite dataset). the input here is some parameter from p, and the output is the return from one trajectory collected at . we now outline our active learning framework for learning robust policies. each policy update involves selectively generating algorithm the effacts framework : for = . . . do : learnperf() : p selectparams( ) : {} : for each p do : trajectory collected at using : : end for : + batchpolopt(, ) : end for : return trajectories to be sent to the batch policy optimization algorithm. this is done in two phases as follows: performance assessment: in the first phase, active learning is used to assess the performance of the current policy. to do this, an active learner sequentially picks some parameters and trajecto- ries are sampled for each of them by setting the environment to these parameters and running the current policy. after a particular number of trials, it is in theory expected to have a reasonably good approximation of the performance as a function of the parame- ters, say (). note that we have used as a subscript since this function is specific to the current policy with parameters . the active learner and this process are encapsulated in a subrou- tine learnperf that takes as input some policy parameters, and returns the function as described above. learnperf could be equipped with persistent memory (e.g. to store data from previous iterations), and no assumptions are made about the form of the output other than that it can be evaluated at any given p. parameter selection: the second phase uses this assessment to decide which parameters from p trajectories need to be collected for. this selection is guided by some given objective that enforces robustness. another subroutine, selectparams carries out this selection, taking as input a performance profile . based on this performance profile, it returns a set p of parameters at which the policy needs to be trained. the policy update is then performed using trajectories collected for each parameter in p, and this is done for a given number () of iterations. we call the resulting framework effacts (efficient active trajectory sampling), and summarize it in algorithm and figure any particular instantiation of effacts is defined by the choice of active learning algorithm and also the scheme used to select which parts of p to sample from, i.e by specifying the learnperf and selectparams subroutines. applying effacts we analyze one such instantiation based on the cvar objective for parameter selection. to generate samples of the bottom percentile of trajectories, a batch of parameters is sampled from p, and trajec- tories are collected only for those that are in the worst percentile of performance according to . the use of bandit algorithms for active learning is well studied in both multi armed bandit as well as linear stochastic bandit settings. the parameter narayanaswami et. al. spaces involved in robust rl are invariably continuous. lsbs and gaussian process regression are two well known approaches that can perform active regression on continuous spaces. lsbs, however, are specialized in the sense that they quickly seek out areas that lead to high reward, a property that we make use of as described shortly. considering that gaussian processes are more computa- tionally expensive, and also that lsbs are simpler to implement and can be very efficient, especially when data is scarce, we turn to lsbs as the active learner in our experiments. its arms are simply a large enough collection of parameters spread across p. pulling an arm p results in a trajectory being sampled at . feedback is given to the bandit in an adversarial manner, being proportional to the negative of the return obtained on that sampled trajectory. this causes it to seek out regions with low performance, and in the process learn about the performance across p. this behaviour is especially useful for identifying the worst case parameters that are used in optimizing the cvar objective. we note that although the bandits learning phase is inherently serial, it is still possible to collect the trajectories for the estimated worst percentile of parameters in parallel. we call this algorithm effacts c b and its learnperf and se- lectparams subroutines described in algorithm the following hyperparameters are introduced: , the number of trajectories sampled by the bandit in the course of its learning, , the total number of parameters chosen (this means that l m parameter values are drawn from the source distribution and their perfor- mance is estimated using the bandit, but only the bottom of those are used to collect trajectories). the most critical component is the lsb learner which incorporates internally a feature trans- former that takes in a parameter from the source distributions support and applies some transformation on it (including possibly standardization), and also scales the negative returns given to it appropriately. sample efficiency due to the fact that effacts c b strategically chooses parameters at which to collect trajectories, we expect it to be able to maintain robustness and performance while collecting fewer samples than existing approaches. consider for instance epopt, which dis- cards a fraction of the trajectories it collects. if epopt collects trajectories, and effacts c bs bandit learner is allowed arm pulls, with the same number of trajectories being used for learning as epopt (i.e = ), the ratio of the total amount of data collected by the two algorithms is \u0010 + \u0011 . for a nominal setting of = , = and = , this results in a dramatic % reduction in the amount of data collected. we later show that efficiency gains of this order are indeed attainable in practice. connections to multi task learning the problem of robust policy search on an ensemble of models can also be viewed as a form of transfer learning from simulated domains to an unseen real domain (possibly without any training on the real domain, which is referred to as direct transfer or jumpstart ). further, the process of learning from an ensemble of models can be viewed as a multi task learning (mtl) problem with the set of tasks corresponding to the set of parameters that constitute the source domain distribution. learning a robust policy corresponds to maintaining performance across this entire set of tasks, as is usually the goal in mtl settings. mtl, which is closely related to transfer learning, has been studied in the drl context in a number of recent works . however, these works consider only discrete and finite task sets, whereas model parameters form a (usually multi dimensional) continuum. more generally, we can think of mtl with the task set being generated by a set of parameters, and refer to such problems as parameterized mtl problems, with robust policy search being an instance of this setting. has employed a bandit based active sampling approach sim- ilar to what we have described here to intelligently sample tasks (from a discrete and finite task set) to train on for each iteration. the feedback to the bandit is also given in an adversarial manner. however, we note several differences when it comes to a parameter- ized mtl setting. the first is the functional dependence of the task performance to an underlying parameter as discussed earlier. in the discrete mtl settings usually studied (such as atari game playing tasks), there is no such visible dependency that can be modeled. this means that any algorithm that is adapted to the parameter- ized setting need to be reworked to utilize such dependencies as effacts does. the task selection procedure in effacts differs from the one in in that the performance is sampled for several tasks in between iterations of policy training. additionally, one single task is not chosen in the end, rather the active learner is used to inform the selection of a group of tasks as necessary for the given objective. ",
                "Subsections": [],
                "Groundtruth": "Linear Stochastic Bandits (LSBs) are important for active learning by finding the optimal arm from a set of arms with rewards as unknown linear functions of arm features. LSB problem is solved by estimating parameters for the linear function using feature transformers. Two common approaches to solving LSBs are based on upper confidence bound algorithm and Thompson sampling. An active learning framework called effacts is proposed to improve sample efficiency in robust policy search by utilizing information on the functional dependence of policy performance on model parameters. Effacts involves performance assessment and parameter selection phases using active learning algorithms. LSBs are chosen as the active learner due to their efficiency in seeking high reward areas. Effacts CB algorithm strategically selects parameters using LSB learner to maintain robustness and performance while reducing the amount of data collected. This approach is connected to multi-task learning, specifically in the context of parameterized MTL for robust policy search."
            }
        ],
        "Groundtruth": ""
    },
    {
        "Section_Num": "4",
        "Section": "4 Active Learning for Efficient Trajectory Sampling",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "5",
        "Section": "5 Connections to Multi-Task Learning",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "6",
        "Section": "6 Experiments",
        "Text": "",
        "Subsections": [
            {
                "Section_Num": "NA",
                "Section": "NA",
                "Text": "as epopt uses the cvar objective, it is a very suitable baseline against which to compare effacts c b for demonstrating the ben- efits of introducing the active learner. we conduct experiments to answer the following questions, which we will reference as rq, rq etc. in later discussions: () do the policies learned using effacts c b suffer any degra- dation in performance from that of epopt? is robustness preserved across the same range of model parameters as in epopt? () does the bandit active sampler identify with reasonable ac- curacy the region corresponding to the worst percentile of performance, and does it achieve a reasonable fit to the performance across the range of parameters (i.e has it ex- plored enough to avoid errors due to the noisy evaluations it receives)? () how much can the sample efficiency be improved upon? this mainly boils down to asking how few trajectories are sufficient for the bandit to learn well enough. implementation details and hyperparameters the experiments are performed on the standard hopper and half- cheetah continuous control tasks available in openai gym , simulated with the mujoco physics simulator . as in , some subset of the following parameters of the robot can be varied: torso an active learning framework for efficient robust policy search algorithm definitions of learnperf and selectparams subroutines for effacts c b : function learnperf() : for = . . . do : optimal arm estimate by . : trajectory collected at using : return from : update with (, ) : end for : function parameterized by s current weights : return : end function : function selectparams( ) : p {}, r {} : for = . . . l m do : parameter sampled from p : () : add to r and to p : end for : psubset of p giving rise to the bottom per- centile of returns in r : return p : end function parameter low high mass friction damping inertias parameter low high mass friction damping inertias table : description of the source domain distribution for the hopper (left) and half cheetah (right) tasks. the values given here specify the means () and standard deviations () for normal distributions truncated at the low and high points mentioned here. this is equivalent to the probability density of a normal distribution with parameters (, ) being zeroed outside the interval , and being normalized so that it integrates to figure : performance (y axis) as a function of torso mass (x axis) for = for the hopper (left) and half cheetah (right) tasks. the bands indicate the confidence intervals of the performance measured across runs of the entire training procedure. all of the configurations tested use at least % fewer trajectories than epopt, and with the exception of = and = in the hopper task alone, perform to the same extent and are as robust as epopt (acccording to results reported in ). mass, friction with the ground, foot joint damping and joint iner- tias. we also use the same statistics for the source distributions of the parameters which are described in table we emphasize that because we are using the same environments in and also the same policy parameterization, results reported there can be used directly to compare against effacts c b. we use the hyperparameter settings for trpo suggested in ope- nai baselines on which our implementation is also based. these are shown in table one difference from epopts implementation is that we use generalized advantage estimation instead of subtracting a baseline from the value function. for value function estimation narayanaswami et. al. hyperparameters median %tile avg %tile std. dev. max %tile =, = =, = =, = =, = table : statistics for the percentiles described in section for the hopper tasks -d model ensemble with = , which are measured every fifth iteration from the th to the th iterations. hyperparameter value timesteps per batch max kl cg iters cg damping vf iterations vf stepsize e- table : hyperparameters for trpo hyperparameter value table : thompson sampling hyperparameters using a critic, we use the same nn architecture as the policy, hidden layers of units each. policies are parameterized with nns and have two hidden layers with units each, and use tanh as the activation function. bandit algorithm ",
                "Subsections": [],
                "Groundtruth": "The text discusses the comparison between the effacts c b active learner and the epopt baseline in the context of policy learning. The experiments aim to answer questions regarding the performance degradation, robustness, accuracy of region identification, exploration efficiency, and sample efficiency. The experiments are conducted on standard hopper and half-cheetah tasks using the mujoco physics simulator. The text also includes implementation details, algorithm definitions for effacts c b, and parameter variations for the robots. Performance results show that effacts c b performs similarly to epopt with fewer trajectories and maintains robustness. Standard hyperparameter settings for TRPO and generalized advantage estimation are used. Thompson sampling hyperparameters and neural network architectures are detailed for the bandit algorithm."
            },
            {
                "Section_Num": "6_2",
                "Section": "6.2 Bandit Algorithm",
                "Text": "for all our experiments, we implement the bandit learner using thompson sampling due to its simplicity, following the version described in . the hyperparameters introduced by this are as in table (they have the same names as in the paper). the first two parameters control the amount of exploration per- formed during thompson sampling, while is a regularization parameter for the parameter estimates. the arms of the bandit are model parameters taken uniformly across the domain and converted to feature values. in order to allow for some degree of expressiveness for the fit, we apply polynomial transformations of some particular degree to the model parameters. the features input to the bandit are th degree polynomial terms generated from the model parameters. this amounts to terms in the -d case and in the -d case. these arm representations are then standardized before being used by the bandit. the negative returns given as feedback to the bandit are scaled by a factor of (rq) performance and robustness in this section, we perform the following experiment to evaluate effacts c b for the objectives of rq in this experiment, only one environment parameter (torso mass) is varied, creating a -d model ensemble on which to evaluate the algorithm. the torso mass is varied in both the hopper and the half cheetah domains keeping the rest of the parameters fixed at their mean values. the performance of the effacts c b learned policy is then tested across this range, and the results are shown in figure we use trust region policy optimization (trpo) for batch policy optimization and run it for iterations. for this part, we use th degree polynomial transformations. we see that the policy is indeed robust as it maintains its perfor- mance across the range of values of the torso mass, and it achieves near or better than the best performance for both tasks as reported in in all but one case, with = and = for hopper. this setting samples the least number of trajectories per iteration, , which is just one eighth of the drawn in epopt. although there is one region where it is unstable, it is still able to maintain its performance everywhere else, thus attaining the same level of robustnes as epopt. further, the performance achieved is almost as good as, and possibly better than epopt. for the other settings which use more trajectories, this does not happen, and even at = and = which samples the most trajectories, a % reduction in samples collected is achieved over epopt (the total number of iterations is the same). the other two settings which perform almost as well in both tasks collect each, which amounts to an even larger reduction of %. in the case of = and = with the half cheetah task, this number is pushed even further to % while still retaining performance and robustness. (rq) performance on a -d model ensemble in this question, we further investigate the scalability of the algo- rithm to larger model ensembles, again in the context of rq in this experiment, we vary the friction with the ground in addition to the torso mass, thus creating a two dimensional ensemble of parameters. here, we run trpo for iterations, and again use th degree polynomial transformations. figure shows the results obtained. full performance is maintained over almost all of the parameter space in both domains, again being comparable to or better than in . notably, with =, =, the same % reduction in an active learning framework for efficient robust policy search figure : performance as a function of torso mass and ground friction on the hopper (left half) and half cheetah (right half) tasks for = for one run or effacts c b. in each row, left: =, = right: =, = collected trajectories is obtained even in a higher dimensional model ensemble. this is also despite the added challenge of an increased number of parameters for the bandit to fit ( as opposed to in the previous experiment). ",
                "Subsections": [],
                "Groundtruth": "Bandit Algorithm using Thompson sampling is implemented with specific hyperparameters controlling exploration and regularization. Model parameters are transformed into feature values using polynomial terms for expressiveness. Scaled negative returns are used as feedback for the bandit. The algorithm is evaluated in varying environment parameters, demonstrating robustness and performance across different scenarios. Trust Region Policy Optimization is used for batch policy optimization, achieving near-optimal performance while reducing the number of trajectories sampled. The algorithm shows scalability to larger model ensembles and maintains performance over diverse parameter spaces."
            },
            {
                "Section_Num": "6_5",
                "Section": "6.5 Visualizing the Bandit Active Learner",
                "Text": "in figure , we present the outcome from one particular iteration of training. the true performance profile is estimated by collecting trajectories to calculate the mean return at each parameter (again, these trajectories are not used for learning). along with this is shown the bandits fit, the trajectories it collects while learning and the trajectories that are sampled based on its estimate of the bottom = percentile (output used for training the policy). we see that the bandit takes exploratory actions (points to the left and in the middle) that dont lead to the worst returns. however, it quickly moves towards the region with low returns, and the final fit is close to the true mean performance. from comparison with the output trajectories, the trajectories in the learning phase are quite clearly not representative of the bottom = percentile according to the source distribution. thus, we cannot reuse these to perform learning with the cvar objective. we also note that a perfectly learned performance profile is not necessary to sample from the worst trajectories. as long as the fit is reasonably accurate in that region, the output trajectories will be of good quality. in practice, we expect lsb algorithms to be capable of doing this as they tend to focus on these regions. (rq) analysis of the bandit active learner here we validate one of our key assumptions, that the active learner learns well enough about the performance that it can produce a decent approximation of a sample batch of the bottom percentile figure : the bandits operation in the th iteration of training in the hopper task. individual trajectories are shown as dots or crosses, with the position indicating the parameter value and the return obtained. of trajectories. for this, we first evaluate the average return for a batch of samples from p by collecting a large number of trajectories at each parameter. we note that these trajectories are solely for the purpose of analysis and are not used to perform any learning. then, the percentile of the trajectory deemed to have the greatest return among those chosen based on the bandit learner is computed using the returns in this batch by using a nearest neighbor approximation. this is done across several iterations during the training, and the median percentiles along with other statistics are reported in table for the hopper task. ideally, we would like this value to come out to around (when written as proper percentiles), i.e the parameter with the greatest performance among the worst percentile should be at the percentile. in our estimates, there are some outliers that cause the average to become large, but as the median value shows, it is indeed reasonably close to the desired value of for = . (rq) non stationary bandits for data reuse in this section, we attempt to answer rq by considering modifica- tions that can be made to effacts c b, so that it uses lesser data, while also achieving a level of performance and robustness that is close to the results above. particularly, we investigate the use of a modified version of the thompson sampling algorithm above that is suited for non- stationary scenarios. this is done in order to reuse performance history from previous iterations to estimate the bandits parameters with the aim of achieving a further reduction in sample complexity. to implement this, past data is weighted down in the linear re- gression step of thompson sampling, and the weight decays after each iteration by a factor . that is, at the iteration, the data from the , (). . .()iterations would have weights , . . .respectively. we compare this version with the original setup in figure we see that the setting with the smallest number of trajectories narayanaswami et. al. figure : results (performance vs. torso mass) for the non stationary bandit version (labeled with history) of effacts c b, along with the original effacts c b results (labeled without history) on the hopper -d ensemble. a value of = was used, with the other hyperparameters remaining unchanged. as before, the plot shows the perfor- mance as a function of torso mass for = , and the bands indicate the confidence intervals of the performance mea- sured across runs of the entire training procedure. this time, with % fewer trajectories than epopt, the configura- tion = , = with history is able to perform compa- rably to epopt, as well as effacts without history and using more trajectories. collected with history (= , = ) is more robust than the vanilla case with = , = and performs nearly as well (despite cutting down on more trajectories). with more data, some loss of robustness is encountered, but the performance improves, and becomes comparable to the best original case (= , = ). ",
                "Subsections": [],
                "Groundtruth": "In the section \"6.5 Visualizing the Bandit Active Learner,\" the text discusses the outcomes of training iterations where the bandit active learner explores different actions to learn performance profiles. It emphasizes the importance of accurate fit in the region with lower returns for quality output trajectories. The bandit active learner is shown to produce trajectories representative of the bottom percentile for training policy. Additionally, the text validates the assumption that the active learner can approximate a sample batch of the bottom percentile. Modifications using a non-stationary bandit algorithm are explored to reduce sample complexity and reuse performance history, resulting in comparable performance with fewer trajectories."
            },
            {
                "Section_Num": "6_8",
                "Section": "6.8 Other Remarks",
                "Text": "we note that we do not perform pre training as in epopt where the entire batch of trajectories is used for policy learning for some iterations at the beginning (corresponding to optimizing for the average return over the ensemble). this has been reported to be necessary, possibly due to problems with initial exploration if using the cvar objective from the beginning. effacts c b on the other hand works without any such step. however, at the very beginning, we collect trajectories for parameters sampled directly from p until time steps have elapsed in that iteration. this is because algorithms like trpo have been known to require at least that much data per iteration. conclusions and further possibilities we developed the effacts framework for using active learning to make an informed selection of model parameters based on agent per- formance, which can subsequently be used to judiciously generate trajectories for robust rl. with an illustration of this framework based on linear bandits and the cvar objective, we have both demonstrated its applicability for robust policy search as well as established its effectiveness in reducing sample complexity by way of empirical evaluations on standard continuous control domains. we also discussed our work in the context of multi task learning along with the similarities and differences between these settings. our work opens up requirements for active learning algorithms that can work well with even lesser data than we need here. methods like gaussian process regression are known to be efficient, but not in high dimensional spaces. for robust policy search methods to be effective for transfer from simulation to reality, they need to be able to handle the complexities of the real world, which necessitates methods that work with high dimensional model ensembles, which in turn entail frameworks such as effacts to help reduce the sample complexity. another possibility for robust policy search itself is to develop objectives that can speedup learning as well as make use of the features of effacts to maintain sample efficiency. with our interpretation of robust policy search as a parameter- ized version of multi task learning, a natural next step would be to adapt developments in the usual discrete mtl setting to robust policy search. it would also be worthwhile to similarly investigate the applicability of meta learning, as it would prove useful for both dealing with large disparities between the source domains and the real world, as well as coping with unmodeled dynamics (which are unavoidable since it is not feasible to model the real world with complete accuracy). ",
                "Subsections": [],
                "Groundtruth": "The text discusses the EffActs framework for robust policy search using active learning to select model parameters based on agent performance, reducing sample complexity. It compares with other methods like pre-training and highlights the need for algorithms that can handle high-dimensional spaces and real-world complexities. The effectiveness of EffActs is demonstrated through empirical evaluations on continuous control domains. Future possibilities include adapting multi-task learning and meta-learning techniques to enhance robust policy search efficiency and deal with unmodeled dynamics in real-world applications."
            }
        ],
        "Groundtruth": ""
    },
    {
        "Section_Num": "7",
        "Section": "7 Conclusions and Further Possibilities",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "8",
        "Section": "8 Acknowledgments",
        "Text": "the authors thank the robert bosch center for data science and ai, iit madras for funding this work and providing the requisite com- puting resources. we also thank aravind rajeswaran for valuable pointers and suggestions, and openai for their excellent codebase of deep rl algorithms. ",
        "Subsections": [],
        "Groundtruth": "The authors express gratitude to the Robert Bosch Center for Data Science and AI at IIT Madras for funding and providing computing resources for their work. They also acknowledge Aravind Rajeswaran for valuable pointers and suggestions, as well as OpenAI for their deep reinforcement learning algorithms."
    },
    {
        "Section_Num": "References",
        "Section": "References",
        "Text": "] yasin abbasi yadkori, dvid pl, and csaba szepesvri. improved algo- rithms for linear stochastic bandits. in proceedings of the th international conference on neural information processing systems. marc abeille and alessandro lazaric. linear thompson sampling revisited. in proceedings of the th international conference on artificial intelligence and statistics. shipra agrawal and navin goyal. thompson sampling for contextual bandits with linear payoffs. in proceedings of the th international conference on international conference on machine learning - volume andrs antos, varun grover, and csaba szepesvri. active learning in multi armed bandits. in proceedings of the th international conference on algorithmic learning theory. greg brockman, vicki cheung, ludwig pettersson, jonas schneider, john schulman, jie tang, and wojciech zaremba. openai gym. arxiv:arxiv: alexandra carpentier, alessandro lazaric, mohammad ghavamzadeh, rmi munos, and peter auer. upper confidence bound algorithms for active learning in multi armed bandits. in algorithmic learning theory. springer berlin heidelberg. an active learning framework for efficient robust policy search prafulla dhariwal, christopher hesse, oleg klimov, alex nichol, matthias plap- pert, alec radford, john schulman, szymon sidor, yuhuai wu, and peter zhokhov. openai baselines. https://github.com/openai/baselines. takuya hiraoka, takahisa imagawa, tatsuya mori, takashi onishi, and yoshi- masa tsuruoka. learning robust options by conditional value at risk optimization. in neurips. http://papers.nips.cc/paper/-learning- robust options by conditional value at risk optimization thanard kurutach, ignasi clavera, yan duan, aviv tamar, and pieter abbeel. model ensemble trust region policy optimization. in iclr. gilwoo lee, brian hou, aditya mandalika, jeongseok lee, and siddhartha s. srinivasa. bayesian policy optimization for model uncertainty. in iclr. https://openreview.net/forum?id=sjgvnsqk lihong li, wei chu, john langford, and robert e. schapire. a contextual- bandit approach to personalized news article recommendation. in proceedings of the th international conference on world wide web. timothy p. lillicrap, jonathan j. hunt, alexander pritzel, nicolas heess, tom erez, yuval tassa, david silver, and daan wierstra. continuous control with deep reinforcement learning. in iclr. i. mordatch, k. lowrey, and e. todorov. ensemble cio: full body dy- namic motion planning that transfers to physical humanoids. in ieee/rsj international conference on intelligent robots and systems (iros). jun morimoto and kenji doya. robust reinforcement learning. in advances in neural information processing systems emilio parisotto, jimmy ba, and ruslan salakhutdinov. actor mimic: deep multitask and transfer reinforcement learning. in iclr. x. b. peng, m. andrychowicz, w. zaremba, and p. abbeel. sim to real transfer of robotic control with dynamics randomization. in ieee interna- tional conference on robotics and automation (icra). lerrel pinto, james davidson, rahul sukthankar, and abhinav gupta. . ro- bust adversarial reinforcement learning. in proceedings of the th international conference on machine learning. pmlr. aravind rajeswaran, sarvjeet ghotra, balaraman ravindran, and sergey levine. epopt: learning robust neural network policies using model ensembles. in international conference on learning representations. fabio ramos, rafael possas, and dieter fox. bayessim: adaptive domain randomization via probabilistic inference for robotics simulators. in robotics: science and systems (rss). https://arxiv.org/abs/ andrei a. rusu, sergio gomez colmenarejo, caglar gulcehre, guillaume desjardins, james kirkpatrick, razvan pascanu, volodymyr mnih, koray kavukcuoglu, and raia hadsell. policy distillation. in iclr. andrei a. rusu, neil c. rabinowitz, guillaume desjardins, hubert soyer, james kirkpatrick, koray kavukcuoglu, razvan pascanu, and raia hadsell. pro- gressive neural networks. corr abs/ (). john schulman, sergey levine, philipp moritz, michael jordan, and pieter abbeel. trust region policy optimization. in proceedings of the nd international conference on international conference on machine learning - volume john schulman, philipp moritz, sergey levine, michael jordan, and pieter abbeel. high dimensional continuous control using generalized advantage esti- mation. in proceedings of the international conference on learning representations (iclr). john schulman, filip wolski, prafulla dhariwal, alec radford, and oleg klimov. proximal policy optimization algorithms. corr abs/ (). b. settles. active learning literature survey. computer sciences technical report university of wisconsinmadison. sahil sharma, ashutosh kumar jha, parikshit s hegde, and balaraman ravindran. learning to multi task by active sampling. in international conference on learning representations. marta soare, alessandro lazaric, and remi munos. active learning in linear stochastic bandits. in nips workshop on bayesian optimization in theory and practice. aviv tamar, yonatan glassner, and shie mannor. optimizing the cvar via sampling. in aaai. matthew e. taylor and peter stone. transfer learning for reinforcement learning domains: a survey. j. mach. learn. res. (). josh tobin, rachel fong, alex ray, jonas schneider, wojciech zaremba, and pieter abbeel. domain randomization for transferring deep neural networks from simulation to the real world. in iros. ieee, e. todorov, t. erez, and y. tassa. mujoco: a physics engine for model- based control. in ieee/rsj international conference on intelligent robots and systems. jack m. wang, david j. fleet, and aaron hertzmann. optimizing walking controllers for uncertain inputs and environments. in acm siggraph papers. ziyu wang, victor bapst, nicolas heess, volodymyr mnih, remi munos, koray kavukcuoglu, and nando de freitas. sample efficient actor critic with experience replay. in iclr. wenhao yu, jie tan, karen liu, and greg turk. preparing for the unknown: learning a universal policy with online system identification. in robotics: science and systems (rss). ",
        "Subsections": [],
        "Groundtruth": "The section covers various references related to improved algorithms for linear stochastic bandits, Thompson sampling, contextual bandits with linear payoffs, active learning in multi-armed bandits, robust policy search, model ensemble trust region policy optimization, deep reinforcement learning, robust reinforcement learning, and transfer of robotic control among others. The references include works presented at prestigious conferences such as Neural Information Processing Systems (NeurIPS), International Conference on Learning Representations (ICLR), and International Conference on Machine Learning (ICML). Additionally, the section features studies on topics like active learning, domain randomization, and optimization techniques for reinforcement learning domains."
    },
    {
        "Section_Num": "NA",
        "Section": "NA",
        "Text": "an active learning framework for efficient robust policy search sai kiran narayanaswami the university of texas at austin nskiran@cs.utexas.edu nandan sudarsanam robert bosch centre for data science and ai indian institute of technology madras nandan@iitm.ac.in balaraman ravindran robert bosch centre for data science and ai indian institute of technology madras ravi@cse.iitm.ac.in abstract r",
        "Subsections": [],
        "Groundtruth": "The text presents an active learning framework for efficient robust policy search, developed by researchers from The University of Texas at Austin and the Indian Institute of Technology Madras. The framework aims to enhance policy search by incorporating active learning techniques. Key contributors include Sai Kiran Narayanaswami, Nandan Sudarsanam, and Balaraman Ravindran."
    },
    {
        "Section_Num": "1",
        "Section": "1. Introduction",
        "Text": "soit k un corps de nombres. on sint eresse ` a etudier les points entiers dune k- vari et e lisse. on note par xc une compactication lisse de x. en g en eral, m eme si on conna t le comportement des points rationnels de xc, on ne peut quen d eduire tr` es peu dinformations sur les points entiers de x. depuis tr` es longtemps, on sint eresse ` a la densit e de zariski de lensemble des points entiers. on sint eresse egalement ` a lapproximation forte la densit e de lensemble des points entiers plong e dans lensemble des points ad eliques. dans la litt erature, de vari et es de types divers ont et e etudi ees. dans cet article, on se limite aux ouverts de certaines vari et es ab eliennes. dans larticle de b. hassett et y. tschinkel , ils ont discut e la question de la densit e potentielle de zariski des points entiers. ils ont d emontr e que tout ouvert dun produit de vari et es ab eliennes simples dont le compl ementaire a grand codimension v erie cette densit e potentielle. a. kresch et y. tschinkel mots cl es : approximation forte, obstruction de brauermanin, vari et es ab eliennes, points de torsion, points entiers. msc : g, g janvier yongqi liang ont aussi les r esultats num eriques qui soutiennent une r eponse armative ` a cette question pour certaines jacobiennes epoint ees en un point rationnel. dun autre point de vue, dans larticle r ecent de y. cao, f. xu et lauteur, nous avons etudi e la densit e des points entiers au sens de la topologie ad elique. nous avons r epondu partiellement ` a une question de o. wittenberg, cf. [aim, problem ] et . parmi nos r esultats, nous avons montr e le th eor` eme suivant. th eor` eme (). soit e une k courbe elliptique de rang de mor- dellweil non nul. soit a une k vari et e ab elienne de dimension strictement positive et de rang de mordellweil z ero. si o d esigne l el ement neutre de e a, alors louvert (e a) \\ o ne v erie pas lapproximation forte avec lobstruction de brauermanin hors de . sa d emonstration est bas ee sur une g en eralisation dun argument de d. harari et j. f. voloch et sur une id ee qui remonte ` a b. poonen . le but de cet article est de g en eraliser ce th eor` eme. nous gardons les m eme hypoth` eses sur e et a, consid erons le compl ementaire x dun ensemble quelconque de points de torsion de e a. en am eliorant lancien argument, nous trouvons une description compl` ete pour la propri et e dapproximation forte sur x. th eor` eme . soit k un corps de nombres. soit s un ensemble ni de places de k contenant les places archim ediennes. soit e une k courbe elliptique de rang de mordellweil non nul. soit a une k vari et e ab elienne de dimension strictement positive et de rang de mordellweil z ero. pour t un ensemble quelconque de points de torsion de e a, on note par x son compl ementaire. si la projection de t sur a contient un point k rationnel, alors x ne v erie pas lapproximation forte avec lobstruction de brauermanin hors de s. en supposant la nitude du groupe de tateshafarevich x(e a), si la projection t sur a ne contient pas de points k rationnels, alors x v erie lapproximation forte avec lobstruction de brauermanin hors de s. dans , nous rappelons le contexte concern e et pr esentons un enonc e plus pr ecis, dont le th eor` eme est une consequence. dans , nous d etaillons sa d emonstra- tion. enonc e pr ecis du r esultat dans cet article, le corps de base k est toujours un corps de nombre quelconque. pour une place v appartient ` a lensemble des places de k, on note par kv le compl et e de k. on note par lensemble des places archim ediennes. pour v \\ , on note par okv lanneau des entiers de kv. on note par a lanneau des ad` eles de k, et par as lanneau des s ad` eles si s est un sous ensemble ni de places. soit x une k vari et e (sch ema s epar e de type ni, g eom etriquement int` egre sur k) lisse. laccouplement de brauermanin est d eni par x(a) br(x) q/z (xv)v, b x v invv(b(xv)), o` u invv : br(kv) q/z d esigne linvariant local en v provenant de la th eorie des corps de classes locaux. pour un sous ensemble b brx, le sous ensemble x(a)b des familles de points locaux qui sont orthogonales ` a tous les el ements appartenant ` a b est ferm e dans lespace des points ad eliques x(a). il contient lensemble des points rationnels x(k) dapr` es la th eorie des corps de classes globaux, et il contient ainsi son adh erence x(k). rappelons que x v erie lapproximation forte avec lobstruction de brauermanin par rapport ` a b hors de s si x(k) x(as) est dense dans prs(x(a)b) x(as), o` u prs : x(a) x(as) est la projection en oubliant les composantes des places dans s. si b = br(x), on dit simplement que x v erie lapproximation forte avec lobstruction de brauermanin hors de s. dans lintroduction, nous avons enonc e le cas particulier o` u a et e sont des vari et es ab eliennes du corollaire de . en eet, ce dernier corollaire etait enonc e et d emontr e dans pour les vari et es semi ab eliennes. dans le reste de cet article, nous d emontrons le th eor` eme suivant qui g en eralise au sens que le ferm e f nest pas forc ement de dimension et que f ne contient m eme pas n ecessairement un point k rationnel. th eor` eme . soit k un corps de nombres. soit s un ensemble ni de places de k contenant les places archim ediennes. soient a et e des vari et es semi- ab eliennes d enies sur k. supposons que a est de dimension strictement positive et que a(k) est discret dans a(as). supposons que e est de dimension et que e(k) nest pas discret dans e(a). soit f e a un ferm e de codimension supposons que la projection de f sur a contient au moins un point k rationnel p tel que la bre f a p, consid er ee comme un ferm e de zariski de e, ne consiste quen points de torsion (pas forc ement k rationnels) de e. alors x = (e a) \\ f ne v erie pas lapproximation forte avec lobstruction de brauermanin hors de s. lorsque e est une courbe elliptique, sur laquelle lhypoth` ese est equivalente ` a la condition que son rang de mordellweil est non nul. la premi` ere conclusion du th eor` eme en d ecoule directement. expliquons la seconde conclusion comme suit. remarque . en supposant la nitude des groupes de tateshafarevich x(aab) du quotient ab elien aab de a et x(e) quand e est une courbe elliptique, lhypo- th` ese partielle du th eor` eme que la projection de f sur a contient au moins un point k rationnel est n ecessaire. si la projection de f ne contient aucun point k- rationnel, alors x v erie lapproximation forte avec lobstruction de brauermanin hors de s. en eet, il sut de consid erer lhypoth` ese la plus faible et la conclusion la plus forte, disons s = . ceci r esulte dun argument simple de bration : si (xv)v est orthogonale ` a br(x), sa projection sur a est alors orthogonale ` a br(a), cela en- tra ne que cette projection (en oubliant les composantes archim ediennes) provient dun point k rationnel p dapr` es un r esultat de d. harari [har, th eor` eme ] car a(k) est suppos e discret dans a(a). comme la projection de f ne contient aucun k point, la bre x ap est identique ` a e. elle contient donc (xv)vbr(e) quitte ` a modier les composantes archim ediennes si n ecessaire. la famille de points locaux (xv)v\\peut etre approxim ee par un point global dapr` es [har, th eor` eme ]. yongqi liang d emonstrations du enonc e on d emontre le th eor` eme dans cette section. tout dabord, on compare certains groupes de brauer dans le lemme . le th eor` eme est une cons equence directe des lemmes , et de la proposition . ` a la n, on d emontre la proposition . lemme . soient a, e, x, f, et p comme dans le th eor` eme . le point p a(k) induit une immersion ferm ee ip : x a p x. si on identie x a p avec louvert e \\ (f a p) de e, alors im[i p : br(x) br(x a p)] = im. d emonstration. consid erons le diagramme commutatif suivant br(e a) / \u000f br(e) \u000f br(x) i p / br(x a p), o` u les ` eches verticales sont induites par les immersions ouvertes et o` u la ` eche horizontale en haut est induite par la section associ ee au point rationnel p a(k). comme f = (e a) \\ x est de codimension , la ` eche ` a gauche est un isomorphisme dapr` es le th eor` eme de puret e pour les groupes de brauer. l egalit e voulue r esulte alors de la surjectivit e de la ` eche en haut. lemme . soit k un corps de nombres. soit s un ensemble ni de places de k. soit f : x a un morphisme entre de vari et es alg ebriques d enies sur un corps de nombres k. supposons que a(k) a(as) est discret. soit p a(k) un point rationnel de a tel que la bre xp = x a p = . si x v erie lapproximation forte avec obstruction de brauermanin hors de s, alors xp v erie lapproximation forte avec obstruction de brauermanin par rapport ` a i p (br(x)) hors de s. d emonstration. cest un argument standard de bration. il existe un ouvert u de a(as) tel que u a(k) = {p}. soit vp xp (as) un ouvert tel que [vp y vs xp (kv)] xp (a)i p (br(x)) = . louvert vp est alors la restriction dun ouvert v f (u) x(as) ` a la bre xp . dapr` es la fonctorialit e de laccouplement de brauermanin, [v y vs x(kv)] x(a)br(x) = . comme x v erie lapproximation forte avec obstruction de brauermanin hors de s, lensemble des points rationnels x(k) intersecte avec v . si q x(k) v , alors f(q) = p et q xp (k) vp . donc xp v erie lapproximation forte par rapport ` a i p (br(x)) hors de s. le th eor` eme r esulte directement des lemmes pr ec edents et de la proposition suivante. proposition . soit k un corps de nombres. soit s un ensemble ni de places de k contenant les places archim ediennes. soit e une vari et e semi ab elienne de dimension d enie sur un corps de nombres k. supposons que e(k) nest pas discret dans e(a). soit m = {m, m, . . . , ms} e un sous ensemble ni non- vide de points de torsion de e. si on note par e le compl ementaire de m dans e, alors e ne v erie pas lapproximation forte avec lobstruction de brauermanin par rapport ` a br(e) hors de s. d emonstration. la preuve g en eralise largument de harari et voloch sur la courbe elliptique y = x + d enie sur q. une partie de largument suivant a et e etablie dans pour traiter le cas o` u m consiste en un seul point rationnel et pour l enonc e sur un corps de nombres k quelconque. la nouveaut e ici est que m peut contient plusieurs de points ferm es non n ecessairement rationnels. dabord, on d emontre la proposition pour le cas o` u e est une courbe elliptique. lorsque e est un tore de dimension , la preuve est essentiellement la m eme. on explique ` a la n les modications n ecessaires pour le cas de tores. ` a partir de maintenant, supposons que e est une courbe elliptique. soit e son mod` ele de n eron sur lanneau des entiers ok. pour tout j n, j s, soit mj ladh erence de zariski de mj dans e. la normalisation de mj est spec(okj), o` u okj est lanneau des entiers du corps r esiduel kj de mj. soit e le compl ementaire dans e de la r eunion de mj. soit n n tel que nmj = o e(kj) pour tout j. dapr` es lhypoth` ese, e est de rang de mordellweil strictement positif. fixons un point rationnel q e(k) dordre inni. il d enit une section de e not ee encore par q e(ok). les sous ensembles suivants de sont alors nis. t = {v \\ : q intersecte avec mj au dessus de v pour un certain j} t = {v \\ : spec(okj) spec(ok) est rami e au dessus de v pour un certain j} t = {v \\ : spec(okj) mj admet une bre non triviale au dessus de v pour un certain j} t = t t t s dapr` es le th eor` eme de seigel, lensemble des points t -entiers e(ot ) = {p = q, p, , pt} est ni. choisissons une place non dyadique v \\ t telle que q pi mod v pour i t et telle que e admet une bonne r eduction en v soit q la caract eristique du corps r esiduel fv de ok en v on trouve soit pgcd(n|e(fv)| + , q) = , soit pgcd(n|e(fv)| , q) = d enissons a = ( n|e(fv)| + , si pgcd(n|e(fv)| + , q) = ; (q )n|e(fv)| + , si pgcd(n|e(fv)| , q) = alors a est toujours premier avec qn|e(fv)|. dapr` es le th eor` eme de la progres- sion arithm etique de dirichlet, soit lensemble inni des nombres premiers l qui v erient l a mod qn|e(fv)|. donc n|e(fv)| divise l et la valuation valq(l ) = valq(n|e(fv)|) est une constante pour tout l . yongqi liang pour v , d esignons (e(kv)) le groupe des composantes connexes du groupe de lie e(kv). consid erons la suite (lq)l plong ee dans lespace compact y v (e(kv)) y v / e(okv). il existe alors une sous suite convergente vers un el ement not e par (xv)vqui est ainsi orthogonal ` a br(e) dapr` es la continuit e de laccouplement de brauermanin. pour toute v , le compos e e(kv) e(kv) (e(kv)) est surjectif. quitte ` a modier xv si n ecessaire, on peut supposer que xv e(kv) pour toute place archim edienne sans aecter lorthogonalit e avec le groupe de brauer. pour toute v / , d emontrons que xv e(kv). en eet, il sut de consid erer le cas o` u il existe une place w de kj au dessus de v telle que lextension kj,w/kv est triviale, sinon le corps r esiduel de chaque point du support du -cycle mj,kv = mj spec(k) spec(kv) contient strictement kv ainsi que xv nest jamais contenu dans mj,kv. soit w une telle place et soit mj,w limage de mj par e(kj) e(kj,w). comme n divise l , on trouve que lmj,w = mj,w dans e(kj,w), ou bien lmj,w = mj,w dans e(okj,w) o` u mj,w = mj spec(ok) spec(okj,w). puisque q nest pas de torsion q = mj,w e(kj,w), il existe un entier r strictement positif tel que les r eductions de q e(ok) et de mj,w e(okj,w) sont di erentes dans e(okj,w/(r w)) o` u w est une uniformisante de kj,w. si la limite xv de lq est egale ` a mj,w, il existe alors un nombre inni de premiers l tel que lq co ncide avec mj,w = lmj,w dans e(okj,w/(r w)). cela contredit au fait que les r eductions de q et de mj,w sont di erentes dans e(okj,w/(r w)). pour toute v t , d emontrons que xv e(okv), autrement dit la r educ- tion xv de xv modulo v ne se trouve pas dans ss j= mj. dapr` es la choix de t , la r eduction mj spec(ok) spec(fv) est une sous sch ema ferm e r eduit de ev = e spec(ok) spec(fv). si elle contient xv, il existe alors une place w de kj non- rami ee et de degr e au dessus de v. la r eduction mj,w ev(fw) de mj,w mod w co ncide avec xv. comme n divise l et lordre de mi divise n, on trouve que mj = lmj e(kj), do` u mj,w = l mj,w ev(fw). pour l susamment grand, la r eduction l qv de lq mod v est egale ` a xv = mj,w = l mj,w ev(fw). ceci entra ne que l qv = l mj,w ev(fw) pour un nombre inni de premiers l, do` u qv = mj,w ev(fw) qui contredit le fait que q et mj nintersectent pas en dehors de t . par cons equent xv e(okv) pour toute v / t . nous concluons que (xv)v[( y vt e(kv)) ( y v / t e(okv))]br(e). an de compl eter la preuve, il reste ` a d emontrer que pour tout s la troncation (xv)vs prs [( y vt e(kv)) ( y v / t e(okv))]br(e) ! ne se trouve pas dans ladh erence de e(k) e(as). comme e(k) [( y vt \\s e(kv)) ( y v / t e(okv))] = e(ot ) = {p = q, p, , pt} est ni, e(k) est discret dans e(as). il sut de d emontrer que (xv)v / s nest limage daucun des points pi. observons que lq q mod v car |e(fv)| divise l pour i t, on trouve que q pi mod v dapr` es la choix de v, et ainsi que xv pi mod v donc (xv)v / s nest pas limage de pi pour i t. par labsurde, supposons que (xv)v / s = p = q. alors une sous suite de (lq)l converge vers q dans e(kv), disons (l )q o pour les premiers l apparus comme indices de la sous suite convergente. de lautre c ot e, e(kv) contient un sous- groupe dindice nie qui est isomorphe ` a (okv , +) comme groupes topologiques dapr` es . comme q nest pas un point de torsion, si on note cette derni` ere indice par n, l el ement q = nq e(kv) est alors non nul. en plus, q se trouve dans le sous groupe qui peut etre identi e avec (okv , +). la convergence (l)q = (l)nq o implique que l dans okv qui contredit le fait que la valuation valq(l ) = valq(n|e(fv)|) est une constante pour tout l . enn, expliquons ladaptation n ecessaire lorsque e est un tore de dimension au lieu dune courbe elliptique. dans la preuve, nous avons besoin dun ok mod` ele e qui est un sch ema en groupe de type ni et lisse sur ok, pour un tore nous prenons la composante connexe de lidentit e du lft mod` ele de n eron, cf. [blr, theorem ]. dapr` es le th eor` eme de dirichlet g en eralis e , le groupe des unit es e(ok) est un groupe ab elien de type ni. il est de rang strictement positif car e(k) e(a) nest pas discret. nous xons q e(ok) un point dordre inni. un point de torsion mj m s etend en une section mj e(ok). tout reste de la preuve fonctionne dans ce contexte. r ef erences open problem session of the amer. inst. math. workshop : rational and integral points on higher dimensional varieties. disponible sur http ://ai- math.org/pastworkshops/ratlhigherdimvarproblems.pdf. s. bosch, w. l utkebohmert, and m raynaud. n eron models, volume of ergebnisse der math. springer verlag, y. cao, y. liang, and f. xu. arithmetic purity of strong approximation for homogeneous spaces. disponible sur arxiv : . d. harari. le d efaut dapproximation forte pour les groupes alg ebriques commutatifs. algebra number theory, () :, b. hassett and y. tschinkel. density of integral points on algebraic va- rieties. in rational points on algebraic varieties, number in progr. math., pages birkh auser, d. harari and j. f. voloch. the brauermanin obstruction for integral points on curves. math. proc. cambridge philos. soc., () :, a. kresch and y. tschinkel. integral points on punctured abelian surfaces. in algorithmic number theory, number in lecture notes in comput. sci., pages springer, a. mattuck. abelian varieties over p adic groud elds. annals of math., () :, b. poonen. insuciency of the brauer manin obstruction applied to etale covers. ann. of math., () :, v. platonov and a. rapinchuk. algebraic groups and number theory. academic press, yongqi liang o. wittenberg. rational points and zero cycles on rationally connected varieties over number elds. pr epublication ` a paraitre dans the procee- dings of the ams summer institute in algebraic geometry, disponible sur arxiv : . yongqi liang university of scinece and technology of china, school of mathematical sciences, jinzhai road, hefei, anhui, china e mail address: yqliang@ustc.edu.cn ",
        "Subsections": [],
        "Groundtruth": "The text discusses the study of integral points on smooth k-varieties and the Zariski density of these points. It focuses on the potential Zariski density of integral points on abelian varieties. The authors discuss the potential density of integral points and strong approximation for these varieties. Results from B. Hassett and Y. Tschinkel are mentioned, highlighting the potential density for certain abelian varieties. The text delves into the strong approximation property with Brauermanin obstruction outside a given set. The main theorems presented extend these ideas, particularly for abelian varieties and semi-abelian varieties. The approach includes mathematical proofs and techniques such as Brauer groups and local field theory. The work aims to provide a thorough understanding of integral points and strong approximation in the context of adelic topology."
    },
    {
        "Section_Num": "2",
        "Section": "2. Énoncé précis du résultat",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "3",
        "Section": "3. Démonstrations du énoncé",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "Références",
        "Section": "Références",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "NA",
        "Section": "NA",
        "Text": "arxiv:v jan approximation forte sur un produit de vari et es ab eliennes epoint e en des points de torsion yongqi liang r esum e. consid erons lapproximation forte pour les vari et es alg ebriques d e- nies sur un corps de nombres k. soit s un ensemble ni de places de k contenant les places archim ediennes. soit e une courbe elliptique de rang de mordellweil non nul et soit a une vari et e ab elienne de dimension strictement positive et de groupe de mordellweil ni. pour un ensemble ni quelconque t de points de torsion de ea, notons par x son compl ementaire. en supposant la nitude de x(e a), nous d emontrons que x v erie lapproximation forte avec lobstruction de brauermanin hors de s si et seulement si la projection de t sur a ne contient aucun point k rationnel. strong approximation for products of abelian varieties punctured at torsion points abstract. consider strong approximation for algebraic varieties dened over a number eld k. let s be a nite set of places of k containing all archimedean places. let e be an elliptic curve of positive mordellweil rank and let a be an abelian variety of positive dimension and of nite mordellweil group. for an arbitrary nite set t of torsion points of e a, denote by x its complement. supposing the niteness of x(e a), we prove that x satises strong approximation with brauermanin obstruction os if and only if the projection of t to a contains no k rational points. introduction ",
        "Subsections": [],
        "Groundtruth": "The section discusses strong approximation for algebraic varieties defined over a number field. It considers products of abelian varieties punctured at torsion points and aims to prove strong approximation with Brauer-Manin obstruction. The text shows that strong approximation holds if and only if the projection of the torsion points onto the abelian variety contains no rational points over the number field."
    },
    {
        "Section_Num": "I",
        "Section": "I Introduction",
        "Text": "i n recent years, due to the availability of massive amounts of credible data (big data: text, video, audio, etc, and tremendous advances in the area of digital electronics technologies that provide immense computing power, there has been a revival in the area of articial intelligence (ai), particularly in the area of deep learning (dl) , a sub- eld of machine learning (ml). the eld of dl emerged in after a long pause in the area of neural networks (nns) research . a key aspect in dl is that the networks and/or their weights are not designed by human beings. instead, they are learned from data using a general purpose learning procedure , . while ml uses algorithms to parse and learn from data, to make informed decisions, dl structures algorithms in layers to create an articial neural network (ann) that can learn, and similar to human intelligence, can make accurate decisions on its own . therefore, instead of designing algorithms by hand, systems can be built and trained to implement concepts in a way similar to what comes naturally to humans, and with accuracy sometimes exceeding human level performance , . in dl, each layer is designed to detect features at different levels. a layer transforms the representation at one level (starting from input data which maybe images, text, or sound) to a representation at a higher, slightly more abstract volume , arxiv:v jan shawahna et al.: fpga based accelerators of deep learning networks for learning and classication: a review level . for example, in image recognition, where input initially comes in the form of pixels, the rst layer detects low level features such as edges and curves. the output of the rst layer becomes input to the second layer which produces higher level features, for example semi circles, and squares . the next layer assembles the output of the previous layer to parts of familiar objects, and a subsequent layer detects the objects. as we go through more layers, the network yields an activation map that represents more and more complex features. the deeper you go into the network, the lters begin to be more responsive to a larger region of the pixel space. higher level layers amplify aspects of the received inputs that are important for discrimination and suppress irrelevant variations. a. applications of deep learning networks with the now widely used convolution neural networks (cnns) , and deep neural networks (dnns) , , it is now possible to solve problems in domains where knowledge is not easily expressed explicitly and implicit information is stored in the raw data. solutions to multifarious problems in the domain of sciences, business, etc., have been possible that were not conceivable for several years, in spite of best attempts by the ai community. this has been primarily possible due to the excellent abil- ity of deep learning in discovering intricate structures in high dimensional data. examples include character recog- nition , gesture recognition , speech recognition (e.g., in google now, siri, or click through prediction on an advertisement) , document processing , natural language processing , , video classi- cation , image classication , face detection and recognition , , robot navigation , real- time multiple object tracking , nancial forecasting , and medical diagnosis systems , to name a few. other recent areas of applications include automated driving (e.g., learning to detect stop signs, trafc lights, pedestrians, etc, aerospace and defense (e.g., identify ob- jects from satellites and identify safe or unsafe zones), medical research (e.g., in identication of cancer cells), industrial automation (e.g., to improve worker safety by detecting when people or objects are within an unsafe distance of machines), and electronics (used in automated hearing, speech translation, etc , . b. emergence of deep learning networks convolutional neural networks are considered as one of the most inuential innovations in the eld of computer vision . the success of deep learning networks grew to prominence in when krizhevsky et al. uti- lized cnns to win the annual olympics of computer vision, imagenet large scale vision recognition challenge (ilsvrc) . using alexnet model, they achieved an astounding improvement as the image classication error dropped from % (in ) to %. imagenet is a stan- dard benchmark dataset used to evaluate the performance figure imagenet competition results . of object detection and image classication algorithms. it consists of millions of different images distributed over tens of thousands of object classes. cnns have achieved even better accuracy in classica- tion and various computer vision tasks. the classication accuracy in ilsvrc improved to % , % , and % in the , , and competitions, respectively. fig. shows the accuracy loss for the winners of imagenet competitions before and after the emergence of deep learning algorithms. thereafter, large host companies started using cnns at the core of their services. google, microsoft, facebook, amazon, pinterest, and instagram are currently using neural networks for their photo search, bings image feeds, auto- matic tagging algorithms, product recommendations, home feed personalization, and for their search infrastructure, respectively . however, the classic use case of cnns is for image and speech processing . a typical cnn is a multi layered feed forward ann with a pipeline like architecture. specically, each layer performs a well known computation on the outputs of the previous layer to generate the inputs for the next layer. in general, cnns have two types of inputs; the data to be tested or classied (also named as feature maps), and the weights. images, audio les, and recorded videos are examples of the input data to be classied using cnns. on the other hand, the network weights are the data generated from training the cnn on a dataset containing similar inputs to the one being tested. c. hardware acceleration of deep learning networks to provide more accurate results as well as real time object recognition, for example in applications such as robots and auto piloted cars, the size of the convolution neural network needs to be increased by adding more neural network layers . however, evolving more and new type of nn layers results in more complex cnn structures as well as high depth cnn models. thus, billions of operations and millions of parameters, as well as substantial computing re- sources are required to train and evaluate the resultant large- scale cnn , , . such requirements represent volume , shawahna et al.: fpga based accelerators of deep learning networks for learning and classication: a review a computational challenge for general purpose processors (gpp). consequently, hardware accelerators such as appli- cation specic integrated circuit (asic), eld programmable gate array (fpga), and graphic processing unit (gpu) have been employed to improve the throughput of the cnn. in practice, cnns are trained off line using the back- propagation process . then, the off line trained cnns are used to perform recognition tasks using the feed forward process . therefore, the speed of feed forward process is what matters. gpus are the most widely used hardware accelerators for improving both training and classication processes in cnns . this is due to their high memory bandwidth and throughput as they are highly efcient in oating point matrix based operations . however, gpu acceler- ators consume a large amount of power. therefore, their use in cnn based applications implemented as a cloud service on large servers or in battery operated devices becomes a challenge. furthermore, gpus gain their performance from their ability to process a large image batch in parallel. for some applications like a video stream, input images should be processed frame by frame as the latency of the result of each frame is critical to the applications performance. for some tracking algorithms, the result of one frame affects the process of the next frame . nurvitadhi et al. recently evaluated emerging dnn algorithms on latest generations of gpus (i.e., nvidia titan x pascal) and fpgas (i.e., intel arria gx and intel stratix ). the experimental results show that current trends in deep neural networks favor fpga platforms as they offer higher power efciency (a.k.a., performance per watt). fpga and asic hardware accelerators have relatively limited memory, i/o bandwidths, and computing resources compared with gpu based accelerators. however, they can achieve at least moderate performance with lower power consumption . the throughput of asic design can be improved by customizing memory hierarchy and assigning dedicated resources . however, the development cycle, cost, and exibility are not satisfactory in asic based acceleration of deep learning networks , . as an alternative, fpga based accelerators are currently in use to provide high throughput at a reasonable price with low power consumption and recongurability , . the availability of high level synthesis (hls) tools, using c or c++, from fpga vendors lowers the programming hurdle and shortens the development time of fpga based hardware accelerators . convolutional neural networks have a very useful prop- erty, that is, each feature map neuron shares its weights with all other neurons . the authors in , proved that the highest energy expense results from accessing the off chip dram memory for data movement rather than computation. in other words, the energy cost of the increased memory accesses and data movement due to the large number of cnn operations often exceeds the energy cost of computation , . thus, cnn accelerators need to carefully consider this to achieve efcient architecture in terms of time and power. in this paper, we review the current status of using fpgas as accelerators for implementing deep learning networks. we highlight the implementation challenges and design directions used to tackle those challenges. we also provide future recommendations to maximize the performance of fpgas as accelerators for deep learning networks and simplify their use. the remainder of the paper is organized as follows. section ii provides background information about cnns, their key operations, and some well known deep learning networks. in addition, it introduces the basic structure of fp- gas and highlights their features enabling them to acceler- ate computationally intensive applications. it also discusses the implementation challenges of deep learning networks on fpgas and how these challenges can be overcome. section iii reviews existing cnns compression techniques and presents the current status of accelerating deep learning networks using asic based and fpga based accelerators. section iv describes the use of metaheuristics in the de- sign and optimization of cnns implementation. section v summarizes existing design approaches for accelerating deep learning networks and provides recommendations for future directions that will simplify the use of fpga based accelerators and enhance their performance. finally, section vi concludes the paper. ",
        "Subsections": [],
        "Groundtruth": "Artificial intelligence, specifically deep learning (DL), has seen a resurgence with the availability of big data and advances in digital electronics providing significant computing power. DL involves training neural networks to learn features from data, making informed decisions autonomously. Applications of DL include image and speech recognition, natural language processing, and medical diagnosis systems. Convolutional neural networks (CNNs) have been instrumental in advancing computer vision, achieving high accuracy in tasks like object detection and image classification. To enhance DL performance, hardware accelerators such as field-programmable gate arrays (FPGAs) are being used to increase computational power and efficiency. FPGAs offer high power efficiency, making them favorable for deep learning implementation compared to GPUs and ASICs. The use of FPGAs in DL networks can provide high throughput, low power consumption, and reconfigurability. High-level synthesis tools simplify programming FPGAs for DL accelerators. Efficient memory management is crucial for optimizing time and power in CNN accelerators. Future directions aim to maximize FPGA performance for DL networks and simplify their implementation."
    },
    {
        "Section_Num": "II",
        "Section": "II Background and Terminology",
        "Text": "this section gives an overview of the key operations and terminology used in convolutional neural networks (cnns) and provides examples of well known deep learning net- works. in addition, it illustrates the basic structure of eld programmable gate arrays (fpgas) and how deep learning methods can benet from the capabilities of fpgas. the last subsection highlights the challenges of implementing deep learning networks on fpgas. a. convolutional neural networks (cnns) in this subsection, we describe the key operations and terminology involved in the construction of cnns including convolution, activation functions, normalization, pooling, and characteristics of fully connected layers. ) convolution (conv) a convolution operation can be thought of as the production of a matrix smaller in size than the original image matrix, representing pixels, by sliding a small window (called lter, feature identier, or kernel) of size k k over the image (called input feature map (fm)), to produce an output feature neuron value . the lter is an array of numbers called weights or parameters. these weights are computed during the training phase. as the lter slides over the feature map, it multiplies the values in the lter with the original pixel values, that is, it rst performs element wise multiplication, and then sums the products, to produce a volume , shawahna et al.: fpga based accelerators of deep learning networks for learning and classication: a review single number. the inputs and outputs of the conv layer are a series of fm arrays. this operation, starting from the top left corner of the fm, is repeated by moving the window s strides at a time, rst in the right direction, until the end of the fm is reached, and then proceeding downwards until the fm is completely scanned and all the elements of the fm are covered. the sliding of the lter window and performing the operation is known by the verb convolving, hence the noun convolution , . normally, the size of the kernel is very small, less than or equals to each output- input fm pair has a set of weights equal to the kernel size and each output fm is computed based on the sum of the convolution operations performed on all input fms. note that different conv layers in the same cnn model vary considerably in their sizes. in summary, the convolution operation comprises four levels of loops; the output fms loop (loop-), the loop across the input fms (loop-), the loop along the di- mensions of a single input fm (scan operation, loop-), and the kernel window size loop (multiply and accumulate (mac) operation, loop-). conv layers are dominant in cnn algorithms since they often constitute more than % of the total cnn operations , , , , , . therefore, many attempts have been made to speedup conv operations using loop unrolling technique , , as will be discussed later. loop unrolling maximizes the parallelism of conv macs computation which requires a special consideration of processing elements (pes) and reg- ister arrays architecture. fig. illustrates the loop unrolling of conv loops levels. ) activation functions (afs) activation function in neural networks is similar to action potential in animal cells such as neurons. a neuron is said to re if it emits an action potential. a popularly used activation function is the sigmoid function which can be expressed as fpxq {p ` exq () where x represents the weighted sum of the neuron inputs and if it is a sufciently large positive number, the sigmoid function approximates to unity. for sufciently large nega- tive values of x, the sigmoid function is close to another popular activation function is fpxq tanhpxq () the above standard sigmoid and tanh non linear functions require long training time . a recently proposed and commonly used af in cnns is rectied linear unit (relu) which is dened as fpxq maxpx, q () relu activation function is known to converge faster in training, and has lesser computational complexity , figure conv loops unrolling : (a) unrolling loop-; (b) unrolling loop-; (c) unrolling loop-; (d) unrolling loop-, where, p kx, p ky, p ix, p iy, p if, and p of are loop unrolling design variables for the kernel window width, kernel window height, input fm width, input fm height, number of input fms, and the number of output fms, respectively. than standard sigmoid and tanh functions. in addition, it does not require input normalization to prevent it from saturating , , . ) normalization in real life, a phenomenon called lateral inhibition appears, which refers to the capacity of an excited neuron to subdue its neighbors, thereby creating a contrast in that area. in cnns, to accomplish this, local response normalization (lrn) or simply normalization is used, particularly when dealing with relu neurons, because they have unbounded activation that needs normalization. it detects high frequency features with a large response. if we normalize around the local neighborhood of the excited neuron, it becomes even more sensitive as compared to its neighbors. at the same time, it will dampen the responses that are uniformly large in any given local neighborhood. if all the values are large, then normalizing those values will diminish all of them. so, basically it performs some kind of inhibition and boosts the neurons with relatively larger activations. normalization can be done within the same feature or across neighboring features by a factor that depends on the neighboring neurons. expressions to compute the response normalized activity can be found in , . ) pooling pooling, also known as subsampling, is employed to pro- gressively reduce the spatial size of the representation, thereby reducing the amount of parameters and computation in the network. pooling layers are periodically inserted in between successive convolutional layers. they operate independently on every depth slice of the input and resize it spatially using the max operation. the most common form is a pooling layer with lters of size applied where the max operation would be taking a maximum over samples thereby discarding percent of the activations . in volume , shawahna et al.: fpga based accelerators of deep learning networks for learning and classication: a review figure alexnet cnn architecture . addition to the popular max pooling, the pooling units in some cnns are also used to perform other functions, such as avg and min operations . ) fully connected layer (fc) a common form of a convolutional neural network archi- tecture comprises stacks of a few convolutional and relu layers, followed by layers for pooling, and this pattern is repeated until the image has merged spatially to a small size. this is followed by one or more fully connected layers, also known as inner product layers, whose neurons have full connections to all activations in the previous layer, hence the name. the last fully connected layer is the classication layer and it holds the output such as the class scores . b. examples of deep learning networks we list in this subsection some of the well known deep learning networks. alexnet () is a convolutional neural network consisting of convolutional layers, interspersed by normalization layers, as well as fully connected layers . each convolutional layer performs the activation function using relu. in addition, pooling layers are employed with the rst, second, and last convolutional layers. the architecture of alexnet cnn is shown in fig. alexnet won the imagenet challenge by classifying input color images to , different output classes. vgg () is a convolutional neural network model similar to alexnet in terms of the number of fully connected layers. however, it consists of groups of convolutional layers , . the exact number of conv layers in each group depends on the version of the vgg, visual geometry group, model. table shows the number of conv and fc layers for the most commonly used vgg models. resnets () are deep residual networks with ex- tremely irregular and complex structures compared to alexnet and vgg cnn models , , . this is due to having more types of layers, where non adjacent layers incorporate shortcuts to compute the residual functions, as well as having highly deep structures, that is, between and conv layers. unlike alexnet and vgg models where the layers are connected in sequence, the interconnections in resnet layers are in the form of a directed acyclic graph (dag). resnet- and resnet- are widely used, especially for image classication. resnet-/ structure contains / conv (most of them are followed by batch normalization (batchnorm), scale, and relu layers), / max pooling, / average pooling, / fc, and, / element wise (eltwise) layers, respectively. c. field programmable gate arrays (fpgas) fpgas are off the shelf programmable devices that provide a exible platform for implementing custom hardware func- tionality at a low development cost. they consist mainly of a set of programmable logic cells, called congurable logic blocks (clbs), a programmable interconnection network, and a set of programmable input and output cells around the device . in addition, they have a rich set of embedded components such as digital signal processing (dsp) blocks which are used to perform arithmetic intensive operations such as multiply and accumulate, block rams (brams), look up tables (luts), ip ops (ffs), clock management unit, high speed i/o links, and others. fig. shows a basic structure of an fpga. fpgas are widely considered as accelerators for computationally intensive applications as they enable mod- els with highly exible ne grained parallelism and as- table cnn layers for vgg models. layers vgg- vgg- vgg- conv (group ) conv (group ) conv (group ) conv (group ) conv (group ) conv (total) fc total volume , shawahna et al.: fpga based accelerators of deep learning networks for learning and classication: a review figure fpga basic structure . sociative operations such as broadcast and collective re- sponse . in , , fpga computing models used for applications acceleration are presented, including data streaming, associative computing, highly parallel memory access, use of standard hardware structures such as rst in rst out (fifo) buffers, stacks and priority queues, and functional parallelism. fpgas have the advantage of maximizing performance per watt of power consumption, reducing costs for large scale operations . this makes them an excellent choice as accelerators for battery operated devices and in cloud services on large servers. fpgas have recently been widely used for deep learning acceleration given the exibility in implementing architectures with large degree of parallelism resulting in high execution speeds . the adoption of software level programming models such as the open computing language (opencl) standard , in fpga tools made them more attractive to use for deep learning , . in addition, the feed forward nature of deep learning algorithms makes fpgas offer a clear advantage as they can create customized hardware circuits that are deeply pipelined and inherently multithreaded . fpgas also have the capability of partial dynamic cong- uration, which allows part of the fpga to be recongured while the rest is being used. this could be of potential benet to deep learning methods where the next layer could be recongured while the current layer is being used. d. challenges of fpga based implementation of deep learning networks implementation of deep learning networks and, in particular, cnns on fpgas has a number of challenges including the requirement of a signicant amount of storage, external memory bandwidth, and computational resources on the order of billions of operations per second . for example, alexnet cnn has over million model parameters which need mb of memory for storing the weights based on -bit oating point representation as well as requires around billion operations for each input image . this large amount of storage required is not supported by existing commercial fpgas and hence the weights have to be stored on external memory and transferred to the fpga during computation. without careful implementation of deep learning networks and maximizing resource sharing, the implementation may not t on fpgas due to limited logic resources. the problem exacerbates with more complex models such as vgg cnn model which have layers. for example, the vgg- cnn model has million weights and needs over gops . although the current trends in implementing cnns is going toward compressing the entire cnn model with dramatically reducing data bit width , it is expected that future cnn models will get more complex with larger number of layers as the amount of training data continues to grow and the problems to be solved get more complex. in addition, different layers in cnns have different characteristics which result in different parallelism and memory access requirements. different layers in a cnn network exhibit vastly different amounts of intra output and inter output parallelism . intra output parallelism parallelizes the computation of a single output image since it is the sum of n input kernel convolutions. however, inter output parallelism is based on computing multiple output fms in parallel. furthermore, convolutional layers are computational centric while fully connected layers are memory centric . for example, the number of operations in each group of convolutional layers in vgg- model are on the order of to gops while the number of weights are on the order of to million. however, the number of operations in fully connected layers are in the order of to gops, while the number of weights are on the order of to million. thus, the developed cnn accelerator must be designed carefully to meet the varying requirements of different layers and needs to be exible to maximize the performance for each cnn layer. as technology advances, fpgas continue to grow in size and capabilities. it is crucial to have some mechanisms for addressing the requirements for efcient implementations of deep learning networks. addressing hardware resource limitations requires reuse of computational resources, and storing of partial results in internal memories. data transfer and computational resource usage are signicantly impacted by the ordering of operations and selection of parallelism in the implementation of cnns on fpgas. careful scheduling of operations can result in signicant reduction in external memory access and internal buffer sizes. external memory bandwidth requirements can be also decreased by using reduced precision for representing the weights with minimal impact on solution quality, which also results in a better energy efciency. in addition, the number of external mem- ory accesses can be reduced by utilizing on chip memory and exploiting data reuse. furthermore, the large number of weights in the fully connected layer can be reduced, based on utilizing singular value decomposition (svd) with a small impact on accuracy. in the next section, we will volume , shawahna et al.: fpga based accelerators of deep learning networks for learning and classication: a review review various design approaches used to cope with those challenges for implementing deep learning networks. iii. acceleration of deep learning networks: current status in this section, we will start by covering convolutional neural networks (cnns) compression techniques as they have a signicant impact on the implementation complexity of cnns. cnns compression techniques target the min- imization of the number of operations and the memory footprint with minimal impact on accuracy. then, we discuss hardware acceleration techniques for deep learning (dl) algorithms and cnns based on both application specic integrated circuit (asic) and eld programmable gate array (fpga) implementations. in general, hardware accelerators focus on designing specic modules and architectures that ensure data reuse, enhance data locality, and accelerate convolutional (conv) layer operations based on performing needed operations in parallel. a. cnns compression in this subsection, we review techniques that target the com- pression of cnns which results in signicantly reducing their implementation complexity with minimal impact on accuracy. denton et al. proposed a technique to reduce the memory footprint for the network weights in object recog- nition systems. they used singular value decomposition (svd) and lter clustering methods for this purpose. the results for convolutional model of layers in show that the proposed technique speeds up the operations in convolutional layers by a factor of , compared to cpu eigen based library implementation . in addition, it successfully achieved memory footprint reduction for the fully connected layers while preserving the recognition accuracy within %. in another work, han et al. employed network pruning techniques to reduce the over tting and complexity of neural network models. their results demonstrated that pruning redundant connections as well as less inuential connections achieved and compres- sion for alexnet and vgg- models, respectively, while achieving zero accuracy loss for both. in a subsequent work, han et al. proposed a deep compression technique for more reduction of the storage requirements of cnns through the enforcement of weights sharing. deep compression basically consists of pruning, trained weights quantization, and huffman coding pipeline stages. the experimental results show that the proposed compression technique successfully reduced the storage requirement of alexnet and vgg- cnn models by and , respectively, without affecting their accuracy. this also improved the power efciency (a.k.a., performance per watt) by to b. asic based accelerators in this subsection, we present some recent work in the area of hardware based accelerators (asics). an asic based hardware accelerator referred to as dian- nao was designed for large scale convolutional neural networks and deep neural networks. diannao accelerates neural networks by minimizing memory transfers, which opened a new paradigm for hardware accelerators. since the weights are repeatedly used in the computations of con- volution layers, frequent memory access can signicantly degrade the overall performance. therefore, the authors exploited the locality properties of neural network layers to design custom storage structures that take advantages of these properties. in addition, they employed dedicated buffers and tiling techniques to reduce the overall external memory trafc through increasing data locality. chen et al. also observed that using short xed- point representation of feature maps (fms) and weights can also signicantly reduce computation resources and memory footprint. they found that the area and power of a - bit multiplier can be reduced by a factor of and , respectively, using -bit multipliers. consequently, diannao has been implemented using nm fabrication technology with -bit xed point arithmetic units, bits of which are used for the integer part and the remaining for the fractional part. the experimental results demonstrated that diannao has an average performance of gops with power consumption of mw. the results depicted that using -bit arithmetic units instead of -bit ones in- troduced only % accuracy loss on mnist dataset . on the other hand, the scalability and efciency of diannao accelerator are severely limited by the bandwidth constraints of the memory system. in a related research work, chen et al. , pro- posed dadiannao multi chip supercomputer which offers sufcient memory capacity suitable for on chip storage of all weights in cnns. this system is mainly important for todays large scale deployments of sophisticated industry and consumers services. dadiannao uses -bit xed point numbers in the inference process like diannao, but it is implemented using nm technology. the results show that dadiannao outperforms the performance of a single gpu architecture by up to and reduces the average energy consumption by with only % accuracy error rate on mnist dataset for a -chip system. another member of the diannao family, called pudian- nao , has been designed using tsmc nm process to support multiple techniques and scenarios of machine learning (ml). pudiannao accelerates different ml tech- niques through extracting their critical locality properties and computational primitives with the use of on chip storage as well as novel functional units. experimental results show that pudiannao is and faster and energy efcient, respectively, than nvidia km gpu architecture. however, both of dadiannao and pu- volume , shawahna et al.: fpga based accelerators of deep learning networks for learning and classication: a review figure processing element (pe) architecture in; (a) flexflow, (b) d mapping . diannao architectures have not been optimized to be used for embedded applications. to improve the scalability and energy efciency of di- annao design discussed in , shidiannao accelerator was proposed . shidiannao is designed especially for real time object recognition applications such as self- driving cars, smartphones, and security using nm cmos technology. the proposed accelerator directly connects with a cmos/ccd sensor in the image processing chip. in addition, all the weights of cnn layers are stored in sram on chip memory, as the target here is small cnn models. shidiannao is embedded inside the processing chip to elim- inate off chip dram memory accesses and minimize data movements between the sram holding the cnn model and the individual processing elements from the sensor. shidiannao has a power consumption of mw with a peak performance of gops under ghz working frequency. moreover, shidiannao has speedup and is more energy efcient than diannao . however, diannao , dadiannao , , pu- diannao , and shidiannao are not implemented using fpga or any other recongurable hardware. there- fore, they cannot be efciently adapted to different appli- cation demands (i.e., different neural network sizes). in addition, asic designs have a long development cycle and lack exibility for handling varying dl network designs. finally, cnn accelerators, which store all weights on chip such as shidiannao , will not be able to support realistic large scale cnn models. similar approaches to the diannao family of techniques are presented in with similar limitations. isaac and prime have explored in memory processing to design an acceleration architecture for neural networks. the proposed isaac architecture has achieved better improve- ments of , , and in throughput, energy, and computational density, respectively, than the state of the art dadiannao architecture. in cnn models, ne grained parallelism appears at fea- ture map level, in the neuron level, and in the synapse level. lu et al. reviewed current accelerators that exploit the intrinsic parallelism and observed a mismatch between the parallel types supported by the computing engine and the dominant parallel types that appear in cnn workloads. they identied that most of the previous techniques pro- posed solutions that fall into one of the three representative architectures: (i) systolic, (ii) d mapping, and (iii) tiling. due to limitations of dataow of each of the above three architectures, most existing accelerators support only one specic parallelism. systolic architectures exploit synapse parallelism (sp), d mapping architectures exploit neuron parallelism (np), and tiling architectures exploit feature map parallelism (fp). however, in a practical cnn, the dominant parallel type depends on the number of input fms, the number of output fms, the size of the output fms, and the size of the kernel. with three components (feature map, neuron, synapse) that can be either left serial, or parallelized, we get possible combinations. an example of processing style could be sfsnms, meaning, single feature map, single neuron, and multiple synapse. to address the above problem, and support all possi- ble processing styles, lu et al. proposed a exible dataow architecture, called flexflow, with minimal con- trols. flexflow supports all types of data paths in each type of parallelism in different layers efciently. as a rst step, a modication to the processing element (pe) micro architecture, and the interconnections between pes, is proposed. pes are arranged in rows where each row can complete one convolution and serve one output neuron. the adders in each pe row are connected to form the adder tree. fig. illustrates the proposed pe in flexflow and that in d mapping architecture. by eliminating de- pendency between adjacent pes, the proposed convolutional unit supports the comprehensive mfmnms parallelisms. to cater to different types of parallelisms, they also proposed a hierarchical dataow with high data routability and low control overhead. the entire dataow can be divided into three sub ows: (i) distribution to local storage in each volume , shawahna et al.: fpga based accelerators of deep learning networks for learning and classication: a review pe, (ii) fetching of data from local storage for operators (multiplier and adder), and, (iii) dataow from neuron and kernel buffers to the distribution layer. they also presented a method to determine parallelization type and degree (i.e., the unrolling parameters) for each conv layer. flexflow architecture was evaluated for computing re- source utilization, performance, power, energy, and area. comparison was made with three typical architectures (i.e., systolic, d mapping, and tiling) using six practical work- loads, including alexnet and vgg. they also examined the scalability of flexflow in terms of resource utilization, power, and area with growing scales of computing engine. from experimental results, it was found that computing resource utilization of each baseline was over % across all workloads in contrast to other baselines that utilized less than % (most of them less than %). in terms of performance, flexflow demonstrated over gops performance with ghz working frequency. it also out- performed others in terms of data reusability and power efciency. c. fpga based accelerators in this subsection, we will review recent techniques employ- ing fpgas for the acceleration of deep learning networks. for each reviewed technique, we will highlight the key features utilized to maximize performance and throughput in the acceleration process. fpga implementations of cnns appeared in the mid- s when cloutier et al. designed the virtual image processor (vip) on altera epf fpga. vip is a single instruction stream multiple data streams (simd) multiprocessor architecture with a d torus connection topology of processing elements (pes). vip improves the performance through the use of low accuracy arithmetic to avoid implementing full edged multipliers. fortunately, recent digital signal processing (dsp)-oriented fpgas in- clude large numbers of multiply and accumulate (mac) units which allow for extremely fast and low power cnn implementations. thereafter, fpga implementations of deep learning net- works have mainly focused on accelerating the computa- tional engine through optimizing conv layer operations. several studies in the literature have reported fpga based implementations of convolution operation. farabet et al. presented an fpga implementation of cnn that uses one dedicated hardware convolver and a soft- processor for data processing and controlling, respectively. the proposed implementation is referred to as convolutional network processor (cnp). cnp exploits the parallelism of conv layers to accelerate the computational engine of cnns while fully utilizing the large number of dsps, the mac hardware units on fpga. the proposed architecture consists of virtex sx fpga platform and external mem- ory. the authors designed a dedicated hardware interface with the external memory to allow simultaneous read/write accesses transparently. in addition, they used rst in rst figure d convolution module of kernel . out (fifo) buffers between the fpga and the external memory chip in both directions to guarantee the steadiness of dataow. the vector arithmetic and logic unit in cnp implements d conv, pooling, and non linear activation function op- erations of convolutional networks. the implementation of d conv with kernel of size (i.e., k ) is shown in fig. , where x is the data from input feature map (fm), y is the partial result to be combined with the current result, z is the result to the output fm, wij is the weight value in the convolution kernel, and w is the width of the input image. it can be seen that the proposed convolutional module accomplishes k mac operations simultaneously in each clock cycle. cnp represents fms and weights using -bit (q) xed point format. the proposed accelerator has been implemented for a face detection system with lenet- architecture . it utilized % and % of the general logic and multipliers, respectively. in addition, cnp consumed less than watts of power. sankaradas et al. proposed a massively parallel coprocessor to accelerate cnns using virtex lxt fpga platform. the proposed accelerator mainly focused on optimizing computation engine by employing the paral- lelism within convolution kernel and fms. the coprocessor can be considered as parallel clusters of vector processing elements (vpes) where each cluster is designed using d convolvers, adders, sub samplers, and look up tables. each vpe consists of multiplier accumulator and programmable register units to hold kernel weights and fm data. to hold the massive intermediate data of cnns, the authors employed a dedicated off chip memory ( ddr memory banks) with a large bandwidth on the coprocessor card. moreover, the proposed accelerator uses a low precision data representation feature with memory packing to further improve the memory bandwidth as well as the throughput. -bit and -bit xed point representations were utilized for kernel weights and fms, respectively. volume , shawahna et al.: fpga based accelerators of deep learning networks for learning and classication: a review figure maple processing core architecture . the authors examined their architecture on cnn with conv layers and without any fully connected (fc) layer for a face recognition application. the results show that the proposed coprocessor is faster than a software implementation on a ghz amd opteron processor with less than watts of power dissipation. however, the proposed accelerator cannot be used to accelerate full cnns as it uses few conv layers without any fc layer. a full cnn model consists of both conv layers and fc layers. thus, an efcient cnn accelerator for real life applications is needed to consider both. similar approaches to the work of sankardas et al. are presented in , to accelerate support vector machines (svm). maple is a programmable fpga prototype sys- tem presented to accelerate both learning and classication tasks in applications with unstructured large amount of data. the authors analyzed ve workload domains to help in designing maple. these workloads are svm , supervised semantic indexing (ssi) , k means , generalized learning vector quantization (glvq) , and cnns . they found that their computations can be structured as parallel streams of vector or matrix operations. thus, they architected maple as a d grid of vector pro- cessing elements as shown in fig. to efciently perform matrix multiplication, they allocate a private local storage to each pe which is used to store a column, or part of it, from the multiplier matrix. in this way, matrix multiplication is accomplished by streaming the multiplicand matrix rows through the pes where each pe performs a mac operation. the pes are organized in clusters, where each group is served by a separate memory bank of the banked off chip memories, which create independent streams for processor- memory computation. moreover, maple uses on chip smart memory blocks to process the large intermediate data on the y using in- memory processing. fig. shows the architecture of the smart memory block. to illustrate the idea of on the y in memory processing, lets consider nding the maximum k elements. the lter compares the input data with the figure maple smart memory block . threshold value (val). if the input value is greater than val, it updates the list by replacing val at address addr with the input value. then, the scanner (scan) searches for the new minimum value in the list and updates the threshold val and addr accordingly. it should be mentioned here that the employment of in memory processing reduced the off chip memory trafc by , , and for ssi, k means, and cnn workloads, respectively. maple pro- totype has been implemented on virtex sxt platform running at mhz. the experimental results for face and digit recognition cnns show that maple is % faster than that for ghz nvidia tesla c gpu implementation. chakradhar et al. proposed a dynamically cong- urable cnn architecture on fpga. the proposed system consists of three main components; a coprocessor, a dy- namically congurable cnn (dc cnn) processing core, and -bank memory subsystem. the coprocessor is designed such that it automatically congures the software and the hardware elements to fully exploit the parallelism at the workload level. dc cnn is responsible for executing cnn applications and its architecture is shown in fig. it consists of m computational elements (each with n d figure the architecture of dc cnn . volume , shawahna et al.: fpga based accelerators of deep learning networks for learning and classication: a review convolvers as well as sub sampling (s) and non linearity (nl) pipelined units), m adders (each with n inputs), and input/output switches. the internal structure of the switches vector encloses m n selectors which are used to help in exploring the entire design space and to provide the congurability function across different layers of cnn model. to determine the best (m, n) feasible combination for each layer, the system analyzes the workload using integer factorization techniques because it is considered fast for small numbers , . dynamic programming is also used to quickly prune infeasible combinations. the authors compared the proposed dc cnn architec- ture, considering d convolvers as well as a mem- ory subsystem of -bit port width, with a ghz nvidias gpu implementation. the results show that dc- cnn achieved , , , , and speedup for face recognition , face detection , mobile robot vision , video surveillance , and automotive safety workloads, respectively. it is worth mentioning that dc cnn is the rst architecture that achieves a perfor- mance suitable for real time processing for video streaming as it processes up to frames per second. in addition, dc- cnn is more energy efcient than the gpu implementation as it consumes watts, while more than watts are consumed by the gpu. on the other hand, the authors modeled their architecture on a cnn with conv layers only without any fc layer which makes it unsuitable for todays other real life applications. a second generation of cnp architecture has been proposed in by designing a stream processor sys- tem. the proposed design replaces the dedicated hardware convolver in cnp with multiple parallel vector processing units, named as alus, laid out in a d grid. each alu is composed of four local routers, one global router, and a streaming operator. the local routers are used to stream data to/from the neighbors. streaming data to and from global data line is done through the global router. the streaming operators in the alu are fully pipelined to produce a result per clock cycle as described in with the use of q coding to represent fms and weights. the proposed system also uses a multi port direct memory access (dma) streaming engine to allow individual streams of data to operate seamlessly within processing blocks. the results show that the proposed stream processor system can run small cnns at up to fps while consuming about watts. an improved version of cnp architectures given in , was presented in and referred to as neuflow. particularly, neuflow has replaced the d grid of alus with a d grid of processing tiles (pts). the proposed architecture contains a d grid of pts, a control unit, and a smart dma module, as shown in fig. each pt consists of local operators and a routing multiplexer (mux). the top three pts have been implemented to perform mac operation. thus, they can be used to perform d convolution, simple dot products, and spatial pooling. general purpose operations, such as dividing and squaring, figure the architecture of neuflow . have been implemented at the middle three pts. therefore, the middle row of neuflow can be used for normalization. finally, neuflows bottom pts row implements non linear operations. moreover, each operator employed input and output fifos to stall its pipeline when required. on the other hand, pts mux is used to connect its local operators with the neighboring pts streaming operators and off chip memory instead of the used local routers and global router discussed in . neuflow uses a dataow compiler, named luaflow, to translate a high level ow graph representation of cnns in torch into hdl scripts with different levels of parallelism. in addition, luaflow produces a binary code conguration le and holds it in the embedded control unit. thereafter, the control unit congures the d grid of pts (connections and streaming operator) and the dma ports through run time conguration buses. a smart memory module has been designed to support multiple asynchronous accesses of off chip memory through its recongurable ports. by targeting the larger xilinx virtex vlxt fpga, neuflow achieved gops at watts for street scene parsing cnn in with the use of bits to represent fms and weights. peemen et al. utilized the exible off chip memory hierarchy method to design a congurable memory centric accelerator template for a variety of models of cnns. this accelerator exploits data reuse in complex access patterns to reduce off chip memory communication, which minimizes the bandwidth requirements. the memory centric accelera- tor maximizes the efciency of on chip memories for better data locality using loop transformation (to optimize the tiling parameters) and block ram (bram)-based multi- bank on chip buffers . at the same time, it minimizes the size of fpga on chip memories to optimize energy and area usage, which are key requirements for embedded platforms. the memory centric accelerator uses a simd cluster of mac pes with exible reuse buffers to accelerate the conv layer. the acceleration template has been imple- mented on virtex fpgas. in addition, a microblaze pro- volume , shawahna et al.: fpga based accelerators of deep learning networks for learning and classication: a review cessor has been utilized to congure and communicate with the accelerator via fifo based fast simplex link (fsl). the proposed accelerator has been analyzed for a cnn vision task of size gmac and the results show that the memory centric accelerator is faster than the standard implementation of similar fpga resources. neural network next (nn x) is a real time system- on chip (soc) computing system for deep learning networks on mobile devices. the architecture of nn x consists of a host processor, a co processor, and external memory. the co processor accelerates the learning networks by paral- lelizing their operations throughout arrays of congurable processing elements referred to as collections. each collec- tion contains one convolution engine, one pooling module, and one non linear operator. the conv engine accelerates the conv operation by fully pipelining the incoming data with the use of cache memories. the collections are able to communicate with one another using the collection route component to achieve cascaded pipelining, which results in reducing accesses to external memory. the data transfer between the collections and the external memory is ac- complished throughout the co processor full duplex memory router, which provides independent data streams. the nn- x has been prototyped on xilinx zc which contains zynq xcz, two arm cortex a processors, and gb ddr eight collections have been employed to achieve large parallelism. the results for face recognition model in show that nn x is faster than the two embedded arm processors. zhang et al. proposed a rooine based model to accelerate convolutional neural networks on fpgas. the rooine model is an intuitive visual performance model used to relate the attainable performance to the peak performance that can be provided by the hardware platform and the off chip memory trafc . the focus in their work is primarily on accelerating the convolutional layers as it consumes more than % of the computational time during the prediction process . in doing so, the authors optimized both the computation operations and the memory access operations in convolutional layers. they considered a cnn application composed of ve convolutional layers that won the imagenet competition in . the proposed accelerator uses polyhedral based data dependence analy- sis to fully utilize all fpga computational resources through loop unrolling, loop pipelining, and loop tile size enumeration. note that loop unrolling maximizes the par- allel computation of conv mac operations. on the other hand, local memory promotion and loop transformation are used to reduce redundant communication operations and to maximize the data sharing/reuse, respectively. subsequently, the rooine performance model is used to identify the optimal design from all possible solutions in the design space. specically, the authors model all possible legal designs delivered from the polyhedral analysis in rooine to nd the optimal unrolling factor xtm, tny for every convolutional layer, where tm and tn are the tile size figure zhang et al. accelerator architecture. for the output fms and input fms, respectively. however, designing a cnn accelerator with different unrolling factors to each convolutional layer is challenging. therefore, the proposed architecture enumerates all possible valid designs to nd uniform cross layer unrolling factors. thereafter, the hardware accelerator is implemented based on the cross- layer optimal unrolling factors. the proposed accelerator composed of a computational engine and memory sub system is depicted in fig. the computation engine is designed as tm duplicated tree- shaped poly structures with tn inputs from the input fms, tn inputs from the weights, and one input from the bias. on the other hand, the memory sub system is implemented as four sets of on chip buffers; two sets to store the input fms and weights, each with tn buffer banks, and two buffer sets of tm independent banks for storing the output fms. to overlap data transfer with computation, on chip buffers are operated in a ping pong manner. in addition, two independent channels are implemented for load and off- load operations to increase the bandwidth utilization. more- over, microblaze processor is used to send conguration parameters and commands for the accelerator over axilite bus. the cnn accelerator communicates with external data transfer engines through fifo interfaces, where the data transfer engines are used to access ddr dram memory through axi bus. the accelerator is designed using vivado high level synthesis tool and implemented on xilinx vc fpga board clocked at mhz. the experimental results depict that the proposed implementation achieves a peak performance of gflops as well as a speedup over the software implementation on intel xeon cpu e- at ghz with mb cache and threads. in addition to this, the results show that the proposed fpga architecture is more energy efcient than the software implementation as the total power consumption is only watts. the proposed implementation has some limitations such as designing the accelerator with new cross layer unrolling factors for different architectures of cnns. fur- volume , shawahna et al.: fpga based accelerators of deep learning networks for learning and classication: a review figure top level archeticture of microsoft cnn accelerator . thermore, using the cnn accelerator with uniform unrolling factors might be sub optimal for some conv layers, which affects the overall performance. in , microsoft research team of catapult project integrated fpga boards into data center applications to successfully achieve speedup for bing ranking (the large scale search engine) . a year later, ovtcharov et al. at microsoft research utilized catapult hardware infrastructure, a dual socket xeon server equipped with stratix v gsmd fpga, to design a specialized hardware for accelerating the forward propagation of deep cnns in a power constrained data center. the top level architecture of the proposed cnn accelera- tor is shown in fig. multi banked input buffer and kernel weight buffer are used to provide an efcient buffering scheme of fms and weights, respectively. to minimize the off chip memory trafc, a specialized network on chip has been designed to re distribute the output fms on the multi banked input buffer instead of transferring them to the external memory. the d convolution operations (such as the dot product) and other cnn operations are indepen- dently performed using spatially distributed scalable vectors of pes. the controller engine is responsible for streaming and data delivery of multi banked input buffer and kernel weight buffer data to each of the pe vectors. in addition, it supports conguring multiple cnn layers at run time. the results show that the proposed design is able to classify images/sec, while consuming about watts, for alexnet model on imagenet k dataset , which is better than the published throughput results for the rooine based fpga accelerator . the authors mentioned that using top of the line fpgas such as arria gx improves the throughput to around images/sec. qiu et al. proposed an fpga design to accelerate cnns for a large scale image classication challenge on embedded systems. the focus was on accelerating both conv and fc layers, since they are considered as the most computational centric and the most memory centric operations in cnns, respectively. the proposed accelerator reduces the resource consumption using specic design of convolver hardware module. in addition, the authors applied singular value decomposition (svd) to the weight matrix of fc layer in order to reduce memory footprint at this layer . to further reduce memory footprint and bandwidth requirement of cnn, they proposed a dynamic- precision data quantization ow component. this compo- nent is responsible for nding the optimal fractional length for weights in each layer as well as the optimal fractional length for fms in adjacent layers, while achieving minimal accuracy loss. then, it converts the oating point numbers representing weights and fms into xed point numbers. in addition, the authors proposed a data arrangement scheme that maximizes the burst length of each transaction to the external memory to accelerate conv and fc layers, as well as to avoid unnecessary access latency. note that maximizing the dram burst length raises up the effective dram bandwidth , . the proposed architecture consists of a processing system (cpu) and programmable logic (fpga). cnn computations are performed through special design of processing element modules in fpga. the main modules in the processing element are convolver complex, max pooling, non linearity, data shift, bias shift, and adder tree, as shown in fig. the convolver complex is designed as a classical line buffer , as shown in fig. , to achieve convolution operations as well as to compute fc layer multiplication of matrix vector. the pooling layer implemented in the max- pooling module is used to output the maximum value in the input data stream with a window of size the activation function of cnn is applied to the input data stream using the non linearity module. the adder tree accumulates the partial sums generated from the convolvers. finally, data shift and bias shift modules are responsible for accomplishing dynamic quantization. the proposed embedded fpga platform has been im- plemented using vgg--svd network with -bit xed- figure processing element module of qiu et al. embedded accelerator architecture. volume , shawahna et al.: fpga based accelerators of deep learning networks for learning and classication: a review figure convolver complex design of qiu et al. embedded accelerator architecture. point numbers on zynq xcz platform. the results demonstrate that applying svd technique reduces memory footprint of fc layer by % with a compression rate of % while introducing an accuracy loss of only %. finally, the overall performance of the proposed acceler- ator reported is gops under mhz working frequency with the top- accuracy of % and a total power consumption of watts. deepburning is an fpga based neural network (nn) design automation tool. it allows for building learning accelerators for specic nn with optimized performance and custom design parameters conguration using a pre- constructed register transfer level (rtl) module library. the rtl library holds the hardware descriptive scripts for nn recongurable components as well as their conguration scripts. in addition, it contains other rtl building blocks for logical and arithmetic operations such as the connection box (used to exchange data between nn layers as well as to approximate the division operation) and approximate look- up table (lut) (used to simplify a function or operation to allow it to be mapped into hardware). in order to design an optimized hardware, deepburning compresses the passed nn model to the greatest extent using temporal and spatial folding which helps also in satisfying the resource constraints and minimizing the re- quired hardware modules. deepburning not only generates the hardware description for neural network scripts, but also analyzes the complex access pattern and data local- ity using an integrated compiler to generate a run time control ow which provides energy efcient, and, better data reuse implementation. in addition, the deepburning compiler investigates the accelerator on chip memory size and throughput to properly tile and partition the nn weights and feature data layouts. moreover, deepburning uses the address ow component to automatically fetch and store off chip memory and on chip memory data. the authors compared the performance of deepburning with that in , considering alexnet cnn model, as they both operate at mhz. they considered a high budget resources constrained deepburning on zynq- device. the results show that deepburning is slower but more energy efcient. an opencl based optimization framework to accelerate large scale convolutional neural network models was pro- posed by suda et al. . they found that the number of performed conv mac operations in parallel (nconv ), simd vectorization factor (sconv ), normalization layer loop unrolling factor (nnorm), the number of parallel pooling outputs in one cycle (np ool), and the number of parallel fc mac operations (nf c) are the key variables that determine the parallelism of the design. subsequently, they analytically and empirically modeled the execution time for each layer as a function of the above mentioned variables. then, genetic algorithm was used to explore the design space for nding the optimal combination of the key design variables considering the resources constraints. the authors implemented the scalable conv block in a similar fashion to that in as a matrix multipli- cation by attening and on the y rearrangement of the feature data. the opencl software has been utilized in their work due to its parallel programming model as well as its ability to integrate the compiled rtl design with external memory interfacing ips , which uses memory coalescing technique with complex load and store units. in addition, it has optimized matrix multiplication and cpu- fpga communication libraries , . the framework is used on both vgg- and alexnet cnn models which are implemented on p d and de net fpga boards with xed point opera- tions according to their precision study. they compared the proposed implementation with ghz core i- cpu implementation that uses caffe tool with atlas optimized library for matrix/vector operations. the results show that the opencl optimized framework on p- d achieved ( gops) and ( gops) speedups for vgg- and alexnet models, respectively. on the other hand, de net fpga achieved less throughput speedup than the p d ( ( gops) for vgg-, and ( gops) for alexnet) as it has less dsps than what is available on p d zhang et al. , analyzed the transformation of conv and fc layers to regular matrix multiplication presented in prior work . for vgg- model, they found that such transformation necessitates up to duplication of input fms. to address this problem and improve the bandwidth utilization, they designed a uniformed matrix multiplication kernel that uses either input major mapping (imm) or weight major mapping (wmm) techniques while computing fc layer. in imm, the designed kernel batches a group of different input fms together, and then performs the matrix multiplication. imm technique improves the data reuse of fc weights. on the other hand, the designed kernel with wmm technique makes use of the fact that the fc layer is communication bound in which the weight matrix volume , shawahna et al.: fpga based accelerators of deep learning networks for learning and classication: a review is much larger than the input fm matrix. in particular, it loads input fm matrix to a weight buffer and loads weight matrix to input fm buffer. subsequently, a regular matrix multiplication is performed on these matrices. as a result, wmm may allow for a higher data reuse than imm, especially for input fms that can be reused multiple times considering the limited hardware resources. for the above, the rooine model was applied to identify the optimal mapping technique under different batch sizes and data precisions. the results demonstrate that wmm is better than imm in term of data reuse and bandwidth uti- lization, especially in small batch sizes which is required for real time inference. hence, the same matrix multiplication kernel is utilized for the computation of both conv and fc layers, but with the use of imm in conv layer and wmm in fc layer. based on this, the authors proposed a software/hardware co design library, which they named caffeine, to accelerate cnns on fpgas. with an easy to use developed tool, caffeine aids in automatically choosing the best hardware parameters, using the model les from caffe and fpga device specications obtained from the user. caffeine fpga engine uses a high- level synthesis (hls)-based systolic like architecture to implement matrix multiplication kernel. it allows changing parameters such as number of pes, precision, and fm size. caffeine further maximizes the fpga computing capability by optimizing multi level data parallelism discussed in and pipeline parallelism using polyhedral based optimiza- tion framework given in . caffeine framework also handles the weights and biases reorganization in off chip dram to maximize the underlying memory bandwidth utilization. in addition, the double buffering technique is employed to prefetch the next data tile for each pe. caffeine has been evaluated by implementing alexnet and vgg- cnns on ultrascale ku (nm and mhz) and on virtex t (nm and mhz) considering different precisions. the vgg- implementation with - bit xed point on ultrascale ku and virtex t provided and overall throughput enhancement, respectively, compared to implementation on a two socket server, each with a -core intel cpu (e- at ghz). a special case of dataow, referred to as synchronous dataow (sdf) , is a paradigm of computation that allows for representing a computing system as a stream- ing problem. in this way, sdf model can represent the hardware implementation of cnns using linear algebra and directed sdf graph (sdfg). each node of sdfg represents a hardware building block that can immediately start its computation as soon as the data are available through its input arcs. such representation of cnn model offers a fast design space exploration. venieris and bouganis employed sdf model to optimize the mapping of cnns onto fpgas based on hls. in particular, the proposed fpgaconvnet framework in takes as input a high level script programmed by dl expert describing the cnn model, along with specications figure sdf graph partitioning . of the targeted fpga platform. thereafter, it parses the input script through a developed domain specic language (dsl) processor to model the cnn in the form of a directed acyclic graph (dag) where each node corresponds to a cnn layer. then, the dag based cnn is transformed into an sdfg representation and modeled as a topology matrix. the topology matrix contains the number of incoming parallel streams, the width of each data stream, and the production or consumption rates at each node. in addition, the dsl processor extracts information about the platform- specic resource constraints. unlike other attempts, instead of exploring the design space for the optimal parameters of loop unrolling and tiling, fpgaconvnet explores the design space of the topology matrix components while considering the resource con- straints. in doing so, fpgaconvnet performs graph parti- tioning, coarse grained folding, and ne grained folding. the graph partitioning splits the original sdfg into sub- graphs and each subgraph is then mapped to a distinct bitstream as shown in fig. note that the proposed multi- bitstream architecture might have multiple conv layer processors (clps), as in the provided example. this away, on chip ram is used for intermediate results and data reuse within the subgraph, while accesss of off chip memory is minimized and limited for input and output streams of the subgraph. however, this scheme adds reconguration penalty due to the need for reconguring the fpga when the data ows between adjacent subgraphs. to amortize this overhead, several input data streams are processed in a pipelined manner. thereafter, each bitstream architecture is optimized using coarse grained folding and ne grained folding. in coarse- grain folding, conv, pooling, non linear, and other major operations of each layer are unrolled to provide the highest possible throughput by having several parallel units of each operation. the ne grain folding controls the unrolling and pipelining of the dot product operations inside conv and average pooling units. instead of fully unrolling the imple- mentation of dot product which produces a dot product per cycle, with the use of a high number of multipliers and adders, fpgaconvnet uses a smaller number of mac units and schedules the execution of different operations using time multiplexing. a trade off between the performance and the required hardware resources can be achieved by changing the unroll factor and the degree of multiplex- volume , shawahna et al.: fpga based accelerators of deep learning networks for learning and classication: a review ing. therefore, fpgaconvnet employed simulated annealing to nd the optimal partitioning points and folding factors. finally, fpgaconvnet uses optimal components to derive the conguration of pes and buffers, and generates a synthesizable vivado hls hardware design. fpgaconvnet framework has been evaluated by mapping lenet- and scene labelling small cnn models with q xed point representation onto a zynq- xcz fpga platform working at mhz. in mapping lenet-, fpgaconvnet achieves up to the performance density of cnp . compared to tegra k gpu implementation of scene labelling cnn, fpgaconvnet surpasses tegra ks power efciency by . ma et al. proposed a python based modularized rtl compiler to accelerate cnns by employing loop unrolling optimization , for conv layer operations. a de- tailed review article of this work has been recently published and referred to as alamo . the proposed compiler integrates both the rtl ner level optimization and the exibility of hls to generate efcient verilog parameterized rtl scripts for asic or fpga platform under the available number of parallel computing resources (i.e., the number of multipliers (nm)). if nm is greater than the number of input fms (nif), the proposed compiler fully unrolls loop- (nif, refer to subsection ii a for more details) while it partially unrolls loop- (nof) to exploit the data reuse of shared features among nm{nif output fms. otherwise, it partially unrolls loop- which results in nif{nm repeated sliding of kernel window. on the other hand, loop- (xy ) is serially computed after loop- (k) to minimize the number of partial sums. the overall modules of the proposed cnn accelerator are shown in fig. the controller is responsible for directing and ensuring in order computation of cnn modules for each layer. the data routers oversee the selection of data read and data write of two adjacent modules as well as the assignment of buffer outputs to shared or pool multipliers of the multiplier bank. the feature buffers hold the fms using on chip rams. the weight buffers are used to ensure the availability of conv and fc layers weights before their computation as well as to overlap the transfer of fc layer weights with its computation. the conv module consists of control logic, groups of adder trees, and relu components. the control logic component parametrizes the loop unrolling factors based on the conguration of each layer (nif, nof, x, y , and k). the conv module contains nm{nif adders to sum nif parallel multiplier results and accumulate them. moreover, the adder trees can be shared by layers with identical nif to be as one single module. the relu component checks the input pixel sign bit to either output zero or the data pixel itself. the pool module contains accumulators or comparators to perform average or maximum operation, respectively. the norm module maintains the required components to perform the operations of local response normalization such as square, non linear (using look up table), and multiplication oper- figure alamo overall acceleration modules . ations. finally, the fc module shares the multiplier bank module with the conv module to perform the matrix- vector multiplication (mvm). alamo architecture permits the output pixels to be only stored in the feature buffers, which makes alamo suitable for cnns with only small intermediate data volumes. the proposed rtl compiler has been tested by accelerating two cnn models; alexnet and nin . the generated parameterized rtl scripts for alexnet and nin are synthe- sized using altera quartus synthesis tool and implemented on de net fpga board. the experimental results for alexnet model are compared with the results for opencl- based design as both use the same fpga board with similar hardware resources for alexnet. alamo achieved and improvement for throughput and power consumption, respectively. moreover, the overall throughput of nin model is better than that of alexnet. this is because nin has more conv layers and many of them have the same nif. liu et al. proposed a parallel framework for fpga- based cnn accelerators that exploits four levels of par- allelism; task level, layer level, loop level, and operator level. task level parallelism involves executing multiple im- age prediction tasks simultaneously. layer level parallelism exploits pipelining across layers to enable parallel execution of all layers with different images. loop level parallelism utilizes loop unrolling in performing convolutions and this can be achieved either through intra output or inter output parallelism. finally, operator level parallelism is achieved by parallelising the k k macs operations needed for convolution operation in convolutional layers or the n macs needed for inner product computation in fully connected layers. fig. shows the parallel framework exploiting these four levels of parallelism. the authors have used -bit xed point format for rep- resenting pixels in input feature maps and output feature maps. however, they have used bits for intermediate results which get truncated to bits. in addition, they have used bits for representing kernels and weights. they have presented a systematic methodology for design space volume , shawahna et al.: fpga based accelerators of deep learning networks for learning and classication: a review figure parallel framework exploiting four levels of parallelism . exploration to nd the optimal solution that maximizes the throughput of an fpga based accelerator under given fpga constraints such as on chip memory, computational resources, external memory bandwidth, and clock frequency. the proposed technique has been evaluated by imple- menting three cnn accelerators on the vc board for lenet, alexnet, and vgg s. it has achieved a throughput of gops, gops, and gops for lenet, alexnet, and vgg s accelerators, respectively. in addition, the performance has been compared with matconvnet tool running the cnn models on intel core i k cpu ( ghz) and nvidia gtx- gpu (, cuda cores, gb gddr, gb/s memory bandwidth). compared to the cpu implementations, the accelerators for lenet, alexnet, and vgg s achieved , , and in performance, respectively, and , , and in power efciency, respectively. compared to the gpu implementations, the accelerators achieved better per- formance in the small scale network lenet (), com- parable performance in the medium scale network alexnet (), and worse performance in the large scale network vgg s (). however, the accelerators achieved higher power efciency than the gpu implementations in all three networks with for lenet, for alexnet and for vgg s. fp dnn is an end to end framework that auto- matically generates optimized fpga based implementations of deep neural networks (dnns) using an rtl hls hy- brid library. fp dnn compiler, programed using c++ and opencl, takes tensorflow symbolic descriptions of dnns, and then performs model inference through the use of model mapper, software generator, and hardware gener- ator modules. the model mapper extracts the topological structure and layers congurations of dnn model from the tensorflow descriptions and generates an execution graph for the target model. the execution graph shows layer by- layer operations and read/write data transactions. fp dnn compiler allocates off chip dram data buffers to store intermediate data, weights, and model parame- ters and congurations. the model mapper maximizes the storage resource reuse through minimizing the number of required physical buffers. specically, it formulates the data reuse problem as a graph coloring problem , and then the left edge algorithm is applied to generate kernel conguration and kernel schedule. subsequently, the software generator uses the kernel schedule to generate a host c++ program which initializes the model, manages the data buffers, and schedules the kernel execution. on the other hand, the hardware generator uses the kernel conguration and the execution graph to generate the fpga hardware codes by instantiating the corresponding optimized templates from an expandable rtl hls hybrid library. each template is comprised of verilog based computational engine and opencl based control logics engine. the architecture of the proposed fpga based accelerator consists of matrix multiplication and data arranger modules. matrix multiplication module is a hand written verilog code that is designed and optimized based on the hardware constraints of altera stratix v gsmd fpga. it applies tiling and ping pong double buffers techniques to improve the throughput. on the other hand, data arranger is an opencl based module that is responsible for mapping the computational part of a layer to matrix multiplication as well as performing data communication with off chip memory and matrix multiplication module. mapping dnns compu- tational operations to matrix multiplication has been widely applied in prior studies , , . fp dnn maps fc layer to matrix multiplication by batching input vectors together. before model deployment, fms and weights are rearranged in dram using the channel major scheme to optimize the communication between the accelerator and off chip dram. on the other hand, both oating point and xed point representations have been supported for implementation, and they can be adjusted by the user. the proposed rtl hls hybrid framework has been eval- uated by accelerating vgg-, lstm lm , resnet- dnns on stratix v gsmd fpga. note that this is the rst work that implements resnet- on fpga. the volume , shawahna et al.: fpga based accelerators of deep learning networks for learning and classication: a review experimental results demonstrated that the speedup of fp- dnn for -bit xed point implementations are about - compared with the server that includes processors each with -core intel xeon e v at ghz. in line with the current trends towards compressed neural networks, with dramatically reduced weights and activations bit width using -bit or -bit quantization , umuroglu et al. conducted a set of experiments to estimate the trade off between the network size and preci- sion using the rooine model. they found that binarized neural networks (bnns) require to times more operations and parameters than an -bit xed point cnn to achieve a comparable accuracy on mnist dataset. however, the performance of bnn is found to be faster than the xed point network. subsequently, the authors proposed a framework, referred to as finn , that maps a trained bnn onto fpga. finn generates a synthesizable c++ network description of a exible heterogeneous streaming architecture. the architecture consists of pipelined compute engines that communicate via on chip data streams. each bnn layer has been implemented using dedicated compute engines with -bit values for weights and fms; + and - are used to represent a set bit and unset bit, respectively. the authors have optimized accumulation, batch normal- ization (batchnorm), activation, and pooling operations of bnns. in particular, the accumulation of a binary dot- product has been implemented as a counter of set bits (popcount operation). the popcount accumulate reduces the number of required look up tables (luts) and ip ops (ffs) by a half, compared to the implementation of signed- accumulation. bnn batchnorm and activation operations have been simplied and implemented together as unsigned comparison with a threshold k, + is produced when the input value is greater than or equals to k, and - otherwise. the value of k is computed during run- time. such an implementation of batchnorm activation op- erations requires much smaller number of luts, without the need for dsps and ffs, compared to regular imple- mentation of batchnorm activation. max pooling, average- polling, and min pooling have been effectively implemented with boolean or operator, boolean majority function, and boolean and operator, respectively. the accelerator architecture is composed of building blocks from the finn hardware library. the matrix vector- threshold unit (mvtu) is the core computational building block as matrix vector operations followed by thresholding form the majority of bnn operations. the design of mvtu consists of an input buffer, an array of p parallel pes each with s simd lanes, and an output buffer. bnn weight matrix is distributed across the pes and stored locally in on- chip memory. subsequently, the input images are streamed through the mvtu and multiplied with the weight matrix. particularly, the pe computes the dot product between an input vector and a row of weight matrix, each of s bits wide, using an xnor gate, as shown in fig. then, it figure the architecture of mvtu pe . compares the number of set bits to a threshold and produces a -bit output value as previously discussed. umuroglu et al. implemented the conv layer using a sliding window unit (swu) and an mvtu, where convo- lutional operation is transformed to matrix multiplication of image matrix and lter matrix. swu generates the image matrix to mvtu by moving the sliding window over the input fms, while the lter matrix is generated by packing the weights from the convolution lters as shown in fig. in order to meet the user throughput requirement, mvtu is folded (time multiplexed) by controlling the values of p and s. folding of mvm decides partitioning of the matrix across pes. every row of matrix tile is mapped to a distinct pe and every column of pe buffer is mapped to a distinct simd lane. in this away, the required number of cycles to compute one mvm (total fold) is obtained as pxy q{pp sq, where x and y are the dimensions of the matrix. the folding factors of bnn layers have been determined such that every bnn layer takes nearly the same number of cycles. to evaluate finn, the authors implemented cnv topol- ogy on xilinx zynq- board at mhz to acceler- ate bnns inference on cifar- . cnv contains figure transforming conv to matrix multiplication , where, ifm and ofm are the input and output feature maps, respectively. volume , shawahna et al.: fpga based accelerators of deep learning networks for learning and classication: a review figure conv acceleration architecture and dataow , where, p ix p ox , p iy p oy , and p of three repetitions of two convs and max- pooling layers. its topology is inspired by vgg- and binarynet . although cnv accepts images with - bits/pixel as an input and produces a -element vector of -bit values, -bits are used for representing intermediate results while -bit is used for representing conv and fc weights. experimental results demonstrated that the proposed design provides high performance ( tops) while incurring low energy consumption ( watts). finn outperforms the design by ovtcharov et al. by over for throughput. in , loop optimization techniques , have been employed in fpga to design a customized cnn accelerator through speeding up conv layer operations. firstly, an in- depth analysis is provided to numerically characterize loop unrolling, loop tiling, and loop interchange optimization techniques. in doing so, conv dimensions parameters (n ), loop unrolling design variables (p ), and loop tiling design variables (t ) have been used with a con- straint, as for a specic loop level, p t n . note that unrolling loop- and loop- requires pkxpky and pif multipliers, respectively, an adder tree with fan in of pkx pky and pif, respectively, and an accumulator. on the other hand, unrolling loop- requires pixpiy par- allel units of mac to reuse the same weight for pixpiy times, while the input feature pixel can be reused by pof times when unrolling loop- with the use of pof parallel mac units. thus, pkx pky pif pix piy pof multipliers are required. please refer to fig. for more details on conv loops levels and their parameters. in loop tile optimization, the authors have numerically set the lower bound on the required size of the input pixel buffer, the weight buffer, and output pixel buffer that ensures reading each input feature pixel and weight from the off- chip memory only once. on the other hand, loop interchange technique has a great impact on the times of memory access as well as the number of partial sums since it determines the order of computing conv loops. secondly, the authors have provided a quantitative anal- ysis of the design variables to minimize each of computing latency, partial sum storage, on chip buffer access, and off- chip dram access. subsequently, matlab scripts are used to randomly sample a subset of the solution space to nd the optimal design congurations. this is due to the large solution space, more than possible congurations for loop tiling variables of width (pox) and height (poy) output fm alone. according to the randomly sampling results for vgg- cnn model on arria gx fpga, uniform unrolling factors for conv layers are used with pix pox piy poy and pof for loop- and loop-, respectively, to reuse input feature pixels and weights. on the other hand, loop- and loop- are serially computed to prevent the movement of the partial sums between the mac units and consume them asap since both loop- and loop- need to be nished in order to obtain one nal output pixel. more importantly, the order of loops computation has been found to be as follows. loop- is computed rst, then comes loop-, and nally loop- and loop- are computed in any order. finally, a customized convolution accelerator module with efcient dataow has been designed based on the previous results and used for all vgg- conv layers. the conv accelerator consists of , (pixpiypof) independent mac units and (pof) input pixel buffers. fig. shows an example of the designed conv accel- erator when pix, piy, and pof are all equal to the input pixels are shifted after fetching them out of the input pixel buffers. subsequently, they can be reused among the input register arrays. then, the input pixels are fed into the associated mac units. the gure also shows that the input pixels and weights are shared by pof and pixpiy mac units, respectively. the overall cnn acceleration system mainly consists of two sdram banks that hold the input feature pixels and weights, two modular scatter gather dma (msgdma) engines to facilitate the simultaneous read/write from/to the sdrams, and a controller to govern the sequential computation of layers as well as the iterations of the four volume , shawahna et al.: fpga based accelerators of deep learning networks for learning and classication: a review conv loops. on the other hand, dual weight buffers have been used to increase the throughput of fc layer through overlapping the inner product computation with off chip communication. the acceleration system has been written as parametrized verilog scripts. the experimental results show that the proposed accelerator has a throughput of gops, which is more than enhancement compared to prior vgg- fpga based implementations , . venieris and bouganis further extended fpga- convnet framework to allow for optimizing either throughput or latency depending on the size of the workload. for large workloads, weights reloading transformation has been introduced to efciently design latency critical cnns on fpga. in contrast with fpgaconvnet, where a distinct architecture is designed for each subgraph, the weights reloading transformation allows for generating a single exible architecture, named as the reference architecture and derived using pattern matching, to execute the workloads of all subgraphs by transitioning to different modes. upon the execution of a new subgraph, the subgraphs weights are read into the on chip memory and the multiplexers are congured to form the appropriate datapath. fig. demon- strates how weights reloading is applied. the authors have mentioned that the required time for transferring subgraphs weights is much smaller than the average time for full fpga reconguration, less when loading mb of weights for a vgg- layer on zynq xcz in the situation discussed above, due to limited on chip memory capacity, it might not be possible to load all weights required for a single conv layer. to handle this, the authors introduced an input fms folding factor (fin) with each conv layer. a conv layer (conv i) is partitioned into fini subgraphs in which each subgraph executes a fraction of conv i to produce a fraction of the output fms. the proposed latency driven methodology has been evaluated by implementing alexnet and vgg- with - bit xed point precision for both on zynq xcz at figure weights reloading . figure overall dla architecture . mhz. the experimental results showed and higher conv throughput than deepburning and the embedded fpga accelerator in for alexnet and vgg- implementations, respectively. lavin and gray demonstrated that cnn algorithms with small lters can be efciently derived using winograd algorithm and fast fourier transform (fft) algo- rithm due to their advantages in improving resource efciency and reducing arithmetic complexity. winograd computation involves a mix of element wise (eltwise) and general purpose matrix multiplication, where some of the matrices need to be transformed. in particular, winograd algorithm exploits the structure similarity among nn tiled input fm pixels given a lter of size rr to generate mm tiled pixels of the output fm, where m represents the stride between winograd tiles (m n r ` ), while minimizing the number of required conv multiplications from mr for conventional conv algorithm to n in another work, zhang et al. implemented fft algorithm for cnn on fpga platform. however, their proposed implementation shows little reduction of computation complexity with small lters such as aydonat et al. presented a deep learning architec- ture (dla) based on opencl. their proposed architecture reduces the external memory bandwidth requirements by an order of magnitude for both the convolutional and fully con- nected layers. this is achieved by caching all intermediate feature maps on chip in stream buffers. for fully connected layers, image batching is used where a batch of images are processed together through the fully connected layers. the approach utilizes the winograd transformation to reduce the multiply accumulate operations, which could reduce the number of needed operations by about %. in addition, it uses half precision (fp) oating point operations with shared exponents, which signicantly reduces the needed computational resources. the overall dla architecture is shown in fig. each pe consists of dot product units, accumulators, and caches, for performing dot products for convolution and fully connected layers. caches are used for storing lter weights. to avoid idle computation cycles, double buffering is used such that lter weights for the next convolution volume , shawahna et al.: fpga based accelerators of deep learning networks for learning and classication: a review layer are prefetched onto the caches while lter weights are loaded from the caches for a particular convolution layer. stream buffers store feature data and stream it to pes. each stream buffer is double buffered similar to lter caches. images are loaded from the ddr and are stored in stream buffers before the rst convolution layer starts execution. during a convolution layer execution, while feature data for a convolution layer is being streamed into the pes, the outputs of convolutions are simultaneously stored in the buffers. the streambuffer unit applies the winograd transformations to features, and streams the transformed features to the rst pe which are forwarded through all the pes via the daisy chained input connections between them. the relu unit receives the outputs of the pes via daisy chained output connections. then, the normalization unit receives the outputs of the relu unit and applies the normalization formula across the feature maps. the pooling unit receives the outputs of the normalization unit and computes the maximum value in a window. the output of the pooling unit is stored back in the stream buffer for further processing, if more convolution layers are to follow. otherwise, the outputs of the pooling unit are stored in external memory. for the fully connected layers, features data are stored on pes caches while lter weights are stored in stream buffers. for the rst fully connected layer, features data are read back from external memory and loaded onto the pe caches. the relu output is sent directly to ddr, without applying normalization or pooling. the sequencer generates the control signals to control the operation of the various blocks in dla according to the topology of the executed cnn. executing a different cnn requires just changing the sequencer conguration. the dla has been evaluated by implementing alexnet cnn on intels arria dev kit which contains a a- device (nm) using a batch size for the fully connected layers. it achieved a performance of images/s. in addition, it achieved x more gflops than the latest ultrascale (ku nm) result reported in , which uses a batch size for the fully connected layers, and more gflops than the latest stratix v result reported in . furthermore, it has achieved energy efciency at images/s/w, which is similar to what is achieved with the best publicly known implementation of alexnet on nvidia titan x gpu. unlike dla architecture where a d winograd algorithm was employed to reduce arithmetic complexity, lu et al. implemented a novel fpga architecture with a two dimensional winograd algorithm to ac- celerate convolutional computation of cnns. the overall architecture consists of line buffer structure and winograd pe engine, as shown in fig. particularly, n ` m input lines and m output lines of on chip buffers are used to effectively reuse fm data among different tiles. while winograd pe engine reads the rst n input lines to perform winograd computation, the next m input lines load pixels from off chip memory using fifos to overlap the data figure winograd based cnn accelerator , where, m is the size of the input fm tile, n is the size of the output fm tile, m is the number of input channels, n is the number of output channels, w is the maximal width of all input fms, c is the width of the output fms. transfer and computation. thereafter, the input lines are rotated in a circular fashion to make the next n input lines ready. on the other hand, winograd pe engine composed of pipelined stages performs transformation, element- wise matrix multiplication, additional transformation, and accumulation of output tiles, respectively. a vector of pes is employed to achieve parallelism through unrolling loop (pof) and loop (pif) similar to that in . to implement fc layer, the proposed accelerator uses the input line buffers to hold fc weights while input neurons are stored on the lter buffers. then, winograd pe engine is reused to implement fc operation but with bypassing the transformation stages. moreover, a batch (nbatch) of input fms are assembled and processed together in order to improve the memory bandwidth. an analytical model has been proposed for a fast design space exploration of optimal design parameters (n, pof, pif, nbatch) constrained by fpga conguration with a -bit xed point representation for both fm data and lter. the proposed accelerator has been evaluated by imple- menting alexnet and vgg- on xilinx zcu fpga. alexnet conv layers have different lters. conventional conv algorithm has been applied to the rst conv layer as it has a lter of size while a uniform lter of size for winograd algorithm has been used to implement the rest of the layers. the design parameters are found to be equal to (, , , ) and (, , , ) for alexnet and vgg-, respectively. the experimental results demon- strated that the proposed winograd based cnn accelerator has an average performance of gops and gops for alexnet and vgg-, respectively, with power consumption of watts for both. the proposed accelera- tor has also been evaluated on xilinx zc platform where the design parameters are found to be as (, , , ) and (, , , ) for alexnet and vgg-, respectively. the ex- perimental results demonstrated that winograd based cnn accelerator has an average performance of gops and gops for alexnet and vgg-, respectively, with power consumption of watts for both. compared to the implementation of vgg- on nvidia titan x with volume , shawahna et al.: fpga based accelerators of deep learning networks for learning and classication: a review figure compute unit with a d bram to pe interconnection . the latest cudnn , titan x gives better performance than xilinx zc but the implementation on xilinx zc achieves higher energy efciency. zhang et al. presented an opencl based architec- ture for accelerating cnns on fpga. they also proposed an analytical performance model to identify the bottleneck in opencl based acceleration of vgg- ccn model on modern fpga platforms such as altera arria gx based on rooine mode analysis, it is shown that the bandwidth requirement of vgg- workload is higher than what is provided by the fpga board. thus, they identied on chip memory bandwidth as the key performance bottle- neck. in addition, they observed that exploited data level parallelism in the existing altera opencl library leads to wasteful replication of on chip memory (bram). this is due to connecting each pe with a dedicated bram port. therefore, a verilog based accelerator kernel has been designed and warped to an opencl ip in order to opti- mally balance on chip memory bandwidth with workload computational throughput and off chip memory accesses. in particular, the proposed kernel consists of a compute subsystem, a local memory subsystem, and a d dispatcher. the compute subsystem is organized hierarchically into compute units (cus) and pes. at pe level, the authors have figure d dispatcher , where, x is the column size of kernel buffer as well as the row size of the input feature buffer, and x is the row size of kernel buffer. figure line buffer design . designed a d multi cast interconnection between brams (-bit data width) and pes to improve the efciency of on chip bram usage by sharing the data of one bram port with several pes as shown in fig. the cu has been designed as a d pe array of size to match the computational bandwidth with the maximum streaming bandwidth (-bit data bus) provided by off chip memory. the d dispatcher divides the work items into work groups each of size (x, x) as shown in fig. thereafter, it adaptively schedules the work items within each work group to the cus starting with the lowest dimension to balance the memory bandwidth with capacity. the d dispatcher is also responsible for host/device memory data transfers. in addition, the authors have limited the maximum fan out for registers to in order to guarantee a higher frequency. the conv layer has been implemented as a matrix multiplication by attening and rearranging the data using line buffer , as shown in fig. , in a similar fashion to that in . the line buffer converts continuous address stream from external memory into a stream conducive for conv operation to substantially reduce the bandwidth requirement of off chip memory. to implement fc layer, the proposed accelerator uses one column of pes in the cu. the proposed implementation has achieved gops and gops with the use of -bit oating point and - bit xed point, respectively, under mhz and mhz working frequencies, respectively. all previously discussed fpga based cnn accelerators, except the ones discussed in , , have employed a single clp to maximize the aggregate throughput of performed consecutive convolutional operations. however, shen et al. noted that using a single globally optimized clp design for the computation of conv layers of radi- cally different congurations and dimensions leads to sub- optimal performance and insufcient utilization of fpga resources. fig. a demonstrates the use of a single clp to iteratively process l , l, and l conv layers where the dimensions of the hardware (clp, clp, and clp) and the layers are represented by the size and shape of the boxes. it is clear that computing l and portions of l leaves fpga resources unutilized as their dimensions are smaller volume , shawahna et al.: fpga based accelerators of deep learning networks for learning and classication: a review figure operation of conv layer processors (clps) on cnn with three conv layers . than the dimension of the used clp. note that processing a conv layer with a dimension bigger than the dimension of clp, such as l, requires the repeated use of clp to process different portions of the layer. the authors have also followed the methodology in to derive an optimal single clp, through nding the optimal unrolling factor xtm, tny, for implementing squeezenet and alexnet on virtex t fpga with a single precision oating point and -bit xed point arithmetic units, respectively. they found that one quarter of dsp slices of squeezenets clp remain unused. even more worse utilization has been observed for alexnet. the optimal single clp has not utilized, on average, more than one quarter of the arithmetic unit resources. on the other hand, they also noted that using one clp for each stage of conv layer in a fashion similar to that in is not efcient due to three reasons. first, it reduces the on chip bram buffer size of each clp which minimizes overall data locality. second, such one to one mapping of conv layers and clps requires orchestrating many off chip memory accesses which incurs latency and bandwidth overheads. third, the overall control overhead scales with the number of clps which leaves insufcient resources for the computation of cnn. to address the above inefciencies, shen et al. proposed a multi clp accelerator system for cnns where the available pfga hardware resources are partitioned across multiple smaller clps. each clp is tailored with a dimension that closely matches the dimensions of a subset of conv layers. thereafter, these specialized clps are used to concurrently operate on a batch of images to achieve a higher overall throughput, as shown in fig. b, where the same hardware in fig. a is partitioned into two parallel clps; clp and clp shen et al. developed an optimization search algo- rithm that uses dynamic programming to nd optimal de- signs. for given congurations of cnn model (i.e., conv layers descriptions) and resource constraints of the targeted fpga platform (i.e., number of dsp slices, bram kb units, and off chip memory bandwidth), it derives the opti- mal number of clps (along with their xtm, tny dimensions) as well as the optimal mapping between conv layers and clps that maximize the performance. the assignment of cnn layers to clps is static, where each cnn layer is mapped and bounded to a particular clp. subsequently, cnn layers are pipelined to their clp, as shown in fig. b, where l and l are pipelined to clp while l is re- peatedly processed on clp with very little idle hardware which improves the performance compared to single clp approach. moreover, the optimization algorithm also nds the optimal partition of on chip bram resources of each clp that minimizes the overall off chip memory accesses. note that the optimal dimension of each clp is found based on the work in . subsequently, c++ (hls) templates are parameterized to design clps and to form a complete implementation of cnn. a standard axi crossbar is used to intercon- nect the independent clps. the ping pong double buffering technique is also used for input fms, output fms, and weights to allow for transferring data while computation is in progress. the experimental results of implementing alexnet with a single precision oating point using multi- clp accelerator on virtex t and t fpgas at mhz demonstrate and higher throughput than the state of the art single clp design in , respectively. for the more recent squeezenet network, the proposed multi clp accelerator results in speedup of and on virtex t and t fpgas at mhz with -bit xed point, respectively. wei et al. presented a systolic architecture for automatically implementing a given cnn on fpga based on opencl description, maximizing clock frequency and resource utilization. the proposed systolic architecture is shown in fig. each pe shifts the data of the weights (w) and inputs (in) horizontally and vertically to the neighboring pes in each cycle. the d structure of pes is designed to match the fpga d layout structure to reduce routing complexity and achieve timing constraints. volume , shawahna et al.: fpga based accelerators of deep learning networks for learning and classication: a review figure systolic array architecture for cnn . the technique rst nds a feasible mapping for the given cnn to the systolic array to guarantee that proper data is available at specic locations in the pe array at every cycle. then, the size of pe array (dimensions) is determined which has an impact on the required number of dsps, the clock frequency, and the dsps efciency. finally, the data reuse strategy is determined by choosing proper tiling sizes. the proposed technique has been evaluated using alexnet and vgg on intels arria gt board. the technique has explored the use of both -bit oating- point and xed point using -bits for weights and -bits for data. evaluation results show that, for the vgg cnn, the technique achieves up to , gops on intels arria device with a clock frequency of mhz and (-)-bit xed point representation. in another recent research work, ma et al. general- ized the previously proposed accelerator in to efciently accelerate resnet- and resnet- on arria gx fpga. in doing so, they designed exible and scalable conv, relu, batchnorm, scale, pooling, fc, and eltwise primitives. in addition, local control logic and registers have been used with each primitive to control their computation order and to hold their congurations, respectively. by doing so, resnets primitives can be efciently reused for different parameters of each layer. for resnets scalable conv primitive, there are four (kernel, stride) size congurations; ( , ), ( , ), ( , ), and ( , ). therefore, a similar architecture and dataow to that shown in fig. has been used for conv but with the use of two sets of register arrays; with shifting between the registers (which is shown in fig. , set-), and without shifting between the registers (set-). the conv primitive with kernel and stride of uses set- register array, while set- is used with ( , ), (, ), and (, ) congurations. in conv primitive with set-, the input pixels are fed from the input pixel buffers into the corresponding registers without shifting, and then to mac units. the skipped input pixels in ( , ) conguration are not stored to the input pixel buffers. on the other hand, the ( , ) conguration of the kernel and stride sizes is retained as the ( , ) case while transferring repeated input pixels into the input pixel buffers and rearranging their storage patterns. the conv primitive also takes care of zero paddings for different (kernel, stride) size congurations. the loop unrolling and tiling techniques in have also been employed to accelerate conv primitive with a uniform mapping of pes to all resnets conv layers. however, designing of efcient cnn modules is not enough, as the memory accesses and data movements between these modules must also be minimized. therefore, the authors have designed a layer by layer computation ow. the global control logic is responsible for governing the sequential op- erations of primitives and their dataow through predened and preloaded layered based execution owchart, as shown in fig. in addition, it has been modeled to recongure resnet primitives according to the parameters of each layer during runtime. for instance, it maps a particular number of pes to conv layer based on loop unrolling parameters as well as it controls the selection of register array type (set- or set-) based on conv (kernel, stride) parameters. on the other hand, a custom dma manager has been designed to control the operations of dma. note that the dma is responsible for transferring the input fm pixels, weights, and output fm pixels between off chip memory and on chip buffers. unlike alamo architecture where the output pixels are only stored in on chip buffers, this work as well as the work discussed in store the output pixels in off chip memory with the use of loop tiling technique in order to have a exible architecture that can process large scale cnns. the dual weight buffers technique has not been used in this work due to the current trend in cnns where either the size of fc weights has been signicantly reduced ( m in resnet compared with m in vgg) or the fc layers are completely removed such as in nin. the experimental results demonstrated that the achieved throughput for resnet- and resnet- are gops and gops, respectively. finally, the authors mentioned that higher throughput can be achieved figure execution flowchart of resnets layers . volume , shawahna et al.: fpga based accelerators of deep learning networks for learning and classication: a review figure dlau accelerator architecture . using batch computing . wang et al. proposed a scalable design on fpga for accelerating deep learning algorithms. in order to provide a scalable architecture and support various deep learning applications, the proposed architecture utilizes the tiling technique in which the large scale input data is partitioned into small subsets. the size of the tile is congured to leverage the trade off between the hardware cost and the speedup. moreover, the authors explored hot spots prol- ing to determine the computational parts that need to be accelerated to improve the performance. the experimental results illustrated that matrix multiplication and activation functions are the key operations in deep learning algorithms as they consume about % and % of the overall execution time, respectively. thus, the proposed accelerator is responsible for speeding up both matrix multiplication and activation function computations. the main components of the proposed architecture are the embedded processor, the ddr memory controller, the dma module, and the deep learning acceleration unit (dlau), as shown in fig. the embedded processor utilizes the jtag uart to communicate with the accel- eration unit . the dlau unit accesses the ddr memory to read the tiled input data and to write the results back through the dma module during the execu- tion. the dlau utilizes three fully pipelined processing units to improve the throughput, while minimizing the memory transfer operations. these units are tiled matrix multiplication unit (tmmu), partial sum accumulation unit (psau), and activation function acceleration unit (afau). tmmu is responsible for multiplication and generating the partial sums. to optimize the performance, tmmu is structured as a pipelined binary adder tree. moreover, it uses two sets of registers alternately to overlap the computation with the communication, one group is used for the computation, while in parallel, the other group is loaded with the next node data every clock cycle. on the other hand, psau is responsible for accumulating the partial sums generated from tmmu. finally, afau implements the sigmoid function using piecewise linear interpolation to speedup the computation with negligible accuracy loss. since the processing units in dlau might have inconsistent throughput rates, each unit has input fifo buffer and output fifo buffer to prevent data loss. the authors implemented the proposed architecture on xilinx zynq zedboard with arm cortex a processors clocked at mhz. in addition, they used the mnist dataset as a benchmark considering the network size as , , and the experimental results demonstrated that the speedup of the dlau accelerator is up to compared with the intel core processors at network size. in addition, the results depict that the proposed architecture is quite energy efcient as the total power consumption was only mw. in , a generalized end to end acceleration system of the previously proposed accelerators in , , , has been developed to support diverse cnn models. in doing so, a user friendly interface and an rtl level compiler have been proposed to automatically generate customized fpga designs. the authors have developed an expandable optimized rtl based library containing the most commonly used cnn operations. these operations have been coded in verilog and designed based on the quantitative analysis and optimization strategies discussed in . the compiler generates a dag based structure for the used cnn model and then compiles it with rtl mod- ules in the library. the proposed compiler allows the user to input the high level information of the used cnn model (previously designed on caffe framework ) as well as the design variables (i.e., loop unrolling and loop tiling variables) with the resource constrains of the targeted fpga platform. such utility facilitates the exploration of the best trade off between the resource usage and the performance. unlike the architecture in where individual conv module is assigned to each conv layer, the scalable rtl computing module proposed in this work is reused by all cnn layers of the same type for different cnns as shown in fig. note that it is not necessary to have all these modules in the architecture. for instance, the rtl compiler figure overall architecture and dataow . volume , shawahna et al.: fpga based accelerators of deep learning networks for learning and classication: a review figure conv recongurable computing module . will not compile or synthesize eltwise and combined batch normalization with scale (bnorm) modules for vgg- model which greatly saves the hardware resources. on the other hand, the authors categorized cnn layers into key layers (i.e., conv, pool, and fc layers) and afliated layers (e.g., relu, bnorm, eltwise, and all other layers). they have also dened layer combos, where each combo is composed of a key layer and several afliated layers. layer combos are sequentially executed according to their order in the dag. moreover, the layer combo is also divided into several sequential tiles. the computation of each combo tile starts by reading its input pixels from off chip dram and ends by writing back its output pixels to off chip dram. the global control logic, inter tile control logic, and intra tile control logic are responsible for governing the sequential operations of layer combos and reconguring the modules, combo tiles, and tile layers (key and afliated layers), respectively, through predened exible execution schedule similar to that in . the authors have also employed special storage pattern of both input pixels and weights on off chip memory before the acceleration process to maximize data reuse and minimize of data communication. the architecture of conv module is designed based on the acceleration strategies in , but with a different organization of mac units as shown in fig. the mac units of conv module have been organized into piy pof independent mac blocks, with each mac block containing pix mac units to further minimize the buffer read operations and the partial sums movements. moreover, such organization enables to handle varying (kernel, stride) sizes congurations through generating different variants of conv register arrays during the compilation. experimental results demonstrated that the achieved throughput on intel stratix v gxa for nin, vgg-, resnet-, and resnet- are gops, gops, gops, and gops, respectively. on the other hand, the achieved throughput on intel arria gx was gops, gops, gops, and gops for nin, vgg-, resnet-, and resnet-, respectively. more than throughput improvements have been achieved on intel arria gx since it has and more logic elements and dsps than the intel stratix v gxa, respectively, which allows for larger loop unrolling variables. recently, the programmable solutions group at intel has developed an fpga software programmable and run time recongurable overlay for deep learning inference . the developed overlay is referred to as the deep learning accelerator (dla). for the hardware side of intels dla, the team has partitioned the congurable parameters into run- time and compile time parameters. the run time parameters allow for easy and quick use of different neural network frameworks, while the compile time parameters provide a tunable architecture for performance. intels dla uses a lightweight very long instruction word (vliw) network, an -bit unidirectional ring network, to support the control and reprogramming logic. compared with typical overlays, intels dla comes with only % overhead while other typical overlays tend to always come with larger over- heads . the reprogramming of intels dla overlay allows for consecutive runs of multiple nns in a single application run without the need for reconguring and recompiling the fpga. fig. shows that a d array of pes is used to perform convolution, multiplication, or any other matrix operations. each pe contains a double buffered lter cache allowing for pre loading of next lters while computing. the stream buffer employed the double buffering mechanism as well to store the inputs and the intermediate data on chip. to have exible nn architecture, intels dla employs an xbar interconnect that connects all the core functions required. thus, deep learning functions can be easily added to the overlay through the xbar by picking them from a suite figure intels dla: neural network inference accelerator . volume , shawahna et al.: fpga based accelerators of deep learning networks for learning and classication: a review of pre optimized functions of the select frameworks that intels dla uses. the width adaptation module has been used to control the throughput of the function. in addition, intels dla supports vectorization across the input width (q_vec), input height (p_vec), input depth (c_vec), output depth (k_vec), lter width (s_vec), and other dimensions as depicted in fig. the authors mention that vectorization depends on the layers dimensions of the considered framework. however, they did not provide a systematic way for nding the optimal balance for the number of used pes and the size of the caches. for efcient use of resources, intels dla maps avg pooling and fc primitives to convolutions in order to avoid having under- utilized (over time) dedicated auxiliary functions. for the software side of intels dla, the proposed accel- erator uses a graph compiler to map a nn architecture to the overlay for maximizing the hardware efciency through slicing, allocation, and scheduling. in the slicing pass, the graph compiler breaks down the architecture into subgraph (a chain of functions) in such a way that they t within the computing and storage resources of the overlay. a single conv layer followed by a pooling layer is an example of cnn subgraph. the graph compiler optimizes the external memory spill points by group slicing technique. the group slicing allows several sequential convolutions, for instance, of a single slice to be computed before moving onto the next slice while using the whole stream buffer. during the allocation pass, the graph compiler optimizes the use of a custom developed lter caches and stream buffer by managing the read and write from the stream buffer for each slice. moreover, it assigns an external memory address when the stream buffer is not big enough to hold the slice data. finally, intels dla compiler schedules the execution of subgraphs using cost based (the ratio of the output size figure design flow from cnn model to hardware acceleration . figure overall architecture of angel eye . to the effective input size) priority queue. the authors utilized the software programmable and run time recon- gurable overlay to optimize the software and hardware implementation of googlenet and resnet cnns. the benchmark results on an arria gx fpga demonstrated that intels dla has a throughput of fps on googlenet. the team pointed that multi fpga deployment might be used to further improve the throughput of intels dla. kaiyuan et al. proposed a complete design ow, referred to as angel eye, for mapping cnns onto fpga. it includes a data quantization strategy, a parameterizable and run time congurable hardware architecture to support various cnns, fpga platforms, and a compiler to map a given cnn onto the hardware architecture. it adopts the approach of using a exible hardware architecture and maps different cnns onto it by changing the software. the proposed design ow from cnn model to hardware acceleration is shown in fig. due to the large dynamic range of data across different layers, the best radix point is found for each layer for a given bit width. they demonstrated that their strategy can simplify state of the art cnns to -bit xed point format with negligible accuracy loss. although -bits are used for representing data, bits are used for representing inter- mediate data in layers, which is then aligned and quantized to bits. fig. and fig. show the overall architecture of angel eye and the structure of a single pe, respectively. the architecture is designed for supporting an instruction interface that supports three types of instructions; load, save, and calc. the overall architecture is divided into four main compo- figure structure of a single pe . volume , shawahna et al.: fpga based accelerators of deep learning networks for learning and classication: a review nents; pe array, on chip buffer, external memory, and con- troller. the pe array implements the convolution operations in cnn and supports kernel level parallelism, and input and output channel parallelisms. it uses a convolution kernel, as this size is most popular in state of the art cnn models. however, larger kernel sizes are supported based on the use of multiple kernels. the use of on chip buffers allows data i/o and convolution calculation to be done in parallel. the controller is responsible for receiving the instructions and issuing them to the other components. the compiler maps the cnn descriptor to the set of instructions that will be executed by the hardware. it follows basic scheduling rules to fully utilize data localization in cnn and reduce data i/o. the block partition step partitions the calculation of one layer to t each block into the hardware. the memory mapping step allocates memory for communication between the host cpu and the cnn accelerator. based on the block partition result, on chip memory is allocated for the input and output feature map blocks and for the convolution kernels and bias values. the dependency check step checks data dependency among instructions and sets appropriate instruction ags to maximize parallelism between convolu- tion calculation and data i/o. based on experimental results, it is shown that the -bit implementation of angel eye on xcz achieves up to higher energy efciency than nvidia tk and higher than nvidia tx in addition, the -bit implementation of angel eye on xcz is faster and higher in power efciency than peer fpga implementation on the same platform . in and , a special register array architecture has been designed to rearrange buffers data and direct them into pes for the purpose of implementing conv module that supports specic stride and zero padding settings. although the designed conv module is not generalized for any (ker- nel, stride) size congurations, it is composed of complex wire routing and control logic as shown in fig. to have exibility in directing the dataow of conv pixels, ma figure conv acceleration architecture and dataow using data router , where, p ix p ox, and p iy p oy. et al. replaced the register array architecture in with a data router as shown in fig. the data router is a scalable set of data bus from buffer to pe (bufpe). the bufpe data bus consists of simple register arrays with fifos in between to form a line buffer similar to that in . the register array uses the fifo to pass its input pixels to the adjacent registers. each bufpe data bus has different data movements within its register arrays to implement specic stride and kernel size settings. unlike the register array architecture in where the west zero paddings are handled by changing the storage pattern within the input pixel buffer, the bufpe handles such kind of paddings by shifting the connection between the register arrays and the input pixel buffers to simplify the data transferring from off chip memory to on chip buffers. however, there is still a need for adjusting the storage pattern within the input buffers in order to handle other zero paddings. the global control logic is responsible for selecting the suitable bufpe data bus from the data router as well as the suitable storage pattern within the input buffers based on the (kernel, stride) size conguration of conv layer. the conv module has also been optimized by reducing the required number of parallel adders that add the partial sums with biases as well as the number of parallel multipliers and adders needed to perform bnorm operation by serializing the parallel outputs using multipliers. in addition, -bit xed point has been used to represent both weights and pixels, while dynamically adjusting the decimal point in different layers to fully utilize the existing data width . the proposed compiler in has been used to cong- ure the parameterized verilog scripts of the overall cnn acceleration system. experimental results show throughput degradation on both intel arria gx and intel stratix v gxa in comparison to the results in . in table and table , we summarize the reviewed fpga based deep learning networks acceleration tech- niques. for each technique, we list the year the technique was introduced, the key features employed for acceleration, the used deep learning model, the number of needed op- erations per image, the fpga platform used to implement the technique, the precision used for the fms and weights, the clock frequency used, the design entry for describing the modeled deep learning network, the type of lut for the used platform, the number of resources available by the used platform in terms of brams, luts, ffs, and dsps, the percentage of each resource utilization, the performance in gops, the speedup in comparison to a given baseline model, and nally the power efciency (gops/w). iv. metaheuristics in the design of convolutional neural networks currently, convolutional neural network (cnn) structures are designed based on human expertise. for a given applica- tion, this consists of determining the number of convolution layers, number of fully connected layers, sizes of feature volume , shawahna et al.: fpga based accelerators of deep learning networks for learning and classication: a review table implementation and performance summary of fpga based accelerators. technique year key features dl model image operations (gop) platform precision frequency (mhz) lut type design entry resources resources utilization performance (gops) speedup baseline power efciency (gops/w) brams/ mk luts/ alms ffs dsps brams/ mk luts/ alms ffs dsps cnp d conv modules, memory interface with simultaneous accesses lenet- virtex sx -bit xed point -input luts lush , , n/a % % % n/a conv coprocessor accelerator parallel clusters of d convolver units, data quantization, off chip memory banks conv layers virtex lxt -bit xed point -input luts c , , % % % % ghz amd opteron maple in memory processing, banked off chip memories, d array of vpes conv layers virtex sxt xed point -input luts c++ , , , n/a ghz c gpu n/a dc cnn integer factorization to determine the best cong for each layer, input and output switches conv layers virtex sxt -bit xed point -input luts rtl , , , n/a - ghz c gpu neuflow multiple full custom processing tiles (pts), pipelining, fast streaming memory interface conv layers in n/a virtex vlxt -bit xed point -input luts hdl , , n/a ghz core duo cpu memory centric accelerator flexible off chip memory hierarchy, data reuse, loop transformation conv layers virtex vlxt xed point -input luts c , , % % n/a % standard virtex implementation n/a nn x cascaded pipelining, multiple stream processing conv layers zynq xcz -bit xed point -input luts lua , , n/a mhz embedded arm cortex a processors rooine based fpga accelerator rooine based model, loop transformation, loop tiling/unrolling alexnet virtex vxt -bit oat point -input luts c , , , , % % % % ghz intel xeon microsoft specialized cnn accelerator multi banked buffers, network on chip for re- distribution of output data, software congurable alexnet stratix v gsmd -bit oat point -input luts c , , , , n/a rooine based fpga accelerator embedded fpga accelerator data quantization and arrangment, svd vgg- zynq xcz -bit xed point -input luts rtl , , % % % % ghz intel xeon deepburning nn model compression, compiler based library, automatic partitioning/ tiling, function approx. alexnet zynq xcz -bit xed point -input luts rtl , , n/a % % % ghz intel xeon opencl based fpga accelerator design space exploration for all cnn layers, genetic algorithm, altera opencl sdk alexnetpaq stratix vpq gxa (-)-bit xed point -input luts opencl , , , % % n/a % paq ghz intel i- n/a pbq n/a vgg pbq stratix vpq gsd , , ,, , % % n/a % paq n/a pbq n/a caffeine , systolic array architecture, loop unrolling, double buffering, pipelining, rooine model alexnetpaq virtexpq vxt -bitpaq xed point -input luts c++ , , , , % % % % pbaq two socket server each with a -core intel cpu e- at ghz % % % % pbaq vgg pbq xilinxpq ku -bitpbq xed point , , , , % % % % ,pbbq n/a paaq n/a fpgaconvnet datapath optimization, synchronous dataow, partitioning and folding, design space exploration lenet- zynq xcz -bit xed point -input luts hls , , % % % % cnp n/a scene labelling % % % % tegra k gpu volume , shawahna et al.: fpga based accelerators of deep learning networks for learning and classication: a review table implementation and performance summary of fpga based accelerators. technique year key features dl model image operations (gop) platform precision frequency (mhz) lut type design entry resources resources utilization performance (gops) speedup baseline power efciency (gops/w) brams/ mk luts/ alms ffs dsps brams/ mk luts/ alms ffs dsps throughput optimized fpga accelerator four levels of parallelism, memory cache mechanism, design space exploration lenet virtex vxt (-)-bit xed point -input luts n/a , , , , % % % % ghz intel core i k cpu alexnet % % % % vgg s % % % % fp dnn rtl hls hybrid compiler, hand written matrix multiply, quantization, tiling and double buffers vgg- stratix v gsmd -bit xed point -input luts c++ and opencl , , , , % % n/a % processors each ghz intel xeon -core e v resnet- % % n/a % finn binarized cnn, pipelining, automatic partitioning and tiling, approximate arithmetic cnv zynq xcz (-)-bit precision -input luts c++ , , % % n/a n/a , microsoft specialized cnn accelerator customized conv loops accelerator numerical analysis of conv loops opt., solution space exploration, dataow opt. vgg- arria gx (-)-bit oat point -input luts verilog , , ,, , % % n/a % energy efcient cnn opencl based fpga accelerator pbq latency driven design for fpga based cnns synchronous dataow, weights reloading and sdf transformations, design space exploration alexnet zynq xcz -bit xed point -input luts hls , , n/a deepburning n/a vgg- embedded fpga accelerator dla winograd transformations, double stream buffers, pes double cache buffers, daisy chained pes alexnet arria gx half precision fp with shared exponent -input luts opencl , , ,, , % % % % , caffeine paaq opencl based fpga accelerator paq winograd based cnn accelerator winograd algorithm, loop unrolling, double buffers, batching, line buffers, design space exploration alexnetpaq zynqpq zcu -bit xed point -input luts c , , , n/a paq paq ,pbq caffeine pbaq vgg pbq xilinxpq zc -input luts , , paq paq pbq titan x (cudnn ) opencl based architecture for accelerating cnns d bram to pe interconnection, d dispatcher, rooine model, opencl vgg- arria gx -bit oat point -input luts opencl , , ,, , % n/a n/a % altera opencl on arria platform -bit xed point , , ,, , % n/a n/a % , n/a multi clp accelerator for cnns multiple conv processors, pipelining, dynamic programming, double buffering alexnetpaq virtexpq vxt -bit oat point -input luts c++ , , , , % % % % paq single clp design based in % % % % paq squeezenetpbq virtexpq vxt -bit xed point -input luts , , , , n/a pbq n/a % % % % pbq automated systolic array architecture for cnn d systolic array architecture, rooine model, automation ow design space exploration alexnetpaq arria gx -bitpq oat point paq -input luts opencl , , ,, , % % n/a % paq n/a n/a vgg pbq (-)-bitpq xed point pbq % % n/a % pbq pbq % % n/a % ,pbq end to end scalable fpga accelerator flexible and scalable resnet modules, conv loops opt., dataow opt., controlled execution ow of resnets layers resnet- arria gx -bit xed point -input luts verilog , , ,, , % % n/a % n/a n/a resnet- % % n/a % dlau pipelined processing units, tiling, fifo buffers dnn n/a zynq xcz -bit oat point -input luts n/a , , % % % % n/a ghz intel core n/a an automatic rtl compiler for high- throughput deep cnns library based rtl compiler, flexible and scalable cnn modules, layer combo computation, conv loops and dataow opt., controlled execution ow of cnn layers ninpaq stratix vpq gxa -bit xed point -input luts verilog , , , % % n/a % paq deepburning n/a % % n/a % pbq n/a vgg pbq % % n/a % pcq n/a % % n/a % pdq fp dnn resnet pcq arria pq gx -input luts , , ,, , % % n/a % paq n/a % % n/a % pbq caffeine pbaq resnet pdq % % n/a % pcq n/a % % n/a % pdq n/a alamo , modularized rtl compiler, loop unrolling, loop tiling alexnet stratix v gxa (-)-bit xed point -input luts verilog , , , % % n/a % rooine based fpga accelerator nin % % n/a % n/a angel eye quantization, parallel pes, compiler, on chip buffers vgg- zynq xcz -bit xed point -input luts n/a , , % % % % nn x zynq xcz -bit xed point , , % % % % optimizing the conv operation to accelerate dnns on fpga scalable set of data buses (bufpe), optimized conv module for different (kernel, stride) size congurations, flexible and scalable cnn modules, conv loops and dataow opt. ninpaq stratix vpq gxa -bit xed point -input luts verilog , , , % % n/a % paq n/a n/a % % n/a % pbq n/a vgg pbq % % n/a % pcq n/a % % n/a % pdq fp dnn resnet pcq arria pq gx -input luts , , ,, , % % n/a % paq n/a % % n/a % pbq n/a resnet pdq % % n/a % pcq n/a % % n/a % pdq n/a volume , shawahna et al.: fpga based accelerators of deep learning networks for learning and classication: a review maps in each layer, along with other operators. recent research has demonstrated that a large number of weights in fully connected layers could be eliminated with minimal impact on accuracy. in addition, although the suggested cnn structures by experts perform well for various appli- cations, the question arises whether the suggested structures could be optimized for performance with minimal impact on accuracy. since the designed cnn has a signicant impact on the complexity of its implementation, we review in this section some approaches attempting to optimize the design of cnns using metaheuristics. np hard combinatorial optimization problems ap- pear in the design of cnns. some examples of areas include design of cnn structures, selection of weights and bias values to improve accuracy, and determination of optimal values of variables to reduce run time. below, we briey touch upon some existing literature in these areas. a. cnn structure optimization in the design of cnns, the number of possible network structures increases exponentially with the number of layers. xie and yuille used genetic algorithm in learning deep network structures . the objective was to nd the best cnn structure that would minimize the error rate. the cost function was the cnn accuracy. they proposed an elegant encoding of chromosome using a xed length binary string to represent each network structure. a cnn string represents only the convolution layers. in each generation, using standard genetic operations new individuals are generated and weak ones eliminated. the quality of an individual was assessed by its recognition ac- curacy which is obtained via the time consuming operation of training the network, and evaluating it on a validation set. two small data sets were used (mnist and cifar-) to run the genetic implementation via which they demonstrated the discovery of new structures. b. cnn weights and bias values optimization an attempt to train cnns using metaheuristics (that is, determine weights and bias values) is presented in . the objective again was to improve accuracy and minimize the estimated error. the authors experiment with three metaheuristic algorithms, namely; simulated annealing, dif- ferential evolution, and harmony search. the algorihtms compute the values of weights and bias in the last layer. these values are used as the solution vector denoted by x which is to be optimized. the move comprised adding a small value of x to perturb the state. the cost function y is modeled as y n inpo uq n () where, o is the expected output, u is the real output, and n is the number of used samples. the stopping criterion is when the iteration count is reached or when the cost function goes below a pre specied value. c. cnn design variables optimization suda et al. presented a systematic methodology for design space exploration with the objective of maximizing the throughput of an opencl based fpga accelerator for a given cnn model (please see subsection iii c). fpga resource constraints such as on chip memory, registers, com- putational resources and external memory bandwidth are considered. the optimization problem comprises nding the best combination of nconv , sconv , nnorm, np ool, and nf c variables, where nconv is size of the lter (or neuron or kernel); sconv is the factor by which computational resources are vectorized to execute in a single instruction stream multiple data streams (simd) fashion; nnorm represents the number of normalization oper- ations performed in a single cycle; np ool is the number of parallel outputs of the pooling layer in a single cycle to achieve acceleration; and, nf c is the number of parallel multiply and accumulate (mac) operations preformed in a single work item within the fully connected layer. the objective function to be minimized is the run time (rt), and is given by t l i rtirnconv , sconv , nnorm, np ool, nf cs () subject to digital signal processing (dsp) slices, logic, and memory constraints, where tl represents the total number of cnn layers including the repeated layers. the convolu- tion layer run time (rt conv ) is analytically modeled as a function of design variables as rt conv i # of convolution opsi nconv sconv frequency () as for the other layers, that is, normalization, pooling, and fully connected, the following general model is proposed rt layeri # of layer opsi unroll factor frequency () the above analytical models are later validated by per- forming full synthesis at selective points and running them on the fpga accelerator. clearly, in order to determine the best values of the discussed design variables, exhaustive search, especially if the number of variables and or fpga resources is large, is infeasible. we have to resort to iterative non deterministic heuristics such as simulated annealing, simulated evolution, tabu search, genetic algorithm, particle swarm optimization, cuckoo search, etc., or any of the modern metaheuristics, to efciently traverse the search space to nd acceptable solutions. volume , shawahna et al.: fpga based accelerators of deep learning networks for learning and classication: a review the proposed methodology employing genetic algorithm was demonstrated by optimizing the implementation of two representative cnns, alexnet and vgg, on two altera stratix v fpga platforms, de net and p d boards, both of which have different hardware resources. peak performance is achieved for both, for the convolution oper- ations, and for the entire cnn network. one major issue related to use of non deterministic iter- ative heuristics in the design of neural networks and cnns is the large amount of memory required to store the state of solution and the amount of time taken to determine the cost of the solution, be it accuracy/error estimation, run time, or any other objective. reasonable estimation techniques and analytical formulations are required to efciently traverse the design space in search of efcient solutions. ",
        "Subsections": [],
        "Groundtruth": "This section provides an overview of key operations and terminology used in convolutional neural networks (CNNs) and explains how deep learning methods can benefit from the capabilities of field programmable gate arrays (FPGAs). The text outlines the process of convolution in CNNs, where a filter is slid over an input feature map to produce an output feature neuron value. It describes activation functions such as sigmoid, tanh, and ReLU, used to introduce non-linearity in neural networks. The section also covers normalization techniques like local response normalization (LRN) and pooling methods for spatial size reduction in CNNs.\n\nFurthermore, the text discusses the basic structure of FPGAs, emphasizing their flexibility in implementing custom hardware functions at a low development cost. It addresses challenges in implementing deep learning networks on FPGAs, including storage requirements, external memory bandwidth, and computational resources. Various techniques and architectures for accelerating deep learning networks on FPGAs are explored, such as convolutional network processors, deep residual networks, and custom hardware accelerators utilizing FPGA-specific optimizations.\n\nOverall, the text offers insights into how optimizing operations and leveraging FPGA capabilities can enhance the performance and efficiency of deep learning networks, particularly in the context of convolutional neural networks."
    },
    {
        "Section_Num": "III",
        "Section": "III Acceleration of Deep Learning Networks: Current Status",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "IV",
        "Section": "IV Metaheuristics in the Design of Convolutional Neural Networks",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "V",
        "Section": "V Summary and Recommendations",
        "Text": "in this section, we highlight the key features discussed in the acceleration of convolutional neural networks (cnns) implemented on fpgas, and provide recommendations to enhance the effectiveness of employing fpgas in the ac- celeration of cnns. all reviewed techniques are centered around accelerating the convolution (conv) operation as it consumes around % of the computational time. this is achieved by utiliz- ing parallel multiply accumulate operations bounded by re- source limitations. in addition, careful design of data access patterns are targeted to minimize the memory bandwidth requirements utilizing internal memory structures and max- imizing data reuse. this is crucial in the acceleration process due to the large memory data that needs to be accessed including feature maps (fms) and weights. to minimize the memory footprint and to achieve effective utilization of resources, some techniques optimize the number of bits used to represent the feature maps and weights with minimal impact on accuracy. this is combined with the optimized selection of the number of fraction bits used for each layer. other techniques optimize the number of used weights in the fully connected (fc) layers as they are memory intensive. coprocessors are also employed to automatically congure both the software and the hardware elements to fully exploit parallelism . to optimize parallelization of convolution operations, several approaches have been attempted. work load analysis has been tried to determine computations that can be struc- tured as parallel streams . the rooine model based accelerator uses polyhedral based data dependence analysis to nd the optimal unrolling factor for every convolutional layer , and to fully utilize all fpga computational resources through loop pipelining. to optimize performance, tiled matrix multiplication is structured as a pipelined binary adder tree for performing multiplication and generating partial sums . an optimization framework has been proposed by suda et al. who identied the key variables of the design and optimize them to maximize parallelism. to reduce computational complexity of conv layers and improve resource efciency, a number of approaches such as , , utilized winograd transformation in performing conv operations as this reduces the computa- tional complexity by around %. to maximize throughput, several techniques such as , , have used multiple conv layer pro- cessors (clps) instead of using a single clp that is opti- mized for all conv layers. this pipelines the operation of the multiple clps achieving layer level parallelism which maximizes resource utilization and enhances performance in comparison to using a single clp. since the computational requirement of fc layers is signicantly less than that of conv layers, to improve performance, and maximize resource utilization, a number of techniques such as , , , create batches by grouping different input fms and processing them together in fc layers. complex access patterns and data locality are used in deepburning tool for better data reuse. in , the authors explored hot spots proling to determine the computational parts that need to be accelerated to improve the performance. acceleration is accomplished by reducing the memory bandwidth requirements. techniques proposed exploit data reuse to reduce off chip memory communica- tions. loop transformations have also been used by reducing tiling parameters to improve data locality, and to reduce redundant communication operations to maximize the data sharing/reuse. efcient buffering, where the weight buffers are used to ensure the availability of conv and fc layers weights before their computation, as well as to overlap the transfer of fc layer weights with its computation, helps in improving performance , . in the catapult project, fpga boards were integrated into data center applications and achieved speedup. microsoft researchs catapult utilized multi banked input buffer and kernel weight buffer to pro- vide an efcient buffering scheme of feature maps and weights, respectively. to minimize the off chip memory trafc, a specialized network on chip was designed to re- distribute the output feature maps on the multi banked input buffer instead of transferring them to the external memory . to further reduce memory footprint and bandwidth re- quirement, optimal fractional length for weights and feature maps in each layer are used. singular value decomposition (svd) has also been applied to the weight matrix of fc layer in order to reduce memory footprint at this layer . tiling techniques have been proposed where large scale input data is partitioned into small subsets or tiles whose size is congured to leverage the trade off between the hardware cost and the speedup . automation tools have been developed that automatically build neural networks with optimized performance . they employ pre constructed register transfer level (rtl) module library that holds hardware (including logical and arithmetic operations) and conguration scripts. deepburn- volume , shawahna et al.: fpga based accelerators of deep learning networks for learning and classication: a review ing, for example, generates the hardware description for neural network scripts. another modularized rtl compiler, alamo, integrates both the rtl ner level optimization and the exibility of high level synthesis (hls) to generate efcient verilog parameterized rtl scripts for asic or fpga platform under the available number of parallel computing resources (i.e., the number of multipliers) , . acceleration is achieved by employing loop unrolling technique for conv layer operations. some of the reviewed techniques also help minimize the size of fpga on chip memories to optimize energy and area usage , . in table and table , we list the optimization mech- anisms utilized by each of the reviewed techniques to maximize performance and throughput of fpga based deep learning networks. to enhance utilization of fpgas in cnns acceleration and to maximize their effectiveness, we recommend the development of a framework that includes a user friendly interface that allows the user to easily specify the cnn model to be accelerated. this includes specifying the cnn model parameters in terms of number of convolution layers and their sizes, and number of fully connected layers along with other intermediate operations. the specied cnn model weights will be read from a le. in addition, the user should have the option of specifying the fpga platform that will be used for implementing the cnn accelerator and the maximum tolerable error, along with the selection of a library from a set of applications to be used for model optimization and evaluation. the framework then should perform optimizations to nd the minimum number of bits that need to be used for representing the weights and feature maps and the number of fraction bits to be used for each layer. in addition, optimization of fully connected layers is performed to minimize the memory requirements. all such optimizations are carried out bounded by the maximum error specied by the user for the specied application library. the framework should be designed based on the devel- opment of a scalable hardware architecture that works for any given fpga platform and achieves higher speedup with the availability of higher resources. based on the available resources, specied by the fpga platform, the tool will perform optimizations to maximize parallelism and data reuse, given the resource limitations. the tool will then automatically generate the cnn model that will t on the given fpga platform and will allow the user to evaluate the performance based on the chosen application library. this will allow the user to evaluate the performance gains while evaluating different fpga platforms with different resources. the tool should have the option to generate per- formance measures based on different performance metrics as selected by the user such as number of frames processed per second or number of operations performed per second. in addition, the tool will report other design metrics such as resource utilization, memory sizes and bandwidth, and power dissipation. furthermore, it is desired to have the option for the user to specify the desired performance for a given cnn model and have the tool perform necessary analysis and evaluation and recommend to the user candidate fpga platforms for achieving the desired performance levels. this will require the development of reasonably accurate analytical models that will estimate the needed resources for achieving the desired performance. the user can then choose the recom- mended fpga platform and perform complete evaluation to verify that the desired performance levels are met. ",
        "Subsections": [],
        "Groundtruth": "The text discusses techniques for accelerating convolutional neural networks (CNNs) on FPGAs, focusing on optimizing convolution operations by utilizing parallel multiply accumulate operations and minimizing memory bandwidth requirements. Various strategies such as optimizing data access patterns, reducing memory footprint, and using coprocessors are employed to enhance performance. Techniques like loop pipelining, tiled matrix multiplication, and Winograd transformation are utilized to improve resource efficiency and throughput. Recommendations include developing a user-friendly framework for specifying CNN models and FPGA platforms, performing optimizations to maximize parallelism and data reuse, and generating performance metrics for evaluation. The goal is to enhance FPGA utilization in CNN acceleration by optimizing hardware architectures and resources to achieve higher speedup and performance levels for different applications."
    },
    {
        "Section_Num": "VI",
        "Section": "VI Conclusion",
        "Text": "in this paper, we reviewed recent developments in the area of acceleration of deep learning networks and, in particular, convolution neural networks (cnns) on eld programmable gate arrays (fpgas). the paper begins with a brief overview of deep learning techniques highlighting their importance, key operations, and applications. special emphasis is given on cnns as they have wide applications in the area of image detection and recognition and require both cpu and memory intensive operations that can be effectively accelerated utilizing fpga inherent ability to maximize parallelism of operations. while the paper briey touches upon the acceleration techniques for deep learning algorithms and cnns from both software and hardware perspectives, the core of this article has been the review of recent techniques employed in the acceleration of cnns on fpgas. a thorough up to- date review is provided that illustrates the employment of various possibilities and techniques such as exploitation of parallelism utilizing loop tiling and loop unrolling, effective use of internal memory to maximize data reuse, operation pipelining, and effective use of data sizes to minimize mem- ory footprint, and, to optimize fpga resource utilization. the paper also presented the use of tools for generating register transfer level (rtl) scripts that not only help in automating the design process, but also help in exploring the design space and suggesting efcient hardware. the paper discusses the use of analytics such as: (i) work load analysis in determining the computations that can be parallelized, (ii) optimal loop unrolling factors, (iii) determining access patterns to improve data locality, etc. in addition, a brief review of the use of non deterministic heuristics in solving np hard combinatorial optimization problems in the design and implementation of cnns has been presented. finally, the paper summarizes the key features employed by the various fpga based cnn acceleration techniques and pro- vided recommendations for enhancing the effectiveness of utilizing fpgas in cnns acceleration. acknowledgment authors acknowledge king fahd university of petroleum & minerals, dhahran, saudi arabia for all support. we also like to acknowledge dr. blair p. bremberg and ms. sumaiya hussain sadiq for their help in professional english editing of this manuscript. volume , shawahna et al.: fpga based accelerators of deep learning networks for learning and classication: a review table optimization mechanisms employed for fpga based acceleration of deep learning networks. technique vip cnp conv coprocessor accelerator maple dc cnn neuflow memory centric accelerator nn x rooine based fpga accelerator embedded fpga accelerator deepburning opencl based fpga accelerator caffeine , fpgaconvnet loop unrolling loop tiling loop interchange pipelining batching multi clps fixed point precision per layer quantization singular value decomposition prefetching rearranging memory data in memory processing line buffer double buffering approximating non linear af eliminating fc layer rooine model polyhedral optimization dynamic programming graph partitioning volume , shawahna et al.: fpga based accelerators of deep learning networks for learning and classication: a review table optimization mechanisms employed for fpga based acceleration of deep learning networks. technique alamo , throughput- optimized fpga accelerator fp dnn finn customized conv loop accelerator latency driven design for fpga based cnns dla winograd- based cnn accelerator opencl based architecture for accelerating cnns multi clp accelerator for cnns automated systolic array architecture for cnn end to end scalable fpga accelerator dlau an automatic rtl compiler for high throughput deep cnns intels dla angel eye optimizing the conv operation to accelerate dnns on fpga loop unrolling loop tiling loop interchange pipelining input batching fc layer batching multi clps binarized cnn fixed point precision per layer quantization prefetching rearranging memory data line buffer double buffering padding optimizations winograd algorithm approximating non linear af rooine model polyhedral optimization dynamic programming graph coloring graph partitioning pattern matching volume , shawahna et al.: fpga based accelerators of deep learning networks for learning and classication: a review ",
        "Subsections": [],
        "Groundtruth": "Recent developments in accelerating deep learning networks, focusing on convolutional neural networks (CNNs) on field programmable gate arrays (FPGAs), were reviewed. The paper highlighted the importance of deep learning techniques, particularly CNNs, for image detection and recognition. Techniques for accelerating CNNs on FPGAs were discussed, including parallelism utilization, loop tiling and unrolling, memory reuse optimization, pipelining, and resource utilization. The paper also covered tool usage for RTL script generation and the use of analytics for design optimization. Non-deterministic heuristics for solving complex optimization problems were mentioned. Various FPGA-based CNN acceleration techniques and recommendations for enhancing FPGA utilization in CNN acceleration were summarized."
    },
    {
        "Section_Num": "REFERENCES",
        "Section": "REFERENCES",
        "Text": "",
        "Subsections": [
            {
                "Section_Num": "NA",
                "Section": "NA",
                "Text": "] y. bengio et al., learning deep architectures for ai, foundations and trends in machine learning, vol. , no. , pp. , j. schmidhuber, deep learning in neural networks: an overview, neu- ral networks, vol. , pp. , i. goodfellow, y. bengio, a. courville, and y. bengio, deep learning. mit press cambridge, , vol. l. zhang, s. wang, and b. liu, deep learning for sentiment analysis: a survey, wiley interdisciplinary reviews: data mining and knowledge discovery, p. e, d. e. rumelhart, g. e. hinton, and r. j. williams, learning represen- tations by back propagating errors, nature, vol. , no. , p. , rumelhart, david e and hinton, geoffrey e and williams, ronald j, neurocomputing: foundations of research, ch. learning representa- tions by back propagating errors, pp. , m. a. nielsen, neural networks and deep learning. determination press, usa, , vol. t. weyand, i. kostrikov, and j. philbin, planet photo geolocation with convolutional neural networks, in european conference on computer vision. springer, , pp. mathworks. what is deep learning? . available: https://www. mathworks.com/discovery/deep learning.html/, , y. lecun, y. bengio, and g. hinton, deep learning, nature, vol. , no. , p. , a. deshpande. a beginners guide to understanding convolutional neural networks . available: https://adeshpandegithub.io/a- beginner%s guide to understanding convolutional neural- networks/, , j. e. dayhoff, neural network architectures: an introduction. van nostrand reinhold new york, y. lecun and y. bengio, convolutional networks for images, speech, and time series, the handbook of brain theory and neural networks, vol. , no. , p. , j. hauswald, y. kang, m. a. laurenzano, q. chen, c. li, t. mudge, r. g. dreslinski, j. mars, and l. tang, djinn and tonic: dnn as a service and its implications for future warehouse scale computers, in acm sigarch computer architecture news, vol. , no. acm, , pp. j. yue hei ng, m. hausknecht, s. vijayanarasimhan, o. vinyals, r. monga, and g. toderici, beyond short snippets: deep networks for video classication, in proceedings of the ieee conference on computer vision and pattern recognition, , pp. y. lecun, b. e. boser, j. s. denker, d. henderson, r. e. howard, w. e. hubbard, and l. d. jackel, handwritten digit recognition with a back- propagation network, in advances in neural information processing systems, , pp. p. barros, s. magg, c. weber, and s. wermter, a multichannel con- volutional neural network for hand posture recognition, in international conference on articial neural networks. springer, , pp. a. graves, a.-r. mohamed, and g. hinton, speech recognition with deep recurrent neural networks, in acoustics, speech and signal processing (icassp), ieee international conference on. ieee, , pp. p.-s. huang, x. he, j. gao, l. deng, a. acero, and l. heck, learning deep structured semantic models for web search using clickthrough data, in proceedings of the nd acm international conference on conference on information & knowledge management. acm, , pp. o. abdel hamid, a.-r. mohamed, h. jiang, l. deng, g. penn, and d. yu, convolutional neural networks for speech recognition, ieee/acm transactions on audio, speech, and language processing, vol. , no. , pp. , p. y. simard, d. steinkraus, and j. c. platt, best practices for convo- lutional neural networks applied to visual document analysis, in null. ieee, , p. s. lai, l. xu, k. liu, and j. zhao, recurrent convolutional neural networks for text classication. in aaai, vol. , , pp. y. kim, convolutional neural networks for sentence classication, arxiv preprint arxiv:, r. collobert and j. weston, a unied architecture for natural language processing: deep neural networks with multitask learning, in proceed- ings of the th international conference on machine learning. acm, , pp. r. sarikaya, g. e. hinton, and a. deoras, application of deep belief networks for natural language understanding, ieee/acm transactions on audio, speech and language processing (taslp), vol. , no. , pp. , a. karpathy, g. toderici, s. shetty, t. leung, r. sukthankar, and l. fei fei, large scale video classication with convolutional neural networks, in proceedings of the ieee conference on computer vision and pattern recognition, , pp. j. mutch and d. g. lowe, multiclass object recognition with sparse, localized features, in computer vision and pattern recognition, ieee computer society conference on, vol. ieee, , pp. a. krizhevsky, i. sutskever, and g. e. hinton, imagenet classication with deep convolutional neural networks, in advances in neural infor- mation processing systems, , pp. , , , , k. simonyan and a. zisserman, very deep convolutional networks for large scale image recognition, arxiv preprint arxiv:, , , o. russakovsky, j. deng, h. su, j. krause, s. satheesh, s. ma, z. huang, a. karpathy, a. khosla, and m. bernstein, imagenet large scale visual recognition challenge, international journal of computer vision, vol. , no. , pp. , c. szegedy, w. liu, y. jia, p. sermanet, s. reed, d. anguelov, d. erhan, v. vanhoucke, and a. rabinovich, going deeper with convolutions, arxiv preprint arxiv:, vol. , , s. ren, k. he, r. girshick, and j. sun, faster r cnn: towards real time object detection with region proposal networks, in advances in neural information processing systems, , pp. k. korekado, t. morie, o. nomura, t. nakano, m. matsugu, and a. iwata, an image ltering processor for face/object recognition us- ing merged/mixed analog digital architecture, in vlsi circuits, digest of technical papers. symposium on. ieee, , pp. h. li, z. lin, x. shen, j. brandt, and g. hua, a convolutional neural network cascade for face detection, in proceedings of the ieee confer- ence on computer vision and pattern recognition, , pp. u. muller, j. ben, e. cosatto, b. flepp, and y. l. cun, off road obstacle avoidance through end to end learning, in advances in neural information processing systems, , pp. r. hadsell, a. erkan, p. sermanet, j. ben, k. kavukcuoglu, u. muller, and y. lecun, a multi range vision strategy for autonomous offroad navigation, proc. robotics and applications (ra), vol. , no. , p. sermanet, r. hadsell, m. scofer, m. grimes, j. ben, a. erkan, c. crudele, u. miller, and y. lecun, a multirange architecture for collision free off road robot navigation, journal of field robotics, vol. , no. , pp. , b. blanco filgueira, d. garca lesta, m. fernndez sanjurjo, v. m. brea, and p. lpez, deep learning based multiple object visual tracking on embedded system for iot and mobile edge computing applications, arxiv preprint arxiv:, p. d. mcnelis, neural networks in nance: gaining predictive edge in the market. academic press, p. j. lisboa and e. c. ifeachor, articial neural networks in biomedicine. springer science & business media, p. w. mirowski, y. lecun, d. madhavan, and r. kuzniecky, comparing svm and convolutional networks for epileptic seizure prediction from intracranial eeg, in machine learning for signal processing, mlsp ieee workshop on. ieee, , pp. g. e. dahl, t. n. sainath, and g. e. hinton, improving deep neural networks for lvcsr using rectied linear units and dropout, in acous- tics, speech and signal processing (icassp), ieee international conference on. ieee, , pp. r. hadsell, p. sermanet, j. ben, a. erkan, m. scofer, k. kavukcuoglu, u. muller, and y. lecun, learning long range vision for autonomous off road driving, journal of field robotics, vol. , no. , pp. , volume , shawahna et al.: fpga based accelerators of deep learning networks for learning and classication: a review l. deng, d. yu et al., deep learning: methods and applications, foundations and trends in signal processing, vol. , no. , pp. , r. girshick, j. donahue, t. darrell, and j. malik, rich feature hi- erarchies for accurate object detection and semantic segmentation, in proceedings of the ieee conference on computer vision and pattern recognition, , pp. b. wu, f. n. iandola, p. h. jin, and k. keutzer, squeezedet: unied, small, low power fully convolutional neural networks for real time object detection for autonomous driving. in cvpr workshops, , pp. k. he, x. zhang, s. ren, and j. sun, delving deep into rectiers: surpassing human level performance on imagenet classication, in pro- ceedings of the ieee international conference on computer vision, , pp. m. d. zeiler and r. fergus, visualizing and understanding convolu- tional networks, in european conference on computer vision. springer, , pp. , k. he, x. zhang, s. ren, and j. sun, deep residual learning for image recognition, in proceedings of the ieee conference on computer vision and pattern recognition, , pp. , , , image net. the imagenet large scale visual recognition challenge (ilsvrc) . available: http://image net.org/challenges/lsvrc/, a.-r. mohamed, g. e. dahl, g. hinton et al., acoustic modeling using deep belief networks, ieee trans. audio, speech & language process- ing, vol. , no. , pp. , o. nomura and t. morie, projection eld type vlsi convolutional neural networks using merged/mixed analog digital approach, in inter- national conference on neural information processing. springer, , pp. t. m. chilimbi, y. suzue, j. apacible, and k. kalyanaraman, project adam: building an efcient and scalable deep learning training system. in osdi, vol. , , pp. y. lecun, b. boser, j. s. denker, d. henderson, r. e. howard, w. hub- bard, and l. d. jackel, backpropagation applied to handwritten zip code recognition, neural computation, vol. , no. , pp. , c. zhang, p. li, g. sun, y. guan, b. xiao, and j. cong, optimizing fpga based accelerator design for deep convolutional neural networks, in proceedings of the acm/sigda international symposium on field programmable gate arrays. acm, , pp. , , , , , , , , , , , , a. yazdanbakhsh, j. park, h. sharma, p. lot kamran, and h. es- maeilzadeh, neural acceleration for gpu throughput processors, in proceedings of the th international symposium on microarchitecture. acm, , pp. g. hinton, l. deng, d. yu, g. e. dahl, a.-r. mohamed, n. jaitly, a. senior, v. vanhoucke, p. nguyen, t. n. sainath et al., deep neural networks for acoustic modeling in speech recognition: the shared views of four research groups, ieee signal processing magazine, vol. , no. , pp. , y. jia, e. shelhamer, j. donahue, s. karayev, j. long, r. girshick, s. guadarrama, and t. darrell, caffe: convolutional architecture for fast feature embedding, in proceedings of the nd acm international conference on multimedia. acm, , pp. , , a. vasudevan, a. anderson, and d. gregg, parallel multi channel convolution using general matrix multiplication, in application specic systems, architectures and processors (asap), ieee th interna- tional conference on. ieee, , pp. k. guo, l. sui, j. qiu, j. yu, j. wang, s. yao, s. han, y. wang, and h. yang, angel eye: a complete design ow for mapping cnn onto embedded fpga, ieee transactions on computer aided design of integrated circuits and systems, vol. , no. , pp. , , , , e. nurvitadhi, g. venkatesh, j. sim, d. marr, r. huang, j. ong gee hock, y. t. liew, k. srivatsan, d. moss, s. subhaschandra et al., can fpgas beat gpus in accelerating next generation deep neural networks? in proceedings of the acm/sigda international sym- posium on field programmable gate arrays. acm, , pp. j. misra and i. saha, articial neural networks in hardware: a survey of two decades of progress, neurocomputing, vol. , no. -, pp. , h. esmaeilzadeh, a. sampson, l. ceze, and d. burger, neural acceler- ation for general purpose approximate programs, in proceedings of the th annual ieee/acm international symposium on microarchi- tecture. ieee computer society, , pp. s. han, x. liu, h. mao, j. pu, a. pedram, m. a. horowitz, and w. j. dally, eie: efcient inference engine on compressed deep neural net- work, in computer architecture (isca), acm/ieee rd annual international symposium on. ieee, , pp. l. du, y. du, y. li, j. su, y.-c. kuan, c.-c. liu, and m.-c. f. chang, a recongurable streaming deep convolutional neural network accelerator for internet of things, ieee transactions on circuits and systems i: regular papers, vol. , no. , pp. , w. vanderbauwhede and k. benkrid, high performance computing using fpgas. springer, a. putnam, a. m. cauleld, e. s. chung, d. chiou, k. constantinides, j. demme, h. esmaeilzadeh, j. fowers, g. p. gopal, j. gray et al., a recongurable fabric for accelerating large scale datacenter services, acm sigarch computer architecture news, vol. , no. , pp. , , y. liang, k. rupnow, y. li, d. min, m. n. do, and d. chen, high level synthesis: productivity, performance, and software constraints, journal of electrical and computer engineering, vol. , p. , j. cong, b. liu, s. neuendorffer, j. noguera, k. vissers, and z. zhang, high level synthesis for fpgas: from prototyping to deployment, ieee transactions on computer aided design of integrated circuits and sys- tems, vol. , no. , pp. , a. canis, j. choi, m. aldham, v. zhang, a. kammoona, j. h. an- derson, s. brown, and t. czajkowski, legup: high level synthesis for fpga based processor/accelerator systems, in proceedings of the th acm/sigda international symposium on field programmable gate ar- rays. acm, , pp. y. lecun, l. bottou, y. bengio, and p. haffner, gradient based learning applied to document recognition, proceedings of the ieee, vol. , no. , pp. , , , r. hameed, w. qadeer, m. wachs, o. azizi, a. solomatnikov, b. c. lee, s. richardson, c. kozyrakis, and m. horowitz, understanding sources of inefciency in general purpose chips, in acm sigarch computer architecture news, vol. , no. acm, , pp. s. w. keckler, w. j. dally, b. khailany, m. garland, and d. glasco, gpus and the future of parallel computing, ieee micro, vol. , no. , pp. , y.-h. chen, j. emer, and v. sze, eyeriss: a spatial architecture for energy efcient dataow for convolutional neural networks, in acm sigarch computer architecture news, vol. , no. ieee press, , pp. , t. serre, l. wolf, s. bileschi, m. riesenhuber, and t. poggio, robust object recognition with cortex like mechanisms, ieee transactions on pattern analysis & machine intelligence, no. , pp. , p. joshi. what is local response normalization in convolutional neural networks . available: https://prateekvjoshi.com////what is local response- normalization in convolutional neural networks/, j. cong and b. xiao, minimizing computation in convolutional neural networks, in international conference on articial neural networks. springer, , pp. , y. ma, n. suda, y. cao, j.-s. seo, and s. vrudhula, scalable and mod- ularized rtl compilation of convolutional neural networks onto fpga, in field programmable logic and applications (fpl), th inter- national conference on. ieee, , pp. , , , , , , d. f. bacon, s. l. graham, and o. j. sharp, compiler transformations for high performance computing, acm computing surveys (csur), vol. , no. , pp. , , , n. suda, v. chandra, g. dasika, a. mohanty, y. ma, s. vrudhula, j.-s. seo, and y. cao, throughput optimized opencl based fpga accelerator for large scale convolutional neural networks, in proceedings of the acm/sigda international symposium on field programmable gate arrays. acm, , pp. , , , , , , , , , , , , , m. denil, b. shakibi, l. dinh, n. de freitas et al., predicting parameters in deep learning, in advances in neural information processing systems, , pp. , volume , shawahna et al.: fpga based accelerators of deep learning networks for learning and classication: a review v. nair and g. e. hinton, rectied linear units improve restricted boltz- mann machines, in proceedings of the th international conference on machine learning (icml-), , pp. y. ma, y. cao, s. vrudhula, and j.-s. seo, optimizing loop operation and dataow in fpga acceleration of deep convolutional neural networks, in proceedings of the acm/sigda international symposium on field programmable gate arrays. acm, , pp. , , , , , , , a. karpathy. convolutional neural networks for visual recognition . available: http://csn.github.io/convolutional networks/, k. he, x. zhang, s. ren, and j. sun, identity mappings in deep residual networks, in european conference on computer vision. springer, , pp. c. szegedy, s. ioffe, v. vanhoucke, and a. a. alemi, inception v, inception resnet and the impact of residual connections on learning. in aaai, vol. , , p. j. villasenor and w. h. mangione smith, congurable computing, scientic american, vol. , no. , pp. , , s. d. brown, r. j. francis, j. rose, and z. g. vranesic, field- programmable gate arrays. springer science & business media, , vol. m. c. herbordt, y. gu, t. vancourt, j. model, b. sukhwani, and m. chiu, computing models for fpga based accelerators, computing in science & engineering, vol. , no. , pp. , b. s. c. varma, k. paul, and m. balakrishnan, architecture exploration of fpga based accelerators for bioinformatics applications. springer, g. lacey, g. w. taylor, and s. areibi, deep learning on fpgas: past, present, and future, arxiv preprint arxiv:, c. farabet, y. lecun, k. kavukcuoglu, e. culurciello, b. martini, p. ak- selrod, and s. talay, large scale fpga based convolutional networks, scaling up machine learning: parallel and distributed approaches, pp. , a. munshi, the opencl specication, in hot chips symposium (hcs), ieee. ieee, , pp. j. e. stone, d. gohara, and g. shi, opencl: a parallel programming standard for heterogeneous computing systems, computing in science & engineering, vol. , no. , pp. , a. r. omondi and j. c. rajapakse, fpga implementations of neural networks. springer, , vol. h. m. waidyasooriya, m. hariyama, and k. uchiyama, design of fpga- based computing systems with opencl. springer, v. sze, y.-h. chen, j. emer, a. suleiman, and z. zhang, hardware for machine learning: challenges and opportunities, in custom integrated circuits conference (cicc), ieee. ieee, , pp. j. qiu, j. wang, s. yao, k. guo, b. li, e. zhou, j. yu, t. tang, n. xu, and s. song, going deeper with embedded fpga platform for convolutional neural network, in proceedings of the acm/sigda international symposium on field programmable gate arrays. acm, , pp. , , , , , , , s. han, j. kang, h. mao, y. hu, x. li, y. li, d. xie, h. luo, s. yao, y. wang et al., ese: efcient speech recognition engine with sparse lstm on fpga, in proceedings of the acm/sigda international symposium on field programmable gate arrays. acm, , pp. s. chakradhar, m. sankaradas, v. jakkula, and s. cadambi, a dynam- ically congurable coprocessor for convolutional neural networks, in acm sigarch computer architecture news, vol. , no. acm, , pp. , , , , , c. f. van loan, matrix computations (johns hopkins studies in mathe- matical sciences), , , e. l. denton, w. zaremba, j. bruna, y. lecun, and r. fergus, exploit- ing linear structure within convolutional networks for efcient evalua- tion, in advances in neural information processing systems, , pp. g. guennebaud, b. jacob, m. lenz et al., eigen v, , url http://eigen. tuxfamily. org, s. han, j. pool, j. tran, and w. dally, learning both weights and con- nections for efcient neural network, in advances in neural information processing systems, , pp. y. lecun, j. s. denker, and s. a. solla, optimal brain damage, in advances in neural information processing systems, , pp. s. j. hanson and l. y. pratt, comparing biases for minimal network construction with back propagation, in advances in neural information processing systems, , pp. b. hassibi and d. g. stork, second order derivatives for network pruning: optimal brain surgeon, in advances in neural information processing systems, , pp. s. han, h. mao, and w. j. dally, deep compression: compressing deep neural networks with pruning, trained quantization and huffman coding, arxiv preprint arxiv:, t. chen, z. du, n. sun, j. wang, c. wu, y. chen, and o. temam, diannao: a small footprint high throughput accelerator for ubiquitous machine learning, acm sigplan notices, vol. , no. , pp. , , y. lecun, the mnist database of handwritten digits, http://yann. lecun. com/exdb/mnist/, y. chen, t. luo, s. liu, s. zhang, l. he, j. wang, l. li, t. chen, z. xu, n. sun et al., dadiannao: a machine learning supercomputer, in proceedings of the th annual ieee/acm international symposium on microarchitecture. ieee computer society, , pp. , t. luo, s. liu, l. li, y. wang, s. zhang, t. chen, z. xu, o. temam, and y. chen, dadiannao: a neural network supercomputer, ieee transactions on computers, vol. , no. , pp. , , d. liu, t. chen, s. liu, j. zhou, s. zhou, o. teman, x. feng, x. zhou, and y. chen, pudiannao: a polyvalent machine learning accelerator, in acm sigarch computer architecture news, vol. , no. acm, , pp. , z. du, r. fasthuber, t. chen, p. ienne, l. li, t. luo, x. feng, y. chen, and o. temam, shidiannao: shifting vision processing closer to the sensor, in acm sigarch computer architecture news, vol. , no. acm, , pp. v. sze, y.-h. chen, t.-j. yang, and j. s. emer, efcient processing of deep neural networks: a tutorial and survey, proceedings of the ieee, vol. , no. , pp. , a. shaee, a. nag, n. muralimanohar, r. balasubramonian, j. p. stra- chan, m. hu, r. s. williams, and v. srikumar, isaac: a convolutional neural network accelerator with in situ analog arithmetic in crossbars, acm sigarch computer architecture news, vol. , no. , pp. , p. chi, s. li, c. xu, t. zhang, j. zhao, y. liu, y. wang, and y. xie, prime: a novel processing in memory architecture for neural network computation in reram based main memory, in acm sigarch com- puter architecture news, vol. , no. ieee press, , pp. w. lu, g. yan, j. li, s. gong, y. han, and x. li, flexow: a exible dataow accelerator architecture for convolutional neural networks, in high performance computer architecture (hpca), ieee interna- tional symposium on. ieee, , pp. j. cloutier, e. cosatto, s. pigeon, f. r. boyer, and p. y. simard, vip: an fpga based processor for image processing and neural networks, in microelectronics for neural networks, , proceedings of fifth international conference on. ieee, , pp. , d. f. wolf, r. a. romero, and e. marques, using embedded proces- sors in hardware models of articial neural networks, in v simposio brasileiro de automao inteligente, brasil, k. r. nichols, m. a. moussa, and s. m. areibi, feasibility of oating- point arithmetic in fpga based articial neural networks, in in caine. citeseer, k. benkrid and s. belkacemi, design and implementation of a d convolution core for video applications on fpgas, in digital and computational video, dcv proceedings. third international workshop on. ieee, , pp. f. cardells tormo and p.-l. molinet, area efcient -d shift variant convolvers for fpga based digital image processing, in signal pro- cessing systems design and implementation, ieee workshop on. ieee, , pp. r. g. girons, r. c. palero, j. c. boluda, and a. s. corts, fpga implementation of a pipelined on line backpropagation, journal of vlsi signal processing systems for signal, image and video technology, vol. , no. , pp. , h. zhang, m. xia, and g. hu, a multiwindow partial buffering scheme for fpga based -d convolvers, ieee transactions on circuits and systems ii: express briefs, vol. , no. , pp. , volume , shawahna et al.: fpga based accelerators of deep learning networks for learning and classication: a review a. w. savich, m. moussa, and s. areibi, the impact of arithmetic representation on implementing mlp bp on fpgas: a study, ieee transactions on neural networks, vol. , no. , pp. , c. farabet, c. poulet, j. y. han, and y. lecun, cnp: an fpga based processor for convolutional networks, in field programmable logic and applications, fpl international conference on. ieee, , pp. , , , , y. lecun et al., lenet-, convolutional neural networks, url: http://yann. lecun. com/exdb/lenet, p. , m. sankaradas, v. jakkula, s. cadambi, s. chakradhar, i. durdanovic, e. cosatto, and h. p. graf, a massively parallel coprocessor for convo- lutional neural networks, in application specic systems, architectures and processors, asap th ieee international conference on. ieee, , pp. , , , h. p. graf, s. cadambi, v. jakkula, m. sankaradass, e. cosatto, s. chakradhar, and i. dourdanovic, a massively parallel digital learning processor, in advances in neural information processing systems, , pp. s. cadambi, i. durdanovic, v. jakkula, m. sankaradass, e. cosatto, s. chakradhar, and h. p. graf, a massively parallel fpga based co- processor for support vector machines, in th ieee symposium on field programmable custom computing machines. ieee, , pp. s. cadambi, a. majumdar, m. becchi, s. chakradhar, and h. p. graf, a programmable parallel accelerator for learning and classication, in proceedings of the th international conference on parallel architectures and compilation techniques. acm, , pp. , , , , j. c. platt, fast training of support vector machines using sequential minimal optimization, advances in kernel methods, pp. , b. bai, j. weston, d. grangier, r. collobert, k. sadamasa, y. qi, o. chapelle, and k. weinberger, learning to rank with (a lot of) word features, information retrieval, vol. , no. , pp. , j. macqueen et al., some methods for classication and analysis of mul- tivariate observations, in proceedings of the fth berkeley symposium on mathematical statistics and probability, vol. , no. oakland, ca, usa, , pp. a. sato and k. yamada, generalized learning vector quantization, in advances in neural information processing systems, , pp. s. lawrence, c. l. giles, a. c. tsoi, and a. d. back, face recognition: a convolutional neural network approach, ieee transactions on neural networks, vol. , no. , pp. , , k. chellapilla, s. puri, and p. simard, high performance convolutional neural networks for document processing, in tenth international work- shop on frontiers in handwriting recognition. suvisoft, , f. nasse, c. thurau, and g. a. fink, face detection using gpu based convolutional neural networks, in international conference on computer analysis of images and patterns. springer, , pp. , j. d. dixon, asymptotically fast factorization of integers, mathematics of computation, vol. , no. , pp. , p. l. montgomery, a survey of modern integer factorization algorithms, cwi quarterly, vol. , no. , pp. , c. farabet, b. martini, p. akselrod, s. talay, y. lecun, and e. culur- ciello, hardware accelerated convolutional neural networks for synthetic vision systems, in circuits and systems (iscas), proceedings of ieee international symposium on. ieee, , pp. c. farabet, b. martini, b. corda, p. akselrod, e. culurciello, and y. le- cun, neuow: a runtime recongurable dataow processor for vision, in computer vision and pattern recognition workshops (cvprw), ieee computer society conference on. ieee, , pp. , , r. collobert, c. farabet, k. kavukcuoglu et al., torch, in workshop on machine learning open source software, nips, vol. , d. grangier, l. bottou, and r. collobert, deep convolutional networks for scene parsing, in icml deep learning workshop, vol. , no. citeseer, , p. , m. peemen, a. a. setio, b. mesman, and h. corporaal, memory centric accelerator design for convolutional neural networks, in computer de- sign (iccd), ieee st international conference on. ieee, , pp. , , , a. beric, j. van meerbergen, g. de haan, and r. sethuraman, memory- centric video processing, ieee transactions on circuits and systems for video technology, vol. , no. , pp. , , v. gokhale, j. jin, a. dundar, b. martini, and e. culurciello, a g ops/s mobile coprocessor for deep neural networks, in proceedings of the ieee conference on computer vision and pattern recognition workshops, , pp. , , , , c. farabet, c. poulet, and y. lecun, an fpga based stream processor for embedded real time vision with convolutional networks, in com- puter vision workshops (iccv workshops), ieee th interna- tional conference on. ieee, , pp. s. williams, a. waterman, and d. patterson, rooine: an insightful visual performance model for multicore architectures, communications of the acm, vol. , no. , pp. , , l.-n. pouchet, p. zhang, p. sadayappan, and j. cong, polyhedral based data reuse optimization for congurable computing, in proceedings of the acm/sigda international symposium on field programmable gate arrays. acm, , pp. k. ovtcharov, o. ruwase, j.-y. kim, j. fowers, k. strauss, and e. s. chung, accelerating deep convolutional neural networks using special- ized hardware, microsoft research whitepaper, vol. , no. , , , , , c. zhang, g. sun, z. fang, p. zhou, p. pan, and j. cong, caffeine: towards uniformed representation and acceleration for deep convolu- tional neural networks, ieee transactions on computer aided design of integrated circuits and systems, , , , , b. bosi, g. bois, and y. savaria, recongurable pipelined -d con- volvers for fast digital signal processing, ieee transactions on very large scale integration (vlsi) systems, vol. , no. , pp. , , , y. wang, j. xu, y. han, h. li, and x. li, deepburning: automatic generation of fpga based learning accelerators for the neural network family, in proceedings of the rd annual design automation confer- ence. acm, , p. , , , , , k. o. w. group et al., the opencl specication version , http://www. khronos. org/registry/cl/specs/opencl-. pdf, m. s. abdelfattah, a. hagiescu, and d. singh, gzip on a chip: high performance lossless data compression on fpgas using opencl, in proceedings of the international workshop on opencl & acm, , p. altera. opencl design examples . available: https://www.altera.com/support/support resources/designexamples/ design software/opencl.html/, nallatech. p d opencl fpga accelerator cards . avail- able: http://www.nallatech.com/wp content/uploads/openclcardspb_v_ pdf/, altera. de net fpga kit user manual . available: ftp://ftp. altera.com/up/pub/altera_material/boards/de/de_user_/, r. c. whaley and j. j. dongarra, automatically tuned linear algebra software, in supercomputing, sc ieee/acm conference on. ieee, , pp. c. zhang, z. fang, p. zhou, p. pan, and j. cong, caffeine: towards uniformed representation and acceleration for deep convolutional neural networks, in computer aided design (iccad), ieee/acm in- ternational conference on. ieee, , pp. , , , , , w. zuo, y. liang, p. li, k. rupnow, d. chen, and j. cong, improving high level synthesis optimization opportunity through polyhedral trans- formations, in proceedings of the acm/sigda international sympo- sium on field programmable gate arrays. acm, , pp. e. a. lee and d. g. messerschmitt, synchronous data ow, proceed- ings of the ieee, vol. , no. , pp. , s. i. venieris and c.-s. bouganis, fpgaconvnet: a framework for map- ping convolutional neural networks on fpgas, in field programmable custom computing machines (fccm), ieee th annual inter- national symposium on. ieee, , pp. , , , , , c. r. reeves, modern heuristic techniques for combinatorial problems. advanced topics in computer science. mc graw hill, l. cavigelli, m. magno, and l. benini, accelerating real time embed- ded scene labeling with convolutional networks, in proceedings of the nd annual design automation conference. acm, , p. , volume , shawahna et al.: fpga based accelerators of deep learning networks for learning and classication: a review y. ma, n. suda, y. cao, s. vrudhula, and j.-s. seo, alamo: fpga acceleration of deep learning algorithms with a modularized rtl compiler, integration, , , , , , , m. lin, q. chen, and s. yan, network in network, arxiv preprint arxiv:, z. liu, y. dou, j. jiang, j. xu, s. li, y. zhou, and y. xu, throughput optimized fpga accelerator for deep convolutional neural networks, acm transactions on recongurable technology and sys- tems (trets), vol. , no. , p. , , , , , , y. guan, h. liang, n. xu, w. wang, s. shi, x. chen, g. sun, w. zhang, and j. cong, fp dnn: an automated framework for mapping deep neural networks onto fpgas with rtl hls hybrid templates, in field- programmable custom computing machines (fccm), ieee th annual international symposium on. ieee, , pp. , , m. abadi, p. barham, j. chen, z. chen, a. davis, j. dean, m. devin, s. ghemawat, g. irving, m. isard et al., tensorow: a system for large- scale machine learning. in osdi, vol. , , pp. m. h. alsuwaiyel, algorithms: design techniques and analysis (re- vised edition). world scientic, , vol. s. chetlur, c. woolley, p. vandermersch, j. cohen, j. tran, b. catanzaro, and e. shelhamer, cudnn: efcient primitives for deep learning, arxiv preprint arxiv:, w. zaremba, i. sutskever, and o. vinyals, recurrent neural network regularization, arxiv preprint arxiv:, w. sung, s. shin, and k. hwang, resiliency of deep neural networks under quantization, arxiv preprint arxiv:, m. rastegari, v. ordonez, j. redmon, and a. farhadi, xnor net: im- agenet classication using binary convolutional neural networks, in european conference on computer vision. springer, , pp. m. kim and p. smaragdis, bitwise neural networks, arxiv preprint arxiv:, s. zhou, y. wu, z. ni, x. zhou, h. wen, and y. zou, dorefa net: training low bitwidth convolutional neural networks with low bitwidth gradients, arxiv preprint arxiv:, m. courbariaux and y. bengio, binarynet: training deep neural net- works with weights and activations constrained to+ or- , , y. umuroglu, n. j. fraser, g. gambardella, m. blott, p. leong, m. jahre, and k. vissers, finn: a framework for fast, scalable binarized neural network inference, in proceedings of the acm/sigda interna- tional symposium on field programmable gate arrays. acm, , pp. , , a. krizhevsky and g. hinton, learning multiple layers of features from tiny images, citeseer, tech. rep., s. i. venieris and c.-s. bouganis, latency driven design for fpga- based convolutional neural networks, in field programmable logic and applications (fpl), th international conference on. ieee, , pp. , , a. lavin and s. gray, fast algorithms for convolutional neural net- works, in proceedings of the ieee conference on computer vision and pattern recognition, , pp. , s. winograd, arithmetic complexity of computations. siam, , vol. , c. van loan, computational frameworks for the fast fourier transform. siam, , vol. c. zhang and v. prasanna, frequency domain acceleration of convo- lutional neural networks on cpu fpga shared memory system, in pro- ceedings of the acm/sigda international symposium on field- programmable gate arrays. acm, , pp. u. aydonat, s. oconnell, d. capalija, a. c. ling, and g. r. chiu, an opencldeep learning accelerator on arria , in proceedings of the acm/sigda international symposium on field programmable gate arrays. acm, , pp. , , , , l. lu, y. liang, q. xiao, and s. yan, evaluating fast algorithms for convolutional neural networks on fpgas, in field programmable custom computing machines (fccm), ieee th annual international symposium on. ieee, , pp. , , , j. zhang and j. li, improving the performance of opencl based fpga accelerator for convolutional neural network, in proceedings of the acm/sigda international symposium on field programmable gate arrays. acm, , pp. , , t. s. czajkowski, d. neto, m. kinsner, u. aydonat, j. wong, d. denisenko, p. yiannacouras, j. freeman, d. p. singh, and s. d. brown, opencl for fpgas: prototyping a compiler, in proceedings of the international conference on engineering of recongurable systems and algorithms (ersa). the steering committee of the world congress in computer science, computer engineering and applied computing (worldcomp), , p. , y. shen, m. ferdman, and p. milder, maximizing cnn accelerator ef- ciency through resource partitioning, in computer architecture (isca), acm/ieee th annual international symposium on. ieee, , pp. , , , , f. n. iandola, s. han, m. w. moskewicz, k. ashraf, w. j. dally, and k. keutzer, squeezenet: alexnet level accuracy with x fewer parameters and< mb model size, arxiv preprint arxiv:, h. li, x. fan, l. jiao, w. cao, x. zhou, and l. wang, a high per- formance fpga based accelerator for large scale convolutional neural networks, in field programmable logic and applications (fpl), th international conference on. ieee, , pp. , w. xuechao, y. cody hao, z. peng, c. youxiang, w. yuxin, h. han, l. yun, and c. jason, automated systolic array architecture synthesis for high throughput cnn inference on fpgas, in proceedings of the design automation conference. acm, , pp. , , , y. ma, m. kim, y. cao, s. vrudhula, and j.-s. seo, end to end scalable fpga accelerator for deep residual networks, in circuits and systems (iscas), ieee international symposium on. ieee, , pp. , , , , c. wang, l. gong, q. yu, x. li, y. xie, and x. zhou, dlau: a scalable deep learning accelerator unit on fpga, ieee transactions on computer aided design of integrated circuits and systems, vol. , no. , pp. , , , , , altera. jtag uart core . available: https://www.altera.com/ en_us/pdfs/literature/hb/nios/ncpu_niipdf, , y. ma, y. cao, s. vrudhula, and j.-s. seo, an automatic rtl compiler for high throughput fpga implementation of diverse deep convolutional neural networks, in field programmable logic and applications (fpl), th international conference on. ieee, , pp. , , , , m. s. abdelfattah, d. han, a. bitar, r. dicecco, s. oconnell, n. shanker, j. chu, i. prins, j. fender, a. c. ling et al., dla: compiler and fpga overlay for neural network inference acceleration, arxiv preprint arxiv:, , a. k. jain, s. a. fahmy, and d. l. maskell, efcient overlay architec- ture based on dsp blocks, in field programmable custom computing machines (fccm), ieee rd annual international symposium on. ieee, , pp. w. liu, d. anguelov, d. erhan, c. szegedy, s. reed, c.-y. fu, and a. c. berg, ssd: single shot multibox detector, in european conference on computer vision. springer, , pp. e. chung, j. fowers, k. ovtcharov, m. papamichael, a. cauleld, t. massengil, m. liu, d. lo, s. alkalay, m. haselman et al., accelerat- ing persistent neural networks at datacenter scale, in hot chips, vol. , y. ma, y. cao, s. vrudhula, and j.-s. seo, optimizing the convolution operation to accelerate deep neural networks on fpga, ieee transac- tions on very large scale integration (vlsi) systems, no. , pp. , , , c. zhang, d. wu, j. sun, g. sun, g. luo, and j. cong, energy efcient cnn implementation on a deeply pipelined fpga cluster, in proceedings of the international symposium on low power electronics and design. acm, , pp. s. m. sait and h. youssef, iterative computer algorithms with appli- cations in engineering: solving combinatorial optimization problems. ieee computer society press, l. xie and a. l. yuille, genetic cnn. in iccv, , pp. l. rere, m. i. fanany, and a. m. arymurthy, metaheuristic algorithms for convolution neural network, computational intelligence and neuro- science, vol. , volume , shawahna et al.: fpga based accelerators of deep learning networks for learning and classication: a review ahmad shawahna ",
                "Subsections": [],
                "Groundtruth": "The text discusses various research papers and books related to deep learning, neural networks, and artificial intelligence. Topics covered include learning deep architectures for AI, deep learning in neural networks, sentiment analysis, handwritten digit recognition, speech recognition, video classification, image recognition, and object detection. Additionally, the text covers the use of convolutional neural networks, recurrent neural networks, and deep belief networks in various applications. There is also mention of FPGA-based accelerators for deep learning networks and the use of OpenCL for implementing convolutional neural networks on FPGAs. The text provides a wide range of resources and references in the field of deep learning."
            },
            {
                "Section_Num": "Ahmad",
                "Section": "Ahmad Shawahna",
                "Text": "obtained m.s. in com- puter engineering from king fahd university of petroleum and minerals (kfupm), saudi arabia, in he also received the b.sc degree in computer engineering from an najah national university, palestine, in ahmad shawahna is a ph.d. student in the department of com- puter engineering of kfupm. in addition, he is currently working at the center for commu- nications and it research (ccitr), kfupm. his research interests include hardware accelerator, deep learning, cnns, fpga, wireless security, network security, internet of things (iot), and cloud computing. ",
                "Subsections": [],
                "Groundtruth": "Ahmad Shawahna holds a Master of Science in Computer Engineering from King Fahd University of Petroleum and Minerals (KFUPM) in Saudi Arabia and a Bachelor of Science in Computer Engineering from An Najah National University in Palestine. He is currently a Ph.D. student in the computer engineering department at KFUPM and works at the Center for Communications and IT Research (CCITR) there. His research interests cover hardware accelerators, deep learning, CNNs, FPGA, wireless security, network security, Internet of Things (IoT), and cloud computing."
            },
            {
                "Section_Num": "Sadiq",
                "Section": "Sadiq M. Sait",
                "Text": "(senior member, ieee) ob- tained his bachelors degree in electronics en- gineering from bangalore university in , and masters and ph.d. degrees in electrical engineering from kfupm in and , respectively. in sait received the best elec- tronic engineer award from the indian institute of electrical engineers, bangalore (where he was born). sait has authored over research papers, contributed chapters to technical books, and lec- tured in over countries. sadiq m. sait is also the principle author of two books. he is currently professor of computer engineering and the director of the center for communications and it research of kfupm. ",
                "Subsections": [],
                "Groundtruth": "Sadiq M. Sait is a senior member of IEEE with a background in electronics engineering. He holds a bachelor's degree from Bangalore University and master's and Ph.D. degrees from KFUPM. Sait has received awards for his work and has authored numerous research papers and books. He currently serves as a professor of computer engineering and the director of the Center for Communications and IT Research at KFUPM."
            },
            {
                "Section_Num": "Aiman",
                "Section": "Aiman El-Maleh",
                "Text": "is a professor in the com- puter engineering department at king fahd uni- versity of petroleum & minerals. he holds a b.sc. in computer engineering, with rst honors, from king fahd university of petroleum & minerals in , a m.a.sc. in electrical engineering from university of victoria, canada, in , and a ph.d in electrical engineering, with deans honor list, from mcgill university, canada, in he was a member of scientic staff with mentor graphics corp., a leader in design automation, from - dr. el maleh received the excellence in teaching award from kfupm in /, / and /, the excellence in advising award from kfupm in / and /, the excellence in research award from kfupm in / and /, and the first instructional technology award from kfupm in / dr. el malehs research interests are in the areas of synthesis, testing, and verication of digital systems. in addition, he has research interests in defect and soft error tolerance design, vlsi design, design automation and efcient fpga implementations of deep learning algorithms and data compression techniques. dr. el maleh is the winner of the best paper award for the most outstanding contribution in the eld of test at the european design & test conference. his paper presented at the design automation conference was also nominated for best paper award. he holds ve us patents. volume , ",
                "Subsections": [],
                "Groundtruth": "Aiman El-Maleh is a distinguished professor in the computer engineering department at King Fahd University of Petroleum & Minerals. He holds a B.Sc. in Computer Engineering, an M.A.Sc. in Electrical Engineering, and a Ph.D. in Electrical Engineering. Dr. El-Maleh has received multiple awards for teaching, advising, and research excellence at KFUPM. His research interests include synthesis, testing, and verification of digital systems, defect and soft error tolerance design, VLSI design, and efficient FPGA implementations of deep learning algorithms. Dr. El-Maleh has won awards for his contributions in the field of test at various conferences and holds five US patents."
            }
        ],
        "Groundtruth": ""
    },
    {
        "Section_Num": "NA",
        "Section": "NA",
        "Text": "date of publication , , date of current version , digital object identier /access..doi fpga based accelerators of deep learning networks for learning and classication: a review ahmad shawahna, sadiq m. sait, , (senior member, ieee), and aiman el maleh, (member, ieee) department of computer engineering, king fahd university of petroleum & minerals, dhahran-, saudi arabia center for communications and it research, research institute, king fahd university of petroleum & minerals, dhahran-, saudi arabia corresponding author: sadiq m. sait (e mail: sadiq@kfupm.edu.sa). this work was supported by the king fahd university of petroleum & minerals, dhahran, saudi arabia. abstract due to recent advances in digital technologies, and availability of credible data, an area of articial intelligence, deep learning, has emerged, and has demonstrated its ability and effectiveness in solving complex learning problems not possible before. in particular, convolution neural networks (cnns) have demonstrated their effectiveness in image detection and recognition applications. however, they require intensive cpu operations and memory bandwidth that make general cpus fail to achieve desired performance levels. consequently, hardware accelerators that use application specic integrated circuits (asics), eld programmable gate arrays (fpgas), and graphic processing units (gpus) have been employed to improve the throughput of cnns. more precisely, fpgas have been recently adopted for accelerating the implementation of deep learning networks due to their ability to maximize parallelism as well as due to their energy efciency. in this paper, we review recent existing techniques for accelerating deep learning networks on fpgas. we highlight the key features employed by the various techniques for improving the acceleration performance. in addition, we provide recommendations for enhancing the utilization of fpgas for cnns acceleration. the techniques investigated in this paper represent the recent trends in fpga based accelerators of deep learning networks. thus, this review is expected to direct the future advances on efcient hardware accelerators and to be useful for deep learning researchers. index terms adaptable architectures, convolutional neural networks (cnns), deep learning, dynamic reconguration, energy efcient architecture, field programmable gate arrays (fpgas), hardware accelerator, machine learning, neural networks, optimization, parallel computer architec- ture, recongurable computing. i. introduction ",
        "Subsections": [],
        "Groundtruth": "This review paper discusses the use of FPGA-based accelerators for deep learning networks. The authors highlight the importance of deep learning in solving complex problems and the challenges faced by CPUs in terms of performance with convolutional neural networks (CNNs). They explore the use of FPGA technology to improve the acceleration performance of CNNs, emphasizing parallelism and energy efficiency. The paper reviews existing techniques, key features, and provides recommendations for enhancing FPGA utilization in CNN acceleration. The findings are expected to guide future advancements in efficient hardware accelerators for deep learning."
    },
    {
        "Section_Num": "1",
        "Section": "1. Introduction",
        "Text": "",
        "Subsections": [
            {
                "Section_Num": "NA",
                "Section": "NA",
                "Text": "consider a random process x = (xv)vv living on the vertex set v of an innite graph g. the process x is said to be nitely dependent if its restrictions to sets which are suciently separated (at least some xed distance apart) are independent. a trivial example of a nitely dependent process is a process y = (yv)vv in which all random variables are independent. a natural question is then how close is a nitely dependent process to such an independent process? before addressing this question, we rst observe that local functions of an independent process y are always nitely dependent. that is, if x is obtained from y by computing each xv as a function only of the random variables yu for which u is at a uniformly bounded distance from v, then x is nitely dependent. suppose now that g is transitive and henceforth restrict attention to processes x which are invariant under all automorphisms of g (or under a transitive subgroup of automorphisms). in particular, the independent process y considered above must now be an i.i.d. process that is, in addition to being independent, the {yv}v are also identically distributed. if x is obtained from y by applying the same local function at each vertex v (i.e., the function applied at u is the composition of the function applied at v with any automorphism taking u to v), then x is said to be a block factor of y . thus, block factors of i.i.d. processes provide a recipe for constructing invariant nitely dependent processes. it was a long standing open problem to determine whether block factors of i.i.d. processes are the only (invariant) nitely dependent processes on z, until nally an example was given by burtongouletmeester of a -dependent process which is not a block factor of any i.i.d. process. recently, holroyd and liggett showed that proper colorings distinguish between block factors of i.i.d. processes and nitely dependent processes no proper coloring of z is a block factor of an i.i.d. process, but nitely dependent proper colorings exist. thus, it is not true that every nitely dependent process is a block factor of an i.i.d. process. in other words, given a nitely dependent process x, one cannot in general hope to nd an i.i.d. process y and an invariant rule for computing x from y , which allows to determine the value of xv by looking at y on a xed size window around v. the goal of this paper is to show the next best thing namely, that it is possible to determine xv by looking at y on a variable sized window around v, where the size of the window, though always nite, may vary according to the input y . we say that x is a nitary factor of y if there is an invariant rule which allows to compute the value of x at any vertex v by only looking at variables yu for which u is within a certain nite, but random, distance from v (formal denitions are given below). thus, a block factor is a nitary factor in which the required distance is not only nite, but is determistically bounded by some constant. the main contribution of this paper is to prove that every nitely dependent process is a date: january , arxiv:v jan yinon spinka nitary factor of an i.i.d. process. this result holds on any amenable graph g. when it is further assumed that no two balls in g with dierent centers are identical, it is also possible to control the entropy of the i.i.d. process involved, and the result becomes that every nitely dependent process is a nitary factor of an i.i.d. process with only slightly larger entropy. . denitions and main result. let v be a countable set, let g be a graph on vertex set v and let be a group acting on v. a random eld (or random process) on g is a collection of random variables x = (xv)vv indexed by the vertices of g and dened on a common probability space. we say that x is -invariant if its distribution is not aected by the action of , i.e., if (xv)vv has the same distribution as x for any . we say that x is k dependent if (xu)uu and (xv)vv are independent for any two sets u, v v such that dist(u, v) > k for all u u and v v . we say that x is nitely dependent if it is k dependent for some nite k. suppose now that g is a transitive locally nite graph and that is a subgroup of the automor- phism group of g. let s and t be two measurable spaces, and let x = (xv)vv and y = (yv)vv be s valued and t valued -invariant random elds. a coding from y to x is a measurable func- tion : t v sv that is -equivariant, i.e., commutes with the action of every element of , and satises that (y ) and x are identical in distribution. such a coding is also called a factor map from y to x, and when such a coding exists, we say that x is a -factor of y . suppose now that s and t are countable. let v be a distinguished vertex. the coding radius of at a point y t v, denoted by r(y), is the minimal integer r such that (y) = (y) for all y t v which coincide with y on the ball of radius r around in the graph distance, i.e., y v = yv for all v v such that dist(v, ) r. it may happen that no such r exists, in which case, r(y) = . thus, associated to a coding is a random variable r = r(y ) which describes the coding radius. while s will always be at most countable, we will allow t to be a larger space, in which case the coding radius may be similarly dened a coding is called nitary if r is almost surely nite. when there exists a nitary coding from y to x, we say that x is a nitary -factor of y . a graph is said to be amenable if inf |v |/|v | = , where the inmum is over all nite non empty subsets v of v, and where v denotes the edge boundary of v . theorem . let g be a transitive amenable graph and let be a transitive group of automor- phisms of g. then any nitely dependent -invariant random eld on g is a nitary -factor of an i.i.d. process. with a minor additional constraint on the geometry of the graph g, we can further control the entropy of the i.i.d. process used in the coding (see section for the denition of entropy). the condition we require is that r(u) = r(v) for any distinct u, v v and r , () where r(u) is the ball of radius r around u. theorem . let g be a transitive amenable graph satisfying (), let be a transitive group of automorphisms of g, and let x be a nite valued nitely dependent -invariant random eld on g. then for any > there exists an i.i.d. process y with entropy h(y ) < h(x) + such that x is a nitary -factor of y . let us make some remarks about how the two theorems compare to one another. in theorem , s is nite (note the assumption that x is nite valued) and, in particular, x has nite entropy, while in theorem , s may be countable and x may have innite entropy. in theorem , y has nite entropy so that t is countable, whereas theorem may require a larger space t for we will only be concerned with spaces t which are nite, countable or of the form t t for nite sets (ti). in the latter case, the coding radius is the smallest r for which there exists n such that (y) = (y) for all y having the property that y v,i = yv,i for all (v, i) such that dist(v, ) r and i n. finitely dependent processes are finitary the conclusion to hold. in fact, in the absence of condition (), even when s is nite, it might not be possible to have t countable (see remark in section ). let us also remark that, while the theorems do not assume connectivity of the graph, there is no loss of generality in assuming this, since transitivity implies that the connected components are isomorphic and nite dependence implies that the random eld is independent on dierent components. thus, the same coding can be used for all components. . discussion.",
                "Subsections": [],
                "Groundtruth": "A random process on an infinite graph is considered finitely dependent if its restrictions to sufficiently separated sets are independent. Block factors of i.i.d. processes provide a way to construct invariant finitely dependent processes, but not all finitely dependent processes are block factors of i.i.d. processes. A new concept of a \"nitary factor\" is introduced, showing that every finitely dependent process is a nitary factor of an i.i.d. process on any amenable graph. The main contribution is proving that a random field on a transitive amenable graph, with a transitive group of automorphisms, is a nitary factor of an i.i.d. process. Theorems provided show how finitely dependent processes are related to i.i.d. processes and introduce the concept of nitary factors as an alternative to block factors."
            },
            {
                "Section_Num": "1_2",
                "Section": "1.2. Discussion",
                "Text": " finite dependence and nitary factors have applications in computer science. for example, if the graph g represents machines in a network and the random eld x represents a common plan in which each machine v is assigned a specic role xv, then nite dependence provides certain security benets in the face of an attacker (if someone gains access to some machines, they learn nothing about the roles of far away machines, thereby conning the security breach), and being a nitary factor of an i.i.d. process provides reliability (e.g., no single point of failure) as it means that the machines can determine their own roles in a distributed manner by following a common protocol, while using local randomness and communicating with nitely many other machines. see e.g. for more information. the nitary coding properties of nitely dependent processes on g = z, and in some cases on g = zd with d , have been studied in various contexts. we give a brief account of these works. we are unaware of any works regarding nitary factors for nitely dependent processes on other graphs. a result by smorodinsky shows that every stationary (i.e., translation invariant) nitely dependent process on z is nitarily isomorphic (a stronger notion than being a nitary factor) to an i.i.d. process. this result is not for the full autormorphism group of the graph (which includes reections), but rather only for the group of translations. in this respect, theorem strengthens this result (if one is content with a nitary factor, rather than a nitary isomorphism), as it provides a nitary factor which is also reection invariant whenever the nitely dependent process is such. the proof in is based on the so called marker ller method of keane and smorodinsky . unfortunately, only a brief sketch of the proof is provided in and the details seem to be missing (after some initial steps, smorodinsky says that the rest of the proof proceeds along the same lines as in with some necessary modications). our proof is based on a dierent approach; see section for an outline. the question of whether there exists a stationary nitely dependent process which is not a block factor of any i.i.d. process was raised by ibragimov and linnik in some progress on this question was made until it was nally resolved in by burtongouletmeester who gave the rst example of a stationary nitely dependent process which is not a block factor of an i.i.d. process. in fact, they showed such an example in which the nitely dependent process is a -dependent hidden markov process with nite energy. some history about nitely dependent processes that cannot be written as block factors is given in . holroyd and liggett constructed a stationary -dependent -coloring and a stationary - dependent -coloring of z, neither of which is a block factor of an i.i.d. process (indeed, no coloring is such ). holroyd subsequently showed that the -dependent -coloring is a nitary factor of an i.i.d. process. regarding the analogous statement for the -dependent -coloring, holroyd writes in that one may attempt to apply our method to the -dependent -coloring, but we will see that it meets a fundamental obstacle in this case. our result shows that either coloring is a nitary factor of an i.i.d. process (with slightly larger entropy), answering armatively question (iii) in . in fact, the two colorings are also reection invariant, and hence the nitary factors may also be taken to commute with reections. related aspects of these two colorings were studied in . in a subsequent paper , holroyd and liggett constructed, for any q , a -dependent q coloring of z which is invariant under translations and reections and is also symmetric under yinon spinka permutations of the colors. it was shown in that each of these colorings is a nitary factor of an i.i.d. process (with exponential tail on the coding radius). our result shows that each of these colorings is a nitary factor of an i.i.d. process, where the factor map commutes with all automorphisms of z and the i.i.d. process has entropy only slightly larger than the coloring. in , holroyd and liggett also constructed stationary nitely dependent colorings of zd with d more specically, they constructed a stationary -dependent d coloring of zd and a sta- tionary nitely dependent -coloring of zd. however, unlike the above one dimensional colorings, these colorings are only translation invariant and not automorphism invariant. in fact, it is still unknown whether there exists a nitely dependent coloring of zd (d ) which is invariant under all automorphisms of zd. in the same paper , holroyd and liggett also investigated the existence of stationary nitely dependent processes on z which are supported on a given shift of nite type. they showed that for any reasonably non degenerate (namely, nonlattice) shift of nite type s on z, there exists a stationary nitely dependent process which almost surely belongs to s. it was later shown that there exists such a process which is also a nitary factor of an i.i.d. process (with exponential tail on the coding radius). a block factor is precisely a nitary factor with bounded coding radius. given a nitary factor which is not a block factor, it is natural to wonder about the typical value of the coding radius. as we have mentioned, the -dependent -coloring of z from , which is not a block factor of any i.i.d. process, was shown in to be a nitary factor of an i.i.d. process. this nitary factor was shown to have (at least) power law tail on the coding radius, thus yielding a perhaps innite expected coding radius. holroydhutchcroftlevy showed that there exist nitely dependent colorings of z which are nitary factors of i.i.d. processes with exponential tail on the coding radius. indeed, they showed that such a k dependent q coloring exists when (k, q) is either (, ), (, ) or (, ). on the other hand, it is believed that when (k, q) is (, ) or (, ), no k dependent q coloring is a nitary factor of an i.i.d. process with nite expected coding radius. we mention that optimal tails for the coding radius of colorings of zd (which are not necessarily nitely dependent) and shifts of nite type on z have been studied in . our main theorem gives no information about the coding radius beyond its almost sure nite- ness. indeed, in light of the above discussion, it would seem that for an arbitrary nitely dependent process on z, there is not much hope to obtain a nitary factor with nite expected coding ra- dius. still, some information about the coding radius may be extracted from the proof given here (see remark ). for example, for -dependent processes on z, the nitary factor provided by theorem has a coding radius r satisfying that p(r > r) /r for all r. in the particular case of the -dependent -coloring of , this improves the power in the power law bound shown in (to an optimal power if the prediction above is indeed correct). to the best of our knowledge, beyond smorodinskys result on z, there do not exist any general results on the nitary coding properties of nitely dependent processes. in particular, theorem and theorem are new for any amenable graph g other than z, and also for g = z in the case when is the full automorphism group of z. finally, we mention that the situation for non- amenable graphs is still poorly understood for example, on a regular tree (of degree at least three), it is not even known whether every automorphism invariant nitely dependent process is a (non nitary) factor of an i.i.d. process . ",
                "Subsections": [],
                "Groundtruth": "The text discusses the importance of finite dependence and nitary factors in computer science, particularly in scenarios involving network security and distributed systems. It highlights the concepts of nitary coding properties, stationarity, and the relationship between nitely dependent processes and i.i.d. processes. Various examples and results in the context of different graph structures are presented, showcasing the complexity and research advancements in this area. The text also touches upon specific constructions and properties of colorings, coding radii, and automorphism invariance, emphasizing the significance of these concepts in understanding the behavior of nitely dependent processes. Overall, the text sheds light on the intricacies and challenges in studying and characterizing nitary factors in different settings, with implications for system reliability and security."
            },
            {
                "Section_Num": "1_3",
                "Section": "1.3. Acknowledgments",
                "Text": " i would like to thank omer angel, nishant chandgotia, tom meyerovitch and mathav murugan for useful discussions. i am especially grateful to nishant chandgotia for suggesting to extend the result from zd to transitive amenable graphs, and to omer angel for jointly proving lemma with me. i would also like to thank the referees for useful comments. finitely dependent processes are finitary ",
                "Subsections": [],
                "Groundtruth": "The author acknowledges Omer Angel, Nishant Chandgotia, Tom Meyerovitch, and Mathav Murugan for their contributions and useful discussions. Chandgotia is credited with suggesting the extension of results from ZD to transitive amenable graphs, and Angel collaborated on proving a lemma. The author also expresses gratitude to the referees for their helpful comments. The text mentions that finitely dependent processes are finitary."
            },
            {
                "Section_Num": "1_4",
                "Section": "1.4. Notation",
                "Text": " throughout the paper, g is always assumed to be an innite, transitive, lo- cally nite, connected graph on a countable vertex set v, and is a subgroup of the auto- morphism group of g that acts transitively on v. the full automorphism group of g is de- noted by aut(g). the graph distance in g is denoted by dist(, ). for sets u, v v, we write dist(u, v ) := minuu,vv dist(u, v) and dist(u, v ) := dist({u}, v ). for r , denote v +r := {u v : dist(u, v ) r} and v r := {u v : dist(u, v c) > r}. the ball of radius r around v is denoted by r(v) := {v}+r. the neighborhood of v is n(v ) := v + \\ v and the edge boundary of v is v := {{u, v} e(g) : v v, u / v }. all logarithms are taken to be in base and we use the convention that log is ",
                "Subsections": [],
                "Groundtruth": "The paper assumes g as an infinite, transitive, locally finite, connected graph with a countable vertex set v. g is also a subgroup of the automorphism group that acts transitively on v. The full automorphism group of g is referred to as aut(g). Graph distance in g is denoted by dist(, ). Distances between sets u and v are defined with respect to the minimum distance between elements. Notations for subsets, distances, balls of radius, neighborhoods, and edge boundaries are established for technical analysis. Logarithms are taken to be in base ."
            }
        ],
        "Groundtruth": ""
    },
    {
        "Section_Num": "2",
        "Section": "2. Outline of proof",
        "Text": "our goal is to express x, a nitely dependent invariant process, as a nitary factor of an i.i.d. process y . the construction of the nitary coding involves the use of three sources of randomness: a random number of random bits located at each vertex, a so called cell process, and a random total order on v. the rst of these three will simply be given by an i.i.d. process, denoted y bits, while the latter two will be obtained as nitary factors of dierent i.i.d. processes, denoted y cell and y ord. in turn, y will be a triplet y = (y bits, y cell, y ord) consisting of three mutually independent i.i.d. processes. to illuminate the main ideas behind our construction, we provide a sketch of the proof below, explaining separately three ingredients: () constructing a nitary coding: the basic and most essential part of the construction is how to obtain x as a nitary factor of y when y is allowed to have innite entropy (this is the setting of theorem ). in this case, y bits v and y ord v may be taken to be uniform random variables in , and the total order may be taken to be the one induced by the usual order on y ord v . () controlling the entropy: the second part is how to control the entropy of the y bits process, requiring only slightly more entropy than that of x. to postpone dealing with the issue of controlling the entropy of y ord, we shall assume in this part of the proof outline that the vertices of g can be deterministically ordered in a -invariant manner so that y ord may be disregarded entirely (e.g., it can be taken to be a constant process). for example, the lexicographical order is such an ordering when g is the graph zd and is the group of translations. () constructing a random order: the third part is how to allow for graphs g and groups which do not admit such a deterministic order. this is of course the case for general graphs, but it may also be the case for simpler graphs, such as z or zd, when is the full automorphism group of g. in these cases, we are led to consider random orders with suitable properties. already the rst part above relies on the aforementioned cell process. before introducing this process, it is convenient to observe that it suces to prove theorem and theorem for -dependent random elds. to see this, let gk denote the graph on vertex set v in which two vertices are adjacent if their distance in g is at most k. it is immediate from the denitions that x is k dependent as a random eld on g if and only if it is -dependent as a random eld on gk. since any automorphism of g is also an automorphism of gk, and since gk is amenable and satises () whenever g is such, we see that we may indeed assume that x is -dependent. this assumption, though not at all essential, is convenient as it obviates the need to work with a dierent connectivity than the usual connectivity in g. a cell process is a random sequence a = (a, a, . . . ) of subsets of v satisfying the following properties almost surely: a a a . yinon spinka a a = v. for each n , all connected components of an are nite. we will obtain a cell process a as a nitary factor of an i.i.d. process y cell with arbitrarily small entropy. we do not explain here how this is done and refer the reader to section for more details and to figure for an illustration of the construction. () constructing a nitary coding: we construct a realization of x as a nitary factor of y in innitely many steps with the idea that at the end of step n {, , . . . }, we will have dened x on the region an. in the rst step, we sample x on the set a this is particularly simple as the cells in a are at pairwise distance at least , and so, due to the -dependence assumption on x, each cell in a can be sampled independently. next, at each step n {, , . . . }, we sample x on the region an \\ an, conditioned on the value of x on an, which has already been sampled in the previous steps the key observation here is that, due again to the -dependence assumption on x, the values of x on dierent cells in an are conditionally independent indeed, if {vi}i are at pairwise distance at least from one another, then for any sets ui vi, given {xui}i, one has that {xvi}i are mutually conditionally independent. since all the cells of every an are nite, the above steps can be carried out in a nitary manner that is, the conditional distribution of x on a given cell depends only on the previously sampled values within that cell. since an increases to v, the value at every given vertex will eventually be sampled, thus producing a realization of x from y in a nitary and -equivariant manner. let us be slightly more specic about the way in which we sample x on a cell. in each cell in a, we distinguish a vertex by choosing the smallest element in the cell according to the order given by y ord. we call these distinguished vertices level agents. similarly, for each n and each cell in an that is not contained in an, we select a level n agent in the cell by choosing the smallest element in the cell which is not in an note that the level n agents are obtained as a nitary factor of (y cell, y ord). with the notion of agents, we may now say more precisely that, in step n above, if c is a cell of an that is not contained in an, then we sample x on the region c \\ an (conditionally on the previously sampled values of x on c an) by accessing a sample of the desired distribution from the random variable y bits u , where u is the unique level n agent in c, using also the order induced by y ord on c \\ an to break any symmetries which may be present in the graph structure of this region (for example, if g = z and includes reections, then when c \\ an is a symmetric interval around u, the left and right sides of u cannot be dierentiated in a -equivariant way without some additional information; ordering all elements in the set is a simple way to get rid of such problems). in this interpretation, we regard y bits u as consisting of independent samples of p(xu | xv = ) for all nite u, v zd and sv , most of which are never used in practice. () controlling the entropy: it is clear from the last observation above that there is plenty of waste in the above construction (in terms of the process y bits). the problem is that we do not know ahead of time which samples of which distributions we will need access to. the basic solution to this is to place an innite sequence of random bits at each site, i.e., y bits v {, }n, from which we may easily construct samples of any desired distributions (hence the name of the process y bits). of course, this idea alone still does not provide any control on the entropy of y bits. for this, we must place a nite (perhaps random) number of random bits at each site, and somehow still be sure that we are able to construct the required samples. by a random number of random bits, we mean a random variable w taking values in {, }, the set of nite words over {, }, and having the property that, conditioned on the length |w| of the word, w is uniformly distributed on {, }|w|. suppose now that there exists a deterministic total order on v that is -invariant in the sense that u v implies that u v for any u, v v and . for instance, the lexicographical order is such an order when g = zd and is the translation group (but there is clearly no such order when is the full automorphism group of zd). for the purpose of this part of the proof outline, finitely dependent processes are finitary it is convenient to further suppose that every v v has a -successor, which we denote by v + , as is the case for the lexicographical order on zd (in which case v + is simply v + (, , . . . , )). the existence of such a deterministic order renders y ord unneeded, allowing us to focus now only on the task of controlling the entropy of y bits. the idea is to associate to each possible distribution we might require, a simulation which outputs a sample of the distribution in question from an input of unbiased random bits. the simulation is fed independent unbiased bits one by one, until at some point (a stopping time) it halts and outputs the sample. such simulations may be done eciently: the expected number of input bits read by the simulation is bounded by the entropy of the target distribution, up to an additive universal constant. we shall use such simulations whenever we sample x on a cell. if the cell c is large, then the entropy of x on c is also large, and thus the above additive error is negligible. when the boundary of the cell is small in comparison to the size of the cell, the average entropy of x on c per site will also be close to h(x), the entropy of x itself. thus, it will be important that the cells in a are typically large with small boundary. this already shows that, in some sense, the average number of random bits needed to generate the samples required throughout the construction is very close to h(x). however, we must place a nite number of bits at each site (more precisely, we need that h(y bits) < h(x) + ), and even if we have more than h(x) such bits at every site, it still may happen at some point during the construction that a simulation carried out by an agent u requires access to many more input bits than are available in y bits u . to solve this, we must allow to transfer bits from one location to another. this aspect of our construction is inspired by the algorithms in . the idea is that whenever an agent u requires access to an additional bit (beyond those available in y bits u ), it may look for an unused bit at u + (the -successor of u). if there are no available unused bits at u + at that time, it may then proceed to look at u + , and so on. one consequence of the above description is that the steps of the construction cannot be directly related to the levels of the cell process. that is, it will no longer be the case that after step n of the construction, we will have dened x on the region an. instead, at any step of the construction, dierent regions of g will be at dierent levels of the cell process. we will continue to use n to denote the levels of the cell process, and will use t to denote the step of the construction (which we henceforth also refer to as time). the way this is done is as follows. initially, at time t = , all level agents are deemed active. an active level agent attempts to collect unused bits until its associated simulation halts, at which point in time the agent becomes inactive and is said to have completed level once all level agents contained in some level cell c have completed, the level agent associated to c becomes active. an active level agent proceeds in the same manner as an active level agent, attempting to read bits in order to complete its associated simulation. in general, a level n agent becomes active once all level n agents contained in its associated cell have completed. in our actual construction, it is more convenient to employ the following policy which makes the details simpler to write down: at time t, an agent u may read at most one bit, and this bit may only be read from site u + t. this has the advantage that it ensures that no two agents ever try to read bits from the same location simultaneously. () constructing a random order: for a general graph g and group , there need not be a deterministic total order of v that is -invariant. instead, we construct a random total order on v whose distribution is -invariant. moreover, we construct as a nitary factor of an i.i.d. process y ord with arbitrarily small entropy. here nitary means that the order induced on any nite set of vertices is determined by a nite (random) subset of {y ord v }vv. in addition, the constructed order will have the property that its order type is almost surely the same as that of z. that is, almost surely, every element v has a successor v+ and a predecessor v , and {v + n}nz = v. though it does not follow from the above denition of nitary, it will yinon spinka turn out to be the case that determining whether some vertex is the successor of some other vertex is also a nitary property (i.e., it is almost surely determined by a nite subset of {y ord v }vv). once such an order is at hand, the proof continues as outlined above. organization. in section , we introduce some preliminaries. in section , we prove the existence of a nitary cell process. in section , we prove the existence of a nitary random total order having the order type of z. in section , we give the construction of the nitary coding for the nitely dependent process x. finally, we end in section with some remarks and open problems. ",
        "Subsections": [],
        "Groundtruth": "The text outlines a proof technique to express a finitely dependent invariant process x as a unitary factor of an i.i.d. process y. The construction involves three sources of randomness: a random number of bits, a cell process, and a random total order on vertices. The process includes constructing a unitary coding, controlling entropy, and generating a random order for graphs and groups that do not admit a deterministic order. The proof involves defining x as a unitary factor of y in infinitely many steps, where each step involves sampling x on specific regions based on the values of x on previous regions. Various techniques are employed to handle entropy control and efficient sampling throughout the construction process. The proof is detailed in different sections, including preliminaries, the existence of a unitary cell process, the construction of a unitary random total order, and the final construction of the unitary coding for x."
    },
    {
        "Section_Num": "3",
        "Section": "3. Preliminaries",
        "Text": "",
        "Subsections": [
            {
                "Section_Num": "NA",
                "Section": "NA",
                "Text": "recall that g is always assumed to be an innite, transitive, locally nite, connected graph on a countable vertex set v, and that is assumed to be a subgroup of the automorphism group of g that acts transitively on v. . entropy.",
                "Subsections": [],
                "Groundtruth": "The text discusses the properties of a graph g and its subgroup within the automorphism group, emphasizing transitivity, local finiteness, and connectivity on a countable vertex set. The focus is on studying entropy in this context."
            },
            {
                "Section_Num": "3_1",
                "Section": "3.1. Entropy",
                "Text": " the shannon entropy of a discrete random variable z is h(z) := x z p(z = z) log p(z = z), where the sum is taken over z in the support of z, or alternatively, we interpret log to be the measure theoretic entropy (or kolmogorovsinai entropy) of a -invariant random eld x on an amenable graph g is h(x) := inf v v nite and non empty h(xv ) |v | . a flner sequence in g is a sequence (fn) n= of non empty nite subsets of v such that lim n |fn| |fn| = it is well known that the entropy of x may be computed along any flner sequence: h(x) = lim n h(xfn) |fn| for any flner sequence (fn) n= in g. it follows that for any > there exists > such that h(xf ) |f| h(x) + whenever f v is non empty and nite and |f| |f|. () of course, since entropy is maximized by the uniform distribution, we also have that h(xf | e) |f| log |s| whenever fv is non empty and nite and e is an event with positive probability, () where s is the nite set in which x takes values. we note that if y is an i.i.d. process, then its entropy h(y ) is equal to the entropy of its single site distribution h(y). ",
                "Subsections": [],
                "Groundtruth": "The text discusses the Shannon entropy of a discrete random variable and the measure-theoretic entropy of an invariant random field on an amenable graph. It explains how to compute entropy along flner sequences and mentions that entropy is maximized by the uniform distribution. Additionally, it notes that for an i.i.d. process, the entropy of the process is equal to the entropy of its single site distribution."
            },
            {
                "Section_Num": "3_2",
                "Section": "3.2. The mass-transport principle",
                "Text": " for u, v v, denote u,v := { : u = v}. note that u,u is the stabilizer of u. we say that is unimodular if |u,uv| = |v,vu| for all u, v v. it is well known (see, e.g., ) that is unimodular if and only if the following mass- transport principle holds: x u f(u, ) = x v f(, v) for any diagonally -invariant function f : v . () finitely dependent processes are finitary by diagonally -invariant, we mean that f(u, v) = f(u, v) for all u, v v and . we note the well known fact that, when g is amenable, any transitive group of automorphisms is unimodular. ",
                "Subsections": [],
                "Groundtruth": "Unimodularity in a group is determined by the mass-transport principle, which states that x ∑ f(u, v) = x ∑ f( ,v) for diagonally-invariant function f. Unimodularity is also linked to amenable groups and transitive group automorphisms.8644"
            },
            {
                "Section_Num": "3_3",
                "Section": "3.3. Simulating distributions from random bits",
                "Text": " we shall use a result about the simulation of a given distribution from unbiased random bits. let be a distribution on a countable set . a simulation of is a pair s = (stime, sout) of measurable functions stime : {, }n n {} and sout : {, }n with the properties: if is a sequence of independent unbiased bits, then sout() has distribution . if stime(x) = n for some x {, }n and n n, then stime(x) = n and sout(x) = sout(x) for any x {, }n which coincides with x on {, . . . , n}. the rst property says that we can use s to simulate the desired distribution from random bits. the second property may be interpreted as saying that stime is a stopping time and that sout is adapted to the -algebra generated by {i : i stime} that is, the simulation reads one input bit at a time, and once the stopping time is reached, the output is determined only by the bits that have already been read. the following theorem follows from a result of knuth and yao (see theorems and there and the corollary just after). theorem . let z be a discrete random variable. there exists a simulation s of z from inde- pendent unbiased bits satisfying that stime() < almost surely and estime() h(z) + knuth and yao show that the above is in fact optimal in a strong sense (they provide a simulation whose stopping time is stochastically dominated by that of any other simulation). a version of this theorem was proved in via a more concrete construction. the results in also provide explicit exponential bounds on the probability that the simulation uses more than n bits, but we shall not need this. ",
                "Subsections": [],
                "Groundtruth": "A simulation method for generating a given distribution using unbiased random bits is discussed in this section. The simulation involves measurable functions stime and sout that can recreate the desired distribution from a sequence of independent unbiased bits. The properties of the simulation include stime acting as a stopping time and sout being adapted to the generated sigma-algebra. A theorem states that a discrete random variable can be simulated from independent unbiased bits with a stopping time less than a certain value. This simulation method is proven to be optimal and efficient."
            }
        ],
        "Groundtruth": ""
    },
    {
        "Section_Num": "4",
        "Section": "4. The cell process",
        "Text": "recall from section that a cell process is a random sequence a = (a, a, . . . ) of subsets increasing to v and satisfying that the connected components of each an are nite. we may identify a cell process a with the n valued random eld (min{n : v an})vv. in particular, when we say that a is -invariant or a nitary factor, we mean that this latter process is such. in this section, we show that nitary cell processes with arbitrarily small entropy exist on any transitive amenable graph. proposition . let g be a transitive amenable graph and let > there exists an i.i.d. process y of entropy at most and a cell process a which is a nitary aut(g)-factor of y . let us mention that in the special case of g = zd, several parts of the argument below can be skipped or simplied, thereby leading to a shorter proof. in the general case, certain technicalities arise which make the proof somewhat longer. the reader may therefore wish to have in mind the case of g = zd on a rst reading. before giving the proof, let us explain the idea behind it; see figure for an illustration. the main idea of the construction is to use the points of a low density bernoulli process to construct voronoi cells (determined from the bernoulli process in a nitary manner), which are then used as the cells of a (after slightly decreasing the voronoi cells to ensure that they are well separated). using another bernoulli process of even lower density, we again construct voronoi cells, which are then used to obtain a from a by lling in some of the empty space between the cells of a, taking care not to connect cells of a which are not in the same voronoi cell (thus ensuring that an innite cluster is not created). repeating in this manner, we obtain an increasing sequence a a of sets (each having only nite cells) as a nitary factor of a small entropy i.i.d. yinon spinka (a) a (b) going from a to a (c) a figure constructing the cell process. the cells of a are simply the voronoi cells of a bernoulli process. to get from a to a, we consider the voronoi cells of a lower density bernoulli process, and merge cells of a which are entirely contained in any such voronoi cell. repeating this procedure produces the cell process. the green shade (light and dark) depicts regions belonging to the cell process. the dark green depicts cells of a which are not cells of a process. it will then only remain to show that an increases to v. this is where amenability comes into play. to ensure that an increases to v, we must be careful in how we dene the voronoi cells. when the graph g has a flner sequence consisting of balls (as is the case when g has subexponential growth), the voronoi cells may be taken with respect to the graph distance in g. however, for the general case considered here, we must adapt the usual voronoi cells to a certain metric (which is not necessarily a true metric) given by a suitable flner sequence. since we will need this metric to be diagonally -invariant, we need to choose a flner sequence (fn)n having the property that each fn is invariant under the stabilizer of some xed vertex v, i.e., fn = ,fn for all n. it is a simple observation that any graph g of subexponential growth has such a sequence for any (since there is a flner sequence consisting of balls), and that the cayley graph g of a nitely generated amenable group also has such a sequence (since the stabilizers are trivial). as it turns out, such a flner sequence always exists in an amenable (not necessarily transitive) graph, even when is its full automorphism group. the following lemma (which appears to be new) shows that every amenable graph admits a flner sequence consisting of sets which are invariant under the stabilizer of some vertex. the lemma was obtained jointly with omer angel. lemma . let g be an amenable graph, let be the full automorphism group of g and let v. there exists a flner sequence (fn)n in g such that fn = ,fn for all n. lemma easily follows by applying the following lemma to each set of some flner sequence, taking to be the stabilizer , of lemma . let g be any graph and let be a group of automorphisms of g under which all orbits of v are nite. for any nite non empty set f v, there exists a nite non empty set e v such that |e| |e| |f| |f| and e = e. proof. we show the existence of the desired e via a probabilistic method that is, we choose a random subset e of v and show that it satises the desired properties with positive probability. finitely dependent processes are finitary let {vi}i be the orbits of v under the action of thus, {vi}i is a partition of v such that each vi is nite (by assumption) and satises vi = vi. let u be a uniform random variable in and dene e := [ i:upi vi, where pi := |f vi| |vi| . thus, each orbit vi is included in e with probability pi, where the choices for dierent i are positively correlated through the use of the common variable u. then e|e| = x i p(u pi) |vi| = x i pi|vi| = x i |f vi| = |f|, and e|e| = x i,j p(pj < u pi) |(vi, vj)| = x i,j max{pi pj, } |(vi, vj)|, where (u, v ) denotes the set of edges between two disjoint sets u and v . let us show that each term in the second sum is at most |(f vi, f c vj)|, i.e., pi pj |(f vi, f c vj)| |(vi, vj)| whenever pi > pj and (vi, vj) = . to see this, note that the right hand side is the probability that an edge e = {u, v} that is uniformly chosen from (vi, vj) belongs to (f vi, f c vj), or equivalently, with the convention that u vi and v vj, that u f and v / f. since this probability is at least p(u f)p(v f), it suces to show that u and v are uniformly distributed in vi and vj, respectively. this follows from the observation that the bipartite graph (vi vj, (vi, vj)) is biregular each vertex in vi is adjacent to the same number of vertices in vj, and similarly, each vertex in vj is adjacent to the same number of vertices in vi. indeed, for any u, v vi and such that u = v, the mapping w w denes a bijection between n(u) vj and n(v) vj. we thus conclude that e|e| x i,j |(f vi, f c vj)| = |f| = e|e|, where := |f|/|f|. thus, |e||e| is a random variable with non negative expectation. since |e| |e| is zero when e is empty, conditioned on e = , we still have that |e| |e| has non negative expectation. in particular, there is positive probability that e = and |e| |e|. finally, since e is nite and satises e = e almost surely, we see that e satises the desired properties with positive probability. suppose that (fn)n is a flner sequence guaranteed by lemma , i.e., fn = ,fn for all n, and further suppose that f f and f f = v (there is clearly no loss in generality in doing so). recall the denition of u,v from section . for u, v v, dene (u, v) := min \b n : v ,ufn . using that ,u = ,u for any , one easily checks that is diagonally -invariant, i.e., (u, v) = (u, v) for all . we stress that is not necessarily symmetric in that (u, v) may not equal (v, u). in particular, we do not claim that is a metric. nevertheless, we still think of (u, v) as a measure of distance from v to u. one nice property of that is easily veriable and which will be important is that, for any sequence of pairs of vertices (ui, vi) i=, we have (ui, vi) as i if and only if dist(ui, vi) as i . () the fact that may not be symmetric presents a certain challenge in the proof, for which we require the following lemma to address. we note that is indeed symmetric when the flner sequence (fn)n consists of balls, and that it is nearly symmetric when g is a cayley graph of yinon spinka in which case switching the roles of u and v in (u, v) has the same eect as replacing each fn with f n . accordingly, in these cases, it is immediate that the two -balls of radius n around , {v : (, v) n} and {u : (u, ) n}, have the same size, namely |fn|. the following lemma shows that this is in fact always true in our setting. lemma . let g be a transitive amenable graph and let be the full automorphism group of g. let f v be invariant under the stabilizer of some vertex , i.e., ,f = f. then |{u v : ,uf}| = |f|. proof. dene f : v by f(u, v) := {v,uf}. since ,u = ,u for any , it follows that f is diagonally -invariant. thus, by the mass transport principle (), |{u v : ,uf}| = x u f(u, ) = x v f(, v) = |,f| = |f|. we are now ready to give the proof of proposition . proof of proposition . let (n) be a sequence to be chosen later which satises that n n. we shall construct a cell process a as a nitary factor of the i.i.d. process y = (yv)vv in which yv = (yv,n)n are independent random variables with yv,n ber(n). the entropy of y can be made arbitrary small, since h(y ) = h(yv) = x n= h(yv,n) = x n= \u0010 n log n + ( n) log n \u0011 log . as explained, the idea of the construction is to use the points in un := {v v : yv,n = }, for any given n, to construct voronoi cells, which are then used to dene the cells of an. precisely, we dene the voronoi cells of a non empty set u v by cu(u) := n v v : (u, v) < (u, v) for all u u \\ {u} o , u u. thus, the voronoi cell cu(u) associated to u consists of all vertices v v which are closer (in the distance measured by ) to u than to any other u u. in particular, voronoi cells associated to dierent vertices in u are disjoint, but they are not necessarily separated (they could be adjacent to one another). we therefore dene modied voronoi cells by slightly shrinking the sets cu(u). precisely, we dene cu(u) := ( cu(u)) using that the voronoi cells are disjoint, it is straightforward to check that dist(cu(u), cu(u)) > for distinct u, u u. we note that cu(u) (in fact, already cu(u)) may be empty and need not be connected. before proceeding with the construction of the cell process, let us rst show that the voronoi cells of u are almost surely nite whenever u is the set of points of an i.i.d. bernoulli process. to this end, it suces to show that the probability that cu() intersects fn \\fn is summable over n. indeed, since a xed vertex v fn \\ fn belongs to cu() only if u \\ {} contains no element of {u : (u, v) n}, it follows from lemma that the probability of this is at most p|fn|, where p is the density of the bernoulli process. since |fn| n, we see that |fn| p|fn| is summable, and hence that the cu() is almost surely nite. finitely dependent processes are finitary we now turn to the construction of the cell process a. the rst level set in the cell process is simply taken to be the vertices in a modied voronoi cell of u, i.e., a := [ uu cu(u). since the voronio cells of u are almost surely nite, we see that cu(u) is almost surely nite for all u u since dist(cu(u), cu(u)) > for distinct u, u u, it follows that all connected components of a are almost surely nite. suppose now that, for some n , an has been dened in such a way that all connected components of an are almost surely nite, and let us now dene an+ let a n+ be the union of the modied voronoi cells of un+, i.e., a n+ := [ uun+ cun+(u), and recall that (as for a) all connected components of a n+ are almost surely nite. intuitively, we would like to obtain an+ from an by adding a n+ however, this might create innite clusters, and we must take care to avoid this by instead only adding a suitable subset of a n+ it will suce to slightly increase the forbidden region (a n+)c as follows: let dn+ denote the union of the connected components of an that intersect (a n+)c, and add d+ n+ to the forbidden region. precisely, we dene an+ := an (a n+ \\ d+ n+). it is straightforward to check that all connected components of an+ are almost surely nite. assuming that a a = v almost surely, it is also easy to check using () that a is a nitary aut(g)-factor of y , which would complete the proof of the proposition. it remains only to show that a a = v almost surely. by aut(g)-invariance, this is equivalent to the fact that p( an) as n . for n , let bn denote the connected component of in an {}. for n , dene the event en := n bn cun(u) for some u un o . note that { an \\ an} = en { / an}. thus, it suces to show that p(en | / an) c for some c > and all n as we now show, this holds when n is suitably chosen. since bn is almost surely nite, there exists a suciently large rn so that p(bn rn | / an) , where r := r(). since (fs)s is a flner sequence, there exists sn suciently large so that |fsn| n and |fsn| |fsn| |rn|. () set n := |fsn| then, noting that an (and thus also bn) is independent of un, p(en | / an) p \u0000bn rn and rn cun(u) for some u un | / an \u0001 = p \u0000bn rn | / an \u0001 p \u0000rn cun(u) for some u un \u0001 . since the rst term on the right hand side is at least by the choice of rn, it remains to show that, for some constant c > which does not depend on n, we have p \u0000rn cun(u) for some u un \u0001 c. yinon spinka let us rst see how to show this when fsn is a ball, say . in this case, it is not hard to see that the event in question occurs when un has a unique point in rn and no other point in +rn, so that p \u0000rn cun(u) for some u un \u0001 p \u0000|un rn| = |un +rn| = \u0001 = p \u0000ber(|rn|, n) = \u0001 p \u0000ber(|+rn \\ rn|, n) = \u0001 c. we now handle the general case in more detail. set u := un. our goal is to bound from below the probability that rn cu(u) for some u u. to this end, we rst nd a simple condition that implies the occurrence of this event. for a set f v, denote m(f) := {u v : ,uf}. set r := rn and s := sn. let us show that |u m(f r s )| = |u m(f +r s )| = = r cu(u) for some u u. () suppose that the left hand side holds. let us show that r cu(u), where u is the unique element in u m(f r s ). by the denition of cu(u), this is equivalent to r cu(u). recalling the denition of cu(u), we see that we must show that (u, w) < (u, w) for all u u \\ {u} and w r. let u u \\ {u} and w r. it suces to show that (u, w) s and (u, w) > s. towards showing this, we rst note that (v )+ = (v +) and (v ) = (v ) for any and v v, due to the fact that acts by an automorphism of g. in particular, (,uv )+r = ,u(v +r) and (,uv )r = ,u(v r), and we may drop the parenthesis when writing such terms. let us now show that (u, w) s. since u m(f r s ), we have that ,uf r s , or equivalently, r ,ufs. since w r, it follows that (u, w) s. next, we show that (u, w) > s. note that u / m(f +r s ) since u m(f r s ) m(f +r s ). thus, / ,uf +r s , or equivalently, r ,ufs = . thus, w / ,ufs and, using that f, f, . . . , fs fs, it follows that (u, w) > s. using (), we obtain p \u0000rn cun(u) for some u un \u0001 p \u0000|un m(f rn sn )| = |un m(f +rn sn )| = \u0001 = p \u0000ber(|m(f rn sn )|, n) = \u0001 p \u0000ber(|m(f +rn sn )| |m(f rn sn )|, n) = \u0001 . by lemma , we have that |m(f rn sn )| = |f rn sn | and |m(f +rn sn )| = |f +rn sn |. finally, by (), |f rn sn | |fsn| |fsn| |rn| |fsn| and |f +rn sn | |fsn| + |fsn| |rn| |fsn|, so that, by standard estimates for bernoulli random variables, both probabilities in question are bounded below by a positive constant. the above proposition established the existence of a nitary cell process a. in particular, an is an invariant set which has high density when n is large. the following proposition shows that the clusters of a dense invariant set typically have relatively small boundary. lemma . let g be a transitive graph of degree d and let be a transitive unimodular group of automorphisms of g. let b v be a random set with no innite clusters and whose distribution is -invariant. let cv denote the cluster of v in b. then, for any > , p \u0000|c| |c| \u0001 ( d + ) p( / b). finitely dependent processes are finitary proof. the proof uses the mass transport principle. dene : v by (u, v) := ( |n(u)cv| |cv| if u / b, v b otherwise . note that, almost surely, x u (u, ) = b |c| x u/ b |n(u) c| = |c| |c| b and x v (, v) = / b x vb |n() cv| |cv| = |n() b| / b d / b. the -invariance of b implies that f(u, v) := e(u, v) is diagonally -invariant. thus, the mass- transport principle () yields that e h |c| |c| b i d p( / b). the proposition now follows from markovs inequality. remark a result of h aggstr om states that any automorphism invariant edge percolation on a d regular tree (d ) with edge density at least /d has an innite cluster with positive probability. in particular, automorphism invariant cell processes do not exist on such a tree. moreover, by lemma (see also the closely related ), we see that -invariant cell processes do not exist on any transitive unimodular non amenable graph. in fact, it is shown in that a closed subgroup of aut(g) is amenable if and only if there is a - invariant site percolation on g with with no innite clusters and density arbitrarily close to it follows that a -invariant cell process on g exists if and only if is amenable. the site percolation constructed in is a factor of an i.i.d. process, though it is not nitary. ",
        "Subsections": [],
        "Groundtruth": "The section discusses the construction of a cell process on a transitive amenable graph. It shows the existence of nitary cell processes with small entropy. The construction involves using low-density Bernoulli processes to create voronoi cells, which are then used to define the cells of the process. This construction ensures that the connected components of the cell process are finite. The section also explains how the choice of a suitable metric is crucial in constructing the voronoi cells to ensure the cell process increases to a limit. A key lemma is introduced to show that every amenable graph has a flner sequence consisting of sets that are invariant under the stabilizer of some vertex. The section concludes by proving a proposition that establishes the existence of a nitary cell process on the graph with high-density invariant sets having relatively small boundaries."
    },
    {
        "Section_Num": "5",
        "Section": "5. Random total orders",
        "Text": "in this section, we construct a random total order on v that has the following properties: it is a nitary factor of an i.i.d. process with arbitrarily small entropy. it is supported on total orders having the same order type as z. the successor/predecessor of any vertex can be found in a nitary manner. let us explain these properties. a random total order on v, or more generally, a random binary relation on v, may be regarded as a random element in {, }v with this viewpoint, the notion of nitary factor easily applies to such relations. namely, such a relation is a -factor of y if it has the same distribution as (y ) for some measurable function : t v {, }v satisfying that (y)(u,v) = (y)(u,v) for all , u, v v and y t v. such a factor is nitary if for every u, v v there almost surely exists a nite (random) set w v such that (y )(u,v) is determined by (yw)ww , in the sense that (y)(u,v) = (y )(u,v) for any y t v which coincides with y on w. a total order on v has the same order type as z if there is an order preserving bijection between the two ordered spaces, i.e., a bijection f : v z such that f(u) f(v) if and only if u v. this may be equivalently formulated as saying that has no minimum or maximum and that there are nitely many elements between any two elements, i.e., every interval of the form {w v : u w v} is nite. in particular, in such an order, every vertex v has a successor (an element w v such that u w for all u v) and a predecessor (an element w v such that u w for all u v). given a factor from y to a random total order on v, we say that successors (predecessors) can be found in a nitary manner if for every u, v v there almost surely exists a nite (random) set w v such that the event that u is the -successor (-predecessor) of v is determined by yinon spinka (yw)ww . we note that, in general, there is no direct relation to the notion of nitary factor: it may be that such a factor is nitary though successors/predecessors cannot be found in a nitary manner, or it may that successors/predecessors can be found in a nitary manner though the factor is not nitary. on the other hand, for a total order having the order type of z almost surely, the second implication is easily seen to hold if successors/predecessors can be found in a nitary manner, then the factor is necessarily nitary. a total order which is a nitary factor of an i.i.d. process with innite entropy is easily obtained from the order induced by uniform random variables in assigned to each vertex. it is easy to see that this order almost surely has the same order type as q. a total order (also with the order type of q) which is a nitary factor (with exponential tails on the coding radius) of an i.i.d. process with nite entropy was constructed in for any quasi transitive graph satisfying a geometric condition similar to (). the application in did not require the i.i.d. process to have arbitrarily small entropy and so this was not stated there, though it easily follows from the proof there that this is possible. since the proof is short, we give it here. the following is essentially a reformulation of for our situation. lemma . let g be a transitive non empty graph satisfying (). for any < there exists a total order on v which is a nitary aut(g)-factor of an i.i.d. bernoulli process with density . proof. let = (v)vv be an i.i.d. bernoulli process with density . for any v v, dene zv = (zv,n)n {, , . . . }{,,... } by zv,n := x uv:dist(u,v)=n u. dene a relation on v in which u v if and only if zu zv, where denotes the lexicographical order on {, , . . . }{,,... }. then is clearly a aut(g)-factor of . it remains to show that is almost surely a total order on v and that the factor is nitary. since is clearly a preorder, to show that it is a total order, it suces to show that p(zu = zv) = for distinct u, v v. it then follows from the denition of the lexicographical order that the factor is nitary. fix u, v v distinct and consider the event en := n \\ i= {zu,i = zv,i}. since p(en) p(zu = zv) as n , it suces to show that p(en | en) for all n by () and the assumption that the graph is non empty, we have n(u) \\ n(u) n(v), as otherwise n(u) n(v), which in turn implies that n(u) = n(v) by transitivity. thus, there exists some wn n(u) \\ (n(u) n(v)). then p \u0000en | v\\{wn} \u0001 max kz p(wn = k) = max{, } = . since en is measurable with respect to v\\{wn}, it follows that p(en | en) . using lemma and the cell processes constructed in the previous section, we are able to construct a total order satisfying all three properties described above. lemma . let g be a transitive amenable non empty graph satisfying () and let > then there exists an i.i.d. process y with entropy at most , and a random total order on v which almost surely has the same order type as z, such that is a nitary aut(g)-factor of y for which successors/predecessors can be found in a nitary manner. proof. by lemma , there exists a total order on v that is a nitary factor of an i.i.d. process y having entropy at most . by proposition , there exist a cell process a that is a nitary factor of finitely dependent processes are finitary an i.i.d. process y (which we take to be independent of y ) having entropy at most . we construct the required total order on v as a nitary factor of (y, y ). the idea is to use to order the sites within the cells given by a. more precisely, we will dene an increasing sequence of partial orders . . . such that each n induces a total order on every cell of an. since s n an = v, this will produce a total order given by the union s n n, which we will show has the desired properties. precisely, we dene to be the relation in which u v whenever u and v belong to the same cell of a and satisfy that u v. then is clearly a partial order that induces a total order on any cell of a in fact, it is the union of these total orders on the cells of a (that is, it only compares vertices that belong to the same cell). next, suppose we have dened the partial order n so that it is a union of total orders on the cells on an, and let us dene n. consider a cell c of an and let d := c an = d dk be the union of the cells d, . . . , dn of an that are contained in c. we dene n in such a way that d n c \\ d by requiring that u n v whenever u d and v c \\ d. to obtain a total order on c, it remains to order the vertices in d and the vertices in c \\ d. the latter is ordered by dening u n v whenever u, v c \\ d and u v. the former is ordered by giving an order to the cells d, . . . , dk and using the n order within each cell that is, we require that n coincides with n on each cell di, and that either di n dj or dj n di for any two cells di and dj. finally, the order of the cells is determined by requiring that di n dj whenever mindi mindj. that is, the i th cell precedes the j cell in n if and only if the -minimal element in the i th cell is -smaller than the -minimal element in the j th cell. it is straightforward that n extends n and that n is a union of total orders on the cells of an. we have thus obtained partial orders , , . . . such that, for each n, n extends n and is a union of total orders on the cells of an. to show that has the order type of z, it remains to show that, almost surely, every -interval is nite and there is no -minimum and no -maximum. it follows from the construction that if u is the n successor of v, then it is also its n+-successor. thus, to conclude that every -interval is nite, it suces to show that, for every u, v v having u v, there exists n such that u n v and the interval n is nite. indeed, since n only compares vertices within the same cell of an and since all such cells are nite, all n intervals are nite. finally, no minimum or maximum can exist as this would contradict the invariance of . we have thus established that almost surely has the same order type as z. it is straightforward from the fact that the n successor of a vertex u (if it exists) is also the n+-successor of u, that the constructed factor from (y, y ) to has the property that successors/predecessors can be found in a nitary manner. as mentioned in the beginning of the section, this implies that the factor is also nitary. ",
        "Subsections": [],
        "Groundtruth": "The section constructs a random total order on a set v with properties such as being a nitary factor of an i.i.d. process with small entropy, supported on total orders with the same order type as Z, and the ability to find the successors/predecessors of any vertex in a nitary manner. The section explains the concepts of nitary factors, total orders with the same order type, and finding successors/predecessors. It also discusses the construction of total orders satisfying these properties using cell processes and i.i.d. processes, as well as establishing the nitary nature of the factors."
    },
    {
        "Section_Num": "6",
        "Section": "6. The finitary coding",
        "Text": "",
        "Subsections": [
            {
                "Section_Num": "NA",
                "Section": "NA",
                "Text": "in this section, we construct a nitary coding for nitely dependent processes. we present the details of the proof of theorem . theorem may be proved in a similar manner (see remark in section ). let x be a -invariant nitely dependent process taking values in a nite set s. recall from the proof outline in section that we shall construct a nitary factor from an i.i.d. process y = (y bits, y cell, y ord) to x. recall also that a will be a cell process that is a nitary factor of y cell and that will be a random total order on v that is a nitary factor of y ord, has the order type of z, and for which successors/predecessors can be found in a nitary manner. given the cell process a and the total order , we will use the additional randomness in y bits to construct a realization of x. yinon spinka . choosing the parameters.",
                "Subsections": [],
                "Groundtruth": "The text presents a method for constructing a unitary coding for finitely dependent processes. The proof details a theorem that can be applied in a similar way to other cases. A process X, which is invariant and finitely dependent, can be constructed from an i.i.d. process Y. By using a cell process A and a random total order, a realization of X can be derived from the additional randomness in Y. The choice of parameters is a crucial aspect in this construction."
            },
            {
                "Section_Num": "6_1",
                "Section": "6.1. Choosing the parameters",
                "Text": " fix > we shall choose the i.i.d. processes y bits, y cell, y ord to satisfy h(y bits) < h(x) + , h(y cell) and h(y ord) so that y has entropy h(y ) < h(x) + we let y bits be any i.i.d. process in which y bits v is a random number of random bits satisfying h(y bits v ) < h(x) + and e|y bits v | > h(x) + () recall that, by a random number of random bits, we mean a random variable w taking values in {, }, the set of nite words over {, }, and having the property that, conditioned on the length |w| of the word, w is uniformly distributed on {, }|w|. the desired random word can be obtained by taking w to be the empty word with probability p or a uniformly chosen sequence in {, }m with probability p, for some suitably chosen m and p < indeed, in this case, h(|w|) = p log p ( p) log( p) and e|w| = pm so that h(|w|) and e|w| h(x) + as m when p = m(h(x) + ). since the entropy and length of w are related via h(w) = e|w| + h(|w|), we see that () holds when m is suciently large. let > be as in (). by decreasing , we may additionally assume that < . () recall from section that we may assume without loss of generality that x is -dependent. by proposition , there exists a cell process a and an i.i.d. process y cell of entropy at most such that a is a nitary factor of y cell. since p( an) as n , lemma implies that p(|an()| |an()|) as n , where an() is the cell of in an. thus, by replacing (a, a, . . . ) with (am, am+, . . . ) for some large m, we may assume that p(|a()| |a()|) is arbitrary small. specically, we require that p \u0010 |a()| |a()| \u0011 < log |s| + () let y ord be an i.i.d. process with entropy at most and let be a total order on v as guaranteed by lemma (note that if the graph g contains no edges, then x is already an i.i.d. process so that there is nothing to prove). . the construction of the nitary coding. recall that the n th -successor of v is denoted by v + n and its n th -predecessor by v n. in particular, v n are random elements of v which are determined from y ord is a nitary manner. recall also that the cell process a is a nitary factor of y cell. it may be helpful from this point onward to think of a and as given, and that our goal is to use them, together with the random bits of y bits, to construct a realization of x. we shall dene, for every time t and every vertex u v, a random variable lt u {, }. we shall dene these inductively with the t = variables given by l u := {u is a level agent and |y bits u |>}. () we think of lt u as indicating whether u read a bit at time t. in particular, if lt u = for some t and u, then necessarily u is an agent (of some level). since at time an agent looks for an available bit at its own location, () says that any level agent reads a bit at time if such a bit is available. before giving the main denitions of the construction, we rst set up some auxiliary notation and denitions. as we have already mentioned, our construction has the property that if an agent u reads a bit at some time t, then the bit it read is located at u + t, i.e., it is one of the bits of the word y bits u+t . in particular, the total number of bits read from location v by time t is mt v := l v + l v + + lt vt. finitely dependent processes are finitary the bits at any location v are read sequentially the rst agent to read a bit at v will read y bits v (), the second will read y bits v () and so on. precisely, the bit read by u at time t is w t u := ( y bits u+t (mt u+t) if lt u = otherwise . for this to be well dened, we must make sure that u does not try to read a non existent bit we mention already here that this does not occur, i.e., our denitions will ensure that mt v |y bits v | for all v v and all t the word read by u by time t is then w t u := w u w u w t u, where denotes concatenation. that is, w t u is the word obtained by concatenating the bits read by u until time t in the order they were read. in particular, w t u is a word in {, }of length |w t u| = l u + l u + + lt u. we emphasize that (mt u, w t u)uv is well dened once (li u, ni u)uv,it is dened, as the former are functions of the latter and of y bits. as explained in the proof outline, we use simulations to obtain samples of distributions from random bits. we rst equip ourselves with simulations of all the possible distributions we may require throughout the construction of the nitary coding. the basic distributions we need are those of xv for a nite set v v. as we aim to obtain a -equivariant factor, we must take care when dealing with random elements of sv , as these are indexed by subsets of vertices. it would be more proper to view xv as a random element of s|v | by using the order . precisely, we proceed as follows. recall the denition of a simulation from section . by theorem , for every ordered sequence v, . . . , vm v of distinct vertices, there exists a simulation s(v,...,vm) of (xv, . . . , xvm) sm satisfying that estime (v,...,vm)() h(xv ) + since the distribution of x is -invariant, we may suppose that s(v,...,vm) = s(v,...,vm) for all (e.g., by choosing a simulation for a single representative of each orbit, and then setting s(v,...,vm) to equal the simulation of its representative). now, for a nite set v v, we let v, . . . , vm be the vertices of v , ordered according to , and set sv to be the simulation s(v,...,vm), where, for notational convenience, we interpret sout v as an element of sv (indexed by v ) through the identication sout v ()vi = sout (v,...,vm)()i. we stress that sv implicitly depends on the order . as we will also encounter situations in which regions of x have already been sampled, we will also need simulations of the distribution of xv conditioned on xu for some nite set u v which is disjoint from v . thus, for every such v and u and every su, we similarly let sv,u, be a simulation of p(xv | xu = ) satisfying that estime v,u,() h(xv | xu = ) + () for ease of notation later on, we allow v and u to intersect and we allow to have any domain containing u, by interpreting sv,u, as sv \\u,u,u in such a case. we also identify sv with sv,,. recall that every cell c in an that is not contained in an has an associated level n agent, and that this agent is responsible for generating the output on c \\ an we denote by an(v) the cell of v in an, where an(v) := if v / an, by un the set of level n agents and, for v an \\ an, by un(v) the level n agent associated to the cell an(v). with the above notation and denitions, we may now proceed to construct the nitary coding. our goal is to dene a random eld zt = (zt v)vv, which represents the output at time t. this output will be a function of (li u, mi u, w i u)uv,it (and of course of the cell process a and the total order ). once zt is dened for some t, it will then only remain to inductively dene (lt+ u )uv, as this then also denes (mi u, w i u)uv,it+ through the denitions above. this will therefore dene yinon spinka zt+ as well. we will then take a limit as t in order to obtain the output z = (zv)vv, which is the desired realization of x. to facilitate the inductive denition of (lt+ u )uv, we require some more denitions. we now regard t as xed and suppose that (li u)uv,it, and hence also (mi u, w i u)uv,it, are already dened. we dene two notions for a level n agent: that of having reached level n at time t, and that of having completed level n of the simulation by time t. we dene these notions inductively on n. we thus begin with level agents. given a level agent u u, we say that u reached level by time t (always, with no condition). u completed level by time t if the stopping time stime a(u) has been reached on input w t u, once a level agent has completed level of the simulation, the output is known on the corre- sponding level cell of the agent. that is, if a level agent u completed level by time t, then we will have zt v = sout a(u)(w t u,)v for all v a(u). to be more precise, let us dene zt, = (zt, v )vv by zt, v := ( sout a(u)(w t u)v if v a(u) for some u u and u completed level by time t otherwise . we will soon also dene zt,n = (zt,n v )vv for n , with the idea that it represents the known output on all cells of level at most n for which the simulation has completed. in particular, if zt,n v = for some v and n, then it will be the case that zt,n+ v = zt,n v . now x n and suppose that we have dened zt,, . . . , zt,n and the two notions (reached and completed) for levels less than n, in such a way that the above property holds namely, if a level m {, . . . , n } agent u completed level m by time t, then the output is known on am(u) at time t in the sense that zt,n v = zt,m v = for all v am(u). then, for a level n agent u un, we say that u reached level n by time t if every level n agent u un an(u) has completed level n by time t. the idea here is that if u reached level n by time t, then the output is known on an(u) an at time t, and we may use this information to start generating the output on the remaining part of the cell, namely, on an(u) \\ an that is, we use the simulation sv,u, with v = an(u), u = an(u) an and = zt,n we thus say that u completed level n by time t if it reached level n by time t and the stopping time stime an(u),an(u)an,zt,n has been reached on input w t u. putting this together leads to dening zt,n = (zt,n v )vv by zt,n v := ( sout an(u),an(u)an,zt,n(w t u)v if van\\an and un(v)=u for some u and u completed level n by time t otherwise . the output zt = (zt v)vv at time t is then dened by zt v := limnzt,n v . that is, to determine zt v, we rst look at the level n at which v enters the cell process, and then consider the level n agent u responsible for generating the output on the cell of v in an. if u has indeed completed level n by time t, then we read the value of zt v from the output of the corresponding simulation. finally, we are ready to dene (lt+ u )uv. as mentioned, these numbers are always zero for non agents, i.e., we set lt+ u := for u / u u . suppose now that u un for some n we say that u is active at time t + if it has reached, but has not completed, level n by time t. thus, if u is active at time t + , then ideally it would like to read a bit at that time, and indeed it finitely dependent processes are finitary may do so as long as there is an available bit at u + t + (recall that u may only read a bit from location u + t + at time t + ). this leads us to dene lt+ u := \u0000u is active at time t + and mt u+t+ < |y bits u+t+| \u0001 . () this completes the inductive denition of lt+ u for all t and u v. we note that, by construction, once the output at a vertex is determined at some time, it remains unchanged at future times that is, if zt v = for some v and t, then zt+ v = zt v. denote by tv := min{t : zt v = } the time at which the output at v is rst determined. the output z = (zv)vv is then given by zv := lim tzt v = ( ztv v if tv < if tv = . this completes the construction of the nitary factor. we record for later use the following simple property of the construction. lemma . for any t , we have that (w t u, zt u)uv and (lt+ u )uv are measurable with respect to y cell, y ord, (|y bits v |)vv and (y bits v (i))vv,imt v. proof. the proof by induction on t is straightforward from the denitions. ",
                "Subsections": [],
                "Groundtruth": "The text discusses the process of choosing parameters for i.i.d. processes in order to satisfy certain entropy conditions. It mentions constructing a random word based on random bits, defining variables for agents reading bits sequentially, introducing simulations for obtaining distributions from random bits, and defining the nitary coding to construct a realization of a random field zt. The construction involves defining outputs based on completed simulations and active agents, ensuring that once an output is determined, it remains unchanged. The final output zv is obtained by taking the limit of zt v."
            },
            {
                "Section_Num": "6_3",
                "Section": "6.3. Concluding Theorem 1.2",
                "Text": " to conclude the proof of theorem , we must establish two properties of the above construction: that the output it produces has the desired distribution, and that the output can be determined from (y bits, y cell, y ord) in a nitary manner. the former is stated in the following proposition whose proof is postponed to section below. proposition . the output z has the same distribution as x. proof of theorem . the random eld z is clearly a deterministic and -equivariant function of (y bits, y cell, y ord). thus, in light of proposition , we must only show that is nitary. since tv is almost surely nite (as zv = almost surely by proposition ), it suces to show that zt is nitary for every t this follows rather easily from the construction. to see this, we explain how to determine the value of zt v in a nitary manner. we begin by nding the level n in which v enters the cell process, i.e., v an \\ an, and then nding the cell an(v) of v in an. since a is a nitary factor of y cell, this may be done in a nitary manner. next, we nd the level n agent un(v) associated to the cell an(v). since this is just the -minimal element in an(v) \\ an, and since the order is a nitary factor of y ord, this may also be done in a nitary manner. let us suppose by induction that all steps of the construction up to time t are nitary. thus, recalling the denition of active, we see that, for any vertex w, we may determine in a nitary manner whether w is active at time t. since successors/predecessors in may be found in a nitary manner from y ord, it then also follows that lt w may be determined in a nitary manner. using again that successors/predecessors may be found in a nitary manner, we conclude that w t w may be found in a nitary manner. we would now like to check whether u completed level n by time t, and if so, nd the output value. to check this, we start at level and work our way up to level n. thus, we rst nd all level agents which are contained in an(v) (since the cell process and total order are nitary, this can be done in a nitary manner). next, for each such agent u, we check whether u completed level by time t. recall that the simulation sa(u) depends on the cell a(u) and on the order induced by on a(u). since the input word w t u, the cell process and the order are nitary, we see that we may determine whether u completed level by time t in a nitary manner, and if so, also determine the output zt, w for all w a(u) in a nitary manner. yinon spinka we now proceed to the next levels. consider some level m n. we again begin by nding all level m agents which are contained in an(v). for each such agent u, we check whether u reached level m by time t. for this we must check whether the level m agents in am(u) completed level m by time t, which, by induction, may be done in a nitary manner. if u reached level m by time t, we then check whether u completed level m by time t. similarly to before, the simulation sam(u),am(u)am,zt,m depends on am(u), am(u)\\am, the order induced by on am(u), and on (zt,m w )wam(u)am since the input word w t u, the cell process and the order are nitary, we see that we may determine whether u completed level m by time t in a nitary manner, and if so, also determine the output zt,m w for all w am(u) in a nitary manner. continuing up to level m = n yields that zt,n v may be determined in a nitary manner. since n is the level in which v enters the cell process, we have by denition that zt v = zt,n v . thus, zt v may be determined in a nitary manner, as required. ",
                "Subsections": [],
                "Groundtruth": "The section concludes the proof of Theorem 1.2 by establishing two key properties of the construction: that the output distribution matches the desired distribution and that the output can be determined in a nitary manner from input components. The proof shows that the output z has the same distribution as x, and then goes on to demonstrate that zt can be determined in a nitary manner for every t using a step-by-step construction process. The text details how to determine the value of zt v in a nitary manner by finding the entry level of v, identifying associated agents, and verifying completion at each level leading to a nitary determination of the output zt v."
            },
            {
                "Section_Num": "6_4",
                "Section": "6.4. The output has the correct distribution",
                "Text": " in this section, we prove proposition . the proof is split up into several steps. the rst step is the following lemma which formalizes the intuition that the simulations used in the construction are fed independent unbiased bits. lemma . let {, }n consist of a sequence of independent unbiased bits. let (u)uv be a collection of i.i.d. copies of , independent of (y bits, y cell, y ord). then, for any t , conditioned on (y cell, y ord), the collection (w t u u)uv has the same distribution as (u)uv. proof. we prove the statement by induction on t, taking t = as a trivial base case (where w u := for all u v). suppose now that we know it for some t and let us show it for t+ recall that w t+ u = w t u w t+ u . thus, we need to show that, conditioned on (y cell, y ord), the collection (w t u w t+ u u)uv has the same distribution as (u)uv. to this end, it suces to show that, conditioned on (y cell, y ord), the collections (w t u)uv and ( w t+ u u)uv are independent and that the conditional distribution of the latter is that of (u)uv. indeed, the induction hypothesis will then yield the desired result. we may restate our goal as showing that, conditioned on (y cell, y ord) and (w t u)uv, the collection ( w t+ u u)uv has the same distribution as (u)uv. let f be the -algebra generated by y cell, y ord, (|y bits v |)vv and (y bits v (i))vv,imt v. by lemma , (w t u)uv and q := \b u : w t+ u = = \b u : lt+ u = are f measurable. since (u)uv is independent of (y bits, y cell, y ord) and hence also of f, it suces to show that, conditioned on f, the random variables ( w t+ u )uq are independent unbiased bits. note that q is the set of vertices (agents) that read a bit at time t + , that q := {v : mt+ v > mt v} is the set of vertices from which a bit was read at time t + , and that u u + t + denes a f measurable bijection from q to q. recall also that w t+ u = y bits u+t+(mt+ u+t+) for u q. thus, it suces to show that, conditioned on f, the random variables (y bits v (mt+ v ))vq are independent unbiased bits. indeed, since q and (mt+ v )vq are f measurable by lemma , since mt+ v > mt v for all v q, and since y bits is an i.i.d. process that is independent of (y cell, y ord), we see that the random variables (y bits v (mt+ v ))vq are conditionally independent given f, and that, for any v q, the conditional distribution of y bits v (mt+ v ) is the same as the distribution of y bits v (mt+ v ) given |y bits v |. since y bits v is a random number of random bits, the latter is the distribution of an unbiased bit, and the proof is complete. finitely dependent processes are finitary we will use the above lemma for xed t and then let t tend to innity. in doing so, we will encounter the limiting word w u := limtw t u. since w t+ u extends w t u, this limit is well dened and is a word in {, }or {, }n (we will see that it is in fact a nite word almost surely). the next step towards proving proposition is to show that the output at every vertex v is eventually determined, i.e., that zv = (equivalently, tv < ) almost surely. for this, we rst show that every vertex is eventually inactive. lemma . every vertex is almost surely eventually inactive. that is, for any u v, there almost surely exists a nite t such that u is not active at any time t t proof. dene (u, v) := {v=u+t and lt u= for some t}. note that (u, v) indicates whether u read a bit located at v. since an agent may read at most one bit from any location, (u, v) also represents the number of bits read by u from location v. thus, recalling that |w t u| = l u + l u + + lt u, we have x v (u, v) = x t= lt u = |w u | and x u (u, v) = x t= lt vt = lim tmt v =: m v . the left hand side describes the number of bits read by a given site u, while the right hand side describes the number of bits read from a given site v. the mass transport principle () tells us that these quantities are the same in expectation: e|w u | = em v . () let eu be the event that u is active at innitely many times t. we wish to show that p(eu) = note that, by (), the event eu is contained in the event that for all but nitely many t , all bits at location u + t have been read by time t, i.e., eu \b mt u+t |y bits u+t | for all suciently large t = \b m u+t = |y bits u+t | for all suciently large t , where the equality follows from the fact that mt v m v |y bits v | for all v v and t suppose now that p(eu) > then by ergodicity, almost surely, ew occurs for some w v, and in particular, there almost surely exists w v such that m w+i = |y bits w+i| for all i since {w + i}iz = v almost surely, it follows by -invariance that m v = |y bits v | for all v v almost surely. thus, by (), e|w u | = e|y bits v |. () that is, the expected number of bits read by each site is precisely the expected number of available bits per site. it remains to show that this is impossible. dene (u, v) := ( |w u | |an(u)\\an| if v an \\ an and un(v) = u otherwise . recall that un(v) is the level n agent associated to the cell an(v). since a level n agent u is responsible for simulating the output on an(u)\\an and does so via the input word w u , we may think of (u, v) as follows: every level n agent u equally divides a total cost of |w u | among the vertices it serviced. observe that x v (u, v) = |w u | and x u (u, v) = |w unv (v)| |anv(v) \\ anv|, yinon spinka where nv is the level at which v entered the cell process, i.e., v anv \\ anv, and where we used that an(v) = an(u) whenever un(v) = u. thus, by () and the mass transport principle (), e|y bits v | = e \" |w unv (v)| |anv(v) \\ anv| # . () this relates the expected number of available bits per site to the length of the input words used by the simulations. we would like to reach a contradiction to the fact that there are many available bits and that the simulation is ecient. suppose that u is a level n agent. it is straightforward from the denitions that the stopping time stime an(u),an(u)an,zt,n is not reached on any prex of w t u that is not w t u itself (it may or may not be reached on the entire word w t u). it therefore follows from lemma that, conditioned on (y cell, y ord), |w t u| is stochastically dominated by stime an(u),an(u)an,zt,n() (u reached level n by time t), where {, }n consists of a sequence of independent unbiased bits, independent of y . note that, if u reached level n by time t, then zt,n coincides with z on an(u) an thus, taking expectations and t , we obtain that e h |w u | | y cell, y ordi e h stime an(u),an(u)an,z() | y cell, y ordi (u eventually reached level n). hence, by () and (), on the event that u un, we have e h |w u | | y cell, y ordi + ( hx(a(u)) if n = |an(u) \\ an| log |s| if n , where we denote hx(v ) := h(xv ) for a nite set v v. therefore, by () and the choice of , e \" |w unv (v)| |anv(v) \\ anv| | y cell, y ord # (h(x) + + ) e + (log |s| + ) ec, where e is the event that nv = and |a(v)| |a(v)|. thus, by (), e|y bits v | (h(x) + + ) p(e) + (log |s| + ) p(ec). using () and (), we see that e|y bits v | < h(x) + , which contradicts (). we therefore conclude that p(eu) = as required. we are now ready to show that the output at every vertex is eventually determined. lemma . for any v v, we have that tv < almost surely. proof. since an almost surely increases to v, it suces to show that p(zv = and v an) = for all n we prove this by induction on n, taking n = as a trivial base case by setting a := . let n and suppose that p(zv = and v an) = by -invariance, we actually have that p(zw = for some w an) = we may thus assume that zw = for all w an suppose now that zv = and v an. let u be the level n agent un(v) associated to the cell an(v). observe that, by the denition of zv and zt,n v , we have that, for all t , u did not complete level n by time t. on the other hand, since zw = for all w an, there exists a nite t such that u has reached level n by time t it follows that u is active at time t for every t > t by lemma , almost surely, no vertex is active at innitely many times, thus completing the proof that p(zv = and v an) = now that we have established that the output at every vertex is eventually determined, it remains to show that the distribution of the output is the correct one, namely, that of x. the following immediately implies proposition . finitely dependent processes are finitary proposition . conditioned on (y cell, y ord), z almost surely has the same distribution as x, where we regard x as independent of (y cell, y ord). proof. throughout the proof, we regard x as independent of y cell and y ord. we also condition on (y cell, y ord) throughout the entire proof, without explicitly mentioning this. in particular, any statement about distributions or independence should be understood as conditional on (y cell, y ord). since every nite subset of v is almost surely contained in some cell of the cell process, it suces to show that, for any n and any cell c of an, zc has the same distribution as xc. we prove this by induction on n, taking n = as a trivial base case (where a := ). suppose now that n let c be a cell of an and denote c := c an we will show that zc d = xc () and p(zc\\c | zc = ) = p(xc\\c | xc = ) for any feasible sc. () by feasible , we mean that p(xc = ) > the desired equality in distribution zc d = xc follows immediately from () and (). both parts require some type of independence, which we now establish. let {, }n consist of a sequence of independent unbiased bits. let (u)uv be a collection of i.i.d. copies of , independent of y bits. by lemma , for any t , (w t u u)u has the same distribution as (u)u. taking the limit as t , we see that (w u u)u also has the same distribution as (u)u. observe that, by construction, if c is some cell of the cell process, then zt c is a function of (w t u)uc. taking the limit as t , it follows that zc is a function of (w u )uc. it also follows from the denition of z and the fact that tv < for all v, that z is unchanged by concatenating any word to any w u . in particular, zc is also a function of (w u u)uc. we now show (). to this end, let c, . . . , cm be the cells in an that are contained in c, so that c = c cm. by the induction hypothesis, zcj d = xcj for every j m. since a is a cell process, we have that dist(cj, cj) > for j < j m. hence, using that x is -dependent, we see that {xcj}jm are independent. thus, it remains to show that {zcj}jm are also independent. since zcj is a function of (w u u)ucj, this follows from the fact that {w u u}uv are independent. to complete the proof, it remains to show (). let w be the agent associated to c and recall that w c \\ c and that c = an(w). note that zc\\c = sout c,c,z(w w ) = sout c,c,z(w w w). recall that sc,c,z is shorthand for sc\\c,c,zc. since w w w is independent of (w u u)u=w, and hence also of zc, we conclude that the conditional distribution of zc\\c given that zc = is equal to the distribution of sout c\\c,c,(w w w). thus, using that w w w has the same distribution as , we see that the distribution in question is that of sout c\\c,c,(), which is by denition p(xc\\c | xc = ), as required. ",
                "Subsections": [],
                "Groundtruth": "The section proves that the output generated has the correct distribution by demonstrating the independence and distributional properties of the simulated output at each step of the construction process. It shows that the output at every vertex is eventually determined and establishes that the distribution of the output matches the desired distribution. The proof involves various steps including the use of lemma and induction, ensuring independence of variables, and verifying the distributional consistency of the output with the desired distribution."
            }
        ],
        "Groundtruth": ""
    },
    {
        "Section_Num": "7",
        "Section": "7. Remarks and open problems",
        "Text": "remark we have given the details of the proof of theorem . theorem follows the same lines of proof, with minor modications, all of which are in fact simplications. to obtain a proof of theorem with the least modications to the existing proof, we may replace the random total order constructed in lemma with the total order induced by an i.i.d. process consisting of uniform random variables. using this order in the proof of lemma yields a random total order with the same properties as in lemma (except for the bound on the entropy of the i.i.d. process). when x is nite valued, the proof then goes through with no yinon spinka further modications. otherwise, we let y bits v consist of innitely many independent random bits, and then the proof goes through after an additional modication to the proof of lemma (which relied on the fact that the entropy of x is nite to deduce a bound on the expected number of bits used by the simulation; instead we only rely on the fact that, almost surely, the simulation uses only nitely many bits; see theorem ). it is instructive to note that a shorter and conceptually simpler proof exists when one does not need to worry about the entropy of the i.i.d. process. this is essentially what is described in constructing a nitary coding in section one way to implement the described coding would be to simply replace u t with u everywhere in the construction in section that is, instead of having an agent u try to read an unused bit from location u + t at time t, it always reads bits located at u. since we may place an innite sequence of bits at every vertex, it will never run out of available bits. in this way, there is no moving around of bits from one location to another. this could be made conceptually even simpler if instead of using simulations from random bits to obtain samples of distributions as they are needed, from the start, each y bits v is a collection (wv,u,)v,u, of independent random variables having distribution p(xv | xu = ) for all nite u, v v and su. either way, a nice feature of this construction is that the coding radius depends only on the cell process a constructed in section namely, the coding radius for determining x is at most the maximum of min{r : a() r(}}, where a() is the cell of in amin{n:an}, the coding radius for determining the cell a() and the coding radius for determining the cell process on a(), i.e., (an a())n we elaborate on this in the next remark. remark our main theorems give no information about the coding radius beyond its almost sure niteness. however, some information about the coding radius may be extracted from the proof given here. specically, theorem may be enhanced to give a universal bound on the tail of the coding radius for any xed graph and nite dependence range. more precisely, for any transitive amenable graph g and any integer k , there exists a sequence (cn) n= tending to zero such that any k dependent invariant random eld x on g is a nitary factor of an i.i.d. process with a coding radius r satisfying that p(r n) cn for all n. the sequence (cn) depends only on the graph g and on the parameter k, and not on the group nor on the random eld x. indeed, the sequence (cn) is governed by the properties of the cell process (see the last part of the previous remark). in particular, for many concrete choices of g (and k), an explicit sequence (cn) may be found. to illustrate this in a simple setting, let us show that for g = z and k = , one may take cn = /n. in this case, instead of using the construction given in section , it is simpler to consider the nitary cell process a given by an := b bn, where (bn)n are independent random subsets of z, each being an independent bernoulli percolation with parameter / then the level n := min{n : an} at which enters the cell process is a geometric random variable with parameter /, conditioned on which, the lengths l := min{m : m / an} of the cell of in an in the positive/negative directions are (independent) geometric random variables with parameter n, and the coding radius r for determining x is bounded by max{l+, l}. thus, p(r > r) e \u0002 ( n)r\u0003 x n= nern r x m= mem r, where we used the substitution n = log rm. we remark that if one would like to simultaneously control also the entropy of the i.i.d. process (as in theorem ), then it is plausible that this can be done by allowing (cn) to depend on the entropy gap (and perhaps on |s|), but we did not pursue this. remark we do not know whether condition () is necessary as stated in theorem , however, as we now explain, some condition of this form is needed (i.e., the condition cannot be completely dropped). let g be an innite transitive graph on vertex set v and let h be a nite transitive graph on m vertices. let g be the graph obtained by replacing each vertex of g with a copy finitely dependent processes are finitary of h that is, the vertex set of g is v {, . . . , m}, and (u, i) and (v, j) are adjacent in g if and only if u and v are adjacent in g, or u = v and i and j are adjacent in h. any graph g obtained in this manner is transitive, but fails to satisfy (). indeed, the balls of radius (or even when h is a complete graph) around (v, i) and (v, j) coincide. a simple case to have in mind is when g = z and h consists of an edge on two vertices, so that the vertices of g are z {, } and there is an edge between (u, i) and (v, j) if and only if |u v| let g be any graph as above and let (wv)vv be independent uniform random variables on {, . . . , m}. consider the random eld x on g dened by x(v,i) := {wv=i}. it is clear that x is - dependent and aut(g)-invariant. we claim that x is not a aut(g)-factor of any i.i.d. process y on g whose single site distribution has at least one atom (in particular, y cannot have nite entropy). indeed, for any such process y , the event y(v,) = = y(v,m) has positive probability, and on this event there is no aut(g)-equivariant way to distinguish between (v, ), . . . , (v, m). that is, any aut(g)-equivariant function : t v{,...,m} {, }v{,...,m} must satisfy (y)(v,) = = (y)(v,m) whenever y t v{,...,m} is such that y(v,) = = y(v,m). in particular, the event (y )(v,) = = (y )(v,m) has positive probability, and hence, (y ) cannot have the same distribution as x. we remark that there are transitive subgroups of aut(g) for which the above obstruction does not exist. for example, let be the subgroup of aut(g) generated by aut(g) and aut(h), both of which are naturally embedded in aut(g). simple modications to the proofs of lemma and lemma yield a -invariant random total order with the desired properties. the rest of the proof then goes through unchanged showing that our main result holds in this case: any nitely dependent -invariant process on g is a nitary -factor of an i.i.d. process with slightly larger entropy. we believe that our main result holds in many similar situations, where condition () is replaced by a suitable condition on . we did not pursue this direction. open problems. () one may wonder about the situation on non amenable graphs such as a regular tree (of degree at least three). namely, is every automorphism invariant nitely dependent process on a tree a nitary factor of an i.i.d. process? in fact, even the more fundamental question of whether such a process is a factor of i.i.d. (without the nitary condition) is still open; see . the same questions may be asked on any transitive non amenable graph. () does there exist a stationary nitely dependent process on z (or, more generally, on some transitive amenable graph) that cannot be expressed as a nitary factor of an i.i.d. pro- cess with nite expected coding radius? as mentioned, the -dependent -coloring and -dependent -coloring of are believed to be examples of such processes, but this is still unproved. () a nitary isomorphism is a nitary factor that is invertible and whose inverse is also nitary. somorodinky showed that every stationary nitely dependent process on z is nitarily isomorphic to an i.i.d. process. is this true in higher dimensions? namely, is every stationary nitely dependent process on zd nitarily isomorphic to an i.i.d. process? ",
        "Subsections": [],
        "Groundtruth": "The text discusses the proof of theorems in a technical context, focusing on simplifications and modifications to the existing proofs. It suggests alternative approaches, such as using an i.i.d. process for constructing a total order, and explores the impact of different conditions on coding radii in random fields. The section also raises open problems related to the factorization of processes on graphs and the existence of stationary processes."
    },
    {
        "Section_Num": "References",
        "Section": "References",
        "Text": "] jon aaronson, david gilat, and michael keane, on the structure of -dependent markov chains, journal of theoretical probability (), no. , jon aaronson, david gilat, michael keane, and vincent de valk, an algebraic construction of a class of one- dependent processes, the annals of probability (), itai benjamini, russell lyons, yuval peres, and oded schramm, group invariant percolation on graphs, geo- metric & functional analysis gafa (), no. , jacob van den berg and jerey e steif, on the existence and nonexistence of nitary codings for a class of random elds, annals of probability (), yinon spinka robert m burton, marc goulet, ronald meester, et al., on -dependent processes and k block factors, the annals of probability (), no. , olle h aggstr om, innite clusters in dependent automorphism invariant percolation on trees, the annals of probability (), matan harel and yinon spinka, finitary codings for the random cluster model and other innite range monotone models, arxiv preprint arxiv: (). nate harvey, alexander e holroyd, yuval peres, and dan romik, universal nitary codes with exponential tails, proceedings of the london mathematical society (), no. , alexander e holroyd, one dependent coloring by nitary factors, annales de linstitut henri poincar e, probabilit es et statistiques, , pp. alexander e holroyd, tom hutchcroft, and avi levy, mallows permutations and nite dependence, to appear in annals of probability, arxiv: (). alexander e holroyd, tom hutchcroft, and avi levy, finitely dependent cycle coloring, electronic communi- cations in probability (). alexander e holroyd and thomas m liggett, symmetric -dependent colorings of the integers, electronic com- munications in probability (). alexander e holroyd and thomas m liggett, finitely dependent coloring, forum of mathematics, pi, alexander e holroyd, oded schramm, and david b wilson, finitary coloring, the annals of probability (), no. , ia ibragimov and yv linnik, independent and stationarily connected variables, izdat. nauka, moscow (). ia ibragimov and yv linnik, independent and stationary sequences of random variables, wolters, noordho pub. (). michael keane and meir smorodinsky, a class of nitary codes, israel journal of mathematics (), no. -, michael keane and meir smorodinsky, bernoulli schemes of the same entropy are nitarily isomorphic, annals of mathematics (), no. , donald knuth and andrew yao, the complexity of nonuniform random number generation, algorithm and complexity, new directions and results (), nathan linial, distributive graph algorithms global solutions from local data, foundations of computer science, , th annual symposium on, , pp. russell lyons, factors of iid on trees, combinatorics, probability and computing (), no. , russell lyons and yuval peres, probability on trees and networks, vol. , cambridge university press, moni naor, a lower bound on probabilistic algorithms for distributive ring coloring, siam journal on discrete mathematics (), no. , meir smorodinsky, finitary isomorphism of m dependent processes, symbolic dynamics and its applications (), yinon spinka, finitary coding for the sub critical ising model with nite expected coding volume, to appear in electronic journal of probability, arxiv: (). university of british columbia. department of mathematics. vancouver, bc vt z, canada. e mail address: yinon@math.ubc.ca ",
        "Subsections": [],
        "Groundtruth": "The text references various research papers and authors who have worked on topics such as -dependent Markov chains, algebraic constructions of one-dependent processes, group invariant percolation on graphs, innite clusters in dependent automorphism invariant percolation, and more. The papers discussed cover a range of topics related to randomness, probability, and coding theory."
    },
    {
        "Section_Num": "NA",
        "Section": "NA",
        "Text": "finitely dependent processes are finitary yinon spinka abstract. we show that any nitely dependent invariant process on a transitive amenable graph is a nitary factor of an i.i.d. process. with an additional assumption on the geometry of the graph, namely that no two balls with dierent centers are identical, we further show that the i.i.d. process may be taken to have entropy arbitrarily close to that of the nitely dependent process. as an application, we give an armative answer to a question of holroyd . introduction ",
        "Subsections": [],
        "Groundtruth": "The text discusses how finitely dependent processes are abstractly connected with finitary yinon spinka. It shows that any finitely dependent invariant process on a transitive amenable graph is a finite factor of an i.i.d. process. Additionally, with a geometric assumption on the graph, where no two balls with different centers are identical, it is further demonstrated that the i.i.d. process can have entropy very close to that of the finitely dependent process. An affirmative answer to a question posed by Holroyd is provided as an application of these concepts."
    },
    {
        "Section_Num": "1",
        "Section": "1 Introduction",
        "Text": "in this work we study the dynamics of randomly switched ordinary dierential equations (odes) of the form dx dt = x = f(x, p), x = x(t) rd, x() =: x, () near bifurcation points. more precisely, we select two parameters p = p r so that () has non equivalent dynamics , which are separated by a distinguished bifurcation point p (p, p+). then we look at the piecewise deterministic markov process (pdmp) generated by switching between the vector elds f(x, p) and f(x, p+). this idea is motivated by several observations. here we just name a few: (m) in parametric families of vector elds, bifurcations occur generically. therefore, they are immediately relevant for the study of pdmps as well. in addition, the interplay between random switching and bifurcation points is not studied well enough yet. (m) from the perspective of pdmps, this setting provides natural examples to test and extend the general theory of invariant measures. (m) stochastic bifurcation theory is a very active area, where still many questions remain open. hence, studying a well dened set of standard cases involving bifurcations and stochasticity is highly desirable. institut de math ematiques, universit e de neuch atel, neuch atel, switzerland technical university of munich, faculty of mathematics, garching bei m unchen, germany arxiv:v jan (m) parameters in many models are usually only known via a possible distribution and not exactly. therefore, our work contributes to the uncertainty quantication for nonlinear systems arising in applications. before describing our main results, we briey review some of the background from pdmps and from nonlinear dynamics to provide a broader perspective. the study of randomly switched deterministic vector elds goes back at least to the works of goldstein and kac . the set up can informally be described as follows: given a starting point x rd and an initial vector eld fi taken from a nite collection {fj} of smooth vector elds on rd, we follow the ow along fi starting at x for an exponentially distributed random time. then a switch occurs, meaning that fi is replaced with a new vector eld fj, j = i. we ow along fj for another exponential time and switch again. this yields a continuous and piecewise smooth trajectory in rd that is, however, not the trajectory of a markov process. to obtain a markov process, one needs to supplement the switching process on rd with a second stochastic process that keeps track of the driving vector eld. the resulting two component process belongs to the class of piecewise deterministic markov processes (pdmps). pdmps were rst introduced by davis in an even more general setting. for instance, pdmps may involve jumps not only on the collection of vector elds but also on rd . the class of pdmps considered in this article is also known under the names of hybrid systems and random evolutions , . randomly switched vector elds have applications to areas such as ecology , gene regulation , molecular motors , epidemiology , queueing theory , and climate science , to name just a few. aside from their uses in modeling, randomly switched vector elds have intriguing theoret- ical properties. for example, switching between stable vector elds can result in an unstable situation, and vice versa. recently, examples of randomly switched vector elds were found that exhibit such a reversal of stability for almost all realizations of switching times . another interesting phenomenon is the regularizing eect random switching can have on a dynamical system. for example, random switching between two lorenz vector elds with just slightly dif- ferent parameter values induces an invariant probability measure that is absolutely continuous with respect to lebesgue measure on r, whereas the dynamics associated to each individual vector eld concentrate on attractors of lebesgue measure zero . another recent topic is the ergodic theory for randomly switched vector elds. important contributions to the ques- tion whether a switching system on a noncompact state space admits an invariant probability measure were made in . in , it was shown that a h ormander type hypoellipticity condition on the vector elds at an accessible point yields uniqueness and absolute continuity of the invariant probability measure. in this work we focus on the invariant probability measure aspect and relate it to bifurcation points. bifurcation theory has become one of the most widely used techniques to study nonlinear systems . informally, the main idea is to study vector elds under parameter variation and to determine at which points the dynamics changes fundamentally, i.e., to detect the points where the phase portraits of the vector elds are not topologically equivalent upon small parameter variation. almost full classication results exist for bifurcations with relatively few parameters, i.e., codimension one or two. these results provide suitable unfoldings, which are basically partitions of parameter space into non equivalent phase portraits . recently, substantial interest has been focused on understanding the interplay between stochas- ticity and bifurcations. yet, the setting in almost all of these works is focused on either stochastic dierential equations (sdes) involving (space-)time stochastic forcing processes , or less frequently on random dierential equations (rdes) with a xed random parameter distribu- tion . particularly interesting dynamics seems to appear for sdes in oscillatory situa- tions . recently numerical and semi analytical work shows that interesting eects also occur for switched systems near bifurcations . therefore, it is very natural that one should try to link pdmps with bifurcation theory. in this paper, we provide a full mathematical classication of the pdmps associated to () switched near local bifurcations for codimension one bifurcations. we not only include the generic fold and hopf bifurcations but also study the frequently occurring one parameter transcritical and pitchfork bifurcations. we prove under which conditions on the switching rates invariant measures occur, when they are unique, when their densities are smooth, and we also provide explicit formulas for these densities in certain cases. in addition, we prove nite time blow up results for certain parameter regimes. in summary, our theorems provide building blocks, which can be employed in various pdmps. in addition, we demonstrate that we may also derive insights from our results in three nonlinear models arising respectively in ecology, nonlinear oscillations, and collective motion. the paper is structured as follows: in section we provide more technical background from local bifurcation theory and pdmps. in section we focus on all cases where below and above the bifurcation point there are non trivial trapping regions. in these cases we characterize the occurring invariant probability measures completely. in section we consider the cases with only one non trivial trapping region. we again study the invariant measures in full detail but now also nite time blow up can appear. in section , we indicate how our results can be used in three models arising from applications. ",
        "Subsections": [],
        "Groundtruth": "This study focuses on the dynamics of randomly switched ordinary differential equations near bifurcation points. The research explores the piecewise deterministic Markov process generated by switching between vector fields, motivated by the interplay between random switching and bifurcation points. The work aims to classify the invariant measures associated with switched systems near local bifurcations, providing conditions for their occurrence, uniqueness, smoothness, and explicit density formulas for certain cases. The findings offer insights applicable to various piecewise deterministic Markov processes and demonstrate implications for nonlinear models in ecology, nonlinear oscillations, and collective motion."
    },
    {
        "Section_Num": "2",
        "Section": "2 Background",
        "Text": "",
        "Subsections": [
            {
                "Section_Num": "NA",
                "Section": "NA",
                "Text": "we briey recall the technical background needed from the two main areas we consider in this work. hence, this section mainly serves as a reference and to x the notation. readers familiar with local bifurcation theory and pdmps can skip ahead to section local bifurcation theory ",
                "Subsections": [],
                "Groundtruth": "The section briefly reviews the technical background required from two main areas relevant to the work, serving as a reference and fixing the notation. It is aimed at readers familiar with local bifurcation theory and PDMPs, allowing them to skip ahead to the section on local bifurcation theory."
            },
            {
                "Section_Num": "2_1",
                "Section": "2.1 Local Bifurcation Theory",
                "Text": "consider an ordinary dierential equation (ode) given by dx dt = x = f(x, p), x = x(t) rd, x() =: x, () where p r is the (main) bifurcation parameter, and we assume that the vector eld f : rd r rd is suciently smooth; in particular, in what follows f c(rd r, rd) is going to suce. we also refer to rd as the phase space of (). the phase space together with the foliation by trajectories x(t) is called phase portrait. suppose xis an equilibrium point (or steady state) of () for the parameter value pso that f(x, p) = without loss of generality, upon translating coordinates in the phase space rd and the parameter space r, we may assume that (x, p) = (, , . . . , ) =: consider the linearized problem near the steady state x = dxf()x = ax, x = x(t) rd. () then xis called hyperbolic if the matrix a rdd has no spectrum on the imaginary axis. in the hyperbolic case, the hartman grobman theorem (see e.g. ) implies that the systems () and () are locally topologically equivalent, i.e., small parameter variations for p (p, p), p > , do not qualitatively alter the phase portrait as the hyperbolic structure of a is robust under small parameter perturbations. more precisely, for any two parameter values p, p (p, p), there exists a homeomorphism h : rd rd such that the phase portraits of f(x, p) and f(x, p) are mapped to each other by h preserving the direction of time on trajectories. suppose a is not hyperbolic so that spec(a) ir = . a local bifurcation occurs at p= if for any p > and any open neighbourhood u = u() of x= , there exist two locally (wrt u) non homeomorphic phase portraits of () for two values p, p (p, p). in particular, a bifurcation just corresponds to the appearance of a topologically non equivalent phase portrait under parameter variation. the general strategy to analyze bifurcation problems proceeds as follows: (i) the system is reduced to the dimension dc of ker(a) using a center manifold w c loc(), (ii) on w c loc() one uses smoothness to taylor expand the vector eld and then simplies it using coordinate changes, and (iii) one proves that a nite number of polynomial terms is locally sucient to determine the topological equivalence class so a truncation yields a nite degree polynomial vector eld. the steps (i)-(iii) lead to dierent classes/families of polynomial vector elds, also called normal forms, depending upon degeneracy of spec(a) and depending upon a nite number of partial derivatives of f. in this work we shall focus on the four most common bifurcations used in practical applications for dc = and dc = , which just require a single bifurcation parameter p, and where the system has already been reduced to normal form. these cases will be the fold, hopf, transcritical, and pitchfork bifurcations. as a motivating example, consider the supercritical pitchfork normal form x = px x, x r, p r. () clearly, the equilibrium x= undergoes a bifurcation at p= as the phase portrait for p < has one globally stable equilibrium, while the phase portrait for p > has three equilibria. for p > , we nd that x= is unstable while the equilibria x = p are both locally stable; see also figure p x sp sp> figure : sketch of the bifurcation diagram for the supercritical pitchfork bifurcation normal form (). there are two classes of topologically non equivalent phase portraits here denoted by sp and sp> however, note that from the viewpoint of applications, treating p just as a static parameter is not always realistic. this approach presumes p is changed innitely slowly to bring the system to and across the bifurcation point. one option is to consider the case when p is just switched across the bifurcation point (e.g., consider shot noise eects, control action, activation of external interfaces of the system, etc). as an example, consider the problem of switching between p < and p > in the context of the pitchfork normal form (). this leads us naturally to consider piecewise deterministic markov processes as introduced in the next section. ",
                "Subsections": [],
                "Groundtruth": "The text discusses local bifurcation theory in the context of ordinary differential equations. It defines equilibrium points, hyperbolicity, and local bifurcations. The general strategy for analyzing bifurcation problems involves reducing the system to a center manifold, taylor expanding the vector field, and determining the topological equivalence class. The text focuses on four common bifurcations for two-dimensional systems and normal forms. A motivating example of the supercritical pitchfork bifurcation normal form is provided. The possibility of switching parameters quickly across bifurcation points is also considered, leading to the discussion of piecewise deterministic Markov processes in the next section."
            },
            {
                "Section_Num": "2_2",
                "Section": "2.2 Piecewise Deterministic Markov Processes",
                "Text": "in this subsection we introduce a class of pdmps characterized by poissonian random switching between a nite number of deterministic vector elds. let i be a nite index set, and let (fi)ii be a collection of vector elds on rd with some degree of smoothness. to introduce the basic framework, we just assume that (fi)ii are in c(rd, rd), but for some of the results stated below higher degrees of smoothness are required. to be able to associate ows to the vector elds, we assume in addition that (fi)ii are forward complete, i.e. for any x rd the initial value problem x = fi(x), x() = x has a unique solution t t i(x) that is dened for all t given a starting point x rd and an initial vector eld fi, the random dynamical system we consider follows the ow associated to x and fi for a random time. then a switch occurs, which means that the driving vector eld fi is replaced by a new vector eld fj chosen at random from {fk : k i \\ {i}}. again, the system ows along fj for a random time until another switch occurs, etc. the stochastic process x = (xt)t that records the position of the switching trajectory on rd is not markov because knowing (xs)st lets us infer the driving vector eld at time t. if the times between consecutive switches are exponentially distributed and independent conditioned on the sequence of driving vector elds, and if the vector elds are chosen according to a markov chain on i, then the two- component process (x, e) is already markov, where et i gives the index of the driving vector eld at time t. for more general distributions of switching times, one needs to adjoin a third component that keeps track of the time elapsed since the latest switch. it is possible to consider the situation where the rate of switching depends continuously on the location of the switching trajectory on rd , . for simplicity we assume that the switching rates do not depend on the process x. we can then give the following rigorous description of (x, e). let e = (et)t be an irreducible continuous time markov chain on the state space i. let x = (xt)t be the solution to the control problem xt = x + z t fes(xs) ds. the markov process (x, e) has innitesimal generator l acting on functions g : rd i r that are smooth in x according to lg(x, i) = fi(x), xg(x, i)+ x j=i i,j(g(x, j) g(x, i)), () where i,j is the rate at which e transitions from state i to state j. we denote the markov semigroup of (x, e) by (pt)t or just by (pt). an invariant probability measure (ipm) of (pt) is a probability measure on rd i such that = pt for all t below, we collect some results on existence, uniqueness and absolute continuity for ipm of (pt) that have been established in the literature. we call a set m rd positive invariant if m is positive invariant under the ows (i)ii associated with the vector elds (fi)ii, i.e. if for any x m, i i and t , we have t i(x) m. thus, trajectories of x starting in a positive invariant set m or entering m at some time remain in m for all future times. if there is a compact positive invariant set m, existence of an ipm is guaranteed by the krylovbogoliubov method , which applies because (x, e) is feller . in the noncompact situation, an ipm is guaranteed to exist, for instance, if (x, e) is on average contracting . by harriss ergodic theorem, existence also holds if the semigroup (pt) admits a lyapunov function as well as a minorizing measure k for every compact set k rd. recall that the support of a borel measure on rd i is the set of points (x, i) rd i such that (u {i}) > for every open neighborhood u rd of x. if xrd is an equilibrium for each of the vector elds fi, then the product of the dirac measure at xand the unique ipm of the continuous time markov chain e is a trivial ipm for (pt). if m rd is a compact positive invariant set containing such a common equilibrium x, the krylovbogoliubov method is not sucient to decide whether there are any additional ipm whose support is contained in m i. this more subtle existence question can often be addressed using the theory of stochastic persistence as developed by bena m , and as applied to the case of a common equilibrium by bena m and strickler . we now outline an existence result from that will be needed later on. let m rd be a compact positive invariant set containing the point x= , which we assume to be an equilibrium for all vector elds fi. as a technical condition, we also require that there is > such that whenever x m and x, then the entire line segment from to x is contained in m. for i i, let ai = dfi() be the jacobian matrix of fi at then, the cone cm = {tx : t , x m, x} rd is positive invariant with respect to the ows of the linear vector elds given by (ai)ii. on cm i, we dene the pdmp (y, e), which is obtained from (x, e) by replacing each vector eld fi with its linearization ai. whenever yt = , we dene the angular process t = yt yt, which evolves on the compact set sd cm. by krylovbogoliubov, (, e) admits at least one ipm. for any ipm of (, e), dene the average growth rate as () = x ii z sdcm ai (d {i}). since ytsatises d dtyt= t aettyt, birkhos ergodic theorem implies that for almost every realization of (, e) with initial distri- bution , we have lim t ln(yt) t = (). recall that an ipm of a markov process with markov semigroup (pt) and state space x is called ergodic if (a) {, } for every measurable a x such that for all t , pt x(a) = for -almost every x a. let denote the inmum and + the supremum of () over all ergodic ipm of (, e). in many situations of interest, (, e) has exactly one ipm, so = +. denition . we call a point x rd reachable from y rd if there is a nite sequence of indices i, . . . , in i and a correspond- ing sequence of positive real numbers t, . . . , tn such that tn in . . . t i(y) = x; accessible from y if for any neighborhood u of x there is z u such that z is reachable from y; accessible from s rd if it is accessible from any y s. if x is accessible from rd, we simply say that x is accessible. a point x rd is accessible if and only if for every neighborhood u of x, for every y rd and for every i, j i there is t > such that pt y,i(u {j}) > if x is accessible, then the points (x, i), i i, are contained in the support of any ipm for (pt). theorem (bena m, strickler, ). let m+ = m \\ {}. the following statements hold. if > , then there exists an ipm of (x, e) such that (m+ i) = in addition, for any starting point x m+, xt almost surely does not converge to as t . if + < and if the point is accessible, then for any starting point x m, xt converges almost surely to as t . in particular, there is no ipm that assigns positive mass to m+ i. now, we review sucient conditions for uniqueness and absolute continuity of the ipm. recall that the lie bracket of c vector elds f and f on rd is dened as (x) = df(x)f(x) df(x)f(x), x rd. let l denote the lie algebra generated by (fi)ii, i.e. l is the smallest collection of cvector elds on rd that contains (fi)ii, and is closed under linear combinations and the lie bracket operation. denition . we say that the weak bracket condition is satised at a point x rd if {f(x) : f l} = rd. the weak bracket condition is essentially h ormanders condition for smoothness of transition densities for a diusion process with the noise acting along (fi)ii, see . theorem (bena m, le borgne, malrieu, zitt, ; bakhtin, hurth, ). let u rd be an open positive invariant set. suppose (pt) admits an ipm such that (u i) = assume in addition that there exists x u such that (i) x is accessible from u and (ii) the weak bracket condition holds at x. then, is the unique ipm assigning full measure to u i, and is absolutely continuous with respect to the product of lebesgue measure on rd and counting measure on i. if d = , the weak bracket condition holds at any point that is not an equilibrium of all (fi)ii. the interesting condition is then existence of an accessible point. suppose now that (pt) admits an absolutely continuous ipm with probability density function (x, i). we refer to the projections i = (, i), i i, as invariant densities. for some simple pdmps on r i, we can give explicit formulas for invariant densities. besides, we have the following regularity result. theorem (bakhtin, hurth, mattingly, ). assume that (fi)ii are cvector elds on r with locally nite sets of critical points each. let x r such that fi(x) = for every i i. then, the invariant densities (i)ii of an absolutely continuous ipm are csmooth at x. ",
                "Subsections": [],
                "Groundtruth": "A class of Piecewise Deterministic Markov Processes (PDMPs) is introduced, characterized by random switching between a finite number of deterministic vector fields. The system follows the flow associated with a chosen vector field until a switch occurs, replacing the driving vector field with another randomly selected one. The position of the switching trajectory is recorded, and the process is not Markovian due to the knowledge of the driving vector field at each time. Under certain conditions such as exponentially distributed switching times and Markov chain selection of vector fields, a two-component process becomes Markovian. Existence, uniqueness, and absolute continuity of invariant probability measures are discussed, along with conditions for accessibility and convergence of trajectories. Sufficient conditions for uniqueness and absolute continuity of invariant probability measures are also outlined, based on the concept of weak bracket condition and the existence of accessible points. Regularity results regarding invariant densities of absolutely continuous invariant probability measures are presented for PDMPs with smooth vector fields."
            }
        ],
        "Groundtruth": ""
    },
    {
        "Section_Num": "3",
        "Section": "3 Two Nontrivial Trapping Regions",
        "Text": "",
        "Subsections": [
            {
                "Section_Num": "NA",
                "Section": "NA",
                "Text": "given a vector eld f on rd with ow function and a set v rd, we call v a trapping region with respect to f if for any x v and any t > we have t(x) v. we split our analysis into two cases, which can occur for our normal forms in dierent parameter regimes. either, there exists a trapping region v rd of nite positive lebesgue measure. or, trajectories leave any bounded set except for a set of measure zero, which is going to consist of unstable equilibria in our case. in this section, we cover the case when such a trapping region exists both below and above the bifurcation value. the case when a trapping region exists only below or only above the bifurcation value is covered in section supercritical pitchfork bifurcation ",
                "Subsections": [],
                "Groundtruth": "A trapping region with respect to a vector field in d-dimensional space is defined as a set where trajectories of the field remain within it for all time. The analysis is divided into two cases based on different parameter regimes. In one case, a trapping region of finite positive Lebesgue measure exists in the entire space. In the other case, trajectories leave any bounded set, except for a set of measure zero which contains unstable equilibria. This section specifically focuses on the scenario where a trapping region exists both below and above the bifurcation value. The cases where a trapping region exists only below or only above the bifurcation value are detailed in another section dealing with supercritical pitchfork bifurcation."
            },
            {
                "Section_Num": "3_1",
                "Section": "3.1 Supercritical Pitchfork Bifurcation",
                "Text": "consider the ode () for d = and assume the existence of a trivial branch of equilibria f(x, p) = for all p. assume that the following conditions hold at (x, p) = (x, p): xf(x, p) = , xxf(x, p) = , xxxf(x, p) < , xpf(x, p) = () then a bifurcation occurs at (x, p), which can be proven to be locally topologically equivalent to the supercritical pitchfork bifurcation normal form x = px x () the dynamics of () is easy to analyze. for p < , there is a unique globally stable equilibrium point x= for p > , x= is unstable while the equilibria x = p are locally stable. for any p r, all trajectories remain bounded so trapping regions of positive measure are easy to nd. we now analyze the normal form () from the viewpoint of pdmps by switching the parameter p. for xed parameters p< and p+ > , we switch between the vector elds f(x) = px x, f(x) = p+x x we denote the rate of switching from f to f by and the rate of switching from f to f by +. since is an equilibrium for both vector elds, the semigroup (pt) associated with the pdmp (x, e) admits at least one ipm, namely the product of the dirac measure at and the measure on i = {, } that assigns probability + ++to and ++to the latter is precisely the ipm of the continuous time markov chain e on the state space i. for ease of reference, we call this trivial ipm . theorem . the following statements hold. if + p+ < p, the semigroup (pt) admits exactly three ergodic ipm: the trivial measure , a measure such that ((, ) i) = , and a measure such that ((, ) i) = if + p+ p, then is the unique ipm for (pt). theorem . suppose that + p+ < p. then, the ergodic ipm and assigning measure to (, ) i and (, ) i, respectively, are absolutely continuous. moreover, the corresponding invariant densities and are given by i (x) = i (x) = cx p + p+ \u0000p+ x\u0001 p (i) \u0000p+ x\u0001 + p+ (+i) (,p+)(x), i i. here, c is a normalizing constant. proof of theorem : let m = and m+ = (, p+]. then, m is a compact posi- tive invariant set containing the common equilibrium moreover, as is globally asymptotically stable for f, is accessible from m. if we linearize f and f at x = , we obtain a = d dxf(x)|x= = p, a = d dxf(x)|x= = p+. we have cm = [, ) and cm s = {}. the angular process (, e) has a unique ipm that assigns probability + ++to {} {} and probability ++to {} {}. thus, + = = () = p+ + p+ + + , which is positive if + p+ < pand negative if + p+ > p. by theorem , if + p+ < p, there exists an ipm such that (m+ i) = ; and if + p+ > p, there is no ipm assigning positive mass to m+ i. suppose now that + p+ < p, and consider the open positive invariant set (, ). let x (, p+) and observe that x is accessible from (, ). since f and f do not vanish at x, the weak bracket condition is satised. by theorem , there is exactly one ipm assigning full measure to (, ) i. this measure is ergodic: by the ergodic decomposition theorem (see, e.g., ), there exists an ergodic ipm assigning positive mass to (, ) i. since (, ) is positive invariant, we have ((, ) i) = and hence = . this argument also shows that is the only ergodic ipm that assigns positive mass to (, ) i. a completely analogous reasoning applies to the positive invariant set (, ). it remains to consider the critical case + p+ = p, where theorem does not apply. to obtain a contradiction, we assume that there is an ergodic ipm that, without loss of generality, assigns measure to (, )i. by theorem , has a density , and by theorem and are smooth in (, p+). therefore, they satisfy the formula in theorem . as p+ + p+ = , and behave asymptotically as x as x since x is not integrable in a neighborhood of , we arrive at a contradiction. proof of theorem : absolute continuity of and follows from theorem . as shown in the proof of theorem , ((, p+]i) = , so the invariant densities ( i )ii vanish outside of . by theorem , ( i )ii are con (, p+) and thus satisfy the fokkerplanck equations, see for instance . written in terms of probability uxes i = i fi, i i, the fokkerplanck equations read for x (, p+) (x) = f(x)(x) + + f(x)(x), () (x) = + f(x)(x) + f(x)(x). () then, + , so + is constant. we even have + . the ode in () becomes (x) = \u0012 f(x) + + f(x) \u0013 (x), which is solved by (x) =c exp \u0012 z dx f(x) + z dx f(x) \u0013 =cx p + p+ \u0000p+ x\u0001 p\u0000p+ x\u0001 + p+ . we obtain the desired formula for with = /f and = /f the formula for follows from the fact that both f and f are odd. ",
                "Subsections": [],
                "Groundtruth": "The text discusses the supercritical pitchfork bifurcation and analyzes the dynamics of a system represented by a differential equation. Conditions for the occurrence of the bifurcation are established, along with stability properties for different parameters. The analysis is extended to piecewise deterministic Markov processes (PDMPs), where switching between vector fields is considered. The text presents theorems related to the ergodic invariant probability measures (IPMs) of the system and derives formulas for the corresponding invariant densities. Proof of the theorems involves concepts such as positive invariant sets, asymptotic stability, and Fokker-Planck equations. The critical case where the parameters coincide is also addressed, leading to a contradiction."
            },
            {
                "Section_Num": "3_2",
                "Section": "3.2 Supercritical Hopf Bifurcation",
                "Text": "consider the ode () for d = assume that x= x(p) is a family of equilibrium points for all p in a parameter space neighbourhood of p. let a = dxf(x(p), p) and assume that spec(a) = {(p) i(p)}, (p) = , (p) = , (p) = () furthermore, consider the rst lyapunov coecient l = l(p), which is computable from f using partial derivatives up to and including third order; see the formulas in . assume that l(p) < then a bifurcation occurs at (x, p), which can be proven to be locally topologically equivalent to the supercritical hopf bifurcation normal form x = px x x(x + x ), x = x + px x(x + x ). () the dynamics of () can be analyzed a lot easier upon changing to polar coordinates (x, x) = (r cos , r sin ), which gives = , r = pr r () analyzing the simple vector eld () and returning to euclidean coordinates, one nds that for p < , there is a unique globally stable equilibrium point x= for p > , x= is unstable while there exists a family of stable periodic orbits {x = p}. for any p r, all trajectories remain bounded so trapping regions of positive measure always exist. we now analyze the normal form () from the viewpoint of pdmps by switching the parameter p, again working in polar coordinates. for xed p< and p+ > , we switch between the vector elds g(, r) = (, pr r), g(, r) = (, p+r r). in analogy to the case of the supercritical pitchfork bifurcation, we denote the rate of switching from g to g by , and the rate of switching from g to g by +. as before, the origin is an equilibrium for both vector elds, so , dened as the product of the dirac measure at the origin and the discrete measure assigning probability + ++to and ++to , is an ipm. let denote the unique ipm for the pdmp induced by switching between the one dimensional vector elds f(r) = pr r, f(r) = p+r r, r > at rates and +, whose existence is guaranteed by theorem . theorem . the following statements hold. if + p+ < p, (pt) admits exactly two ergodic ipm: the measure and a measure that is the product of lebesgue measure on the unit circle s, normalized by the factor , and the ipm . if + p+ p, then is the unique ipm for (pt). proof: suppose rst that + p+ < p. let denote the product of lebesgue measure on s, normalized by the factor , and the ipm . then, for t > , i, j i, s, r > and measurable sets a s, b (, ), we have pt ,r,j(a b {i}) = a( + t)b pt r,j(b {i}), where + t should be understood modulo , and where b p denotes the semigroup associated with the pdmp induced by f and f this form of independence for and r holds because the evolution of the angular component is entirely deterministic and in particular not aected by the switching times. thus, pt(a b {i}) = z s a( + t) d x j{,} z b pt r,j(b {i}) (dr {j}) =leb(a) b pt(b {i}) = leb(a) (b {i}) = (a b {i}). hence, is an ipm for (pt). next, we show that is the only ipm such that (s(, )i) = first we show that any point in s (, p+) is accessible from s (, ). fix two points (, p) s (, p+) and (, q) s (, ). for i i, we denote the ow associated with the vector eld gi by i. as s , the radial component of s (, q) tends to let s > such that s (, q) has angular component and radial component u < p. as p (, p+), a short computation shows that t (, p) has radial component \u0012 ep+tp+c + ep+tc \u0013 , where c = p+p p +p+p > this shows that the vector eld g is both forward and backward complete on the punctured disk s (, p+), with limtt (, p) = hence, there is t < such that t (, p) has angular component and radial component v < u. for t , let h(t) denote the dierence of the radial components of t (, u) and t (, v). then, h() = uv > and h(t) u p < as h is continuous, there is (, t) such that h() = as the points t (, u) and t (, v) have the same angular component for any t , we have (, u) = (, v). thus, we can reach the point (, p) from (, q) as follows: first, ow along the vector eld g for time s + , then make a switch and ow along g for time t . for (, p) s (, p+), the vectors g(, p) and g(, p) are clearly transversal, so the weak bracket condition holds as well. by theorem , is indeed the only ipm assigning mass to s (, ). the fact that and are the only ergodic ipm follows along the same lines as in the proof of theorem . now, we consider the case + p+ p. to obtain a contradiction, suppose that there is an ipm for (pt) such that (s (, ) i) > by the ergodic decomposition theorem, we may assume without loss of generality that (s (, ) i) = consider the marginal b () = (s ), which is a probability measure on (, ) i. for t > and with b p dened as above, we have for measurable b (, ) and i i b b pt(b {i}) = x ji z b pt r,j(b {i}) b (dr {j}) = x ji z s z b pt r,j(b {i}) (d dr {j}) = x ji z s z s( + t)b pt r,j(b {i}) (d dr {j}) = x ji z s z pt ,r,j(s b {i}) (d dr {j}) = (s b {i}) = b (b {i}). this computation shows that b is an ipm for (b pt). but theorem implies that (b pt) has no ipm if + p+ p, a contradiction. ",
                "Subsections": [],
                "Groundtruth": "The text discusses a supercritical Hopf bifurcation in the context of a system of differential equations. It introduces the concept of the bifurcation occurring at a specific point (x, p) and analyzes the dynamics of the system in different coordinate systems. The text further delves into the analysis of the system using piecewise deterministic Markov processes (PDMPs) by switching parameters and vector fields. The existence of ergodic invariant probability measures (IPMs) is discussed, along with conditions under which these measures are unique. The analysis involves detailed mathematical computations and proofs to showcase the behavior of the system under various conditions."
            },
            {
                "Section_Num": "3_3",
                "Section": "3.3 Transcritical Bifurcation",
                "Text": "consider the ode () for d = and assume the existence of a trivial branch of equilibria f(x, p) = for all p. assume that the following conditions hold at (x, p) = (x, p): xf(x, p) = , xxf(x, p) = , xpf(x, p) = () then a bifurcation occurs at (x, p), which can be proven to be locally topologically equivalent to the transcritical bifurcation normal form x = px x () the dynamics of () works as follows. there are two families of equilibrium points x= and x= p. for p < , xis locally stable, while xis unstable. for p > , the stabilities switch. there are bounded trapping regions of positive measure given in the dierent parameter regimes by vp< = and vp> = with the special case vp= = for any k > for xed p< and p+ > , consider the vector elds f(x) = px x, f(x) = p+x x these vector elds are not forward complete: trajectories for f that start to the left of p and trajectories for f that start to the left of move oto in nite time. to obtain a well dened pdmp, we therefore restrict ourselves to the positive invariant set [, ), where both f and f have bounded trajectories and are in particular forward complete. we switch from f to f at rate and from f to f at rate +, and we let denote the product of the dirac measure at and the ipm of e. remark: it is possible to dene a pdmp that involves switching between f and f on the larger interval (p, ). since (p, ) is not a trapping region for f, one needs to ensure that we switch away from f before reaching the point p. this can be achieved by letting the switching rate + depend on the location x of the switching trajectory, with +(x) blowing up as x pfrom the right. theorem . the following statements hold. if + p+ < p, there are exactly two ergodic ipm: the trivial measure and a measure such that ((, ) i) = if + p+ p, then is the only ipm. this statement can be shown along the same lines as theorem . we therefore omit the proof. theorem . suppose that + p+ < p. then, the ergodic ipm assigning mass to (, )i is absolutely continuous. moreover, the corresponding invariant density is given by i(x) = cx p + p+ (p+ x) p (i)(p+ x) + p+ (+i)(,p+)(x), i i. proof: absolute continuity of follows from theorem . as ((, p+] i) = , the invariant densities (i)ii vanish outside of . by theorem , (i)ii are con (, p+) and thus satisfy the fokker planck equations. for the probability ux , we have (x) = c exp \u0012 z dx f(x) + z dx f(x) \u0013 = cx p + p+ (p+ x) p(p+ x) + p+ . as in the case of the supercritical pitchfork bifurcation, we obtain the desired formula with = /f and = /f if the switching rates + and do not depend on x, the pdmp (x, e) starting at a point to the left of will tend to in nite time with positive probability. to make this statement more precise, we dene for a < pthe stopping time a = inf{t : xt a}. proposition . let be a probability measure on r i such that ((, ) i) = , and let pa denote the law of (x, e) with initial distribution and stopped at time a. there is a nonincreasing function g : (, p] (, ) such that r p g(a) da < and pa \u0000p < \u0001 > , pa \u0000a a+ < g(a) | p < \u0001 = , a p proposition essentially says that xt goes oto in nite time with positive probability if the initial distribution assigns full measure to (, )i: there is a positive probability that x reaches the interval (, p] in nite time. and once x is in (, p], it blows up to with probability in time less than \u0000p p \u0001 + \u0000p p \u0001 + . . . x k= g(pk) < . proof of proposition : fix a p let us rst show that pa(p < ) > let > be so small that ((, ] i) > , and let r, s > such that r () = , s ( ) = p if pa(s + r > a) > , we also have pa(p < ) > if pa(s + r a) = , we use the estimate pa(p < ) pa(p < , x , et = t ). () suppose that s + r a and x . then, we have xr if in addition et = for all t , it follows that p < . hence, the term on the right side of () equals pa(x , et = t ) > now, we come to the second statement. we will specify the function g later in the proof. since there is c > such that f(x) f(x) c for all x (, p], we have a < for all a p whenever p < . by the strong markov property, pa(a a+ < g(a) | p < ) = pa(a < g(a)), where is the distribution of (x, e)a+ under pa( | p < ), and thus satises ((, a + ] i) = in light of f f c, we have under pa xt t (a + ), t as a result, if we let g(a) be dened by the relation g(a) (a + ) = a, we have a < g(a) under pa. since trajectories of f starting in (, p] tend to in nite time, we also have r p g(a) da < . proposition and the ergodic decomposition theorem imply that there is no ipm assigning positive mass to (, ) i. looking at proposition , it is natural to ask under which conditions a blow up of xt to in nite time happens almost surely. the answer follows from theorem below. theorem . let a p and let be a probability measure on ri such that ((, ) i) = if + p+ < p, we have pa(p < ) = if + p+ > pand if ((p, ) i) > , we have pa(p < ) < and pa \u0010 {p < } n lim txt = o\u0011 = as stated in the following lemma, with probability , xt either diverges to in nite time or converges to as t . lemma . if a p and if is a probability measure on ri such that ((, )i) = , we have pa \u0010 {p < } n lim txt = o\u0011 = proof of lemma : under pa, the complement of {p < } {limtxt = } is {p = } [ n= \\ k= [ tk \u001a xt n \u001b . for xed n n, consider the event \\ k= [ tk \u001a xt n \u001b . on this event, there is t > such that xt n for every t t, or there is a sequence of times tj such that (x, e)tj = ( n, ) for every j. in the former case, let s > such that s ( n) = p then, p < or, pa almost surely, there is r > t such that et = for r r r + s, which also yields p < . in the latter case, observe that the rst return time to state ( n, ) is nite with probability strictly less than , so by the strong markov property the event {(x, e)tj = ( n, ) j} has probability proof of theorem : assume rst that + p+ < p. by lemma it suces to show that pa \u0010 lim txt = \u0011 = () let m = [ p , ] and m+ = [ p , ). let f be a smooth vector eld that coincides with f on the interval [p , ], is strictly negative on (p , ), and has p as an equilibrium point. in addition, we assume that f(x) f(x) for all x r. then, m is positive invariant for the vector elds f and f, and is accessible from m. let ( x, e) denote the pdmp with vector elds f, f and switching rates , +. following the proof of theorem and applying theorem , we see that for any starting point x m+, xt almost surely does not converge to as t . the markov property and the fact that any switching trajectory starting in (, ) and converging to has to visit points in m+ imply that this result extends to starting points x (, ). since f f, we nally infer (). now, we consider the case + p+ > p, assuming that ((p, ) i) > dening f and ( x, e) as above, we obtain with theorem that for any starting point x m = [ p , ], xt converges almost surely to as t . fix x [ p , ). then, x, p \u0010 lim t xt = \u0011 = , where x, p is the distribution of ( x, e) with initial distribution x, the rst return time for state (x, ) must then be innite with positive probability. in other words, there is a positive probability that the pdmp ( x, e) starting in (x, ) stays in [ p , ) i for all t and thus coincides with (x, e) starting in (x, ). in particular, pa x,(limtxt = ) > for all x ( p , ). let > be so small that ((p+ , ) i) > , and let s > such that s (p+ ) = p . then, for any (y, i) (p+ , ), (ps y,i)a(( p , ) {}) > it follows that pa \u0010 lim txt = \u0011 x ii z p+ z p pa x, \u0010 lim txt = \u0011 (ps y,i)a(dx {}) (dy {i}) > the claim made in part of theorem then follows from lemma . ",
                "Subsections": [],
                "Groundtruth": "The text discusses transcritical bifurcations in a dynamical system described by a specific ordinary differential equation and conditions that lead to these bifurcations. It introduces concepts such as equilibrium points, stability, trapping regions, and transitions between vector fields. The text also explores the dynamics of the system under different parameter regimes and conditions, providing theorems and propositions to explain the behaviors observed. Additionally, it discusses ergodic measures, invariant densities, and the probability of blow-up to infinite time for trajectories starting in certain intervals. Lemmas and proofs are presented to support the theoretical findings, illustrating how the system evolves and converges under various circumstances."
            }
        ],
        "Groundtruth": ""
    },
    {
        "Section_Num": "4",
        "Section": "4 One Nontrivial Trapping Region",
        "Text": "",
        "Subsections": [
            {
                "Section_Num": "4_1",
                "Section": "4.1 Subcritical Pitchfork Bifurcation",
                "Text": "consider the ode () for d = and assume the existence of a trivial branch of equilibria f(x, p) = for all p. assume that the following conditions hold at (x, p) = (x, p): xf(x, p) = , xxf(x, p) = , xxxf(x, p) > , xpf(x, p) = () then a bifurcation occurs at (x, p), which can be proven to be locally topologically equivalent to the subcritical pitchfork bifurcation normal form x = px + x () the dynamics of () works as follows. for p < , there are three equilibrium points x= and x = p. xis locally stable, while x are unstable. for p > , x= is the only equilibrium point and it is unstable. for p there is no trapping region of positive measure. however, = v is a trapping region for the dynamics when p < for xed p< and p+ > , we switch between f(x) = px + x, f(x) = p+x + x at rates and +. the trivial measure is dened exactly as for the supercritical pitchfork bifurcation. as for the transcritical bifurcation, the vector elds f and f are not forward complete. e.g., any trajectory of f not starting at blows up in nite time. if one wishes to dene a pdmp outside of the common equilibrium point , one can either let the rate + of switching from f to f depend on the location x, with +(x) diverging to as x approaches pfrom the right and pfrom the left. or one can stop the pdmp with constant switching rates once it reaches certain thresholds. for the latter model, is the unique ipm. besides, we have the following result that is reminiscent of proposition and theorem . since f and f are odd functions, we may restrict ourselves to the interval (, ), with the understanding that there are completely analogous statements about (, ). theorem . let be a probability measure on r i such that ((, ) i) = , and let a = inf{t : xt a} for a > p. let pa denote the law of (x, e) with initial distribution and stopped at time a. there is a nonincreasing function g : [p+, ) (, ) such that r p+ g(a) da < and pa(p+ < ) > , pa(a a < g(a) | p+ < ) = , a p+ if + p+ < p, we have pa(p+ < ) = for a p+ if + p+ > pand ((, p) i) > , we have pa(p+ < ) < and pa \u0010 {p+ < } n lim txt = o\u0011 = the proof is analogous to the ones of proposition and theorem , and we omit it. ",
                "Subsections": [],
                "Groundtruth": "In the context of a subcritical pitchfork bifurcation described by the given ODE, certain conditions lead to a bifurcation at a specific point. The dynamics involve stability changes of equilibrium points based on the bifurcation parameter. Switching between different vector fields occurs depending on the parameter's value. Furthermore, for a defined probability measure and with specific conditions, a function g is introduced, leading to certain results about the behavior of the system. The text also mentions the potential of defining a piecewise deterministic Markov process with constant switching rates and provides a comparative discussion about the characteristics of the functions and equilibrium points involved."
            },
            {
                "Section_Num": "4_2",
                "Section": "4.2 Subcritical Hopf Bifurcation",
                "Text": "consider the same setting as in section , except that we now assume that the rst lyapunov coecient satises l(p) > this leads to a subcritical hopf bifurcation normal form x = px x + x(x + x ), x = x + px + x(x + x ). () here the unstable bifurcating family of periodic orbits {x = p} exists for p < and in this case x= is locally stable. xis unstable for p for p < , there is a trapping region of positive measure vp< = {x r : x p}. after a change of variables to polar coordinates, the system in () becomes =, r =pr + r for xed p< and p+ > , we then switch between g(, r) = (, pr + r), g(, r) = (, p+r + r) at rates and +. here, we encounter the same issue as for the subcritical pitchfork bifurcation. then, theorem applies to the pdmp induced by the vector elds f(r) = pr + r, f(r) = p+r + r, and thus to the evolution of the radial component of the pdmp induced by g and g ",
                "Subsections": [],
                "Groundtruth": "In the context of a subcritical Hopf bifurcation, assuming the first Lyapunov coefficient satisfies a specific condition, a normal form equation is derived leading to the existence of an unstable bifurcating family of periodic orbits for certain parameter values. By changing variables to polar coordinates, the system undergoes a specific transformation and encounters challenges similar to those seen in the subcritical pitchfork bifurcation. Theorem is then applied to the piecewise deterministic Markov process (PDMP) induced by certain vector fields, impacting the evolution of the radial component of the PDMP."
            },
            {
                "Section_Num": "4_3",
                "Section": "4.3 Fold Bifurcation",
                "Text": "consider the ode () for d = assume that the following conditions hold at (x, p) = (x, p): f(x, p) = = xf(x, p) = , xxf(x, p) = , pf(x, p) = () then a bifurcation occurs at (x, p), which can be proven to be locally topologically equivalent to the fold (or saddle node) bifurcation normal form x = p x () for p > , there are two equilibrium points x = p. x+ is locally stable, while xis unstable. for p < , there are no equilibria. only for p > , there is a trapping region given by vp> = . for p< , p+ > , we switch between f(x) = px, f(x) = p+ x at rates and +. theorem . let be a probability measure on r i, and let a = inf{t : xt a} for a < p+. let pa be the law of (x, e) with initial distribution and stopped at time a. then, there is a nonincreasing function g : (, p+] (, ) such that r p+ g(a) da < and pa(p+ < ) = , pa(a a+ < g(a) | p+ < ) = , a p+ in words, xt diverges to in nite time almost surely. proof of theorem : let us show that pa(p+ < ) = the rest is analogous to the proof of proposition . for xed x p+ , let s such that s (x) = p+ , and let s > such that s (p+) = p+ set s = max{s, s} and let i i. with x,ipa- probability , we have p+ < or there is r such that et = for all t . but the latter case also implies p+ < because any switching trajectory starting from x cannot move to the right of max{x, p+}. as a result, pa \u0000p+ < \u0001 = x ii z x,ipa \u0000p+ < \u0001 (dx {i}) = ",
                "Subsections": [],
                "Groundtruth": "In the context of fold bifurcation, considering the ODE () for d = and certain conditions at (x, p) = (x, p), a bifurcation can occur locally topologically equivalent to the fold (or saddle node) bifurcation normal form x = p x () for p > . Under these conditions, two equilibrium points x = p exist, where x+ is locally stable and x- is unstable. For p < , there are no equilibria, but for p > , a trapping region is defined by vp> = . A theorem states that for a probability measure , there exists a nonincreasing function g such that xt diverges to infinity almost surely, with the proof involving certain conditions and stochastic processes."
            }
        ],
        "Groundtruth": ""
    },
    {
        "Section_Num": "5",
        "Section": "5 Applications",
        "Text": "",
        "Subsections": [
            {
                "Section_Num": "NA",
                "Section": "NA",
                "Text": "in this section, we provide several very brief examples of systems where the switching viewpoint near bifurcations via pdmps can yield insight into concrete dynamical systems arising in ap- plications. in particular, the normal form results can be used suciently close to bifurcation points after a normal form transformation. furthermore, they can also be used directly to form conjectures about the dynamics of the applications. the paradox of enrichment ",
                "Subsections": [],
                "Groundtruth": "The text provides examples of systems where switching viewpoints near bifurcations using Piecewise Deterministic Markov Processes (PDMPs) can offer insights into concrete dynamical systems in applications. Normal form results can be effectively utilized near bifurcation points post a normal form transformation. These results can also aid in forming conjectures about the dynamics of the applications, as highlighted in the paradox of enrichment."
            },
            {
                "Section_Num": "5_1",
                "Section": "5.1 The Paradox of Enrichment",
                "Text": "the paradox of enrichment is a classical topic in ecology. one simple variant can be found in classical predator prey systems, such as the rosenzweig macarthur model x = x \u0010 x p \u0011 xy +x, y = xy +x my, () where x, y [, ) are population densities of prey and predator, > is a parameter repre- senting a conversion factor, m > is the mortality of the predator, while p > is the carrying capacity for the prey. the basic concept of the paradox of enrichment is that increasing the carrying capacity p > can actually lead to more likely extinction events triggered by additional stochastic eects, which can be supported by a classical bifurcation analysis of () as follows: let us x m = and = while varying p as the main bifurcation parameter. besides the two boundary equilibrium points (x, y) = (, ) and (x, y) = (p, ), we nd the nontrivial co existence equilibrium point (x, y) = (x, y(p)) = \u0012 , (p ) p \u0013 which is in the relevant domain given by the non negative quadrant for p > / linearizing () around (x, y) shows that the coexistence equilibrium is locally asymptotically stable for p (, ). another direct calculation shows that a supercritical hopf bifurcation occurs at p= the resulting locally asymptotically stable limit cycle generated in the hopf bifurcation for p > grows in phase space. hence, solutions can get closer to the two coordinate axes {x = , y } and {y = , x }, which could make it more likely that a stochastic eect triggers an extinction event of a species. therefore, enrichment may lead to a potential increase in extinction events. of course, it is important to mention that there is still a debate in the literature on the mechanisms and possible variations of the paradox of enrichment . we do not provide here a full discussion of the various arguments made in favor or against the paradox but instead point towards the eect of randomness in the parameter p. from an ecological perspective, it can be plausible to view p as a parameter, which switches randomly between dierent carrying capacities since the environment might be driven by external random events such as droughts, oods, storms, earthquakes, sudden human intervention, or even just dierent seasonal climate conditions. suppose we switch p randomly in a range near p= with rates and values p as dened in section , where the parameters p are chosen so that the supercritical hopf normal form () is a good local approximation of () near p. then theorem suggests an interesting dichotomy of the ergodic ipm. either, we have only the invariant measure , which is concentrated on the equilibrium branch (x, y(p)) with probabilities determined by the switching rates, or we have two probability measures given by and , where is a non trivial product measure also supported on the periodic orbit. one natural ecological interpretation of this eect is that we can actually avoid the paradox of enrichment from the viewpoint of measures if we restrict to those switching rates, which only lead to the ipm , i.e., that we switch frequently enough from the periodic stable regime above the bifurcation to the stationary stable regime below the bifurcation. ",
                "Subsections": [],
                "Groundtruth": "The paradox of enrichment in ecology discusses how increasing the carrying capacity of prey can lead to more likely extinction events due to additional stochastic effects. Through bifurcation analysis, it is shown that a supercritical Hopf bifurcation occurs at a certain parameter value, leading to instability and potential extinction events. The text also highlights the debate in the literature on this paradox and suggests that randomness in environmental factors, represented by parameter switching, can mitigate the paradox. By intelligently switching parameters, it is possible to avoid the negative effects of enrichment and maintain ecological stability."
            },
            {
                "Section_Num": "5_2",
                "Section": "5.2 Relaxation Oscillations",
                "Text": "consider the van der pol (vdp) / fitzhugh nagumo (fhn) system x = p x + x, p = x, which is a classical model used for bistable systems with relaxation oscillations and excitability. the parameter is usually assumed to be small, so that in the singular limit , we obtain the fast subsystem ode x = p x + x, () where p r becomes a parameter. we can also view p as a random parameter for the dynamics. observe that there are several branches of equilibrium points for () given by solving p = x x. if p (/, /), then there are three equilibria, two locally asymptotically stable and one unstable. at p= / and p= /, there are non degenerate fold bifurcations. while for |p| > /, there is always only one globally stable equilibrium point x. let us focus on the case of switching p near p= /; the case p= / simply follows by a symmetry argument. if we switch the dynamics randomly above and below the fold bifurcation, theorem suggests that with probability one, we are going to diverge away from the region of the fold, i.e., we are going to obtain a point measure eventually concentrated on single remaining equilibrium xexisting for p> / hence, if we have random switching across both folds, then it is possible to obtain the classical structure of a relaxation oscillations . this conrms similar observations made already numerically for a similar class of randomly switched van der pol oscillators in . ",
                "Subsections": [],
                "Groundtruth": "The van der Pol (VDP) / FitzHugh Nagumo (FHN) system is a classical model for bistable systems with relaxation oscillations and excitability. In the singular limit with a small parameter, the fast subsystem ODE shows multiple equilibrium points. When the parameter is near specific values, there are fold bifurcations leading to stable and unstable equilibria. By randomly switching dynamics above and below the fold, it is possible to observe relaxation oscillations with a single globally stable equilibrium point. This behavior has been confirmed both theoretically and numerically for similar systems."
            },
            {
                "Section_Num": "5_3",
                "Section": "5.3 Adaptive Swarming",
                "Text": "the next ode model we are going to discuss is motivated by the swarming motion of locusts in a ring shaped arena . an adaptive network model for this situation was proposed in . the network model views locusts in clockwise moving and anti clockwise moving nodes and keeps track of interactions between dierent locusts/nodes via the links of the network. the model is reduced to a low dimensional ode via moment closure and we focus here only on the essential features of the following low dimensional ode model x = q(x x) + w(y /(x) y /(x)), x = q(x x) + w(y /(x) y /(x)), y = q(y y) + w(y + y /l yy/x), +w(y /x + y /(x ) y y/x ) + aex dey y = q(y y) + w(y + y /x yy/x), +w(y /x + y /(x ) y y/x ) + aex dey () and the conservation equation (y + y + y) = axx dy + ae(x + x ) de(y + y), () where q, w, w, ae, de, a, d are positive parameters. basically, x and x correspond to propor- tions of clockwise r (right) and anti clockwise l (left) moving nodes, while y,, capture the link densities rr, ll and rl between the two classes of nodes respectively. one checks that for ae = = de, one can solve the steady state problem, which provides a branch of solutions given by x = = x this state corresponds to an equal number of left and right moving nodes. this disordered state is locally asymptotically stable up to a supercritical pitchfork bifurcation at a = d p q/w the parameter a controls the rate at which new connections between left and right moving nodes form. therefore, for a high connectivity between dierent groups, the system can move into an ordered phase given by the steady state (x) := q qd /(wa ) and similarly for x with reversed signs. this corresponds to a classical symmetry breaking and above the supercritical pitchfork, the two majority states are locally asymptotically stable. clearly, we can also view a =: p as our randomly switched parameter across the pitchfork bifurcation. then theorem provides us with the case of either one or three ipm if we switch near a the interpretation for swarming is that we eectively can allow for a certain percentage of disordered motion as long as the switching rate back into the ordered phase is large enough to get an eective ordered phase. furthermore, if we have the case of three ipm, then we are bound to observe not only a pure ordered state but intermittent phases of disordered motion as the non trivial measures are supported also near the locally unstable state above the bifurcation point. references vladimir v. anisimov. switching processes in queueing models. iste ltd, john wiley & sons, inc., l. arnold. random dynamical systems. springer, berlin heidelberg, germany, yuri bakhtin and tobias hurth. invariant densities for dynamical systems with random switching. nonlinearity, ():, yuri bakhtin, tobias hurth, and jonathan c. mattingly. regularity of invariant densities for d systems with random switching. nonlinearity, :, m. bal azs, g. horv ath, s. kolumb an, p. kov acs, and m. telek. fluid level dependent markov uid models with continuous zero transition. performance evaluation, (): , special issue: performance m. baudel and n. berglund. spectral theory for random poincar e maps. siam j. math. anal., ():, m. bena m. stochastic persistence, part i. preprint. m. bena m and e. strickler. random switching between vector elds having a common zero. https://arxiv.org/abs/. michel bena m, st ephane le borgne, florent malrieu, and pierre andr e zitt. qualitative properties of certain piecewise deterministic markov processes. annales de linstitut henri poincar e, : , michel bena m, st ephane le borgne, florent malrieu, and pierre andr e zitt. quantitative ergodicity for some switched dynamical systems. electron. commun. probab., :no. , , michel bena m, st ephane le borgne, florent malrieu, and pierre andr e zitt. on the sta- bility of planar randomly switched systems. ann. appl. probab., ():, michel bena m and claude lobry. lotkavolterra with randomly uctuating environments or how switching between benecial environments can make survival harder. ann. appl. probab., ():, n. berglund and b. gentz. noise induced phenomena in slow fast dynamical systems. springer, m. breden and c. kuehn. exploring invariant sets of random dynamical systems via poly- nomial chaos. preprint, pages , paul c bresslo. stochastic switching in biology: from genotype to phenotype. journal of physics a: mathematical and theoretical, ():, j. buhl, d.j. sumpter, i.d. couzin, j.j. hale, e. despland, e.r. miller, and s.j. simpson. from disorder to order in marching locusts. science, ():, g. da prato and j. zabczyk. ergodicity for innite dimensional systems, volume of london mathematical society lecture note series. cambridge university press, cambridge, m. h. a. davis. piecewise deterministic markov processes: a general class of nondiusion stochastic models. j. roy. statist. soc. ser. b, ():, with discussion. m. h. a. davis. markov models and optimization, volume of monographs on statistics and applied probability. chapman & hall, london, m. engel, j.s. lamb, and m. rasmussen. bifurcation analysis of a stochastically driven limit cycle. arxiv:, pages , stewart n. ethier and thomas g. kurtz. markov processes. wiley series in probability and statistics. john wiley & sons, inc., hoboken, new jersey, characterization and convergence. a. faggionato, d. gabrielli, and m. ribezzi crivellari. non equilibrium thermodynamics of piecewise deterministic markov processes. j. stat. phys., ():, a. faggionato, d. gabrielli, and m. ribezzi crivellari. averaging and large deviation princi- ples for fully coupled piecewise deterministic markov processes and applications to molecular motors. markov processes and related elds, :, s. goldstein. on diusion by discontinuous movements, and on the telegraph equation. quart. j. mech. appl. math., :, t. gross and h. sayama, editors. adaptive networks: theory, models and applications. springer, j. guckenheimer and p. holmes. nonlinear oscillations, dynamical systems, and bifurca- tions of vector fields. springer, new york, ny, martin hairer. ergodic properties of markov processes. lectures given at the university of warwick, http://www.hairer.org/notes/markov.pdf, reuben hersh. the birth of random evolutions. math. intelligencer, ():, c. huepe, g. zschaler, a.-l. do, and t. gross. adaptive network models of swarm dynam- ics. new j. phys., page (), mark kac. a stochastic model related to the telegraphers equation. rocky mountain j. math., :, k.l. kirk. enrichment can stabilize population dynamics: autotoxins and density depen- dence. ecol., :, c. kuehn. deterministic continuation of stochastic metastable equilibria via lyapunov equations and ellipsoids. siam j. sci. comp., ():aa, c. kuehn. moment closure - a brief review. in e. sch oll, s. klapp, and p. h ovel, editors, control of self organizing nonlinear systems, pages springer, c. kuehn. quenched noise and nonlinear oscillations in bistable multiscale systems. epl (europhysics letters), :, yu.a. kuznetsov. elements of applied bifurcation theory. springer, new york, ny, rd edition, sean d. lawley, jonathan c. mattingly, and michael c. reed. sensitivity to switching rates in stochastically switched odes. commun. math. sci., ():, dan li, shengqiang liu, and jingan cui. threshold dynamics and ergodicity of an sirs epidemic model with markovian switching. journal of dierential equations, (): , andrew j. majda and xin t. tong. geometric ergodicity for piecewise contracting pro- cesses with applications for tropical stochastic lattice models. communications on pure and applied mathematics, ():, florent malrieu. some simple but challenging markov processes. ann. fac. sci. toulouse math. (), ():, e. mccauley and w.w. murdoch. predator prey dynamics in environments rich and poor in nutrients. nature, :, e.f. mishchenko and n.kh. rozov. dierential equations with small parameters and re- laxation oscillations (translated from russian). plenum press, v. nair, s. sarkar, and r.i. sujith. uncertainty quantication of subcritical bifurcations. probab. eng. mech., :, david nualart. the malliavin calculus and related topics. probability and its applications (new york). springer verlag, berlin, second edition, m.l. rosenzweig. paradox of enrichment: destabilization of exploitation ecosystems in ecological time. science, :, m.l. rosenzweig and r.h. macarthur. graphical representation and stability conditions of predator prey interactions. american naturalist, :, s. sadhu and c. kuehn. stochastic mixed mode oscillations in a three species predator prey model. chaos, ():, edouard strickler. randomly switched vector elds sharing a zero on a common invariant face. available at https://arxiv.org/abs/, s.h. strogatz. nonlinear dynamics and chaos. westview press, g. teschl. ordinary dierential equations and dynamical systems. ams, g. george yin and chao zhu. hybrid switching diusions, volume of stochastic modelling and applied probability. springer, new york, properties and applications. ",
                "Subsections": [],
                "Groundtruth": "The section discusses an adaptive network model inspired by the swarming motion of locusts in a ring-shaped arena. The model tracks interactions between locusts/nodes moving clockwise and anti-clockwise, represented by low-dimensional ordinary differential equations. By adjusting parameters, the system can reach a steady state with equal numbers of left and right moving nodes, exhibiting symmetry breaking above a certain threshold. Through a supercritical pitchfork bifurcation, the system can transition into an ordered phase with stable majority states. The model allows for a percentage of disordered motion, emphasizing the importance of a high switching rate back to the ordered phase for effective swarm dynamics."
            }
        ],
        "Groundtruth": ""
    },
    {
        "Section_Num": "NA",
        "Section": "NA",
        "Text": "random switching near bifurcations tobias hurthand christian kuehn january , abstract the interplay between bifurcations and random switching processes of vector elds is studied. more precisely, we provide a classication of piecewise deterministic markov pro- cesses arising from stochastic switching dynamics near fold, hopf, transcritical and pitchfork bifurcations. we prove the existence of invariant measures for dierent switching rates. we also study, when the invariant measures are unique, when multiple measures occur, when measures have smooth densities, and under which conditions nite time blow up occurs. we demonstrate the applicability of our results for three nonlinear models arising in appli- cations. introduction ",
        "Subsections": [],
        "Groundtruth": "The text explores the interaction between random switching processes and bifurcations in vector fields. It presents a classification of piecewise deterministic Markov processes resulting from stochastic switching dynamics near specific types of bifurcations. The study establishes the existence of invariant measures for varying switching rates and investigates factors such as uniqueness of measures, occurrence of multiple measures, smooth densities of measures, and conditions for finite time blow up. The applicability of the findings is demonstrated through analysis of three nonlinear models in practical applications."
    },
    {
        "Section_Num": "Acknowledgments",
        "Section": " Acknowledgments",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "References",
        "Section": " References",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "S1:",
        "Section": " S1: Exact results on complete graphs",
        "Text": "consider a complete graph in which every vertex interacts with every other vertex. the energy of a color congu- ration c = (c, c, . . . , cn) is e(c) = n n x i= n x j=i+ cj ci , () where the rescaling factor n is introduced to make the total energy an extensive quantity. suppose there are nc vertices of color c in the conguration c, then the total energy can be rewritten as e(c) = n q x c= \u0010nc n \u0011 = n q x c= c , () where c nc n is the density of color c. in the thermodynamic limit of n the energy density u is simply u = q x c= c . () the total number of microscopic congurations corresponding to the coarse grained state (n, n, . . . , nq) is (n, n, . . . , nq) = n! n!n!...nq!. in the thermodynamic limit then the entropy density s is s = n x c= c ln c . () the task is now to nd the values of (, , . . . , q) which lead to the maximum of s under the constraints of xed energy density u and xed number n of vertices. this can be achieved by introducing a function z(, . . . , q) with two lagrange multipliers and : z = n x c= c ln c + q x c= c + n x c= c . () the rst derivative of this function with c is z c = ln c + c + therefore, from the condition z c = we obtain that c = ec pq c= ec (c = , , . . . , q) . () the color symmetric xed point solution of eq. () is c = q for all colors c. the energy density of this disordered symmetric (ds) solution is the maximum value u = q, and its entropy density is ln q. if the energy density u decreases from the maximum value, then color symmetry has to be broken. therefore the critical energy density for spontaneous symmetry breaking (ssb) is simply umic = q. the other xed point solutions of eq. () can be characterized by two parameters, and m. the real parameter [ q, ] is the density of a dominant color, and the integer m {, , . . . , q } is the number of dominant colors. without loss of generality we assume that i = for i = , , . . . , m and j = m qm for j = m + , m + , . . . , q. at a xed integer value of m, the order parameter is expressed as = q + r \u0000 m q \u0001 (umic u) . () - - - - - u q= - - - - - s fig. : q state potts model on a complete graph. the maximum energy density is u = q and the minimum energy density is u= there is a continuous microcanonical ssb transition at energy density umic = q, which is identical to the maximum energy density (umic = in the example of q= shown here). the density of the dominant color deviates continuously from q (marked by the horizontal dashed line). the inset shows the non concave relationship between the entropy density s and the energy density u. notice that is a continuous function of energy density u, so the ssb transition at umic must be a continuous phase transition (fig. ). the entropy density s at the polarized xed point of eq. () is s = m ln ( m) ln m q m , () where is determined by eq. (). the parameter m should be set to an integer value which maximizes s. it turns out that m = for all values of q. therefore, in the ssb phase there is only one dominant color, and all the other colors are equally abundant in the system. we nd that in the general case of q the entropy density function s(u) is convex in the vicinity of umic (fig. ). this non concave property means that there is a discontinuous ssb phase transition in the canonical ensemble at certain critical value c of the inverse temperature. to summarize, for the complete graph q state potts model (q ), the ssb transition is always a discontinuous phase transition in the canonical ensemble but it is always a continuous phase transition in the microcanonical ensemble. ",
        "Subsections": [],
        "Groundtruth": "The section discusses the energy and entropy properties of a complete graph with every vertex interacting with every other vertex in a color configuration setting. It explores the concept of energy density, entropy density, and the critical energy density for spontaneous symmetry breaking (ssb). The analysis reveals that the system exhibits a continuous phase transition in the microcanonical ensemble and a discontinuous phase transition in the canonical ensemble. Key parameters such as dominant color density and number of dominant colors play significant roles in determining the system's behavior and phase transitions."
    },
    {
        "Section_Num": "S2:",
        "Section": " S2: The random-graph Potts model in the canonical ensemble",
        "Text": "in the canonical ensemble the inverse temperature of the environment is the control parameter, and the energy density u of the system is not xed. we now briey describe some of the results obtained by the bethe peierls mean eld theory and by canonical monte carlo (mc) simulations. for concreteness we consider regular random (rr) graphs of vertex degree k = and set the number of colors to q=, as in the main text. first, depending on the value of , the bp equation q = b(q) may have one, two, or three xed point solutions, see fig. (a). the trivial xed point q = q corresponds to the disordered symmetric (ds) phase and it is locally stable for < ds = . when cp = there is another stable xed point with q much larger than q, which corresponds to the canonical polarized (cp) phase. in this cp phase one color is much more abundant than each of all the other q colors (which are equally abundant among themselves), that is, the density of the dominant color is much higher than q. the free energy density of the cp phase becomes lower than that of the ds phase as exceeds the critical value c = , see fig. (a). therefore there is a discontinuous equilibrium phase transition at c, at which jumps from q = to a much higher value and u drops from to . because of the high free energy barrier between the ds and cp phases, there is a strong hysteresis eect in the canonical mc simulation dynamics in the vicinity of c, see figs. (b) and (c). b(q) q = = = (a) cp ds mp mc mc (b) - - - - u cp ds mp mc mc (c) fig. : q state potts model with q= in the canonical ensemble, on k = regular random graphs. (a) the bp xed points are the intersection points of the curve b(q) and the dashed diagonal line. depending on there might be one, two, or three xed points. (b) and (c): the density of the dominant color and the mean energy density u for the disordered symmetric (ds, solid line), the canonical polarized (cp, dashed line), and the microcanonical polarized (mp, dotted line) xed points. the up- and down triangles are canonical mc simulation results obtained on a single rr graph of size n = , with the initial color conguration being completely disordered and random (mc) or being completely ordered (mc). the vertical dashed lines mark the canonical phase transition point c =. when (cp, ds) the bp equation also has an unstable xed point, referred to as the microcanonical polarized (mp) one, whose free energy density is higher than those of the ds and cp xed points. this xed point therefore is physically irrelevant in the canonical ensemble, see fig. ",
        "Subsections": [],
        "Groundtruth": "In the canonical ensemble of the random-graph Potts model, the energy density of the system is not fixed, and the control parameter is the inverse temperature of the environment. Results obtained from Bethe Peierls mean-field theory and canonical Monte Carlo simulations on regular random graphs with a certain vertex degree and number of colors show different fixed point solutions depending on the value of the control parameter. Specifically, there are fixed points representing disordered symmetric (DS) and canonical polarized (CP) phases, with a discontinuous equilibrium phase transition at a critical value. Hysteresis effects are observed in the simulation dynamics near the transition point. The canonical ensemble also includes a physically irrelevant fixed point referred to as microcanonical polarized (MP) in the Bethe Peierls equation."
    },
    {
        "Section_Num": "S3:",
        "Section": " S3: The entropy kink at umic and the microcanonical inverse temperature",
        "Text": "for the potts model of q = on the rr graph of degree k = , the upper left inset of fig. (c) in the main text has shown how the entropy densities of the ds and mp xed point solutions change with energy density u, but the entropy kink is not visually obvious in that gure. to clearly demonstrate the entropy kink, let us dene a modied entropy density function s(u) as s(u) s(u) ucp . () since ucp is linear in u, if s(u) has a kink then the entropy density s(u) will also have a kink. we redraw the theoretical data of fig. (c) at the vicinity of the critical energy density umic = in fig. the kink of the - - - - s - u cp u ds mp - - fig. : this gure is complementary to fig. (c) of the main text. the same theoretical data in the upper left inset of fig. (c) is redrawn here, but with the vertical axis being s(u) s(u) ucp, with cp = . the inset here is an enlarged view of the kink of s(u) at umic =. entropy density is now quite evident. associated with the entropy kink at umic is the discontinuity of the microcanonical inverse temperature . to verify this discontinuity by the microcanonical monte carlo (mc) simulation dynamics, we notice that the microcanonical inverse temperature can be estimated by = log \u0010 + edemon \u0011 , () where edemonis the mean energy of the demon (see ref. ). the non negative demon energy edemon is simply the dierence between the objective energy eo and the actual energy e(c) of the color conguration c, namely edemon = eo e(c). therefore edemonis easy to compute through the microcanonical mc evolution process. figure shows the good agreement between the theoretically predicted and actually measured microcanonical inverse temperatures for the rr graph of degree k = at q= this gure also shows the measured microcanonical inverse temperatures at dierent energy densities u for two of the eight dimensional bond diluted lattice systems used in fig. (d). an interesting feature is that, given a xed value of energy density u at the ds phase (u > umic), the microcanonical inverse temperature of d= diluted lattice systems is considerably lower than that of the rr graph - - - - - - u k=, q= ds mp rr d, n = d, n = fig. : the relationship between the microcanonical inverse temperature of the potts model (q=) and the energy density u. solid line (for the ds phase) and dotted line (for the mp phase) are theoretical predictions for rr graphs of degree k =, and the vertical dashed line marks the predicted microcanonical phase transition point umic. symbols are microcanonical monte carlo simulation results obtained on the graph instances of fig. (d), including the rr graph (n =) and the two bond diluted eight dimensional periodic hypercubic lattices of side length l = , (d, n =l), degree k = instance, and the dierence increases as u further increases. further research is needed to fully understand such dierences. ",
        "Subsections": [],
        "Groundtruth": "The text discusses the entropy kink at the critical energy density umic in the context of the Potts model on the RR graph. A modified entropy density function is defined to emphasize the kink in the entropy density. The discontinuity of the microcanonical inverse temperature is associated with the entropy kink at umic. The microcanonical inverse temperature is estimated using a formula involving the mean energy of the demon in Monte Carlo simulations. The results show good agreement between predicted and measured microcanonical inverse temperatures for the RR graph and bond-diluted lattice systems at different energy densities. The microcanonical inverse temperature is considerably lower for the bond-diluted lattice systems compared to the RR graph, especially in the DS phase. Further research is necessary to explore and understand these differences."
    },
    {
        "Section_Num": "S4:",
        "Section": " S4: The Potts model on RR graphs of large degree K",
        "Text": "we investigate here the asymptotic property of the microcanonical ssb phase transition of the potts model as the degree k of the rr graphs approaches innity. for this purpose, it turns out to be convenient to dene a parameter, c, as c e + e q . () let us denote the bp xed point solution as q = q + . () then the mean energy density u is expressed as u = \u0000 + (q)c q \u0001 k q \u0010 + q q + qc q \u0011 . () after some careful derivations, we nd that the free energy density dierence between the mp phase and the ds phase, at energy density value u, is expressed as sdi= k ln \u0010 + qc q \u0011 + k ln \u0000 + c \u0001 + ln h ( /q) \u0010 \u0000 ( + /(q ))c + c \u0001k\u0011i +k ln \u0010 q\u0000 + (q)c q \u0001 (q )\u0000 + qc q\u0001\u0011 \u0000 + (q)c q \u0001 k q \u0010 + q q + qc q \u0011 ln \u0010 q (q) + q q \u0011 . () if we assume to be small, then based on the bp equation the expression for is = q(q ) (q )c (k )(k ) \u0010 (k )c q \u0011 . () in the limit of k , we nd that, to fourth order of , sdi= \u0010 qkc (q ) k(k )c (q ) \u0011 + (q )k(k )(k )c (q ) + kq (q ) \u0000 + (q )c q \u0001\u0000 c q \u0001 . () in the limit of k , it turns out that c(k )/q is very close to unity, so we write c = q k () with being a small quantity. to the leading order of , we have = (q ) q(q ) . () then we obtain from the condition sdi= that = (q )(q ) kq , = q(q ) k . () because = q + for k , we see that the asymptotic behavior of the jump at umic is (q )(q ) kq . () / (a) k q= q= q= q= - - - (a) / (a) k q= q= q= q= - - - - - (b) fig. : asymptotic behaviors of the gap of the dominant color density (a) and the gap of the microcanonical inverse temperature (b), at the microcanonical ssb phase transition on rr graphs of degree k. the dierent sets of theoretical curves are for dierent values of q. in the main panel of (a) is rescaled by (a) = (q)(q) q k, while in the main panel of (b) is rescaled by (a) = q(q) k the insets of (a) and (b) demonstrate the k asymptotic decay of and the k asymptotic decay of , with the k and k power laws marked by the two dashed lines. this asymptotic scaling behavior is in agreement with numerical computations, see fig. (a). at a given energy density u, the dierence between the microcanonical inverse temperature of the mp and ds phases is = ln \u0010 + q q q (q) \u0011 . () at the microcanonical ssb phase transition point, the scaling behavior of is then q (q ) q(q ) k . () this asymptotic scaling behavior of is also in agreement with numerical computations, see fig. (b). these asymptotic results suggest that the microcanonical ssb phase transition on rr graphs will be discontinuous for any nite value of degree k, and it becomes continuous only at k = , i.e., when the graph becomes completely connected. s: the potts model with large q values on rr graphs of xed degree k it is also interesting to see how the microcanonical ssb phase transition behaves at the limit of large q. for rr graphs of xed degree k we can determine the gaps and at umic as a function of q, see fig. we observe that is not monotonic in q but it attains a maximum value at q and then decays slowly as a power law ( q) with exponent much smaller than unity. on the other hand, is monotonic in q and approaches a nal negative value as q . a nite value of at q is reasonable because the microcanonical inverse temperature is the slope of s(u). it would also be interesting to know the limiting behavor of the potts model as both q and k approach innity. the results in fig. (a) and fig. (a) indicate that will decay to zero no matter whether q approaches innity faster or slower than k. for , as it is expected to have a nite limiting value at q at each xed value of k and it decays as k at k for each xed value of q, we conjecture that will decay to zero as both q and k approach innity, and consequently the microcanonical ssb phase transition will become continuous at the limit of q and k both approach innity. - - - q k= k= k= k= k= k= (a) - - q k= k= k= k= k= k= (b) fig. : asymptotic behaviors of the gap of the dominant color density (a) and the gap of the microcanonical inverse temperature (b), at the microcanonical ssb phase transition on rr graphs of degree k, as the number q of colors changes. the dierent sets of theoretical curves are for dierent values of k. the dashed line in (a) marks the scaling behavior q, which is much steeper than the actual decaying behaviors of ",
        "Subsections": [],
        "Groundtruth": "The text explores the asymptotic properties of the microcanonical ssb phase transition of the Potts model on RR graphs as the degree k approaches infinity. A parameter c is defined as c = e + eq, and the mean energy density is expressed in terms of q and c. Through careful derivations, the free energy density difference between phases is calculated. The text discusses the behavior of certain variables as k approaches infinity, indicating that the microcanonical ssb phase transition becomes continuous only when k = infinity. It also analyzes the behavior of the transition as the number of colors q approaches infinity. The findings suggest that the transition will become continuous when both q and k approach infinity. Graphs illustrate the gap of dominant color density and microcanonical inverse temperature at the transitional points."
    },
    {
        "Section_Num": "S5:",
        "Section": " S5: The Potts model with large Q values on RR graphs of fixed degree K",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "S6:",
        "Section": " S6: Potts model with kinetic energies",
        "Text": "the potts model discussed in the main text only considers the interaction energies between neighboring vertices in the graph. here we introduce local kinetic energies to the vertices to make the model more general . suppose there is a particle of mass m on top of each vertex i and this particle can move in a small conned space so it has a kinetic energy p i m, where pi is the momentum of this particle. the dimensionality of pi is denoted as d. the total energy of this extended system then depends on the color conguration c = (c, c, . . . , cn) and the momentum vectors {pi} of all the vertices: etotal = x (i,j)g cj ci + n x i= p i m . () when the total energy of this system is restricted to a tiny interval [e, e +e), where e is extensive and e , the partition function of the system is z = e+e z e detotal x c n y i= z dpi pd \u0010 etotal e(c) x i p i m \u0011 , () where p is certain characteristic momentum value needed to count the number of microscopic states in the momentum space, and e(c) is simply the color energy. by integrating out the momentum degrees of freedom we obtain that z = mnd/ \u0000 nd \u0001 pnd e+e z e detotal x c \u0002 m(etotal e(c) \u0003 nd . () let us denote the total energy density of the system as utotal, that is, e = nutotal. by noticing that the total number of color congurations at energy e(c) = nu is exp \u0000s(u) \u0001 , where s(u) is the entropy density of color congurations at given interaction energy density u, we can re write the above expression as z utotal z du exp \u0010 n \u0000s(u) + d ln utotal u \u0001\u0011 , () - - - - - - u utotal - - - - fig. : the potts model with kinetic energies for regular random graphs (k =, q=, and d =). the discontinuous phase transition occurs at utotal =, at which the value of color interaction energy density u drops from u= to u=, the density of the dominant color jump from = / to = , and the microcanonical inverse temperature drops from = to =. where p m. from eq. () we see that at a given value of total energy density utotal, the mean interaction energy density uof the system will be determined by u= arg max u \u0010 s(u) + d ln utotal u \u0011 . () therefore umust be a root of utotal u= d (u) , () where (u) ds(u) du is the microcanonical inverse temperature of the system. notice that this equilibrium interaction energy density udoes not depend on equation () simply says that the mean kinetic energy of a vertex is equal to d (u). for intermediate values of utotal, eq. () has a pair of solutions u, and the one which corresponds to higher total entropy density will be the physically relevant solution. as demonstrated in fig. for regular graphs (k =, q=, and momentum dimensionality setting to be d =), there is a discontinuous phase transition when the total energy density is decreased to the critical value utotal =. such a discontinuous phase transition will occur for other values of k and q as well. ",
        "Subsections": [],
        "Groundtruth": "The Potts model discussed in the text is extended to include kinetic energies for each vertex in a graph system. This modification adds momentum vectors to the vertices, increasing the model's complexity. The total energy of the system is dependent on both the color configuration and the momentum vectors. The partition function is derived by integrating out the momentum degrees of freedom. The system's total energy density is expressed as utotal, with a focus on the equilibrium interaction energy density u. The model exhibits a discontinuous phase transition at the critical value utotal =, leading to changes in color interaction energy density and microcanonical inverse temperature. The equilibrium interaction energy density u is determined by maximizing the total entropy density subjected to a constraint. The model shows a discontinuous phase transition for regular graphs at specific values, with characteristics varying based on the total energy density."
    },
    {
        "Section_Num": "S7:",
        "Section": " S7: Bond-diluted lattice systems and short-range interaction range l",
        "Text": "the vertices in a d dimensional hypercubic lattice of side length l are located at positions (x, x, . . . , xd) where xd (d = , , . . . , d) are integer values. periodic boundary conditions are imposed, so that (x, x, . . . , xd) and x , x , . . . , x d) are the same position if (xd mod l) = (x d mod l) () for every d = , , . . . , d. the total number of vertices in the system is n = ld. each vertex has d bonds linking itself to its nearest neighboring vertices in space. the length of the shortest loops in such a hypercubic graph is equal to four and it does not increase with system size l. to make it more dicult for nucleation to occur (see the next section), we dilute this hypercubic graph by deleting a large fraction of bonds and keeping only k bonds for each vertex. a maximally random bond diluted lattice graph is constructed according to the following procedure: construct an initial d dimensional bond diluted hypercubic lattice graph in which every vertex has exactly k active bonds (the remaining d k bonds of this vertex are all regarded as inactive). pick a vertex i uniformly at random from the lattice graph and pick uniformly at random an active bond (i, j) from its k active bonds; then move to vertex j and pick uniformly at random an inactive bond (j, k) from its d k inactive bonds; then move to vertex k and pick uniformly at random an active bond (k, l) from its k active bonds; ...... this chain of alternative active and inactive bonds is further extended until it visits a vertex that is already in the chain (loop closure). if the length of this sampled loop is odd, nothing is changed. but if the length of this loop is even, then all the active bonds in this loop are deleted from the graph (i.e., they change to be inactive) while all the originally inactive bonds of this loop are added to the graph (i.e., they now become active). this switching action keeps the active degree of every involved vertex unchanged. repeat steps () and () a large number of times (e.g., n) to make the bond diluted lattice graph as random as possible. this loop switching algorithm is similar to the algorithm used in ref. . it is easy to prove that this algorithm is ergodic and it leads to a uniform distribution among all the valid k regular lattice graphs. because all the bonds in the original hypercubic lattice are between spatial nearest neighbors, the constructed bond diluted graphs naturally contain only bonds between nearest neighbors. we also consider diluted lattice graphs with longer interaction ranges to explore the eect of interaction range to the microcanonical ssb transition. for this purpose, we consider a lattice system in which each vertex i at position (x, x, . . . , xd) of the periodic hypercubic lattice has (l + )d bonds to all the other vertices located in or at the surface of the hypercubic box of side length (l + ) centered on vertex i. that is, there is a bond between position (x, x, . . . , xd) and the positions (x + x, x + x, . . . , xd + xd) where xd {l, l + , . . . , , , , . . . , l , l} (d = , , . . . , d) . () this lattice graph is much more densely connected than the simplest hypercubic graph of degree d, since each vertex has (l + )d attached edges. to make it sparse we only retain k edges for each vertex and delete all the other edges. such a maximally random diluted graph can be sampled by the same loop switching algorithm as described above. we have performed some preliminary computer simulations for the physically relevant dimension d = the side length of the periodic cubic lattice is xed to l = , the vertex degree is xed to k = , while four dierent values are tried for the interaction range parameter l, namely l = , , , the microcanonical mc simulation results shown in fig. clearly demonstrate that the interaction range has a dramatic eect on the ssb transition. when the interaction range l the simulation results on these nite size lattice graphs are very similar to the predicted results for random rr graphs. ",
        "Subsections": [],
        "Groundtruth": "In a d-dimensional lattice system with side length l, vertices are positioned with periodic boundary conditions and connected by bonds. By diluting the lattice with fewer active bonds per vertex, a maximally random bond-diluted lattice graph is created using a loop switching algorithm. This method ensures a uniform distribution among valid k-regular lattice graphs. Varying the interaction range in the lattice system affects the microcanonical spontaneous symmetry breaking (SSB) transition. Computer simulations show that different interaction ranges lead to significant effects on the SSB transition behavior."
    },
    {
        "Section_Num": "S8:",
        "Section": " S8: Droplet nucleation and phase separation",
        "Text": "here we review some of the key ideas of the droplet nucleation theory and discuss when droplet formation will be severely suppressed. to be concrete, we consider the d dimensional hypercubic lattice of side length l with periodic boundary conditions. the total number of vertices in the lattice system is n = ld, and the total number of bonds (edges) is m = dn if every vertex only interacts with its d nearest neighbors. in the canonical statistical ensemble and at the thermodynamic limit l , the q state potts model dened on such a lattice will experience an equilibrium phase transition at certain critical inverse temperature c, between the disordered symmetric (ds) phase and the canonical polarized (cp) phase. in the ds phase all the q colors are equally abundant, while in the cp phase one randomly picked color is favored over all the other colors. we consider the case of this canonical ssb phase transition being discontinuous. let us denote the energy densities of these two phases at c as uc ds and uc cp, respectively. similarly, the entropy densities of these two phases at c are denoted as sc ds and sc cp. because the dscp phase transition is an equilibrium one, we have sc ds sc cp uc ds uc cp = c . () now we consider the microcanonical ensemble of xed energy density u and assume u takes an intermediate value between uc cp and uc ds. because of the existence of the equilibrium canonical dscp phase transition, the entropy density s(u) at this intermediate energy density must satisfy the following inequality: s(u) rsc cp + ( r)sc ds , () - - - - - - u ds mp l= l= l= l= fig. : the potts model (q = ) on four graph instances of three dimensional short range interaction lattice systems. symbols are microcanonical mc simulation results. the side length of the diluted lattice graphs is l = so the total number of vertices is n = each vertex has k = attached edges in these graphs, with the interaction range being l = (squares), l = (diamonds), l = (triangle), and l = (circles). the theoretical predictions for rr graphs of degree k = concerning the density of the dominant color are the solid line (the ds phase) and the dotted line (the mp phase). the vertical dashed line marks the predicted microcanonical ssb phase transition point umic = for rr graphs of degree k = where the parameter r is dened by r uc ds u uc ds uc cp . () at the thermodynamic limit l it turns out that s(u) achieves the upper limit of the inequality () by phase separation. the argument goes as follows. suppose the ds and cp phases coexist in the system and r is the relative size of the cp phase. in the case of r close to zero, to minimize the surface area between these two phases, we may assume that the cp phase is conned within a hyperspherical droplet of radius r. the total number nv of vertices in this droplet is then nv = crd while the total number ns of vertices on its surface is ns = crd, where c, c are two constants. when the droplet becomes large in size, we see that lim r ns nv = . () a consequence of eq. () is that, when r becomes suciently large the surface interaction energy between the cp and ds phases will be negligible in comparison with the volume interaction energy of the droplet. then the cp droplet and the ds subsystem can be treated as two independent systems and the entropy density of the combined system is then s(u) = rsc cp + ( r)sc ds . () because of eq. (), the rst derivative of s(u) at u=uc ds is equal to c. consequently s(u) is c continuous at uc ds. according to this droplet picture, in the thermodynamic limit l, when the energy density u is decreased to uc ds, phase separation starts to occur and the relative size r of the cp droplet increases gradually from r= at u=uc ds to r = as u is gradually decreased to u = uc cp. the inverse temperature of the system keeps the value c during this cp phase expansion process. since r continuously increases from zero, the density of the dominant color must also deviate from /q in a continuously manner. for a stable droplet to form in the system, however, the size r of the droplet must exceed certain threshold length rth(u), which depends on the energy density u of the whole system. if r is too small, the energy (and free energy) gain of forming a partially ordered droplet will not be enough to compensate for the penalty of interfacial energy, and then the droplet will be suppressed. if the side length l of the system is comparable or even smaller than rth(u), then it is likely the system as whole will change directly from the ds phase to a partially ordered phase, without experiencing the intermediate phase coexistence stage. this partially ordered phase for such nite size systems may contain a percolating cluster of connected vertices which are all in the dominant color state (see, for example, ref. for related simulation results obtained on the supercooled liquid/gas system). the required minimum radius rth for droplet formation and phase separation might be greatly increased in a bond diluted lattice system as compared to an intact lattice system. when bonds are deleted in a maximally random manner and the active degree k of every vertex is quite small, the graph will be locally quite similar to a random graph, and the typical length loop of the shortest path of unbroken bonds linking two spatial neighbors will be relatively large. indeed, as the spatial dimensionality d of the lattice increases while the active degree k of the vertices is xed, short loops will be more and more dicult to form and the graph will be more and more like a completely random graph. if the radius r of a hyperspherical region in such a bond diluted spatial graph is of the same order as loop, the vertices of this region will be well approximated by a tree and its surface interaction energy will be comparable to the volume interaction energy of this region. therefore, for phase separation to occur the side length l of the lattice system must be much larger than loop. now we oer a rough estimate of loop. consider a rooted tree in the bond diluted hypercubic lattice and assume the path length between two leaf vertices is ltree (the distance from the root to a leaf vertex is ltree/). since each internal vertex of this tree has k attached edges, the total number of vertices in the tree is approximately kltree/ because the directions of the edges in this tree are quite random, the length of the spatial region which contains this tree is approximately ltree, and the total number of vertices of this region is then approximately \u0000ltree \u0001d. to guarantee the absence of loops the total number of vertices in the tree must not exceed the allowed number of vertices in the region. therefore, we estimate loop to be the largest integer value ltree which satises the following condition: kltree/ \u0000ltree \u0001d/ . () in the case of k = and d = as in fig. (d), the above condition suggests that loop = , which indicates a threshold number of vertices exceeding nth = (loop)d = (which is much larger than the accessible graph sizes in our computer simulations). if the interaction range l in the lattice system increases while the vertex degree k keeps xed (see the preceding section), because the euclidean length of an edge of this system is approximately l, the above condition probably needs to be modied as kltree/ ld\u0000ltree \u0001d/ , () and the characteristic length loop will increase with l. for the d = graph instances of fig. , the estimated lengths are loop = for l = (the lower bound of threshold number of vertices is then nth = (lloop)d = ), loop = for l = (nth = ), and loop = for l = (nth = ). these quantitative estimates may help explain why at l the simulation results of fig. for the d = spatial graphs are quite close to the theoretical results predicted for rr graphs. ",
        "Subsections": [],
        "Groundtruth": "The text discusses the droplet nucleation theory and phase separation in the context of the d-dimensional hypercubic lattice. It focuses on the equilibrium phase transition in the Potts model, where a canonical ssb phase transition leads to phase separation between the disordered symmetric (ds) and canonical polarized (cp) phases. The text highlights how the system undergoes phase separation as the energy density decreases, leading to the formation of a droplet of the favored color within the ds phase. It explains that the formation of stable droplets depends on the relative size of the droplet and the energy density of the system, with a threshold length determining whether phase separation will occur. The text also discusses the impact of bond dilution on phase separation and provides estimates for the characteristic length loop in bond diluted lattice systems."
    },
    {
        "Section_Num": "NA",
        "Section": "NA",
        "Text": "kinked entropy and discontinuous microcanonical spontaneous symmetry breaking hai jun zhou, cas key laboratory for theoretical physics, institute of theoretical physics, chinese academy of sciences, beijing , china school of physical sciences, university of chinese academy of sciences, beijing , china (dated: april , ) spontaneous symmetry breaking (ssb) in statistical physics is a macroscopic collective phe- nomenon. for the paradigmatic q state potts model it means a transition from the disordered color symmetric phase to an ordered phase in which one color dominates. existing mean eld theories imply that ssb in the microcanonical statistical ensemble (with energy being the control parameter) should be a continuous process. here we study microcanonical ssb on the random graph potts model, and discover that the entropy is a kinked function of energy. this kink leads to a dis- continuous phase transition at certain energy density value, characterized by a jump in the density of the dominant color and a jump in the microcanonical temperature. this discontinuous ssb in random graphs is conrmed by microcanonical monte carlo simulations, and it is also observed in bond diluted nite size lattice systems. spontaneous symmetry breaking (ssb) is a fundamen- tal concept of physics and is tightly linked to the origin of mass in particle physics, the emergence of superconduc- tivity in condensed matter system, and the ferromagnetic phase transition in statistical mechanics, to name just a few eminent examples . in statistical physics a theoret- ical paradigm for ssb is the potts model, a simple two- body interaction graphical system in which each vertex has q discrete color states . the equilibrium ssb transition of the potts model in the canonical ensemble, where inverse temperature is the control parameter, has been extensively investigated (see refs. for some of the recent results). driven by energyentropy com- petitions, this transition is a discontinuous phenomenon when q is suciently large, with the density of the dominant color jumps from /q to a much larger value at the critical inverse temperature c. to compensate for the extensive loss of entropy, such a discontinuous tran- sition is always accompanied by a discontinuous decrease of the systems energy density u . when the system is isolated and cannot exchange en- ergy with the environment (the microcanonincal ensem- ble ), it is generally believed that the ssb transi- tion will occur gradually, with the dominant color density deviating from /q continuously at certain critical energy density umic. indeed if the microscopic entropy density s(u) is a c continuous function of energy den- sity u (i.e., both s(u) and its rst derivative are contin- uous), there is no reason to expect a discontinuity of the order parameter the c continuity of s(u) can be easily veried for the mean eld potts model on a com- plete graph . for nite dimensional lattices the phase separation mechanism (the nucleation and expansion of droplets ) will guarantee a c continuous entropy prole in the thermodynamic limit. for random graph systems one would na vely expect umic to be an inection point of s(u) , which ensures c continuity. in this letter we investigate the microcanonical potts model on random graphs using the bethe peierls mean eld theory, and discover that the entropy density s(u) is actually not c continuous but is kinked at u = umic for any q (fig. ). consequently, there is a discon- tinuous microcanonical phase transition at umic, with a jump in the dominant density and a drop in the mi- crocanonical inverse temperature. this ssb transition is driven completely by entropy competitions between the microcanonical polarized (mp) phase and the disor- dered symmetric (ds) phase, and at umic the mp phase is hotter than the ds phase. these theoretical predic- tions for random graphs are veried by microcanonical monte carlo simulations. the discontinuous ssb tran- sition is also observed in three- and higher dimensional bond diluted lattices, but only for system sizes not too large . the phenomenon of kinked entropy may per- sist in other multiple state spin glass systems or com- binatorial optimization problems . our work also adds new insight on the debate about ensemble inequiv- alence . mean eld theory. consider a graph g formed by n vertices and m edges. each vertex i has a discrete color umic + umic umic s(u) u fig. : schematic drawing of kinked entropy density s(u). as the energy density u of the q state potts model decreases to umic, s(u) changes from concave to convex and its slope drops from to the system is color symmetric at umic+ ( ) with a lower microcanonical temperature /, but at umic it has a highly dominant color and a higher micro- canonical temperature / arxiv:v apr ci {, , . . . , q} and an edge (i, j) between vertices i and j has a ferromagnetic interaction energy eij(ci, cj) = cj ci , where cj ci = () if ci = cj (ci = cj). the total energy of a color conguration c (c, c, . . . , cn) is the summed edge energies, e(c) = p (i,j)g eij(ci, cj), which is symmetric with respect to color permutations. the partition function z() at a given inverse tempera- ture is z() x c ee(c) = x c y (i,j)g h + (e )cj ci i . () we now review the bethe peierls theory for this model . for simplicity we describe the theoreti- cal equations for random regular (rr) graphs, which are maximally random except that every vertex has exactly k attached edges. (the mean eld theory for general graphs can easily be derived following the cavity method of statistical physics or through loop expansion of the partition function this theory is exact for tree graphs, and because random graphs are locally tree- like (loop lengths diverge logarithmically with n) and there is no intrinsic frustration in the edge interactions, we expect it to be exact for rr graphs as well. without loss of generality we assume c = to be the dominant (most abundant) color. to compute the marginal probability of this color state for a randomly chosen vertex i we rst delete i and all its attached edges from the graph. because short loops are rather rare in the graph, the k nearest neighbors of i will now be far sep- arated in the perturbed cavity graph and consequently their color states will be independent. we denote by q (/q) the probability of such a neighboring vertex j to be in state cj = in the perturbed graph, and assume that vertex j has equal probability ( q)/(q ) to be in any of the other color states. when vertex i and its k edges are added back to the graph, its probability of being in state ci = is then = \u0014 + (q ) \u0010 + (e ) q q + (e )q \u0011k\u0015 . () this quantity is also the dominant color density of the rr graph. a similar expression for the cavity probability q of the neighboring vertex j can be written down (j has k edges in the cavity graph): q = b(q) \u0014 + (q ) \u0010 + (e ) q q + (e )q \u0011k\u0015 . () this self consistent expression is referred to as a belief- propagation (bp) equation . the free energy density f (/n) ln z() of the system can be computed by rst summarizing the indi- vidual contributions of all the vertices, and then sub- tracting the individual contributions of all the edges (be- cause each edge contributes to the free energies of two vertices) . at a bp xed point the explicit expression of f is f = ln n\u0002 + (e )q \u0003k +(q ) \u0002 + (e ) q q \u0003ko + k ln h + (e ) \u0000q + ( q) q \u0001i . () one can verify that f q = when q = b(q). the mean energy density u is obtained from eq. () as u (f) = k e\u0000q + (q) q \u0001 + (e ) \u0000q + (q) q \u0001 . () the entropy density s of the system is then determined by s = (u f) . the bp equation () always has a trivial xed point q= /q which corresponds to the disordered symmetric (ds) phase with all the colors being equally abundant . this xed point becomes unstable with respect to the it- eration qt+ b(qt) when >ds ln \u0000 + q/(k ) \u0001 . for k and q, eq. () has a stable xed point with q and strictly larger than /q at >cp, which cor- responds to the canonical polarized (cp) phase of broken color symmetry. here cp (< ds) is the lowest inverse temperature at which the cp phase becomes possible. the cp and ds phases have equal free energy density at a critical inverse temperature c (cp, ds), so an equilibrium phase transition occurs at c, with a sudden drop in energy density u . microcanonical ssb. for (cp, ds) the bp equation () has another xed point which is unstable with respect to qt+ b(qt) . this xed point is usually neglected because its free energy is higher than those of the ds and cp phases (fig. (a)). but we nd that it reveals a discontinuous microcanonical phase tran- sition between the ds phase and a new microcanonical polarized (mp) phase of the conguration space. plotting the predicted thermodynamic values of the mp xed point (fig. (b)), we observe that while q and are monotonic functions of as anticipated, the energy density u and entropy density s both are non- monotonic. this surprising feature of u and s leads to the two branched entropy prole shown in the upper left inset of fig. (c). these two entropy branches merge and stop at umax, which is the maximal achievable en- ergy density of the mp phase. the entropy of the lower mp branch is slightly lower than that of the ds phase so this branch has no physical signicance. on the other hand, the entropy of the upper mp branch exceeds that of the ds phase as u decreases below certain critical value umic which is strictly lower than umax, indicating the sys- tem will jump from the color symmetric phase to a color- symmetry broken mp phase which is stable only in the - - - f cp ds mp (a) - - - - mp fixed point u s q, q (b) - - - - s u cp ds mp - - - - - - - s (x ) (c) - - - u cp ds mp rr d, n = d, n = d, n = - - - (d) fig. : potts model on regular random graphs, k = and q= (a) free energy densities f() for the disordered symmetric (ds, solid line), the canonical polarized (cp, dashed line), and the microcanonical polarized (mp, dotted line) xed points of the bp equation. the ds solution is stable at inverse temperature <ds =, the cp solution exists for cp =, and the dscp phase transition occurs at c = with the energy density u dropping from to . (b) energy density u(), entropy density s(), xed point value q() and density () of the dominant color (inset), for the mp xed point. the maximal achievable mp energy density is umax = . (c) and (d): entropy density s and dominant color density versus energy density u for the ds (solid line), mp (dashed line), and cp (dotted line) xed points. upper left and lower right insets of (c) show an enlarged view of the mp entropy prole and the dierence s between the mp and ds entropy density values (s = at energy density umic = ). symbols in (d) are microcanonical monte carlo simulation results obtained on a single rr graph (n =) and several bond diluted eight dimensional periodic hypercubic lattices of side length l = , , (d, n =l), degree k = and q= the inset of (d) is an enlarged view of the transition region, and the phase transition point umic for rr graphs is marked by the vertical dashed line, at which jumps from / to . microcanonical ensemble. the dominant color density at umic is strictly higher than /q, so the spontaneous breaking of color symmetry is a discontinuous emerging phenomenon. notice that at u slightly below umic the entropy density of the mp phase is higher than that of the ds phase. because the entropy densities of the ds and mp phases are equal at u=umic but have dierent slopes (fig. (c)), the systems entropy density function s(u) is not c- continuous but is kinked at umic . since the micro- canonical inverse temperature is equal to the rst deriva- tive of s(u), ds(u) du , there will be a sudden drop of the microcanonical and an associated sudden drop of the free energy density f (= u s) as the system changes from the ds to the mp phase at umic. in other words, at umic the partially ordered mp phase is hotter than the disordered symmetric phase and has a lower free energy density. this peculiar feature of s(u) is qualita- tively dierent from the recently discussed entropy inec- tion phenomenon, which is associated with the vanishing of the second order derivative of s(u) . we have checked that as long as q, the discontinu- ous ssb phenomenon holds for all the rr graph ensem- bles of degree k as demonstrated in table i, at each xed value of q the and gaps at umic both decrease with degree k (and vanish gradually as k ). the discontinuous microcanonical phase transition will also occur in an extended potts model with additional kinetic energies . monte carlo simulations. we carry out microcanon- ical monte carlo (mc) simulations to check the theoret- ical predictions. there are many discussions on micro- canonical mc methods , and here we employ the simple demon method to draw a set of independent congurations which are located slightly be- low a prescribed objective energy level eo. starting from table i: the critical energy density umic, the jump of the dominant color density and the drop of the micro- canonical inverse temperature at umic, for the q state potts model on rr graphs of degree k. k q umic k q umic an initial conguration c of energy e eo, an elementary mc step unfolds as follows: () pick a vertex i uniformly at random and change its color ci to a uniformly ran- dom new value c i (= ci); () accept this color change if the energy e of the resulting new conguration satises e eo, otherwise keep the old color ci; () increase the evolution time t by a tiny amount /n (one unit time therefore corresponds to n single ip trials). this mc dynamics obeys detailed balance, so the sampled color congurations all have the same statistical weight. the simulation results obtained on a large rr graph instance are shown in fig. (d) (k = , q = ). we indeed ob- serve a discontinuous transition at the predicted critical energy density umic. the numerical results on the dom- inant color density also agree perfectly with theory. the predicted inverse temperature gap is also quan- titatively conrmed by computer simulations . we also consider bond diluted d dimensional hyper- cubic lattices of side length l with periodic boundary conditions (n = ld). by keeping only k bonds in a maximally random manner for every vertex (see for construction details), the shortest loops passing through the vertices rapidly increase their lengths as k decreases and d increases, and the diluted lattice is locally resem- bling a random graph . a discontinuous ssb tran- sition is observed in the mc dynamics for such bond- diluted lattice systems at high dimensions (e.g., d = , fig. (d)) and also at the physical dimension d= . however, unlike the case of truly random graphs, we ex- pect that the ssb transition in these lattice systems will become continuous in the thermodynamic limit , be- cause phase separation is deemed to occur as the system size l becomes suciently large . conclusion. in summary, we predicted and con- rmed a discontinuous microcanonical ssb phase transi- tion in the q state potts model on random graphs. such a discontinuous transition was also observed in bond- diluted nite size lattice systems (even down to three di- mensions ). in the future we need to investigate the geometric property of the congurations in the mp phase (e.g., the possibility of a percolating cluster of connected same color vertices) , and possible latent structures prior to the microcanonical transition , and to study systematically the microcanonical ssb transition in nite dimensional nite size systems and the associ- ated inequivalence between the microcanonical and the canonical ensembles . the discovered property of kinked entropy may be a general feature of random graphical models with a canonical discontinuous phase transition, and it may have important computational consequences in optimization tasks . the following funding supports are acknowledged: na- tional natural science foundation of china grants no. and no. ; the chinese academy of sciences grant no. qyzdj ssw sys numeri- cal simulations were carried out at the hpc cluster of itp cas and also at the tianhe- platform of the na- tional supercomputer center in guangzhou. the author thanks youjin deng, gaoke hu, hao hu, shaomeng qin, mutian shen, and jinhua zhao for helpful discussions and/or valuable comments on the manuscript. k. brading, e. castellani, and n. teh, symmetry and symmetry breaking, in the stanford encyclopedia of philosophy (metaphysics research lab, stanford univer- sity, winter edition, ), edited by e. n. zalta. r. b. potts, some generalized order disorder transfor- mations, proc. cambridge phil. soc. , (). f. y. wu, the potts model, rev. mod. phys. , (). r. j. baxter, exactly solved models in statistical me- chanics (academic press, london, uk, ). v. gorbenko, s. rychkov, and b. zan, walking, weak rst order transitions, and complex cfts ii. two- dimensional potts model at q > , scipost phys. , (). h. w. j. bl ote, w. guo, and m. p. nightingale, scaling in the vicinity of the four state potts xed point, j. phys. a: math. theor. , (). h. hu and y. deng, universal critical wrapping proba- bilities in the canonical ensemble, nuclear phys. b , (). s. wang, z.-y. xie, j. chen, b. normand, and t. xi- ang, phase transitions of ferromagnetic potts model on the simple cubic lattice, chinese phys. lett. , (). c. h. lee and a. lucas, simple model for multiple choice collective decision making, phys. rev. e (). l. tian, h. ma, w. guo, and l.-h. tang, phase tran- sitions of the q state potts model on multiply laced sier- pinski gaskets, eur. phys. j. b , (). q. n. chen, m. p. qin, j. chen, z. c. wei, h. h. zhao, b. normand, and t. xiang, partial order and nite- temperature phase transitions in potts models on irreg- ular lattices, phys. rev. lett. , (). y. deng, y. huang, j. l. jacobsen, j. salas, and a. d. sokal, finite temperature phase transition in a class of four state potts antiferromagnets, phys. rev. lett. , (). d. h. e. gross, a. ecker, and x. z. zhang, microcanoni- cal thermodynamics of rst order phase transitions stud- ied in the potts model, ann. physik , (). d. h. e. gross, microcanonical thermodynamics and sta- tistical fragmentation of dissipative systems: the topo- logical structure of the n body phase space, phys. rep. , (). v. martin mayor, microcanonical approach to the sim- ulation of rst order phase transitions, phys. rev. lett. , (). f. moreno, s. davis, c. loyola, and j. peralta, ordered metastable states in the potts model and their connection with the superheated solid state, physica a , (). see supplementary information for additional theoreti- cal and numerical results, some technical details on con- structing a bond diluted lattice system, and a qualita- tive discussion of the nucleation phenomenon in nite- dimensional systems. m. biskup, l. chayes, and r. koteck y, on the forma- tion/dissolution of equilibrium droplets, europhys. lett. , (). k. binder, theory of the evaporation/condensation tran- sition of equilibrium droplets in nite volumes, physica a , (). l. g. macdowell, v. k. shen, and j. r. errington, nucle- ation and cavitation of spherical, cylindrical, and slablike droplets and bubbles in small systems, j. chem. phys. , (). t. nogawa, n. ito, and h. watanabe, evaporation- condensation transition of the two dimensional potts model in the microcanonical ensemble, phys. rev. e , (). y.-z. xu, c. h. yeung, h.-j. zhou, and d. saad, en- tropy inection and invisible low energy states: defensive alliance example, phys. rev. lett. , (). m. m ezard and a. montanari, information, physics, and computation (oxford univ. press, new york, ). d. mukamel, statistical mechanics of systems with long range interactions, aip conf. proc. , (). a. campa, t. dauxois, and s. ruo, statistical me- chanics and dynamics of solvable models with long range interactions, phys. rep. , (). y. murata and h. nishimori, ensemble inequivalence in the spherical spin glass model with nonlinear interac- tions, j. phys. soc. jpn. , (). h. touchette, equivalence and nonequivalence of ensem- bles: thermodynamic, macrostate, and measure levels, j. stat. phys. , (). k. huang, statistical mechanics (john wiley, new york, second edition, ). m. m ezard, g. parisi, and m. a. virasoro, spin glass theory and beyond (world scientic, singapore, ). j.-q. xiao and h.-j. zhou, partition function loop series for a general graphical model: free energy corrections and message passing equations, j. phys. a: math. theor. , (). h.-j. zhou and c. wang, region graph partition func- tion expansion and approximate free energy landscapes: theory and some numerical results, j. stat. phys. , (). j. pearl, probabilistic reasoning in intelligent systems: networks of plausible inference (morgan kaufmann, san franciso, ca, usa, ). m. creutz, microcanonical monte carlo simulation, phys. rev. lett. , (). k.-c. lee, rejection free monte carlo technique, j. phys. a: math. gen. , (). p. schierz, j. zierenberg, and w. janke, first order phase transitions in the real microcanonical ensemble, phys. rev. e , (r) (). l. a. fern andez, v. martin mayor, g. parisi, and b. seoane, spin glasses on the hypercube, phys. rev. b , (). h.-j. zhou and h. ma, communities of solutions in sin- gle solution clusters of a random k satisability formula, phys. rev. e , (). h.-j. zhou and c. wang, ground state conguration space heterogeneity of random nite connectivity spin glasses and random constraint satisfaction problems, j. stat. mech.: theor. exp. , p (). kinked entropy and discontinuous microcanonical spontaneous symmetry breaking hai jun zhou supplementary information s: exact results on complete graphs ",
        "Subsections": [],
        "Groundtruth": "The text discusses kinked entropy and discontinuous microcanonical spontaneous symmetry breaking in the Potts model on random graphs. It reveals a discontinuous phase transition at a critical energy density, characterized by a jump in dominant color density and microcanonical temperature. This phenomenon challenges existing mean field theories and is confirmed through Monte Carlo simulations. The study also observes similar behavior in bond-diluted lattice systems. The findings highlight the non-continuity of the entropy function at the critical energy density, leading to a sudden transition between phases. The implications extend to understanding ensemble inequivalence and may have computational implications for optimization tasks."
    },
    {
        "Section_Num": "References",
        "Section": " References",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "I",
        "Section": "I Introduction",
        "Text": "edge computing is one of the recent developments in the eld of articial intelligence. the amount of data being pro- cessed with the ever increasing inter connectivity of devices and internet of things, ranging from sensors to autonomous ve- hicles, demand for high real time data processing at the edge. the edge devices are usually selected from neuromorphic chips, embedded devices, fpga, gpu/cpu etc depending on the application. among these devices, neuromorphic chip this research is supported by programmatic grant no. ab from the singapore governments research, innovation and enterprise plan (advanced manufacturing and engineering domain). a version of this paper is submitted to ijcnn has proven to be the efcient or potential candidate in terms of computational power and latency. neuromorphic chips are developed in digital , analog or mixed signal integrated circuit designs. usual design trend is that mostly the computation and memory section is done in analog domain whereas, the communication between cores are maintained in digital domain. the neuromorphic chip discussed in this paper is based on crossbar architecture of non volatile memory synapses. however, one of the main challenges is to efciently map the neurons on to the neuromorphic chip with hardware constraints such as core size, number of cores and fan in/fan- out . the existing neuromorphic chips have a mapping framework which is more hardware specic. ibms truenorth chip uses corelet language based on matlab, a programming language specic to their hardware. within this matlab framework, a mapping technique is integrated as a minimization problem . spinnaker and brainscales uses a simulator independent language, pynn based on python. sequential mapping is used in spinnaker. neural engineering framework (nef) is developed for neurogrid . neu- trams addresses an optimized mapping technique based on graph partition problem: kernighan lin (kl) partitioning strategy for network on chips. even though, every neuro- morphic chip simulator tools are addressing certain mapping techniques, optimized mapping onto a single neuromorphic core is often neglected and left unexplored by default. most of these mapping techniques are hidden within a neuromorphic hardware specic simulators, which mitigate the requirement of an algorithm developer to understand the details of a neuromorphic chip. but, for an optimized co development of a neural network model for a specic neuromorphic chip, the knowledge of hardware constraints is a must. over the years, convolutional neural networks evolved to arxiv:v jan become more deep and wide with respect to the evolution of different classication tasks i.e. from simple mnist hand- written digit classication to much more complex imagenet image classication. for mnist classication task, as the neural network is small, the neurons can be mapped manually onto a neuromorphic core. but, for large networks in the case of imagenet classication, it is near impossible to manually mention how the neurons in every layers are mapped to each core in a neuromorphic chip. hence, an automated procedure is necessary for identifying the neuron addresses with corresponding synaptic weights and input values. in this paper, aforementioned issues are mitigated with the help of mad framework and its optimizations. mad frame- work is a generic python wrapper which has an optimized algorithm for mapping any feed forward neural network such as mlp, cnn, snn onto a crossbar array of synapses with corresponding synaptic weights, thereby tting the neurons in minimum possible number of neuromorphic cores. python wrapper is also suitable as a debugging tool for verication of the inferencing of neural network architectures on the neuromorphic chip. thus together the framework is called as mapping and debugging (mad) framework. this python wrapper is developed in connection with the simulator in , where most of the techniques are quite similar to neutrams . the paper is organized as follows. section ii briey describe about the crossbar array of synapses and the spiking neuron in a neuromorphic chip. section iii illustrates the details of mad framework. section iv shows the implementation of mnist and cifar- classication task on mad framework. finally the paper is concluded with discussion in section v. ",
        "Subsections": [],
        "Groundtruth": "Edge computing in artificial intelligence is essential due to the increasing data volume from connected devices, leading to a demand for real-time data processing at the edge. Neuromorphic chips are favored for their computational power and efficiency. Challenges in mapping neurons onto these chips exist due to hardware constraints. Existing neuromorphic chips use hardware-specific mapping frameworks. The MAD framework, a Python wrapper, addresses these challenges by optimizing the mapping of neural networks onto neuromorphic cores. MAD aids in debugging neural network architectures on neuromorphic chips, providing efficient mapping and enhancing performance."
    },
    {
        "Section_Num": "II",
        "Section": "II Materials and Method",
        "Text": "a. spiking neuron integrator comparator reset spikes input current fig. block diagram of a spiking neuron. biological neuron computes the signal received through multiple dendrites and transmits the output signal through axons to other neurons connected in the network . fig shows a block diagram representation of the biological neuron. neuron has mainly two blocks, an integrator and a comparator. the integrator sums up all the input currents (excitatory post- synaptic current, epsc) and build up the membrane potential. this membrane potential is being monitored by the comparator to cross certain threshold. if the membrane potential crosses the set threshold, neuron emits an output spike and then resets the membrane potential back to its initial value. the communication between neurons in the biological network or in a spiking neural network (snn) is with the help of these output spikes. the entire mechanism of a spiking neuron can be modelled with the leaky integrate and re neuron model and its mathematical expression is given below: m du dt = + ri(t) () where, m = rc, is the membrane time constant of leaky integrator. u(t) = membrane potential i(t) = synaptic current urest = membrane resting potential b. crossbar array of synapses input axons output neurons word line (wl) memory device synapse fig. crossbar array of synapses in a neuromorphic core. fig. shows a crossbar array of synapses. the crossbar structure is very suitable for performing matrix dot vector multiplication (mvm) along each column in a crossbar architecture. for instance, a neuromorphic core with a core size of , input voltages from respective axons out of are given through word line. bit line collects all the weighted current at each synaptic nodes () and delivers to respective output neurons () for integration. the weighted current depends on the memory element used in the intersection of word line and bit line as synapse. the synaptic weights, which draws analogy to conductances, are represented in the form of blue dots at the cross points. from kirchoffs current law, the total current owing into each neuron from respective bit lines is the sum of currents owing through each intersection in every column. in fact, in conventional neural networks, total current of a particular column is the value of a single neuron activation in a particular layer, formed by summation of products of input voltages and corresponding synaptic weights (conductances) taking part in convolution operation. ",
        "Subsections": [],
        "Groundtruth": "A spiking neuron consists of an integrator and a comparator, where the integrator sums up input currents to build up membrane potential monitored by the comparator. When the potential crosses a threshold, the neuron emits an output spike before resetting. Neurons communicate through these spikes in biological and spiking neural networks. This mechanism can be modeled using the leaky integrate-and-fire neuron model. In a neuromorphic core, a crossbar array of synapses allows for matrix dot vector multiplication, with input voltages sent through word lines to synapses at the intersection of word and bit lines. The synaptic weights, analogous to conductances, determine the weighted current output to neurons for integration, following Kirchoff's current law."
    },
    {
        "Section_Num": "III",
        "Section": "III MaD Framework",
        "Text": "this section illustrates the details of the construction of mad framework. the complete usage of the framework is explained with a owchart as shown in g. a particular neural network is chosen for a classication or a detection task. the parameters like lter size, strides and padding among each layers are xed. the chosen network is trained using deep learning tool for obtaining the weight les to be given as input to the mapping function. core utilization is dened as the number of axons and neurons utilized in a single neuromorphic core. core utilization, as shown in the owchart, is an output from another function which calculates the number of axons and number of neurons used for mapping a section of particular layer onto a single core. core utilization is represented as . the details of the mapping function, core utilization and padding techniques are given in the subsequent subsections. this section is ended by including optimizations to be considered while mapping. fix parameter values (filter size, stride, padding) select a neural network for a classification/detection task padding? python wrapper (mapping function) train using deep learning framework weights yes no connectivity matrix in dictionary format virtual padding technique core utilization total number of utilized cores connections between cores for simulator fig. flowchart of python wrapper: the details of the python wrapper is shown with a owchart. the input and output of the mapping function that is used in the python wrapper is illustrated in the owchart. the core utilization and weight les are marked in different color to show that these inputs are the results from other functions. a. mapping function the mapping function is the core of the python wrapper as shown in g. fig. shows the input and output of the mapping function. the inputs to mapping function are input size, lter size, stride, padding, core utilization and weight les. the input size is the size of the input datasets, for eg. in the case of mnist or in the case of cifar- filter size is the size of lters used for convolution in each layers, here it is selected as throughout the layers of the chosen neural networks in section iv. stride and padding depends on the layers of the convolutional neural network. the detailed calculation of the core utilization is mentioned in subsection iii b. weight les are the weights obtained after training the chosen neural network using deep learning tool. the output section in g. shows the necessary outputs that is obtained from the mapping function. there are mainly three outputs, a connectivity matrix for verifying the interconnectivity between the cores and within the core, to verify the cores utilized and an automated generation of connection list for simulator. the steps for mapping are as follows: all the neurons are rst named to follow a regular pattern eg. l f n this implies layer:, feature map:, and neuron in row: and column: prepare a connectivity list of population of neurons in a particular layer connected to the previous layer. choose a population of neurons from a particular layer, based on the core utilization, to be mapped on to a particular core. repeat this process until entire neurons in every layers are completely mapped onto the core. since the naming and connectivity list are xed at the beginning, the neurons and axons will be automatically duplicated among the cores for mapping. b. core utilization x x x x neuron_col neuron_row layer n layer n- fig. two layers of convolution layer to illustrate the optimization of core utilization. layer n- neurons are in green, whereas layer n neurons are in red. synaptic connections are shown for two neurons in layer n. consider two layers of a convolutional neural network shown in g the neurons in layer n is marked as red and neurons in layer n- is marked as green. first two neurons in layer n is connected to layer n- and the synaptic connections are shown with straight lines. the convolution lter size used is , hence you can see connections from each red neurons in layer n to green neurons in layer n- likewise, the synaptic connections can be imagined throughout the layer with respect to the kernel size and strides used for convolution. while mapping these two layers in g. onto a core with crossbar array, the green neurons in layer n- will be the axons and the red neurons in layer n will be the neurons as in g notice the overlap of lter window when it strides across the layer. in fact these overlapped green neurons can be mapped onto the crossbar array connections without any duplication. duplicating the axons, while one to one mapping of neurons connected to axons onto a core, is not a good design with respect to core utilization as input needs to be duplicated into many axons and also mapping requires bigger core sizes and ends up utilizing many cores . hence, the toeplitz matrix method is utilized for efcient mapping of these layers onto a neuromorphic core without input duplication. toeplitz method for convolution is illustrated in . inorder to calculate the core utilization, the number of neurons and axons connected together has to be chosen which could be entirely mapped onto a single core. the number of axons can be evaluated as an algorithmic condition in the mapping function as there are overlapping axons whereas neurons selection become bit straight forward. the overlapping axons are dened as the axons which share connections with more than a single neuron, the term overlapping is because of the overlapping nature of the axons with the neighbourhood of the kernel lter with respect to strides (see layer n- in g. , the overlapping axons among the green and yellow synaptic connections are ). depending on this overlap, kernel lter size and strides, the total number of axons to be selected follows the formula as given below: n axons = kxk + kxsx(neuron col )+ sxsx(neuron col )x(neuron row )+ kxsx(neuron row ) () where, n axons = total number of axons to be selected k = convolution lter size s = stride neuron row = number of neurons across row neuron col = number of neurons across column the selection of neurons, neuron row and neuron col, in a layer depends on the condition: number of axons, n axons <= number of physical axons (eg. or or ) in the neuromorphic core. eq. is considering only a single feature map, this can be easily extended to multiple feature maps by multiplying with respective channel size. c. mad framework optimizations ) core utilization: referring to g. , consider a case for calculating core utilization, suppose neurons has to be chosen from layer n for mapping onto a core. this can be done by choosing rows and columns of neurons or rows and columns of neurons. here, rows and columns of neurons correspond to neuron row and neuron col in eq. if the convolution kernel size, k used is and stride, s is , then for rows and columns of neurons the axons required are rows and columns, similarly for rows and columns of neurons the axons required are rows and columns. this can be easily estimated from the formula to calculate the output size of convolutions as given below: o width = i width f width stride width + o height = i height f height stride height + () where, o width and o height = width and height of the convolution output respectively i width and i height = input width and height respectively f width and f height = width and height of lter kernel s width and s height = width and height of strides the above case suggest that choosing neurons from rows and columns are much better for core utilization than from rows and columns as input number of axons in former case is only whereas, in the later case it is that means the core utilization is in the former case and in the later case. the intuition from this example case is that the neurons to be selected for mapping onto the core is better to be in square shape than in rectangular shape. the section below provides a mathematical proof for choosing square shape rather than rectangular shape while mapping: - - - - - - - - - - - - - - - - - - yi xi min ((xi ,yi)) fig. graphical illustration of the theorem. theorem: given a, nd xi and yi such that: xi yi = a and minimum of p(xi, yi). proof: the graphical illustration of the theorem is shown in g. consider xy = a and x + y = z where z is a real number x + a x = z dz dx = a x at minima dz dx = , a x = x = + a if x > , z is minimum x = a also y = a and x + y is the minimum. () ) padding: padding is a common technique used in deep learning for maintaining the shape of the convolution layers throughout the network. padding simply adds extra zeros around the input activations in a convolution layer during con- volution operation. in fact such added zeros doesnt provide any computational signicance as mathematically zeros are multiplied and added. while mapping, these padded zeros are in fact physical neurons, but need not be participating in computation. if these neurons are considered as physical neurons during mapping, then there will be a lot of wastage on axon usage. this will reduce the optimized utilization of core. hence, as shown in g. (mentioned as virtual padding technique), when padding is used in a particular convolution layer, a virtually padded neuron address is created and is assigned in the connectivity list. later, while mapping onto the core these virtually padded neurons are removed from the connectivity list, reducing the fan in connection of those particular neurons in the periphery of a layer connected to those padded neurons in the previous layer. ",
        "Subsections": [],
        "Groundtruth": "The MaD Framework construction is detailed, emphasizing core utilization, mapping function, and padding techniques. A neural network is selected for classification or detection tasks, with fixed parameters like filter size and strides. The mapping function, core utilization calculation, and padding optimizations are explained. The mapping function's inputs include dataset size, filter size, stride, padding, core utilization, and weight files obtained post-training. Core utilization involves mapping neurons and axons onto a single core efficiently, considering factors like convolution filter size and neuron connections. Optimizations for core utilization suggest choosing neurons in a square shape over a rectangular one for better efficiency. Additionally, a virtual padding technique is proposed to optimize core usage by excluding padded neurons during mapping, reducing unnecessary axon wastage."
    },
    {
        "Section_Num": "IV",
        "Section": "IV Results",
        "Text": "this section mainly provides an instance of the utilization of neuromorphic chip using a classication task on mnist and cifar- datasets through the parameters, core utilization and number of cores utilized. here, the focus is not on improving the accuracies, but to show the neuromorphic core utilization while mapping, with a much simpler handcrafted neural network. all the accuracies mentioned in this section is iterated for ten times and then averaged it out. two sets of experiments are done for that purpose, one is to choose a particular neural network architecture for classication task on mnist and cifar- datasets and keep that architecture constant among different core sizes. different core sizes cho- sen here are , and (core sizes need not be in square shape but any other shapes are also possible). here, the accuracy will be same, as architecture is constant, while the core utilization and number of cores utilized will be different among different core sizes. second set of experiment is to change the neural network architecture for different core sizes. this will change the accuracy of neural network architectures for different core sizes, but the number of cores utilized will remain same. table i neural network (nn) architecture for mnist dataset nn core size architecture input layer layer layer table ii neural network (nn) architecture for cifar- dataset nn core size architecture input layer layer layer ) keeping architecture constant: consider the architecture shown in table i and ii. the softmax classier output layer is not shown in the tables. for this set of experiment, the architecture is maintained same irrespective of different core sizes neural network architecture chosen for mnist and cifar- datasets are given in the rst column under the core size, respectively in both tables i and ii. the neural network architecture is kept constant while mapping onto other core sizes as well. the convolutional lter size used is throughout the layers. in table i, between input layer and layer , stride used is and with padding in the input activations. between layer and layer , stride used is and with padding in the input. between layer and layer , stride used is again but without padding in the input. in table ii, between input layer and layer , stride used is and without any padding in the input. between layer and layer , stride used is and without padding in the input. between layer and layer , stride used is again but with padding in the input. from table iii and iv, the results for mnist and cifar- classication accuracy is constant among all the core sizes as the architecture remains same, while the core utilization and number of cores utilized changes with core sizes. table iii keeping architecture constant: mnist dataset core size acc (%) core no of core no of core no of utilization cores utilization cores utilization cores total cores table iv keeping architecture constant: cifar- dataset core size acc (%) core no of core no of core no of utilization cores utilization cores utilization cores total cores ) keeping architecture different: the different neural net- work architectures chosen for mnist and cifar- datasets for different core sizes are shown in table i and ii. for this set of experiment, the architecture is changed slightly to t onto the respective core sizes. the modication of the network is only done on the number of feature maps or channels in different layers. this modication will not really affect the mapping much. but, rather better accuracies are obtained with same number of cores utilized. the convolutional lter size used is throughtout the layers. the strides and padding used between all the layers are exactly same as mentioned in the previous subsection. from table v and vi, the results for mnist and cifar- classication accuracy is shown for different core sizes and can be seen that the accuracy improves with increase in core sizes. this is obvious that bigger network can be mapped on to neuromorphic chips with bigger core sizes, bigger the network, better the accuracy. the core utilization varies with mapping but the number of cores utilized remains same with core sizes. table v keeping architecture different: mnist dataset core utilization no: of (cores) acc % table vi keeping architecture different: cifar- dataset core utilization no: of (cores) acc % ",
        "Subsections": [],
        "Groundtruth": "The IV Results section presents an experiment involving the utilization of neuromorphic chips for a classification task on mnist and cifar- datasets. The focus is on showcasing neuromorphic core utilization rather than improving accuracies. Two sets of experiments are conducted: one maintains a constant neural network architecture while varying core sizes, and the other changes the architecture for different core sizes. The results show that accuracy remains constant when the architecture is the same, but increases with larger core sizes when the architecture is altered. Core utilization differs based on core sizes, while the number of cores utilized remains constant."
    },
    {
        "Section_Num": "V",
        "Section": "V Discussion and Conclusion",
        "Text": "random access memories are popular in terms of in- memory computation. resistive random access memory core fig. division of a convolutional neural network layer into different neuromorphic cores. (rram) became more popular in the eld of neuromorphic computing chips with the capability of doing both computation and memory at the same time. these two terminal rram devices are very much compatible with the crossbar array of synapses architecture, which enhanced its acceptance in the eld of neuromorphic chips. apart from rram, there are other devices like oating gate mosfet , memristors , thin lm devices and spin devices to be the contender of synaptic devices in a neuromorphic chip. the mapping of different portions of a convolutional layer onto different cores is shown in the g. different colors within the layer shows that those neurons are mapped onto particular core. for example, neurons in yellow are mapped onto core and neurons in brown are mapped onto core etc. the challenges in mapping onto a single neuromorphic core are mainly explained in the section for optimizations. one of the major priority while mapping is to choose the shape of the neurons in a layer that the chosen neurons and its correspond- ing axons could map completely onto a neuromorphic core without splitting the matrix between cores. another concern is to avoid the padded neurons while inferencing or mapping as these padded neurons during training is necessary to keep the size of the input activations but during inference these padded neurons become hardware overhead. in this paper, these two challenges are mitigated using simple techniques in the mapping function. from the results, it can be seen that bigger the core size, easier to map a bigger network and better the accuracy. similarly, if the accuracy is xed, then the lesser number of cores are utilized in a neuromorphic chip with bigger core size. this is infact better compared to usage of more number of cores in a neuromorphic chip with smaller core sizes because the communication between neuromorphic cores will consume more power than the computations. eventhough the neuromorphic chip with bigger core size is preferable, the bottleneck is the design possibility of such bigger crossbar array of synapses with the latest cmos technology. number of cores in a neuromorphic chip depends on the core size and the available silicon area for the chip. hence, number of cores and core size become a neuromorphic hardware constraint other than the major hardware constraints like synaptic noise, precision of weight and outputs. this paper gives an overview of mapping in neuromorphic chip with respect to the utilization of number of cores. the python wrapper for mad framework can output a visual representation of each core in a format easily veriable by the users (.csv or .xls). the verication of network activations and inferencing becomes quite simple as well. the code for python wrapper can be shared upon request. acknowledgment ",
        "Subsections": [],
        "Groundtruth": "The use of resistive random access memory (RRAM) in neuromorphic computing chips allows for simultaneous computation and memory capabilities. RRAM devices are compatible with the crossbar array of synapses architecture, enhancing their acceptance in the field. Other devices like floating gate MOSFETs, memristors, thin film devices, and spin devices also compete for synaptic device roles in neuromorphic chips. Challenges in mapping portions of a convolutional layer onto neuromorphic cores are addressed, including considerations for neuron shape and avoiding padded neurons for optimized performance. Larger core sizes facilitate easier mapping of networks and improved accuracy. The design of larger crossbar arrays with the latest CMOS technology presents a bottleneck in utilizing bigger core sizes. The number of cores in a neuromorphic chip is constrained by core size and available silicon area. This work provides an overview of core mapping in neuromorphic chips, with a Python wrapper for visual representation and verification of network activations."
    },
    {
        "Section_Num": "References",
        "Section": "References",
        "Text": "] eustace painkras, luis a. plana, jim garside, steve temple, si- mon davidson, jeffrey pepper, david clark, cameron patterson and steve furber, spinnaker: a multi core system on chip for massively- parallel neural net simulation, proceedings of the ieee custom integrated circuits conference, pp. -, san jose, ca, ben varkey benjamin, peiran gao, emmett mcquinn, swadesh choud- hary, anand r. chandrasekaran, jean marie bussat, rodrigo alvarez- icaza, john v. arthur, paul a. merolla and kwabena boahen, neuro- grid: a mixed analog digital multichip system for large scale neural simulations, proceedings of the ieee, vol. , issue. , pp. -, j. schemmel, d. briiderle, a. griibl, m. hock, k. meier and s. millner, a wafer scale neuromorphic hardware system for large scale neural modeling, proceedings of ieee international symposium on circuits and systems, pp. -, paris, j. s. seo, b. brezzo, y. liu, b. d. parker, s. k. esser, r. k. montoye, b. rajendran, j. a. tierno, l. chang, d. s. modha, and d. j. friedman, a nm cmos neuromorphic chip with a scalable architecture for learning in networks of spiking neurons, in ieee custom integrated circuits conference (cicc), pp. -, sept m. prezioso, f. merrikh bayat, b. d. hoskins, g. c. adam, k. k. likharev and d. b. strukov, training and operation of an integrated neuromorphic network based on metal oxide memristors, nature vol. , pp. -, may f. akopyan et al., truenorth: design and tool flow of a mw million neuron programmable neurosynaptic chip, in ieee transac- tions on computer aided design of integrated circuits and systems, vol. , no. , pp. -, oct. arnon amir, pallab datta, william p. risk, andrew s. cassidy, jeffrey a. kusnitz, steve k. esser, alexander andreopoulos, theodore m. wong, myron flickner, rodrigo alvarez icaza, emmett mcquinn, ben shaw, norm pass, and dharmendra s. modha, cognitive computing programming paradigm: a corelet language for composing networks of neurosynaptic cores, ieee international joint conference on neural networks (ijcnn), pp. -, aug, davison ap, brderle d, eppler jm, kremkow j, muller e, pecevski da, perrinet l and yger p, pynn: a common interface for neuronal network simulators, front. neuroinform., : doi:/neuro.. pande, s., morgan, f., cawley, s., mcginley, b., carrillo, s., harkin, j., and mcdaid, l., embrace sysc for analysis of noc based spiking neural network architectures, in system on chip (soc), international symposium on ieee, pp. -, sep, a. r. voelker, b. v. benjamin, t. c. stewart, k. boahen and c. eliasmith, extending the neural engineering framework for nonideal silicon synapses, ieee international symposium on circuits and sys- tems (iscas), pp. -, baltimore, md, yu ji, youhui zhang, shuangchen li, ping chi, cihang jiang, peng qu, yuan xie and wenguang chen, neutrams: neural network transformation and co design under neuromorphic hardware constraints, in the th annual ieee/acm international symposium on microar- chitecture (p. ). ieee press. m.k.f. lee, y. cui, t. somu, t. luo, j. zhou, w.t. tang, w.f. wong, and r.s.m. goh, a system level simulator for rram based neuromorphic computing chips, accepted by acm transactions on architecture and code optimization (taco). jacek m. zurada, introduction to articial neural system, west publishing company, st. paul, mn, wulfram gerstner, werner m. kistler, richard naud and liam paninski, neuronal dynamics, cambridge university press, july m. hu et al., dot product engine for neuromorphic computing: pro- gramming tm crossbar to accelerate matrix vector multiplication, nd acm/edac/ieee design automation conference (dac), pp. - , austin, tx, alom, m. z., josue, t., rahman, m. n., mitchell, w., yakop- cic, c., and taha, t. m. deep versus wide convolutional neural networks for object recognition on neuromorphic system, in international joint conference on neural networks (ijcnn). doi:/ijcnn. rathinakumar appuswamy and tapan k. nayak and john v. arthur and steven k. esser and paul merolla and jeffrey l. mckinstry and timothy melano and myron flickner and dharmendra s. modha, structured convolution matrices for energy efcient deep learning, corr, arxiv, r. m. gray, toeplitz and circulant matrices: a review. now pub- lishers, roshan gopalakrishnan and arindam basu, robust doublet stdp in a oating gate synapse, in proceedings of the international joint conference on neural networks, pp. -, beijing, china, jul. roshan gopalakrishnan and arindam basu, on the non stdp behav- ior and its remedy in a floating gate synapse, ieee transactions on neural networks and learning systems, vol. , no. , pp. -, feb. roshan gopalakrishnan and arindam basu, triplet spike time de- pendent plasticity in a oating gate synapse, in proceedings of the international symposium on circuits and systems, pp. -, lisbon, portugal, may. roshan gopalakrishnan and arindam basu, triplet spike time depen- dent plasticity in a oating gate synapse, ieee transactions on neural networks and learning systems, vol. , no. , pp. -, april s. sagui, c. mayr, t. serrano gotarredona, h. schmidt, g. lecerf, j. tomas, j. grollier, s. boyn, a. vincent, d. querlioz, s. la barbera, f. alibart, d. vuillaume, o. bichler, c. gamrat, and b. linares barranco, plasticity in memristive devices for spiking neural networks, fron- tiers in neuromorphic engineering. front. neurosci. :, -march- m. rahimi azghadi, b. linares barranco, d. abbott and p. h.w. leong, a hybrid cmos memristor neuromorphic synapse, ieee trans. on biomedical circuits and systems, vol. , no. , pp. - , april jose m cruz albrecht, timothy derosier, and narayan srinivasa. a scalable neural chip with synaptic electronics using cmos integrated memristors, nanotechnology, ():, rohit abraham john, fucai liu, nguyen anh chien, mohit r. kulka- rni, chao zhu, qundong fu, arindam basu, zheng liu, and nripan mathews, synergistic gating of electro iono photoactive d chalco- genide neuristors: coexistence of hebbian and homeostatic synaptic metaplasticity, advanced materials, (), may, a. sengupta, k. yogendra and k. roy, spintronic devices for ultra- low power neuromorphic computation (special session paper), ieee international symposium on circuits and systems (iscas), pp. -, montreal, qc, ",
        "Subsections": [],
        "Groundtruth": "The text discusses various research papers and works related to neuromorphic hardware systems for large-scale neural simulations, including Spinnaker, Neurogrid, wafer-scale neuromorphic hardware, TrueNorth, and others. It covers topics such as neuromorphic chips, memristors, neural network transformation, and system-level simulators for neuromorphic computing. Additionally, it mentions works on energy-efficient deep learning, structured convolution matrices for deep learning, spike time-dependent plasticity in floating gate synapses, plasticity in memristive devices, and spintronic devices for low-power neuromorphic computation."
    },
    {
        "Section_Num": "NA",
        "Section": "NA",
        "Text": "mad: mapping and debugging framework for implementing deep neural network onto a neuromorphic chip with crossbar array of synapses roshan gopalakrishnan institute for infocomm research (ir) astar singapore roshan@ir.a star.edu.sg ashish jith sreejith kumar school of electrical and electronic engineering nanyang technological university singapore ashishji@e.ntu.edu.sg yansong chua institute for infocomm research (ir) astar singapore chuays@ir.a star.edu.sg abstractneuromorphic systems or dedicated hardware for neuromorphic computing is getting popular with the advance- ment in research on different device materials for synapses, especially in crossbar architecture and also algorithms specic or compatible to neuromorphic hardware. hence, an automated mapping of any deep neural network onto the neuromorphic chip with crossbar array of synapses and an efcient debugging framework is very essential. here, mapping is dened as the deployment of a section of deep neural network layer onto a neuromorphic core and the generation of connection lists among population of neurons to specify the connectivity between various neuromorphic cores on the neuromorphic chip. debugging is the verication of computations performed on the neuromorphic chip during inferencing. together the framework becomes mapping and debugging (mad) framework. mad framework is quite general in usage as it is a python wrapper which can be integrated with almost every simulator tools for neuromorphic chips. this paper illustrates the mad framework in detail, considering some optimizations while mapping onto a single neuromorphic core. a classication task on mnist and cifar- datasets are considered for test case implementation of mad framework. index termsmapping, debugging, neuromorphic computing, neuromorphic chip, spiking neuron, synapse, crossbar array, deep neural network, mnist, cifar- i. introduction ",
        "Subsections": [],
        "Groundtruth": "This section introduces the \"mad\" framework, which is designed for implementing deep neural networks on neuromorphic chips with crossbar arrays of synapses. The framework includes automated mapping of neural networks to the chip and an efficient debugging process. Mapping involves deploying network layers onto neuromorphic cores and creating connection lists among neurons. Debugging verifies computations during inferencing. The framework is a Python wrapper compatible with various simulator tools for neuromorphic chips. The text explains the mad framework in detail, emphasizing optimizations for mapping on a single core. Test cases using MNIST and CIFAR datasets demonstrate the framework's application."
    },
    {
        "Section_Num": "Introduction",
        "Section": "Introduction",
        "Text": " ",
        "Subsections": [],
        "Groundtruth": "The introduction provides background information on the topic and highlights the importance of understanding the subject matter for the upcoming technical presentation."
    },
    {
        "Section_Num": "1",
        "Section": "1. Background material",
        "Text": " ",
        "Subsections": [],
        "Groundtruth": "The Background material section provides essential information for a technical presentation, offering a foundational understanding of the topic being discussed. This includes key concepts, historical context, and relevant background information that will enable the audience to grasp the subsequent technical content more effectively."
    },
    {
        "Section_Num": "2",
        "Section": "2. Construction of the maximal surface",
        "Text": " description of the boundary at innity parameterisation of wild anti de sitter structures references introduction globally hyperbolic maximal (ghm) anti de sitter three manifolds are a special class of lorentzian manifolds that share many similarities with hyperbolic quasi- fuchsian manifolds. mess initiated the study of the deformation space gh(s) of such structures () showing that if s is a closed, oriented surface of genus at least , then gh(s) is parameterised by two copies of the teichmller space of s. after that, many progress has been made in the understanding of the geometry of these manifolds (, , , ): in particular, krasnov and schlenker () noticed that they behave more like almost fuchsian hyperbolic manifolds in the sense that they always contain a unique embedded maximal sur- face (i.e. with vanishing mean curvature) with principal curvatures in (, ). they exploited this fact in order to construct a new parameterisation of gh(s) by the cotangent bundle of the teichmller space of s by associating to a ghm anti de sitter manifold m the conformal class of the induced metric and the holomorphic quadratic dierential that determines the second fundamental form of the maximal surface embedded in m. arxiv:v jan wild ghm ads structures this construction has been later generalised by the author to include non compact surfaces (, ). in particular, if is a connected, oriented surface with punctures and negative euler characteristic, we introduced a special class of globally hyperbolic maximal anti de sitter structures on r that we called regular and are parameterised by the bundle over teichmller space of of meromorphic quadratic dierentials with poles of order at most at the punctures. these man- ifolds play a role also in the theory of ghm anti de sitter structures with closed cauchy surfaces, as they can be seen as the geometric limits of such structures along pinching sequences in the cotangent bundle parameterisation (). in this paper we extend our previous results in order to include higher order poles. we expect this theory to be relevant for the study of degeneration of ghm anti de sitter structures along more general diverging sequences (). we rst show existence and uniqueness of the maximal surface with given embed- ding data: theorem a. given a complete hyperbolic metric h of nite area on and a meromorphic quadratic dierential q with poles of order at least at the punctures, there exists a unique (up to global isometries) complete, conformal equivariant max- imal embedding : ads into anti de sitter space whose second fundamental form is the real part of q. the embedding comes together with a representation : () isom(ads) that, by identifying isom(ads) with psl(, r) psl(, r), is equivalent to a pair of representations l,r : () psl(, r). by the recent work of gupta (), we will deduce that l,r are faithful and discrete and send peripheral curves to hyper- bolic elements. the main part of the paper is devoted to the study of the boundary at innity of the maximal surface, that, unlike the closed case, is only partially de- termined by the representation . recall that the boundary at innity of anti de sitter space can be identied with rp rp and the action of = (l, r) extends naturally on each factor. theorem b. the boundary at innity of ( ) is a locally achronal curve that contains the closure of the set of pairs of attracting xed points of (l, r). this set is completed to a topological circle by inserting, in a -equivariant way, a light like polygonal curve at each end. we will dene precisely in section what we mean by light like polygonal curve. here it suces to mention that it consists of an innite family of light like segments on the boundary at innity of ads belonging to the right foliation and the left- foliation in an alternate way, which is equivariant by the action of the cyclic group wild ghm ads structures generated by the hyperbolic translation along the corresponding peripheral curve. the boundary at innity of ( ) determines then a domain of dependence on which (()) acts properly discontinuously and the quotient gives the desired wild globally hyperbolic anti de sitter manifold dieomorphic to r. moreover, using the relation between maximal surfaces and minimal lagrangian maps, we are able to give an analogue of mess parameterisation for wild anti de sitter structures. recall that an orientation preserving dieomorphism m : (, h) (, h) between hyperbolic surfaces is minimal lagrangian if there exists a riemann sur- face x and harmonic maps f : x (, h) and f : x (, h) with opposite hopf dierentials such that m = f f these are in one to one correspondence with (l, r)-equivariant maximal surfaces in anti de sitter space via the gauss map (): the riemann surface x is determined by the conformal structure of the maximal surface, h and h are hyperbolic metrics on with holonomy l and r respectively, and the harmonic maps f and f are the projections of the equivari- ant gauss map (that in this lorentzian context takes value into h h). as a consequence of the work of gupta (), we deduce that in our case the image of the gauss map is a pair of crowned hyperbolic surfaces and we prove the following: theorem c. the deformation space of wild globally hyperbolic maximal anti de sitter structures on r is parameterised by the quotient of two copies of the te- ichmller space of crowned hyperbolic surfaces by the innite cyclic group generated by the diagonal action of dehn twists along the boundary curves and relabelling of the boundary cusps. outline of the paper. in section we recall well known facts about anti de sitter geometry, meromorphic quadratic dierentials and crowned hyperbolic surfaces. in section we prove existence and uniqueness of the equivariant maximal embedding starting from the data of a complete hyperbolic metric of nite area on and a meromorphic quadratic dierential with poles of order at least the boundary at innity of this surface is described in section we prove theorem c in section acknowledgement. the author would like to thank subhojoy gupta for answering specic questions about crowned hyperbolic surfaces. background material we recall here some well known facts about anti de sitter geometry, (meromor- phic) quadratic dierentials on riemann surfaces, and crowned hyperbolic surfaces that will be used in the sequel. throughout the paper, we will denote with a closed, connected, oriented surface and with = \\ {p, . . . , pn} a surface with a nite number of punctures. we will always assume that () < moreover, we wild ghm ads structures will denote with t() the teichmller space of , i.e. the space of marked complete hyperbolic structures of nite area on up to isotopy. . anti de sitter geometry. consider the vector space r endowed with a bi- linear form of signature (, ) x, y= xy + xy xy xy . we denote d ads = {x r | x, x= } . it can be easily veried that d ads is dieomorphic to a solid torus and the restriction of the bilinear form to the tangent space at each point endows d ads with a lorentzian metric of constant sectional curvature anti de sitter space is then dened as ads = p({x r | x, x< }) rp . the natural map : d ads ads is a two sheeted covering and we endow ads with the induced lorentzian structure. the isometry group of [ ads that preserves the orientation and the time orientation is so(, ), the connected component of the identity of the group of linear transformations that preserve the bilinear form of signature (, ). the boundary at innity of anti de sitter space is naturally identied with ads = p({x r | x, x= }) . it coincides with the image of the segre embedding s : rp rp rp, and thus, it is foliated by two families of projective lines, which we distinguish by calling s(rp {}) the right foliation and s({} rp) the left foliation. the action of an isometry extends continuously to the boundary, and preserves the two foliations. moreover, it acts on each line by a projective transformation, thus giving an identi- cation between pso(, ) and psl(, r) psl(, r). the lorentzian metric on ads induces on ads a conformally at lorentzian structure. to see this, notice that the map f : d s d ads (z, w) \u0012 z z, + z z w \u0013 is a dieomorphism, hence ds is a model for anti de sitter space if endowed with the pull back metric f gads = ( z) |dz| \u0012 + z z \u0013 d . wild ghm ads structures therefore, by composing with the projection : d ads ads, we deduce that f continuously extends to a homeomorphism f : s s ads (z, w) (z, w) and in these coordinates the conformally at lorentzian structure is induced by the conformal class c = . notice, in particular, that the light cone at each point p ads is generated by the two lines in the left- and right- foliation passing through p. . complete maximal surfaces in ads let u c be a simply connected domain. we say that : u ads is a space like embedding if is an embedding and the induced metric i = gads is riemannian. the fundamental theorem of surfaces embedded in anti de sitter space ensures that such a space like embedding is uniquely determined, up to post composition by a global isometry of ads, by its induced metric i and its shape operator b : tu tu, which satisfy ( db = (codazzi equation) ki = det(b) (gauss equation) where is the levi civita connection and ki is the curvature of the induced metric on (u). we say that is a maximal embedding if b is traceless. in this case, the codazzi equation implies that the second fundamental form ii = i(b, ) is the real part of a quadratic dierential q, which is holomorphic for the complex structure compatible with the induced metric i, in the following sense. for every pair of vector elds x and y on (u), we have re(q)(x, y ) = i(bx, y ) . in a local conformal coordinate z, we can write q = f(z)dz with f holomorphic and i = eu|dz| thus, re(q) is the bilinear form that in the frame {x, y} is represented by re(q) = \u0012 re(f) im(f) im(f) re(f) \u0013 , and the shape operator can be recovered as b = ire(q). if the induced metric is complete, the space like condition implies that, identifying d ads with d s via f, the surface is the graph of a -lipschitz map ([tamb, proposition ]) and its boundary at innity is a locally achronal topological circle in ads () such that if two points are causally related, then a light like segment joining them is entirely contained in ([tamb, lemma ]). wild ghm ads structures . ghmc anti de sitter manifolds. this paper deals with the moduli space of a special class of manifolds locally isometric to ads we say that an anti de sitter three manifold m is globally hyperbolic maximal (ghm) if it contains an embedded, oriented, space like surface s that intersects every inextensible causal curve in exactly one point, and if m is maximal by iso- metric embeddings. it turns out that m is necessarily dieomorphic to a product s r (). moreover, we say that m is cauchy compact (c) if s is closed. we denote by gh(s) the deformation space of ghmc anti de sitter structures on sr. the theory is well developed when s is closed of genus at least : theorem (). gh(s) is parameterised by t(s) t(s). the homeomorphism is constructed as follows. given a ghmc anti de sitter structure, its holonomy representation : (s) isom(ads) = psl(, r) psl(, r) induces a pair of representations (l, r) by projecting onto each factor. mess proved that both are faithful and discrete and thus dene two points in t(s). on the other hand, given a pair of fuchsian representations (l, r), there exists a unique homeomorphism : rp rp such that r() = l() for every (s). the graph of denes a curve on the boundary at innity of ads and mess constructed a maximal domain of discontinuity d() for the action of ((s)), called domain of dependence, by considering the set of points whose (projective) dual space like plane is disjoint from . the quotient m = d()/((s)) is the desired ghmc anti de sitter manifold. later krasnov and schlenker () introduced another parameterisation of gh(s) by the cotangent bundle over t(s), which is what inspired our construction. let us recall it briey here. let m be a ghmc anti de sitter manifold. it is well- known that m contains a unique embedded maximal surface s (). lifting s to ads, we obtain an equivariant maximal embedding of h into ads, which is completely determined (up to global isometries of ads) by its induced metric and a holomorphic quadratic dierential. by equivariance, these dene a riemannian metric i and a holomorphic quadratic dierential q on s. we can thus dene a map : gh(s) t t(s) m (h, q) associating to a ghmc anti de sitter structure the unique hyperbolic metric in the conformal class of i and the holomorphic quadratic dierential q. in order to prove that is a homeomorphism, krasnov and schlenker () found an explicit inverse. they showed that, given a hyperbolic metric h and a quadratic dierential q that is holomorphic for the complex structure compatible wild ghm ads structures with h, it is always possible to nd a smooth map v : s r such that i = evh and b = ire(q) are the induced metric and the shape operator of a maximal surface embedded in a ghmc anti de sitter manifold. this is accomplished by noticing that the codazzi equation for b is trivially satised since q is holomorphic, and thus it is sucient to nd v so that the gauss equation holds. now, det(b) = det(ev(h)re(q)) = ev det((h)re(q)) = evq h and ki = ev(kh hv) = ev(kh hv) hence the gauss equation translates into the quasi linear pde () hv = ev evq h + kh . they proved existence and uniqueness of the solution to equation () on closed surfaces and on surfaces with punctures, when q has simple pole sigularities at the punctures. in an analogous result was obtained for meromorphic quadratic dierentials with poles of order at most at the punctures. in section , we will extend this result to include higher order poles and describe the geometry of the associated maximal surface. . meromorphic quadratic dierentials. suppose that is endowed with a complex structure. a meromorphic quadratic dierential q on is a (, )-tensor, locally of the form q(z)dz, where q(z) is a meromorphic function with poles at the punctures {p, . . . , pn}. in this paper, we are interested in meromorphic quadratic dierentials with poles of order n at the punctures. in this case, we can always nd a local coordinate chart around the puncture such that q(z)dz = \u0010an zn + an zn + + a z \u0011 dz for some coecients aj c. meromorphic quadratic dierentials with poles at points p, p, . . . , pn of orders bounded above by n, n, . . . , nn n form a vector space over c of real dimension d = |()| + p i ni, by the riemann roch theorem. in particular, the space of meromorphic quadratic dierentials with poles of order exactly n, . . . , nn is parameterised by rdn (s)n. a meromorphic quadratic dierential q induces a singular at metrics |q| on that in local coordinates is written as |q| = |q(z)||dz| the metric has cone singularities of angle (m + ) at a zero of order m of q. when poles have order at least , the metric is complete of innite area, and poles are at innite distance from any point on the surface. moreover, strebel () described the local picture of the singular at metric around a pole p of order n as a cyclic arrangement of (n ) half- planes glued along half lines in their boundaries. these half planes are constructed as follows (see also ). first, we choose a local coordinate w adapted to the wild ghm ads structures quadratic dierential in the sense that wq = wn dw if n is odd \u0010 wn/ + a w \u0011 dw if n is even . then, for every k = , . . . , n , the half planes are the images of the natural charts k : h v = { < |w| < r} dened by the property that kq = d if p is a pole of odd order, these natural coordinates can be written explicitly as () k() = \u0012n \u0013 n exp \u0012 n log( + ib) + ki n \u0013 where b > is big enough to ensure that k(h) v . for even order poles the above construction needs to be slightly modied: for > small, we consider h = { c | < arg() < + } . notice that there is a constant () > depending on such that any pair of points , h is connected by a path in h with length bounded above by ()||. the natural coordinates are then dened as a composition k = k : h h v where and k are dened as follows. the map k is given by () extended to h for a suitable choice of b that guarantees that k(h ) v . an easy computation shows that kq = \u0012 + c + ib \u0013 d for a constant c c depending only on a. up to increase b further we can assume that c + ib < () for every h . with this choice, the map f() = + c log( + ib) sends h injectively into a domain in the complex plane containing h + id fo some constant d > large enough, thus the function () = f ( + id) is well dened as map : h h and the composition k = k is the desired natural coordinate. moreover, by construction, the union of the images k(h) for k = , . . . , n is a punctured neighbourhood of p, and two consecutive charts only intersect along a half ray in their boundary. wild ghm ads structures . crowned hyperbolic surfaces. a crown c with m boundary cusps is an incomplete hyperbolic surface bounded by a closed geodesic boundary c and a crown end consisting of bi innite geodesic {i}m i= arranged in cyclic order, such that the right half line of the geodesic i is asymptotic to the left half line of the geodesic i+, where indices are intended modulo m. a crown comes equipped with a labelling of the boundary cusps compatible with the cyclic order. a polygonal end p of a crown is the z invariant bi innite chain of geodesic lines in h obtained by lifting the cyclically ordered collection of geodesics {i}m i= in c to its universal cover, where z is the group generated by the hyperbolic translation corresponding to the geodesic boundary c. notice that the ideal points of the chain of geodesics of the polygonal end limit to the end point of the axis of the lift of c. the hyperbolic crowns we will consider come with an additional real parameter, the boundary twist, that we associate with the geodesic boundary. in the corre- sponding polygonal end in the universal cover, this can be thought of as the choice of a marked point on the axis and the parameter is the signed distance of this point from the foot of the orthogonal arc from the cusp labelled with to . let s denote a compact, oriented surface of genus and b boundary com- ponents. a crowned hyperbolic surface is obtained by attaching crowns to a compact hyperbolic surface with geodesic boundaries by isometries along their closed bound- aries. this results in an incomplete hyperbolic metric of nite area on the surface. we denote with t(s, m, . . . , mb) the teichmller space of crowned hyperbolic sur- faces such that the i th crown has mi boundary cusps, for every i = , . . . , b. in this context the marking is a homeomorphism f : s x sending a neighbourhood of the boundary to the crown end. two marked hyperbolic surfaces with crowns (x, f) and (y, g) are equivalent if there is an isometry i : x y that is homotopic to g f via a homotopy that keeps each boundary component xed, and g f does not dehn twist around any crown end. proposition (lemma ). the teichmller space of crowned hyper- bolic surfaces is homeomorphic to rn, where n = + pb i=(mi + ). here is a possible way to give coordinates to t(s, m, . . . , mb): the rst +b parameters are the familiar fenchel nielsen coordinates on the teichmller space of surfaces of genus and b geodesic boundaries. then, after xing an identica- tion of the universal cover of the surface with boundary with a domain in h, the marked crowned hyperbolic surface is determined by the end points of the lifts of the boundary cusps in a fundamental domain. in order to keep track also of the twist parameters, we x, in an equivariant way, a point on the lifts of each geodesic boundary so that the remaining pb i= mi parameters can be dened as follows: b real parameters are given by the signed distance between the base point xed above and the foot of the geodesic arc exiting from the boundary cusp labelled with \"\" intersecting the geodesic boundary orthogonally, and the other pb i=(mi ) are wild ghm ads structures positive real numbers determined by the relative distance between the intersection points of the geodesic rays emanating from two consecutive cusps and orthogonal to the geodesic boundary. construction of the maximal surface in the next sections we are going to construct globally hyperbolic maximal anti de sitter structures on r starting from the data of a complete hyperbolic metric h on of nite area and a meromorphic quadratic dierential q with poles of order at least at the punctures. we rst nd a complete equivariant maximal embedding into ads with induced metric i = evh and second fundamental form ii = re(q). we will then describe its boundary at innity and prove that () acts by isometries and properly discontinuously on its domain of dependence, thus inducing a globally hyperbolic anti de sitter structure on the quotient. let h t () be a complete hyperbolic metric of nite area on and let q be a meromorphic quadratic dierential with poles of order at least at the punctures. recall that nding an equivariant maximal conformal embedding of into ads is equivalent to nding a solution to the quasi linear pde (section ) hv = ev evq h + kh . this is an example of vortex equation, recently studied in the context of riemann surfaces with punctures in . we recall here for the convenience of the reader the main steps for the construction of the unique solution and the asymptotic esti- mates that will be used in the sequel. the main idea consists in choosing another complete background metric g in the same conformal class as h such that q g + kg at pi . in this way, the function u : r that satises evh = eug is the solution of the dierential equation gu = eu euq g + kg and the assumptions on g guarantees that u = is an approximate solution in a neighbourhood of the punctures. this metric g is dened as a smooth interpolation between the metric h of constant curvature and the at metric induced by the quadratic dierential. more precisely, we introduce a local coordinate zi in a neighbourhood ui of the puncture pi disjoint from the zeros of q and dene g = |q| for |zi| < ci ei|dzi| for ci |zi| ci h for |zi| > ci and on \\ ui wild ghm ads structures for smooth interpolating functions i. moreover, we can assume that there exists i > such that q g i on ui because q(zi) g when zi proposition . there exists a bounded smooth function u : r satisfying () gu = eu euq g + kg . proof. let f(x, u) = eu euq g + kg. since f is an increasing function of u, the solution to equation () is guaranteed () by the existence of two continuous functions u : r such that u+ f(u+, x), uf(u, x) and uu+ . let us start with the supersolution u+. let f : r be a positive smooth function such that f(zi) = |zi|i on the neighbourhood {|zi| < ci} of the puncture pi for some i > to be chosen later. for any r we consider u+ = f. we claim that it is possible to nd > large enough and i > suciently small so that u+ is a supersolution. in fact, on vi = {|zi| < ci} ui, we can nd a constant di > such that |q| di|z| and we have g(|zi|i) e|zi|i + e|zi|iq g kg i di|zi|i e|zi|i + e|zi|i \u0012 i di \u0013 u+ + (eu+ eu+ + u+) + eu+ , which can be made negative, because the term in the middle is always non positive and we can choose i small enough and large enough so that the sum of the rst and last term is negative. therefore, u+ is a supersolution on vi for every i > and > outside vi, we do not have control on the curvature of g and on the laplacian of f, but knowing that they are bounded, we can increase so that gf ef + efq g kg because ef grows the fastest when +. this proves that u+ is a supersolution everywhere on . as for the subsolution, let w : \\q() r be half of the logarithmic density of the at metric |q| with respect to g, that is ewg = |q|. we claim that w is a solution outside the zeros of q: in fact, gw ew + ewq g kg = (gw kg) ew + ewq |q| = because q|q| = and the rst term vanishes because the metric |q| is at outside the zeros of q. notice that w tends to at a zero of q and, by our denition wild ghm ads structures of the open sets ui and of the metric g, the background metric on has constant curvature in a small neighbourhood of the zeros of q. since any negative constant is a subsolution where the metric g has constant curvature , the function u= ( w on ui max(w, b) on \\ ui for a suciently large b > is a continuous subsolution, being it the maximum of two subsolutions. we remark that the resulting metric i = eug is complete because g is com- plete and u is bounded. moreover, the subsolution we found implies that i |q|. uniqueness follows then from a general result about vortex equations: proposition ( theorem ). for every non zero holomorphic quadratic dierential on there exists a unique complete solution to equation (). combining the above results we obtain: theorem . for any complete hyperbolic metric h on of nite area and for any meromorphic quadratic dierential q on with poles of order at least at the punctures there exists a unique complete equivariant maximal embedding : ads with induced metric i conformal to h and second fundamental form ii = re(q). moreover, the principal curvatures are in (, ). proof. existence and uniqueness of such embedding follows from the above discus- sion. let be the positive principal curvature of the maximal surface. by denition of q, we have = det(b) = euq g at the punctures. therefore, is bounded and a classical fact about maximal surfaces in anti de sitter space () implies that [, ). . asymptotic estimates. in order to describe the geometry of the maximal surface, we will also need the following precise estimate for the solution v in a neigh- bourhood of a puncture. recall that such a neighbourhood is covered by a collection of half planes, in which the quadratic dierential pulls back to d by an abuse of notation, we will still indicate with v the function such that ev|d| equals the induced metric on ( ) in the -coordinates. proposition . let be a natural coordinate for q dened on a standard half plane in a neighbourhood of a puncture p. then v() = o e || p || ! as || +. proof. in a natural coordinate the function v satises the pde () v = ev ev wild ghm ads structures because |q()| = and the background metric is at. the subsolution and su- persolution in proposition also show that v is non negative and innitesimal. in particular, we can assume that v on every half plane and we have that ev ev v . the asymptotic estimate will then follow from [dw, lemma ] provided we show that the restriction of v to the boundary of a half plane is integrable. in order to show this, we prove that v is exponentially decaying. let be a point in the boundary of the half plane. if || is suciently large, this point is actually contained also in the precedent or the following standard half plane. in both cases, we can nd a constant c > depending only on the gluing map between the half planes (hence only on q) and a ball of radius r() = || c centered at , which is entirely contained in these two coordinate charts. using a coordinate in this ball br(), the function v satises the same equation () on this ball. therefore, the solution of the dirichlet problem ( h = h h|br() = is a supersolution and as such is greater than v. it is then well known that the solution of the above dirichlet problem is the function h() = i( ||) i( |r(||)) where i is the modied bessel function of the rst kind (). hence, () v() h() = o(|| e ||) as || +. ",
        "Subsections": [],
        "Groundtruth": "The section describes the construction of maximal surfaces in globally hyperbolic maximal (GHM) anti de Sitter three manifolds, detailing the parameterization of wild GHM anti de Sitter structures. The study includes the geometry of these manifolds, the uniqueness of maximal surfaces, existence and uniqueness of embeddings, and the boundary at infinity of maximal surfaces. The authors also extend results to include higher order poles and provide a precise characterization of the deformation space of wild GHM anti de Sitter structures. Additionally, the text covers the background information related to anti de Sitter geometry, meromorphic quadratic differentials, and crowned hyperbolic surfaces that are used in the construction process."
    },
    {
        "Section_Num": "3",
        "Section": "3. Description of the boundary at infinity",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "4",
        "Section": "4. Parameterisation of wild anti-de Sitter structures",
        "Text": " ",
        "Subsections": [],
        "Groundtruth": "The text discusses the parameterisation of wild anti-de Sitter structures. These structures are geometric objects used in certain mathematical models. Parameterisation is a technique that assigns variables to these structures, allowing for easier analysis and manipulation within the models."
    },
    {
        "Section_Num": "References",
        "Section": "References",
        "Text": " introduction globally hyperbolic maximal (ghm) anti de sitter three manifolds are a special class of lorentzian manifolds that share many similarities with hyperbolic quasi- fuchsian manifolds. mess initiated the study of the deformation space gh(s) of such structures () showing that if s is a closed, oriented surface of genus at least , then gh(s) is parameterised by two copies of the teichmller space of s. after that, many progress has been made in the understanding of the geometry of these manifolds (, , , ): in particular, krasnov and schlenker () noticed that they behave more like almost fuchsian hyperbolic manifolds in the sense that they always contain a unique embedded maximal sur- face (i.e. with vanishing mean curvature) with principal curvatures in (, ). they exploited this fact in order to construct a new parameterisation of gh(s) by the cotangent bundle of the teichmller space of s by associating to a ghm anti de sitter manifold m the conformal class of the induced metric and the holomorphic quadratic dierential that determines the second fundamental form of the maximal surface embedded in m. arxiv:v jan wild ghm ads structures this construction has been later generalised by the author to include non compact surfaces (, ). in particular, if is a connected, oriented surface with punctures and negative euler characteristic, we introduced a special class of globally hyperbolic maximal anti de sitter structures on r that we called regular and are parameterised by the bundle over teichmller space of of meromorphic quadratic dierentials with poles of order at most at the punctures. these man- ifolds play a role also in the theory of ghm anti de sitter structures with closed cauchy surfaces, as they can be seen as the geometric limits of such structures along pinching sequences in the cotangent bundle parameterisation (). in this paper we extend our previous results in order to include higher order poles. we expect this theory to be relevant for the study of degeneration of ghm anti de sitter structures along more general diverging sequences (). we rst show existence and uniqueness of the maximal surface with given embed- ding data: theorem a. given a complete hyperbolic metric h of nite area on and a meromorphic quadratic dierential q with poles of order at least at the punctures, there exists a unique (up to global isometries) complete, conformal equivariant max- imal embedding : ads into anti de sitter space whose second fundamental form is the real part of q. the embedding comes together with a representation : () isom(ads) that, by identifying isom(ads) with psl(, r) psl(, r), is equivalent to a pair of representations l,r : () psl(, r). by the recent work of gupta (), we will deduce that l,r are faithful and discrete and send peripheral curves to hyper- bolic elements. the main part of the paper is devoted to the study of the boundary at innity of the maximal surface, that, unlike the closed case, is only partially de- termined by the representation . recall that the boundary at innity of anti de sitter space can be identied with rp rp and the action of = (l, r) extends naturally on each factor. theorem b. the boundary at innity of ( ) is a locally achronal curve that contains the closure of the set of pairs of attracting xed points of (l, r). this set is completed to a topological circle by inserting, in a -equivariant way, a light like polygonal curve at each end. we will dene precisely in section what we mean by light like polygonal curve. here it suces to mention that it consists of an innite family of light like segments on the boundary at innity of ads belonging to the right foliation and the left- foliation in an alternate way, which is equivariant by the action of the cyclic group wild ghm ads structures generated by the hyperbolic translation along the corresponding peripheral curve. the boundary at innity of ( ) determines then a domain of dependence on which (()) acts properly discontinuously and the quotient gives the desired wild globally hyperbolic anti de sitter manifold dieomorphic to r. moreover, using the relation between maximal surfaces and minimal lagrangian maps, we are able to give an analogue of mess parameterisation for wild anti de sitter structures. recall that an orientation preserving dieomorphism m : (, h) (, h) between hyperbolic surfaces is minimal lagrangian if there exists a riemann sur- face x and harmonic maps f : x (, h) and f : x (, h) with opposite hopf dierentials such that m = f f these are in one to one correspondence with (l, r)-equivariant maximal surfaces in anti de sitter space via the gauss map (): the riemann surface x is determined by the conformal structure of the maximal surface, h and h are hyperbolic metrics on with holonomy l and r respectively, and the harmonic maps f and f are the projections of the equivari- ant gauss map (that in this lorentzian context takes value into h h). as a consequence of the work of gupta (), we deduce that in our case the image of the gauss map is a pair of crowned hyperbolic surfaces and we prove the following: theorem c. the deformation space of wild globally hyperbolic maximal anti de sitter structures on r is parameterised by the quotient of two copies of the te- ichmller space of crowned hyperbolic surfaces by the innite cyclic group generated by the diagonal action of dehn twists along the boundary curves and relabelling of the boundary cusps. outline of the paper. in section we recall well known facts about anti de sitter geometry, meromorphic quadratic dierentials and crowned hyperbolic surfaces. in section we prove existence and uniqueness of the equivariant maximal embedding starting from the data of a complete hyperbolic metric of nite area on and a meromorphic quadratic dierential with poles of order at least the boundary at innity of this surface is described in section we prove theorem c in section acknowledgement. the author would like to thank subhojoy gupta for answering specic questions about crowned hyperbolic surfaces. background material we recall here some well known facts about anti de sitter geometry, (meromor- phic) quadratic dierentials on riemann surfaces, and crowned hyperbolic surfaces that will be used in the sequel. throughout the paper, we will denote with a closed, connected, oriented surface and with = \\ {p, . . . , pn} a surface with a nite number of punctures. we will always assume that () < moreover, we wild ghm ads structures will denote with t() the teichmller space of , i.e. the space of marked complete hyperbolic structures of nite area on up to isotopy. . anti de sitter geometry. consider the vector space r endowed with a bi- linear form of signature (, ) x, y= xy + xy xy xy . we denote d ads = {x r | x, x= } . it can be easily veried that d ads is dieomorphic to a solid torus and the restriction of the bilinear form to the tangent space at each point endows d ads with a lorentzian metric of constant sectional curvature anti de sitter space is then dened as ads = p({x r | x, x< }) rp . the natural map : d ads ads is a two sheeted covering and we endow ads with the induced lorentzian structure. the isometry group of [ ads that preserves the orientation and the time orientation is so(, ), the connected component of the identity of the group of linear transformations that preserve the bilinear form of signature (, ). the boundary at innity of anti de sitter space is naturally identied with ads = p({x r | x, x= }) . it coincides with the image of the segre embedding s : rp rp rp, and thus, it is foliated by two families of projective lines, which we distinguish by calling s(rp {}) the right foliation and s({} rp) the left foliation. the action of an isometry extends continuously to the boundary, and preserves the two foliations. moreover, it acts on each line by a projective transformation, thus giving an identi- cation between pso(, ) and psl(, r) psl(, r). the lorentzian metric on ads induces on ads a conformally at lorentzian structure. to see this, notice that the map f : d s d ads (z, w) \u0012 z z, + z z w \u0013 is a dieomorphism, hence ds is a model for anti de sitter space if endowed with the pull back metric f gads = ( z) |dz| \u0012 + z z \u0013 d . wild ghm ads structures therefore, by composing with the projection : d ads ads, we deduce that f continuously extends to a homeomorphism f : s s ads (z, w) (z, w) and in these coordinates the conformally at lorentzian structure is induced by the conformal class c = . notice, in particular, that the light cone at each point p ads is generated by the two lines in the left- and right- foliation passing through p. . complete maximal surfaces in ads let u c be a simply connected domain. we say that : u ads is a space like embedding if is an embedding and the induced metric i = gads is riemannian. the fundamental theorem of surfaces embedded in anti de sitter space ensures that such a space like embedding is uniquely determined, up to post composition by a global isometry of ads, by its induced metric i and its shape operator b : tu tu, which satisfy ( db = (codazzi equation) ki = det(b) (gauss equation) where is the levi civita connection and ki is the curvature of the induced metric on (u). we say that is a maximal embedding if b is traceless. in this case, the codazzi equation implies that the second fundamental form ii = i(b, ) is the real part of a quadratic dierential q, which is holomorphic for the complex structure compatible with the induced metric i, in the following sense. for every pair of vector elds x and y on (u), we have re(q)(x, y ) = i(bx, y ) . in a local conformal coordinate z, we can write q = f(z)dz with f holomorphic and i = eu|dz| thus, re(q) is the bilinear form that in the frame {x, y} is represented by re(q) = \u0012 re(f) im(f) im(f) re(f) \u0013 , and the shape operator can be recovered as b = ire(q). if the induced metric is complete, the space like condition implies that, identifying d ads with d s via f, the surface is the graph of a -lipschitz map ([tamb, proposition ]) and its boundary at innity is a locally achronal topological circle in ads () such that if two points are causally related, then a light like segment joining them is entirely contained in ([tamb, lemma ]). wild ghm ads structures . ghmc anti de sitter manifolds. this paper deals with the moduli space of a special class of manifolds locally isometric to ads we say that an anti de sitter three manifold m is globally hyperbolic maximal (ghm) if it contains an embedded, oriented, space like surface s that intersects every inextensible causal curve in exactly one point, and if m is maximal by iso- metric embeddings. it turns out that m is necessarily dieomorphic to a product s r (). moreover, we say that m is cauchy compact (c) if s is closed. we denote by gh(s) the deformation space of ghmc anti de sitter structures on sr. the theory is well developed when s is closed of genus at least : theorem (). gh(s) is parameterised by t(s) t(s). the homeomorphism is constructed as follows. given a ghmc anti de sitter structure, its holonomy representation : (s) isom(ads) = psl(, r) psl(, r) induces a pair of representations (l, r) by projecting onto each factor. mess proved that both are faithful and discrete and thus dene two points in t(s). on the other hand, given a pair of fuchsian representations (l, r), there exists a unique homeomorphism : rp rp such that r() = l() for every (s). the graph of denes a curve on the boundary at innity of ads and mess constructed a maximal domain of discontinuity d() for the action of ((s)), called domain of dependence, by considering the set of points whose (projective) dual space like plane is disjoint from . the quotient m = d()/((s)) is the desired ghmc anti de sitter manifold. later krasnov and schlenker () introduced another parameterisation of gh(s) by the cotangent bundle over t(s), which is what inspired our construction. let us recall it briey here. let m be a ghmc anti de sitter manifold. it is well- known that m contains a unique embedded maximal surface s (). lifting s to ads, we obtain an equivariant maximal embedding of h into ads, which is completely determined (up to global isometries of ads) by its induced metric and a holomorphic quadratic dierential. by equivariance, these dene a riemannian metric i and a holomorphic quadratic dierential q on s. we can thus dene a map : gh(s) t t(s) m (h, q) associating to a ghmc anti de sitter structure the unique hyperbolic metric in the conformal class of i and the holomorphic quadratic dierential q. in order to prove that is a homeomorphism, krasnov and schlenker () found an explicit inverse. they showed that, given a hyperbolic metric h and a quadratic dierential q that is holomorphic for the complex structure compatible wild ghm ads structures with h, it is always possible to nd a smooth map v : s r such that i = evh and b = ire(q) are the induced metric and the shape operator of a maximal surface embedded in a ghmc anti de sitter manifold. this is accomplished by noticing that the codazzi equation for b is trivially satised since q is holomorphic, and thus it is sucient to nd v so that the gauss equation holds. now, det(b) = det(ev(h)re(q)) = ev det((h)re(q)) = evq h and ki = ev(kh hv) = ev(kh hv) hence the gauss equation translates into the quasi linear pde () hv = ev evq h + kh . they proved existence and uniqueness of the solution to equation () on closed surfaces and on surfaces with punctures, when q has simple pole sigularities at the punctures. in an analogous result was obtained for meromorphic quadratic dierentials with poles of order at most at the punctures. in section , we will extend this result to include higher order poles and describe the geometry of the associated maximal surface. . meromorphic quadratic dierentials. suppose that is endowed with a complex structure. a meromorphic quadratic dierential q on is a (, )-tensor, locally of the form q(z)dz, where q(z) is a meromorphic function with poles at the punctures {p, . . . , pn}. in this paper, we are interested in meromorphic quadratic dierentials with poles of order n at the punctures. in this case, we can always nd a local coordinate chart around the puncture such that q(z)dz = \u0010an zn + an zn + + a z \u0011 dz for some coecients aj c. meromorphic quadratic dierentials with poles at points p, p, . . . , pn of orders bounded above by n, n, . . . , nn n form a vector space over c of real dimension d = |()| + p i ni, by the riemann roch theorem. in particular, the space of meromorphic quadratic dierentials with poles of order exactly n, . . . , nn is parameterised by rdn (s)n. a meromorphic quadratic dierential q induces a singular at metrics |q| on that in local coordinates is written as |q| = |q(z)||dz| the metric has cone singularities of angle (m + ) at a zero of order m of q. when poles have order at least , the metric is complete of innite area, and poles are at innite distance from any point on the surface. moreover, strebel () described the local picture of the singular at metric around a pole p of order n as a cyclic arrangement of (n ) half- planes glued along half lines in their boundaries. these half planes are constructed as follows (see also ). first, we choose a local coordinate w adapted to the wild ghm ads structures quadratic dierential in the sense that wq = wn dw if n is odd \u0010 wn/ + a w \u0011 dw if n is even . then, for every k = , . . . , n , the half planes are the images of the natural charts k : h v = { < |w| < r} dened by the property that kq = d if p is a pole of odd order, these natural coordinates can be written explicitly as () k() = \u0012n \u0013 n exp \u0012 n log( + ib) + ki n \u0013 where b > is big enough to ensure that k(h) v . for even order poles the above construction needs to be slightly modied: for > small, we consider h = { c | < arg() < + } . notice that there is a constant () > depending on such that any pair of points , h is connected by a path in h with length bounded above by ()||. the natural coordinates are then dened as a composition k = k : h h v where and k are dened as follows. the map k is given by () extended to h for a suitable choice of b that guarantees that k(h ) v . an easy computation shows that kq = \u0012 + c + ib \u0013 d for a constant c c depending only on a. up to increase b further we can assume that c + ib < () for every h . with this choice, the map f() = + c log( + ib) sends h injectively into a domain in the complex plane containing h + id fo some constant d > large enough, thus the function () = f ( + id) is well dened as map : h h and the composition k = k is the desired natural coordinate. moreover, by construction, the union of the images k(h) for k = , . . . , n is a punctured neighbourhood of p, and two consecutive charts only intersect along a half ray in their boundary. wild ghm ads structures . crowned hyperbolic surfaces. a crown c with m boundary cusps is an incomplete hyperbolic surface bounded by a closed geodesic boundary c and a crown end consisting of bi innite geodesic {i}m i= arranged in cyclic order, such that the right half line of the geodesic i is asymptotic to the left half line of the geodesic i+, where indices are intended modulo m. a crown comes equipped with a labelling of the boundary cusps compatible with the cyclic order. a polygonal end p of a crown is the z invariant bi innite chain of geodesic lines in h obtained by lifting the cyclically ordered collection of geodesics {i}m i= in c to its universal cover, where z is the group generated by the hyperbolic translation corresponding to the geodesic boundary c. notice that the ideal points of the chain of geodesics of the polygonal end limit to the end point of the axis of the lift of c. the hyperbolic crowns we will consider come with an additional real parameter, the boundary twist, that we associate with the geodesic boundary. in the corre- sponding polygonal end in the universal cover, this can be thought of as the choice of a marked point on the axis and the parameter is the signed distance of this point from the foot of the orthogonal arc from the cusp labelled with to . let s denote a compact, oriented surface of genus and b boundary com- ponents. a crowned hyperbolic surface is obtained by attaching crowns to a compact hyperbolic surface with geodesic boundaries by isometries along their closed bound- aries. this results in an incomplete hyperbolic metric of nite area on the surface. we denote with t(s, m, . . . , mb) the teichmller space of crowned hyperbolic sur- faces such that the i th crown has mi boundary cusps, for every i = , . . . , b. in this context the marking is a homeomorphism f : s x sending a neighbourhood of the boundary to the crown end. two marked hyperbolic surfaces with crowns (x, f) and (y, g) are equivalent if there is an isometry i : x y that is homotopic to g f via a homotopy that keeps each boundary component xed, and g f does not dehn twist around any crown end. proposition (lemma ). the teichmller space of crowned hyper- bolic surfaces is homeomorphic to rn, where n = + pb i=(mi + ). here is a possible way to give coordinates to t(s, m, . . . , mb): the rst +b parameters are the familiar fenchel nielsen coordinates on the teichmller space of surfaces of genus and b geodesic boundaries. then, after xing an identica- tion of the universal cover of the surface with boundary with a domain in h, the marked crowned hyperbolic surface is determined by the end points of the lifts of the boundary cusps in a fundamental domain. in order to keep track also of the twist parameters, we x, in an equivariant way, a point on the lifts of each geodesic boundary so that the remaining pb i= mi parameters can be dened as follows: b real parameters are given by the signed distance between the base point xed above and the foot of the geodesic arc exiting from the boundary cusp labelled with \"\" intersecting the geodesic boundary orthogonally, and the other pb i=(mi ) are wild ghm ads structures positive real numbers determined by the relative distance between the intersection points of the geodesic rays emanating from two consecutive cusps and orthogonal to the geodesic boundary. construction of the maximal surface in the next sections we are going to construct globally hyperbolic maximal anti de sitter structures on r starting from the data of a complete hyperbolic metric h on of nite area and a meromorphic quadratic dierential q with poles of order at least at the punctures. we rst nd a complete equivariant maximal embedding into ads with induced metric i = evh and second fundamental form ii = re(q). we will then describe its boundary at innity and prove that () acts by isometries and properly discontinuously on its domain of dependence, thus inducing a globally hyperbolic anti de sitter structure on the quotient. let h t () be a complete hyperbolic metric of nite area on and let q be a meromorphic quadratic dierential with poles of order at least at the punctures. recall that nding an equivariant maximal conformal embedding of into ads is equivalent to nding a solution to the quasi linear pde (section ) hv = ev evq h + kh . this is an example of vortex equation, recently studied in the context of riemann surfaces with punctures in . we recall here for the convenience of the reader the main steps for the construction of the unique solution and the asymptotic esti- mates that will be used in the sequel. the main idea consists in choosing another complete background metric g in the same conformal class as h such that q g + kg at pi . in this way, the function u : r that satises evh = eug is the solution of the dierential equation gu = eu euq g + kg and the assumptions on g guarantees that u = is an approximate solution in a neighbourhood of the punctures. this metric g is dened as a smooth interpolation between the metric h of constant curvature and the at metric induced by the quadratic dierential. more precisely, we introduce a local coordinate zi in a neighbourhood ui of the puncture pi disjoint from the zeros of q and dene g = |q| for |zi| < ci ei|dzi| for ci |zi| ci h for |zi| > ci and on \\ ui wild ghm ads structures for smooth interpolating functions i. moreover, we can assume that there exists i > such that q g i on ui because q(zi) g when zi proposition . there exists a bounded smooth function u : r satisfying () gu = eu euq g + kg . proof. let f(x, u) = eu euq g + kg. since f is an increasing function of u, the solution to equation () is guaranteed () by the existence of two continuous functions u : r such that u+ f(u+, x), uf(u, x) and uu+ . let us start with the supersolution u+. let f : r be a positive smooth function such that f(zi) = |zi|i on the neighbourhood {|zi| < ci} of the puncture pi for some i > to be chosen later. for any r we consider u+ = f. we claim that it is possible to nd > large enough and i > suciently small so that u+ is a supersolution. in fact, on vi = {|zi| < ci} ui, we can nd a constant di > such that |q| di|z| and we have g(|zi|i) e|zi|i + e|zi|iq g kg i di|zi|i e|zi|i + e|zi|i \u0012 i di \u0013 u+ + (eu+ eu+ + u+) + eu+ , which can be made negative, because the term in the middle is always non positive and we can choose i small enough and large enough so that the sum of the rst and last term is negative. therefore, u+ is a supersolution on vi for every i > and > outside vi, we do not have control on the curvature of g and on the laplacian of f, but knowing that they are bounded, we can increase so that gf ef + efq g kg because ef grows the fastest when +. this proves that u+ is a supersolution everywhere on . as for the subsolution, let w : \\q() r be half of the logarithmic density of the at metric |q| with respect to g, that is ewg = |q|. we claim that w is a solution outside the zeros of q: in fact, gw ew + ewq g kg = (gw kg) ew + ewq |q| = because q|q| = and the rst term vanishes because the metric |q| is at outside the zeros of q. notice that w tends to at a zero of q and, by our denition wild ghm ads structures of the open sets ui and of the metric g, the background metric on has constant curvature in a small neighbourhood of the zeros of q. since any negative constant is a subsolution where the metric g has constant curvature , the function u= ( w on ui max(w, b) on \\ ui for a suciently large b > is a continuous subsolution, being it the maximum of two subsolutions. we remark that the resulting metric i = eug is complete because g is com- plete and u is bounded. moreover, the subsolution we found implies that i |q|. uniqueness follows then from a general result about vortex equations: proposition ( theorem ). for every non zero holomorphic quadratic dierential on there exists a unique complete solution to equation (). combining the above results we obtain: theorem . for any complete hyperbolic metric h on of nite area and for any meromorphic quadratic dierential q on with poles of order at least at the punctures there exists a unique complete equivariant maximal embedding : ads with induced metric i conformal to h and second fundamental form ii = re(q). moreover, the principal curvatures are in (, ). proof. existence and uniqueness of such embedding follows from the above discus- sion. let be the positive principal curvature of the maximal surface. by denition of q, we have = det(b) = euq g at the punctures. therefore, is bounded and a classical fact about maximal surfaces in anti de sitter space () implies that [, ). . asymptotic estimates. in order to describe the geometry of the maximal surface, we will also need the following precise estimate for the solution v in a neigh- bourhood of a puncture. recall that such a neighbourhood is covered by a collection of half planes, in which the quadratic dierential pulls back to d by an abuse of notation, we will still indicate with v the function such that ev|d| equals the induced metric on ( ) in the -coordinates. proposition . let be a natural coordinate for q dened on a standard half plane in a neighbourhood of a puncture p. then v() = o e || p || ! as || +. proof. in a natural coordinate the function v satises the pde () v = ev ev wild ghm ads structures because |q()| = and the background metric is at. the subsolution and su- persolution in proposition also show that v is non negative and innitesimal. in particular, we can assume that v on every half plane and we have that ev ev v . the asymptotic estimate will then follow from [dw, lemma ] provided we show that the restriction of v to the boundary of a half plane is integrable. in order to show this, we prove that v is exponentially decaying. let be a point in the boundary of the half plane. if || is suciently large, this point is actually contained also in the precedent or the following standard half plane. in both cases, we can nd a constant c > depending only on the gluing map between the half planes (hence only on q) and a ball of radius r() = || c centered at , which is entirely contained in these two coordinate charts. using a coordinate in this ball br(), the function v satises the same equation () on this ball. therefore, the solution of the dirichlet problem ( h = h h|br() = is a supersolution and as such is greater than v. it is then well known that the solution of the above dirichlet problem is the function h() = i( ||) i( |r(||)) where i is the modied bessel function of the rst kind (). hence, () v() h() = o(|| e ||) as || +. description of the boundary at infinity the equivariant maximal embedding : ads comes with a representation : () :pso(, ) such that ( x) = () (x) x () . identifying pso(, ) with psl(, r) psl(, r), determines and is determined by a pair of representations l,r : () psl(, r). in order to understand them, we make use of the theory of harmonic maps between surfaces. infact, the maximality of ( ) implies that the gauss map g : h h is harmonic and (l, r)-equivariant. moreover, the bound on the principal curvatures ensures that, if we denote with l and r the two projections onto the left and right factors, the maps (g l) and (g r) are harmonic dieomorphisms into their image (, ). then, the hyperbolic metrics (g l)gh and (g r)gh descend to hyperbolic metrics hl and hr on with holonomy l and r, respectively. now, since the hopf dierentials of these harmonic maps are iq, where re(q) wild ghm ads structures is the second fundamental form of the maximal embedding (, ), a recent result by gupta () implies that (, hl) and (, hr) are crowned hyperbolic surfaces, thus showing that the representations l and r are discrete and faithful, with hyperbolic peripheral elements. since the maximal surface is complete, its boundary at innity is a locally achronal curve and determines a domain of dependence d() ads by considering points whose dual space like plane is disjoint from , on which the representation = (l, r) acts properly discontinuously. our knowledge on the representation allows us to describe the curve at least partially. recall that we can identify the boundary at innity of ads with rp rp and r,l act on each factor by projective transfor- mations. given an element (), let x () denote the attracting and repelling xed points of (). these dene four points on the boundary at innity of ads: x++(()) = (x+ l (), x+ r ()) x+(()) = (x+ l (), x r ()) x(()) = (x l (), x r ()) x+(()) = (x l (), x+ r ()) it follows immediately from the denition that lim n+()n x = x++(()) for every x ads \\ {x+(()), x+(()), x(())}. therefore, the limit set = {x++(()) ads | ()} is the smallest closed (())-invariant subset in the boundary at innity of anti- de sitter space. since the boundary of the maximal surface is (())-invariant, it must contain . because l and r are holonomies of hyperbolic metrics on with geodesic boundary, the limit set is a cantor set and we need to describe the remaining part of the boundary at innity of the maximal surface. this will be studied in the next subsections by comparing, in a neighbourhood of the punctures, the maximal embedding and a particular maximal surface, called the horospherical surface (). . the frame eld of a maximal embedding. let us consider r c and extend the r bilinear form of signature (, ) to the hermitian product on c given by z, w= z w + z w z w z w . given a maximal conformal embedding : h ads, with a slight abuse of notation, we still denote with : h [ ads c one of its lifts. let n be the unit normal vector eld such that { w, w, n, } is an oriented frame in c we dene q = nw, w. the embedding being maximal implies that q is a holomorphic quadratic dierential on h since the embedding is conformal, we can dene a function : h r such that w, w= w, w= e . wild ghm ads structures these are related to the embedding data of as follows: the induced metric on (h) is i = e|dw| and the second fundamental form is ii = re(q). the vectors v = w e v = w e n, and give a unitary frame of (c, , ) at every point w h taking the derivatives of the fundamental relations n, n= , = vj, n= vj, = nz, w= q vj, vj= one deduces that n w = e qv v = wv + e and v = wv + qen . therefore, the pull back of the levi civita connection of (c, , , ) via can be written in the frame {v, v, n, } as () = v d w+udw = w e q w e e q e d w+ w e w qe qe e dw . notice that the atness of is equivalent to being a solution of the pde = e e|q| which coincides with equation () when the background metric is at. viceversa, if a holomorphic quadratic dierential q and a solution of the above equation are given, the -form v d w + udw can be integrated to a map f : h sl(, c), which is the frame eld of a maximal embedding into ads with induced metric i = e|dw| and second fundamental form ii = re(q). moreover, this is unique once the initial conditions are xed. . the horospherical surface. the frame eld can be written explicitly in the special case when q is a constant holomorphic quadratic dierential, and the associ- ated maximal surface in ads appears in the literature as the horospherical surface (, , ). see also and . suppose q = d is a holomorphic quadratic dierential dened on the complex plane c. the corresponding solution to the atness equation is then clearly = the -form becomes vd + ud = d + d . wild ghm ads structures the frame eld of the horospherical surface is thus f() = a exp(u + v ) , for some constant matrix a sl(, c). for our convenience, we choose a = i i . a simple computation shows that the matrix u + v is diagonalisable by a con- stant unitary matrix r so that r(u + v )r = diag(re(), im(), re(), im()) . therefore, we can write f() = ardiag(ere(), eim(), ere(), eim())r . the resulting maximal embedding is given by the last column of f(), that is = (sinh(re()), sinh(im()), cosh(re()), cosh(im())) . in particular, we can describe explicitly the boundary at innity of : it consists of four light like segments as the following table shows. direction projective limit of (tei + iy) ( , ) v = = vy = for some s(y) r+ ( , ) v = = vy = for some s(y) r+ ( , ) v = = vy = for some s(y) r+ ( , ) v = = vy = for some s(y) r+ table limits of the standard horospherical surface along rays . comparison with the horospherical surface. we saw in section that in a neighbourhood of a pole p of order n we can nd n standard half planes (uk, k) in which the quadratic dierential q pulls back to d k. moreover, the estimates in proposition , show that very close to the puncture the induced metric on the maximal surface is approximated by the at metric |dk| this suggests that the equivariant maximal embedding restricted to each half plane should behave asymptotically as the horospherical surface. in order to make this idea precise, we adapt to this lorentzian context the techiniques developed in . wild ghm ads structures let (u, ) be a standard half plane. in this discussion we remove the dependence on the index k with the understanding that the argument should be applied to each half plane. let f : u sl(, c) be the frame eld of the maximal surface found in theorem restricted to u. we dene the osculating map g : u so(, ) by g() = f()f () where f : u sl(, c) denotes the frame eld of the horospherical surface. notice that the map actually takes value in so(, ) because both frames f() and f() lie in the same right coset of so(, ) within sl(, c). evidently, g is constant if and only if is itself a horospherical surface. a computation using the structure equation for a maximal surface shows that gdg = ff , where () = v ev v ev ev ev d + v ev v ev ev ev d and i = ev|d| is the induced metric on the maximal surface in the -coordinate. notice that the estimates in proposition show that () is rapidly decaying to as || increases. ignoring the conjugation by the matrix f(), this suggests that g() should converge to a constant as goes to innity, which would mean that the maximal surface ( ) is asymptotic to a horospherical surface. however, the frame eld f is itself exponentially growing, with a precise rate depending on the direction. thus the actual asymptotic behaviour of g depends on the comparison between the growth of the error () and the frame eld f(). in most directions, the exponential decay of () is faster than the growth of f(), giving a well dened limiting horospherical surface. in exactly directions there is an exact balance, which allow the horospherical surface to shift. we start by pointing out the stable directions: denition . we say that a ray (t) = eit + iy is stable if / {/, /}. notice that the possible directions of stable rays in a standard half plane form three open intervals j+ = (, /) j = (/, /) and j= (/, ). wild ghm ads structures lemma . if is a stable ray, then limt+g((t)) exists. furthermore, among all such rays only three limits are achieved: there exist l, l so(, ) such that lim t+g((t)) = l+ if j+ l if j l if j proof. let (t) = eit + iy be a ray in a stable direction . for brevity we denote g(t) = g((t)). we know that g(t)g(t) = f((t))(t)( (t))f ((t)) . since f(w) = ardiag(ere(w), e(w), ere(w), e(w))r, for constant matrices r and a, the asymptotic behaviour of f((t))(t)( (t))f ((t)) depends only on the action by conjugation by the diagonal matrix d(t) = diag(ere((t)), e((t)), ere((t)), e((t))) . a direct computation shows that rr is equal to ei (ev + ev ) v + ( i)(ev ev) v + ( + i)(ev ev) v ( i)(ev ev) i(ev + ev ) v + ( + i)(ev ev) v ( + i)(ev ev) (ev + ev ) v ( i)(ev ev) v ( + i)(ev ev) v + ( i)(ev ev) i(ev + ev ) dt+ ei (ev + ev ) v ( + i)(ev ev) v ( i)(ev ev) v + ( + i)(ev ev) i(ev + ev ) v ( i)(ev ev) v + ( i)(ev ev) (ev + ev ) v + ( + i)(ev ev) v + ( i)(ev ev) v ( + i)(ev ev) i(ev + ev ) dt and conjugating by d(t) multiplies the (i, j)-entry by ij = exp \u0012 t \u0012 cos \u0012 + (i ) \u0013 + cos \u0012 + (j ) \u0013\u0013\u0013 = o \u0010 ec()t\u0011 , where c() achieves its maximum at = / (here we have considered only the pairs (i, j) so that ij multiplies a non zero entry). combining the bounds for rr and ij, we nd that for every stable ray, g(t)g(t) = o \u0012et t \u0013 where = c() > it is then standard to show that the limit limt+g(t) exists (). now suppose that and are stable rays with respective angles and that belong to the same interval. for any t , let t(s) = ( s)(t) + s(t) be the constant speed parameterisation of the segment from (t) to (t). let gt(s) = g(t())g(t(s)), which satises g t (s)g t(s) = f(t(s))t(s)( t(s))f (t(s)) gt() = id gt() = g(t)g(t) , wild ghm ads structures where gi(t) = g(i(t)) for i = , since | t(s)| = o(t), the analysis above shows that g t (s)g t(s) = o \u0010 tet\u0011 , where = sup{c() | }. in particular, by making t large we can arrange g t (s)g t(s) to be uniformly small for all s . ode methods ([dw, lemma b.]) ensure that gt() = g (t)g(t) id as t +. this shows that g has the same limit along and next we analyse the behaviour across unstable rays in order to understand the relationship between l and l lemma . let l and l be as in the previous lemma. then there exist unipotent matrices u such that l + l = aru+ra and l l= arura . proof. we give the detailed proof for l + l, for the other case we only underline the dierences at the end. consider the rays +(t) = ei/t and (t) = it. by the previous lemma g+(t) = g(+(t)) and g(t) = g((t)) have limit l+ and l, respectively. for any t > , we join +(t) and (t) by an arc t(s) = eist , where s . let gt(s) = g(t(/))g(t(s)). then gt : so(, ) satises the dierential equation () g t (s)g t(s) = f(t(s))t(s)( t(s))f (t(s)) gt(/) = id gt() = g+(t)g(t) . unlike the previous case, the coecient mt(s) = d(t(s))rt(s)( t(s))rd(t(s)) is not exponentially small in t throughout the interval. at s = /, conjugation by d(t(/)) multiplies the (, )-entry and the (, )-entry by a factor exp( t), exactly matching the decay rate of rr and giving mt(/) = o \u0012| t(/)| t \u0013 = o( t) because | t(/)| = t. however, this growth is seen only in the (, )-entry and in the (, )-entry because all the others are scaled by a smaller exponential factor. moreover, for we have = = exp(t(cos + sin )) exp \u0012 t \u0010 \u0011 t \u0013 , wild ghm ads structures thus we can separate the unbounded entry in mt(s) and write mt(s) = m t (s) + t(s)(e + e) where m t (s) = o(et) for some > , e and e are the elementary matrices, and t(s) = o \u0010 | t(s)| exp( t) \u0011 = o \u0010 te(/)t\u0011 . this upper bound is a gaussian function centered at = /, normalised such that its integral is independent of t. therefore, the function t(s) is uniformly absolutely integrable over s as t +. now, under this condition the solution to the initial value problem () satises () gt(/) ar exp (e + e) z t(s) ! ra as t +. since gt() = g(+(t))g((t)) l + l, this gives the desired unipotent form. the proof for l lfollows the same line with the only dierence given by the fact that at = /, the leading term in the matrix mt(s) lies in the (, )- and (, )-entry. . light like polygonal ends. we are now going to use the above estimates in order to describe the boundary at innity of the -equivariant maximal surface ( ). we already know that contains the limit set of the representation = (l, r) which is a cantor set consisting of the pairs of attracting xed points of the holonomy. on the other hand, must be a locally achronal topological circle. by equivariance, what remains to be understood is how the points x++(()) and x(()) are connected, for every peripheral element (). we will prove the following. theorem . let p be a puncture and suppose that the quadratic dierential q has a pole of order n at p. let () be a peripheral element around p. then the points x++(()) and x(()) are connected by a ()-equivariant achronal light like polygonal end with (n ) fundamental vertices and accumulation points x++(()) and x(()). let us rst explain the terminology. a light like polygonal end lp in the boundary at innity of ads is a concatenation of light like segments {i}iz. this can be constructed by alternating segments belonging to the right- and left- foliation of ads we say that the polygonal end is ()-equivariant if there exists an integer k > such that ()i = i+k for every i z. in this case, we can reconstruct the polygonal end by knowing a nite number of vertices, that we called fundamental, as they can be obtained by considering the vertices contained in a fundamental domain of the action of () in ads the accumulation points are the limits accum(lp) = lim i+i lim ii . clearly, if lp is equivariant with respect to the action of (), the accumulation points always coincide with the attracting and repelling xed points of (). wild ghm ads structures the vertices of the polygonal end will arise as limits of the equivariant maximal embedding along lifts of paths on directed towards the puncture. the limit will depend on the homotopy class of the paths and we will pin down special represen- tatives in each homotopy class in order to be able to apply the estimates in section . we are going to consider paths starting from a xed base point in and converging to the puncture p which in a natural coordinate are rays. this allows us to talk about the direction in which is approaching the puncture. notice that paths in dierent homotopy classes can converge to the puncture along the same direction in the same half plane, as they may dier by a complete rotation along the puncture. a way to construct such paths is the following: let {(uk, k)}n k= be the collection of standard half planes that cyclically cover a neighbourhood of the puncture. a path k, converging to the puncture in the half plane uk with direction can be obtained by concatenating a path k : that connects the xed base point on and the boundary of uk with the ray (t) = eit + k(). morover, we can choose the paths k so that is contractible and k is obtained from by following the boundary of the halfplanes (see figure ). figure denition of the paths k,,m in this way, when m z rotations are completed around the puncture we get paths in dierent homotopy classes approaching the puncture in the same direction and in the same half plane uk as k,. we will denote such paths by k,,m with the convention that k,, = k,. each homotopy class of paths converging to the puncture has then a representative of the form k,,m for some (). proof of theorem . let us consider the family of paths k,,m converging to the puncture p as above. by lemma , the limit lm k, = limt+g(k,,m(t)) ex- ists as long as is a stable direction, and only depends on the interval j or j in which lies. we will thus denote the limiting matrix as lm k, where = , , wild ghm ads structures keeping track only on such interval. since the frame eld of the maximal embed- ding is f(k,,m(t)) = g(k,,m(t))f(k,,m(t)), the limiting point along the path can be expressed as m k, = lm k,v, where v is the point at innity of the standard horospherical surface along a ray with direction (see table ). we deduce that in each standard half plane uk, for a xed m z, we see three points at innity as the following table shows: direction projective limit of m k, of (, ) m k,+ = lm k,+ ( , ) m k, = lm k, ( , ) m k,= lm k, table limits along rays in a half plane a direct computation, using the formulas provided by lemma , shows that lm k, = lm k, and lm k, = lm k,+ . in particular, the three limit points that appear in each half plane are causally re- lated, being them the image under an element of so(, ) of causally related points, and the light like segment joining them is entirely contained in the boundary at in- nity. therefore, for m = in each standard hall plane uk, we see a \"vee\" in the boundary at innity of the maximal surface given by l k,( ) with s varying in r+. given two consecutive half planes uk and uk+ the two \"vees\" share an extreme vertex: in fact, by considering another standard half plane w that intersects uk and uk+ in a sector of angle /, the arguments in lemma show that the direction is stable, so the ending point of the \"vee\" in uk coincides with the rst point of the \"vee\" in uk+ notice that when k = n this procedure makes the index m of the path increase by one. the above discussion thus produces a collection of vertices {m i }(n) i= in the boundary at innity of the maximal surface that arise as limits along the paths k,,m with m (n) = m+ , such that two consecutive vertices are connected by light like segments that belong to the left- and right foliation alternately. moreover, since for every m z the path k,,m is homotopic to k,, m, where is the peripheral element that goes once around the puncture, we have that, if i = lim t (k,,(t)) then m i = lim t (k,,m(t)) = lim t ((k,, m)(t)) = ()m i wild ghm ads structures hence the polygonal end is ()-equivariant and there are (n ) fundamental vertices. parameterisation of wild anti de sitter structures from the results of the previous sections, we can construct a globally hyperbolic anti de sitter structure from the data of a complete hyperbolic metric h of nite area on and a meromorphic quadratic dierential q with poles of order at least at the punctures. namely, theorem provides a unique complete equivariant maximal embedding into ads whose boundary at innity is an achronal curve (h, q) that contains the limit set of the holonomy completed to a topological circle by inserting light like polygonal ends for each peripheral element. let (h, q) be the domain of dependence of this boundary curve. the holonomy representation acts properly dis- continuously on (h, q) (,) and the quotient is a globally hyperbolic maximal anti de sitter manifold m(h, q) dieomorphic to r. on the other hand, for a xed pair of discrete and faithful representations l,r : () psl(, r) with hyperbolic peripheral elements, the space of ghm anti de sitter structures gh() on r is quite large: if is the limit set of the action of = (l, r), then there is a one to one correspondence between elements of gh() and (())-equivariant completions of to an achronal topological circle (). the aim of this section is thus to characterise the image of the map : mq() gh() (h, q) m(h, q) associating to a point (h, q) mq() in the bundle over teichmller space of meromorphic quadratic dierentials with poles of order at least at the punctures the corresponding ghm anti de sitter structure. proposition . the map is injective. proof. suppose by contradiction that is not injective. then we can nd (h, q) = (h, q) mq() such that (h, q) = (h, q). by denition, this means that the equivariant maximal embeddings associated to (h, q) and (h, q) have the same holonomy representation and the same boundary at innity. on the other hand, the arguments of show that given an achronal curve ads the maximal surface bounding is unique. this gives a contradiction because the pair (h, q) is uniquely determined by the embedding data of the maximal surface. proposition . the map is continuous. proof. we endow gh() with the topology induced by the one to one correspon- dence between elements of gh() and pairs (, ), where = (l, r) is pair of discrete and faithful representations into psl(, r) with hyperbolic peripheral ele- ments and is an achronal completion of the limit set of to a topological circle. we thus consider on gh() the topology induced by the product of the usual topol- ogy in the space of representations and the hausdortopology for compact sets in ads wild ghm ads structures let (hn, qn) mq() be a sequence converging to (h, q) mq(). we need to prove that the holonomy representation of m(hn, qn) converges to the holonomy representation of m(h, q) and the boundary curves (hn, qn) converge to (h, q) in the hausdortopology. let vn and v be the solution to equation () associated to the data (hn, qn) and (h, q) respectively. on every compact set k , the superso- lution and subsolution found in proposition provide a uniform bound for hnvn. since hn is a convergent sequence, standard theory for elliptic pde gives a uniform w , bound for vn. thus vn subconverges to a weak solution of the equation vh = ev evq h + kh , in w , on every compact set. by elliptic regularity v is smooth and the convergence is actually smooth. we deduce that the embedding data of the unique maximal surface in m(hn, qn) converges smoothly on compact sets to the embedding data of the unique maximal surface in m(h, q). by lifting to the universal cover, this implies that the corresponding equivariant maximal embeddings n : ads are converging smoothly on compact sets (up to post composition by a global isometry) to : ads, and thus the boundary at innity (hn, qn) converges to (h, q) in the hausdortopology. the convergence of the holonomy follows from the general fact below, which was proved in . lemma . let : ads be a sequence of n equivariant space like embed- dings. if n converges to a space like embedding smoothly on compact sets, then n converges, up to subsequences, to a representation and is -equivariant. we dene the subset of wild ghm anti de sitter structures on r as the image of the map : ghwild() = (mq()) gh() . from section we know that the curves at innity (h, q) are always obtained by completing the limit set of the holonomy with light like polygonal ends, but at the moment we do not know if any representation = (l, r) is attained and if any light like polygonal end can be realised. let us denote with ghlp() the space of globally hyperbolic anti de sitter manifolds (up to dieomorphisms isotopic to the identity) with holonomy given by a pair of faithful and discrete representations with hyperbolic peripheral elements and boundary at innity given by a light like polygonal completion of the limit set of . hence ghwild() ghlp(). proposition . the map : mq() ghlp() is proper. proof. let (n, n) = (hn, qn) be a sequence of globally hyperbolic maximal anti- de sitter structures with holonomy n and boundary at innity n that lie in the image of the map . suppose that n and n converge to and in ghlp(). in particular, the boundaries at innity n of the maximal surfaces sn embedded in these manifolds converge to in the hausdortopology. the arguments of [tamb, section ] show that the sequence sn converges to a maximal surface s bounding smoothly on compact sets. in particular, the embedding data of sn converge to the wild ghm ads structures embedding data of s. we deduce that hn converges to h in t() and qn converges to a meromorphic quadratic dierential q. moreover, q has poles of order at least because the order of the poles determines the number of fundamental vertices in the light like polygonal completion of the boundary at innity of the maximal surface. therefore, the map is a homeomorphism onto its image, and we are going to prove that ghwild() = ghlp(), by showing that for every n, . . . , nn the subset ghlp(, n , . . . , nn ) ghlp() consisting of light like polygonal completions with ni fundamental vertices is a manifold of the same dimension as the subbundle mq(, n, . . . , nn) of meromorphic quadratic dierentials with poles of order exactly ni. . parameterisation of ghlp(, n, . . . , nn ). a curve on the bound- ary at innity of ads can be seen as a graph of a function f : rp rp in the following way. fix a totally geodesic plane p in ads its boundary at innity is a circle in ads any point ads lies in a unique line belonging to the right foliation and a unique line belonging to the left foliation of ads those two lines intersect the circle at innity of p in exactly one point that we denote by l() and r() respectively. we can thus associate to a curve the map f : rp rp dened by the property that f(l()) = r() for every . this procedure gives a well dened map, as soon as is an acausal curve (). however, if is a light like polygonal completion of the limit set of a represen- tation = (l, r) : () psl(, r) psl(, r), we can make this construction work and associate a unique function f : rp rp as follows. by denition, the projections l = l() and r = r() are the limit sets of the representa- tions l and r acting on p, which is isometric to the hyperbolic plane. we can set f : l r as the unique map such that f(l()) = r() f for every . notice that, in particular, f sends the attracting (resp. repelling) xed points of l to the attracting (resp. repelling) xed points of r. we then want to extend this function to the whole rp consider a connected component c of rp \\ l: this corresponds to a lift of an end of the hyperbolic surface l = h/l(()). let () be the associated peripheral element and let c be the lift of so that c is a connected component of rp \\ {c} where c are the end points of c. similarly c = f(c) are the ideal points of a lift c of a geodesic boundary of the hyperbolic surface r = h/r(()) and the oriented arc between cand c+ is a connected component of rp \\ r. the pairs of points (c+, c+) and (c, c) belong to the limit set and are connected in by a ()-equivariant light like polygonal end {i}iz. without loss of generality we can assume that the segments i with even indices belong to the left foliation and the segments i+ of odd indices lie in the right foliation. the projections l(i+) give a collection of arcs with end points wild ghm ads structures i = l(i) limiting to cwhen i and to c+ for i +. let us denote by i the projections r(i+). we can extend the function f to the connected component c in such a way that i are the points of discontinuity of f and the following relations hold f(int(l(i+))) = i f(i) = i for all i z. this determines the function f in all connected components r(())c by equiv- ariance, and repeating the same construction for all geodesic boundaries of l we obtain the desired function f whose graph is represented by the curve . notice that the function f is uniquely determined by the representations l and r, and by labelled collections of points {i}iz and { i}iz in rp for each geodesic bound- ary of the hyperbolic surfaces l and r. since this data can be obtained from the universal cover of two crowned hyperbolic surfaces, we get the following: theorem . the set ghlp(, n , . . . , nn ) is parameterised by (t(s, n , . . . , nn ) t(s, n , . . . , nn )) /zn where zn acts diagonally by relabelling the lifts of the boundary cusps in the universal cover of each crown. proof. this is a consequence of the above discussion together with the remark that a diagonal change of labelling produces the same function, hence the same curve on ads corollary . the map : mq(, n, . . . , nn) ghlp(, n , . . . , nn ) is a homeomorphism. proof. we already know that is a proper, injective and continous map. by the previous theorem, ghlp(, n , . . . , nn ) is a manifold of dimension ( + p i(ni + )) which equals the dimension of the subbundle mq(, n, . . . , nn), hence is a homeomorphism. references milton abramowitz and irene a. stegun. handbook of mathematical functions with formu- las, graphs, and mathematical tables, volume of national bureau of standards applied mathematics series. for sale by the superintendent of documents, u.s. government printing oce, washington, d.c., thierry barbot. causal properties of ads isometry groups. i. causal actions and limit sets. adv. theor. math. phys., ():, thierry barbot. causal properties of ads isometry groups. ii. btz multi black holes. adv. theor. math. phys., ():, thierry barbot, franois bguin, and abdelghani zeghib. constant mean curvature fo- liations of globally hyperbolic spacetimes locally modelled on ads geom. dedicata, :, thierry barbot, franois bguin, and abdelghani zeghib. prescribing gauss curvature of surfaces in -dimensional spacetimes: application to the minkowski problem in the minkowski space. ann. inst. fourier (grenoble), ():, wild ghm ads structures francesco bonsante, kirill krasnov, and jean marc schlenker. multi black holes and earthquakes on riemann surfaces with boundaries. int. math. res. not. imrn, (): , francesco bonsante and jean marc schlenker. maximal surfaces and the universal te- ichmller space. invent. math., ():, francesco bonsante, andrea seppi, and andrea tamburelli. on the volume of antide sitter maximal globally hyperbolic three manifolds. geom. funct. anal., (): , david dumas and michael wolf. polynomial cubic dierentials and convex polygons in the projective plane. geom. funct. anal., ():, robert geroch. domain of dependence. j. mathematical phys., :, subhojoy gupta. harmonic maps and wild teichmller spaces. subhojoy gupta. limits of harmonic maps and crowned hyperbolic surfaces. arxiv:, kirill krasnov and jean marc schlenker. minimal surfaces and particles in -manifolds. geom. dedicata, :, georey mess. lorentz spacetimes of constant curvature. geom. dedicata, :, xin nie. poles of cubic dierentials and ends of convex rp surfaces. arxiv:, andrea seppi. maximal surfaces in anti de sitter space, width of convex hulls and qua- siconformal extensions of quasisymmetric homeomorphisms. to appear in journal of the ems, kurt strebel. quadratic dierentials, volume of ergebnisse der mathematik und ihrer grenzgebiete () . springer verlag, berlin, andrea tamburelli. constant mean curvature foliation of domains of dependence in ads to appear in trans. of the ams, andrea tamburelli. entropy degeneration of globally hyperbolic maximal compact anti- de sitter structures. arxiv:, andrea tamburelli. polynomial quadratic dierentials on the complex plane and light like polygons in the einstein universe. arxiv:, andrea tamburelli. degeneration of globally hyperbolic maximal anti de sitter structures along pinching sequences. arxiv:, andrea tamburelli. regular globally hyperbolic maximal anti de sitter structures. arxiv:, tom yau heng wan. constant mean curvature surface, harmonic maps, and universal teichmller space. j. dierential geom., ():, department of mathematics, rice university e mail address: andrea_tamburelli@libero.it ",
        "Subsections": [],
        "Groundtruth": "The text discusses various topics related to globally hyperbolic maximal anti-de Sitter manifolds, including their structure, geometry, parameterization, and the relationship to other mathematical concepts such as harmonic maps and crowned hyperbolic surfaces. Additionally, it emphasizes the significance of understanding the holonomy representations, boundary at infinity, light-like polygonal ends, and the construction of these manifolds from given data such as complete hyperbolic metrics and meromorphic quadratic differentials. The text also presents theorems and propositions to explain the parameterization of these structures and the topology of the associated spaces, ultimately aiming to provide a detailed mathematical analysis of these complex geometric structures."
    },
    {
        "Section_Num": "NA",
        "Section": "NA",
        "Text": "wild globally hyperbolic maximal anti de sitter structures andrea tamburelli abstract. let be a connected, oriented surface with punctures and negative euler characteristic. we introduce wild globally hyperbolic anti de sitter struc- tures on r and provide two parameterisations of their deformation space: as a quotient of the product of two copies of the teichmller space of crowned hyper- bolic surfaces and as the bundle over the teichmller space of of meromorphic quadratic dierentials with poles of order at least at the punctures. contents introduction ",
        "Subsections": [],
        "Groundtruth": "The text discusses wild globally hyperbolic maximal anti de Sitter structures on a connected, oriented surface with punctures and negative Euler characteristic. It introduces two parameterisations of the deformation space for these structures: as a quotient of the product of two Teichmüller spaces of crowned hyperbolic surfaces, and as a bundle over the Teichmüller space of meromorphic quadratic differentials with poles of order at least at the punctures."
    },
    {
        "Section_Num": "I",
        "Section": "I Introduction",
        "Text": "deep learning is recognized to be a state of the art scheme in articial intelligence and machine learning and has recently triggered enormous research activities. deep neural networks (deep nets for short) is believed to be capable of discovering deep features of data which are important but are impossible to be found by shallow neural networks (shallow nets for short). it, however, simultaneously produces a series of challenges such as the efcient computation, algorithmic solvability, robustness, interpretability and so on. a direct consequence of these challenges is that users hesitate to utilize deep learning in learning tasks with high risk such as the clinical diagnosis and nancial investment, since it is not clear whether deep nets perform essentially better than the scheme in hand. thus, it is urgent and crucial to provide the theoretical guidance on when do deep nets perform better than shallow nets? generally speaking, there are three steps to study the above problem. the rst step is to correspond specic real world applications to some data features. for example, gures are assumed to be local similarity ; earthquake forecasting is related to rotation invariant features ; and computer vision requires the spareness of activated neurons on the receptive eld . the second step is to connect these data fea- tures with a priori information which can be mathematically reected by specic properties of functions. in particular, local similarity usually corresponds to piece wise smooth functions ; rotation invariance generally corresponds to radial functions and sparseness on the receptive eld frequently corresponds to sparseness in the spacial domain . the last step is to pursue the outperformance of deep nets in approximating or learning these application related z. c. guo is with school of mathematical sciences, zhejiang university, hangzhou, china. l. shi is with shanghai key laboratory for contemporary applied mathematics, school of mathematical sciences, fudan university, shanghai, china. s. b. lin is with department of mathematics, wenzhou university, wenzhou, china. z. c. guo and l. shi are the co rst author. the corresponding author is s. b. lin (email: sblin@gmail.com). functions. in fact, the outperformance of deep nets has been rigorously veried in approximating piece wise smooth func- tions , rotation invariant functions and sparse functions , which coincides with the empirical evidences on image classication , earthquake prediction and computer vision . with the rapid development in deep nets approximation theory, there are numerous features that are proved to be realizable by deep nets , , , , , with much less neurons than shallow nets. different from these encouraging results, studies in learning theory showed that, however, to realize these features, capacities of deep nets are much larger than those of shallow nets with comparable number of free parameters. in particular, under some specied capacity measurements such as the number of linear regions , betti numbers , number of monomials , it was proved that the capacity of deep nets increases exponentially with respect to the depth but polynomially with respect to the width. an extreme case is that there exist deep nets with two hidden layers whose capacity measured by the pseudo- dimension is innite , . the large capacity of deep nets inevitably makes the deep nets learner sensitive to noise and requires a large amount of computations to nd a good estimator. in a nutshell, previous studies on advantages of deep nets showed that deep nets are capable of realizing various application related data features, but it requires additional capacity costs. the rst purpose of our study is to gure out whether the large capacity of deep nets to realize data features is necessary. our study is based on two interesting observations from the literature , , , , , , . one is that the number of layers of deep nets to realize various data features is small, the order of which is at most the logarithm of the number of free parameters. the other is that the magnitude of free parameters is relatively small, which is at most a polynomial with respect to the number of free parameters. with these two ndings, we adopt the well known covering number , to measure the capacity of deep nets with controllable number of layers and magnitude of weights and present a rened estimate of the covering number of deep nets. in particular, we prove that the covering number of deep nets with controllable depth and magnitude of weights is similar as that of shallow nets with comparable free parameters. this nding together with existing results in approximation theory shows that, to realize various features such as sparseness, hierarchy, rotation invariance and manifold structures, deep nets improve the performance of shallow nets without bringing additional capacity costs. as is well known, advantages of deep nets in realizing some special features do not mean that deep nets are always better than shallow nets. our second purpose is to demon- strate the necessity of deepening networks in realizing some simple data features. after building a close relation between approximation rates and covering number estimates, we prove that if only the smoothness feature is explored, then up to a logarithmic factor, approximation rates of shallow nets and deep nets with controllable depth and magnitude of weights are asymptotically identical. combining the above two statements, we indeed present rigorous theoretical verications to support that deep nets are necessary in a large number of applications corresponding to complex data features, in the sense that deep nets realize data features without any additional capacity costs, but not all. the rest of paper is organized as follows. in the next section, after reviewing some advantages of deep nets in approximation, we present a rened covering number estimate for deep nets. in section iii, we give a lower bound for deep nets approximation to show the limitation for deep nets in realizing simple features. in the last section, we draw a simple conclusion of this paper. ",
        "Subsections": [],
        "Groundtruth": "Deep learning, a leading scheme in artificial intelligence and machine learning, presents challenges such as efficient computation and algorithmic solvability. A crucial question is when deep neural networks (deep nets) perform better than shallow nets. Studies have shown that deep nets excel in approximating piece-wise smooth, rotation invariant, and sparse functions. However, the capacity of deep nets is significantly larger than that of shallow nets, leading to sensitivity to noise and increased computation. Research aims to determine if the large capacity of deep nets is necessary, finding that deep nets can improve performance without additional capacity costs. While deep nets excel in realizing complex data features, for simple features, both shallow and deep nets have similar approximation rates. Overall, deep nets are deemed necessary in many applications for complex data features."
    },
    {
        "Section_Num": "II",
        "Section": "II Advantages of Deep Nets in Realizing Feature",
        "Text": "in this section, we study advantages of deep nets in ap- proximating classes of functions with complex features. after introducing some mathematical concepts associated with deep nets, we review some important results in approximation theory which show that deep nets can realize some application- related features that cannot be approximated by shallow nets with comparable free parameters. then, we present a rened covering number estimate for deep nets to show that deepening networks in some special way does not enlarge the capacity of shallow nets. a. deep nets with xed structures great progress of deep learning is built on deepening neural networks with structures. deep nets with different structures have been proved to be universal, i.e., , for deep convolutional nets, for deep nets with tree structures and for deep fully connected neural networks. let i := and x = (x(), . . . , x(d)) id = d. let l n and d, d, . . . , dl n with d = d. assume k : r r, k = , . . . , l, be univariate nonlinear func- tions. for h = (h(), . . . , h(dk))t rdk, dene k( h) = (k(h()), . . . , k(h(dk)))t . deep nets with depth l and width dj in the j th hidden layer can be mathematically represented as h{d,...,dl,}(x) = a hl(x), () where hk(x) = k(wk hk(x) + bk), k = , , . . ., l, () h(x) = x, a rdl, bk rdk, and wk = (w i,j k )dk,dk i=,j= be a dk dk matrix. denote by h{d,...,dl,} the set of all these deep nets. when l = , the function dened by () is the classical shallow net. the structure of deep nets can be reected by structures of the weight matrices wk and parameter vectors bk and a, k = , , . . ., l. for examples, deep convolutional neural net- works corresponds to toeplitz type weight matrices and (a) deep fully connected nets (b) deep nets with tree structure fig. structures for deep nets deep nets with tree structures usually correspond extremely sparse weight matrices . throughout this paper, a deep net with specic structures refers to a deep nets with specic structures of all wk, bk, k = , . . . , l and a. figure shows two structures for deep nets. although deep fully connected neural networks possess better approximation ability than other networks, the number of free parameters of this type networks is al = dl + l x k= (dkdk + dk), () which is huge when the width and depth are large. a recent focus in deep nets approximation is to pursue the approx- imation ability of deep nets with xed structures. up till now, numerous theoretical results , , , showed that the approximation ability of deep fully connected neural networks can be maintained by deep nets with some special structures with much less free parameters. in this paper, we are interested in deep nets with structures. for k = , . . . , l, we assume that the structure of deep nets is xed and there are fk,w free parameters in wk, fk,b free thresholds in bk and fl,a free parameters in a. then, there are totally n := l x k= (fk,w + fk,b) + fl,a () free parameters in the deep nets. we assume further n al. throughout the paper, we say there are fk,w free parameters in wk, if the weight matrix wk is generated through the following three ways. the rst way is that the matrix has fk,w entries that can be determined freely, while the reminder dkdkfk,w entries are xed, e.g., the weight matrix in deep nets with tree structures. the second way is that the weight matrix wk is exactly generated by fk,w free parameters, e.g., the toeplitz type weight matrix in deep convolutional neural networks. the third way is that the weight matrix is generated jointly by both way above, that is, part of the weight matrix is xed, while the remaining part are totally generated by fk,w free parameters. denote by h{n,l,} the set of all these deep nets with l hidden layers, xed structure and n free parameters. denote further h{n,l,,r} = {hn,l, h{n,l,} : |wi,j k |, |bi k|, |ai| r, i dk, j dk, k l} () the set of deep nets whose weights and thresholds are uni- formly bounded by r, where r is some positive number which may depend on n, dk, k = , . . . , l and l. we aim at studying the approximation ability and capacity of h{n,l,,r}. it should be mentioned that the boundedness assumption in () is necessary. in fact, without such an assumption, , proved that for arbitrary > and arbitrary continuous function f, a deep net with two hidden layers and nitely many free parameters is fully able to generate an approximation hf, such that f hflp(id) . () this implies that the capacity of deep nets with two hidden layers and nitely many free parameters is comparable with that of lp(id), showing its extremely large capacity. therefore, to further control the capacity of deep nets, the boundedness assumption has been employed in large literature , , . b. a fast review for realizing data features by deep nets in approximation and learning theory, data features are usually formulated by a priori information for corresponding functions, like the target function for approximation, re- gression function for regression and bayes decision func- tion for classication. studying advantages of deep nets in approximating functions with different a priori information is a classical topic. it can date back to , when deduced the localized approximation property of deep nets which is far beyond the capability of shallow nets. the localized approximation of a neural network shows that if the target function is modied only on a small subset of the euclidean space, then only a few neurons, rather than the entire network, need to be retrained. we refer to [, def.] for a formal denition of localized approximation. since the localized approximation is an important step stone in approximating piecewise smooth functions and sparse functions in spacial domains , deep nets perform much better than shallow nets in related applications such as image processing and computer vision . the following proposition, which can be found in (see also ), shows the localized approximation property of deep nets. proposition suppose that : r r is a bounded measurable function with the sigmoidal property lim t(t) = , lim t+(t) = () then there exists a deep net with two hidden layers, d + neurons and activation function provides localized approx- imation. rotation invariance, is another popular data feature, which abounds in statistical physics , earthquake early warn- ing and image rendering . mathematically, rotation- invariant property corresponds to a radial function which is by denition a function whose value at each point depends only on the distance between that point and the origin. in the nice papers , , shallow nets were proved to be incapable of embodying rotation invariance features. to show the power of depth in approximating radial functions, we present the denition of smooth radial function as follows. denition let a r, c > and r = s+v with s n := {}n and < v we say a univariate function g : a r is (r, c)-lipschitz continuous if g is s times differentiable and its s th derivative satises the lipschitz condition |g(s)(t) g(s)(t)| c|t t|v, t, t a. () denote by lip(r,c) a the set of all (r, c)-lipschitz continuous functions dened on a. denote also by lip(,r,c) the set of radial functions f = g(x ) with g lip(r,c) . the following proposition, which can be found in , shows that deep nets can realize rotation invariance and smoothness features of target functions, simultaneously. proposition let d and p . if is the logistic function, i.e. (t) = +et , then for arbitrary f lip(,r,c), there is an h h{n,,,r} such that f hlp(id) cnr. () furthermore, for arbitrary h h{n,,,r}, there always exists a function f lip(,r,c) satisfying f hl(id) cnr/(d), () where c, c are constants independent of d, d, . . . , dl or n. numerous learning problems in computer vision, gene analysis and speech processing involve high dimensional data. these data are often governed by many fewer variables, producing manifold structure features in a high dimensional ambient space. a large number of theoretical studies , , have revealed that shallow nets are difcult to realize smooth and manifold structure features simultaneously. conversely, deep nets, as studied in , , is capable of reecting these features, which is shown by the following proposition (see also ). proposition let id be a smooth d dimensional compact manifold (without boundary) with d d. if is the relu activation function, i.e. (t) = max{t, }, and f is dened on and twice differentiable, then there exists a g hn,,,r such that f gl(id) cn d . () where c is a constant independent of d, d, . . . , dl or n. the previous studies showed that, compared with shallow nets, deep nets equipped with fewer parameters are enough to approximate functions with complex features to the same accuracy. in the following table i, we list some literature on studying the advantages of realizing data futures. c. covering number estimates in the above subsection, we have reviewed some results on the advantages of deep nets in realizing data features. however, it does not mean that deep nets are better than shallow nets, since we do not know what price is paid for references features l , localized approximation sigmoidal sparse+smooth sigmoidal smooth+manifold relu , hierarchical+smooth sigmoidal hierarchical piecewise smooth relu finite radial+smooth relu log() , sparse (frequency) analytic log() table i powers of deep nets in approximation (within accuracy ) such advantages in approximation. in this subsection, we use the covering number, which is widely used in learning theory , , , , , to measure the capacity of hn,l,,r and then unify the comparison within the same framework to show the outperformance of deep nets. let b be a banach space and v be a subset of b. denote by n(, v, b) the -covering number of v under the metric of b, which is the minimal number of elements in an -net of v . if b = l(id), we denote n(, v ) := n(, v, l(id)) for brevity. our purpose is a tight bound for covering numbers of hn,l,, r. to this end, we need the following assumption. assumption for arbitrary t r and every k {, . . ., l}, assume |k(t) k(t)| c|t t| () and |k(t)| c(|t| + ) () for some c, c to be detailed, () shows the lipchitz continuous prop- erty of k and () exhibits the linear increasing condition of k. these assumptions have been utilized in , , to quantify covering numbers of neural networks with different structures. we can see that almost all widely used activation functions such as the logistic function, hyperbolic tangent sigmoidal function (t) = (tanh(t) + ) with tanh(t) = (et )/(et + ), arctan sigmoidal function (t) = arctan(t) + , gompertz function (t) = eaebt with a, b > , relu (t) = max{t, }, and gaussian function (t) = et satisfy assumption with this assumption, we present our rst main result in the following theorem, whose proof can be found in appendix a. theorem let hn,l,,r be dened by (). under assump- tion , there holds n \u0000, hn,l,,r} \u0001 (crdmax)(l+)n n, where dmax := maxl dand c is a constant depending only on c, c and d. for satisfying assumption , it was deduced in , that log n(, hn,,,r) = o \u0012 n log cr \u0013 , () where c is a constant independent of or n. from theorem , we can derive log n(, hn,l,,r) = o \u0012 ln log cr \u0013 () for some c independent of , l, d, d, . . . , dl or n. compar- ing () with (), we nd that, up to a logarithmic factor, deep nets do not essentially enlarge the capacity of shallow nets, provided that they have same number of free parameters and the depth of deep nets is at most log n. noting that the depths of deep nets in table i all satisfy this constraint, theorem shows that to realize various data features presented in table i, deep nets can improve the performance of shallow nets without imposing additional capacity costs. therefore, theorem together with table i yields the reason why deep nets perform much better than shallow nets in some complex learning tasks such as image processing and computer vision. recently, presented a tight vc dimension bounds for piecewise linear neural networks. in particular, they proved that v cdim(sgn(h{d,...,dl,})) = o(ln log n), () where v cdim(v ) denotes the vc dimension of the set v and sgn(v ) := {x sgn(f(x)) : f v }, where sgn(f(x)) = if f(x) and sgn(f(x)) = otherwise. using the standard approach in , we can derive log n(, hn,l,,r) = o \u0012 ln log cr \u0013 () provided that = = = l are piecewise linear, where c is a constant independent of , l, d, . . . , dl or n. comparing (), there is an additional l in our analysis. the reason is that we focus on all activation functions satisfying () rather than piecewise activation functions. it should be also mentioned that similar covering number estimates for deep nets with tree structures has been studied in , , . we highlight that different structures yield essentially non trivial approaches. in fact, due to tree structures, the approach in , , is just to decouple layers by using the boundedness and liptchiz property of activation functions. however, in estimating covering number of deep nets with arbitrarily xed structure, we need a novel matrix- vector transformation technique, as presented in appendix a. ",
        "Subsections": [],
        "Groundtruth": "Deep neural networks offer advantages in approximating functions with complex features that shallow networks cannot achieve with comparable parameters. Deep nets with fixed structures, such as convolutional nets and tree structures, demonstrate universality in function approximation. Specialized deep nets maintain approximation ability with fewer parameters compared to fully connected networks. Deep nets excel in realizing data features like localized approximation, rotation invariance, smoothness, and manifold structures. The covering number estimates show that deep nets do not significantly increase capacity compared to shallow nets, given the same number of parameters and limited depth. This explains why deep nets outperform shallow nets in tasks like image processing and computer vision."
    },
    {
        "Section_Num": "III",
        "Section": "III Necessity of the Depth",
        "Text": "previous studies showed that, to realize some complex data features, deep nets can improve the performance of shallow nets without additional capacity costs. in this section, we study in a different direction to prove that, to realize some simple data features, deep nets are not essentially better than shallow nets. a. limitations of deep nets approximation smoothness or regularity is a widely used feature that has been adopted in a vast literature , , , , , , . to present the approximation result, we at rst introduce the following denition. denition let c > and r = s+v with s n := {}n and < v we say a function f : id r is (r, c)- smooth if f is s times differentiable and for every j n, j = , . . . , d with + +d = s, its s th partial derivative satises the lipschitz condition sf x . . . xd d (x) sf x . . . xd d (x) cx xv, () where x, x id and xdenotes the euclidean norm of x rd. denote by lip(r,c) the set of all (r, c)-smooth functions dened on id. approximating smooth functions is a classical topic in neural networks approximation. it is well known that the approximation rate can be as fast as o(nr/d) for neural networks with n free parameters. in particular, the jackson- type error estimate dist(lip(r,c), h{n,,,r, lp(id)) c nr d () has been established for shallow nets with analytic activation functions, where dist(u, v, lp(id)) := sup fu dist(f, v, lp(id)) := sup fu inf gv f glp(id) denotes the deviations of u from v in lp(id) for u, v lp(id). similar results has been derived in with deep nets with two hidden layers and a sigmoidal activation function. recently, derived an error estimate taking the form of dist(lip(r,c), hn,l,,r, lp(id)) c nr d log n () for deep nets with l = log n and relu activation functions. we would like to point out that, for shallow nets with relu activation functions, estimates () holds only for < r , which is also considered as the approximation bottleneck of shallow nets. the paper showed that deepening the networks can overcome this bottleneck for shallow nets. however, it should be mentioned from () that for other activation functions except the relu activation functions, such a bottleneck does not exist. thus, the paper indeed conduct a nice analysis on the necessity of deepening relu nets. however, their established results can not illustrate the necessity of depth. in the following theorem that will be proved in appendix c, we show that deep nets cannot be essentially better than shallow nets in realizing the smoothness feature. theorem let p , l n. then dist(lip(r,c), hn,l,,r, l(id)) cr d , () where c is a constant depending only on c, c, c, d and r. combining the estimates () and (), and noting fl(id) cd,pflp(id) with cd,p a constant depending only on d and p, we see that, when l is not too large, deep nets cannot essentially improve (a) approximation by shallow nets (b) approximation by dfcns fig. comparison between deep and shallow nets the approximation rate if one only considers the smoothness feature. when l is too large, it follows from theorem that we will need additional capacity cost for deep nets to improve the approximation ability of shallow nets. in other words, the smoothness feature is not sufcient to judge whether the depth of neural networks is necessary. b. remarks and discussions limitations of the approximation capabilities of shallow nets were rstly studied in in terms of providing lower bounds of approximation of smooth functions in the minimax sense. recently, highlighted that there exists a probabilistic measure, under which, all smooth functions cannot be ap- proximated by shallow nets very well with high condence. in another two interesting papers , , limitations of shallow nets were presented in terms of establishing lower bound of approximating functions with some variation restrictions. however, due to these results, it is still not clear whether the depth of neural networks is necessary, if only the smoothness information is given. theorem goes further along this direction and presents a negative answer. in theorem , to realize smoothness features, deep nets perform almost the same as shallow nets. this result veries the common consensus that deep learning outperforms shallow learning in some difcult learning tasks , but not always. moreover, our result also implies that whether deep nets can help to improve the performance of the existing learning schemes depends on what features for data we are exploring. combing our work with , , , , , , , , we can illustrate the comparison between shallow and deep nets in figure we declare that theorem only presents limitations of deep nets in realizing smooth features. as shown in figure , if more features are explored, we believe that the approximation rate of deep nets can break through the lower bound presented in (). ",
        "Subsections": [],
        "Groundtruth": "Previous studies have shown that deep neural networks can enhance performance for complex data features compared to shallow networks without incurring extra capacity costs. This section discusses how, contrary to this belief, deep networks may not necessarily outperform shallow networks for simple data features. The limitations of deep networks in approximating smoothness or regularity are explored, with specific focus on the bottleneck faced by shallow networks with relu activation functions. The analysis reveals that deep networks may not significantly improve upon shallow networks in achieving smoothness features, thus questioning the necessity of depth in neural networks. Further discussions highlight that the depth of neural networks may not always be essential for improved performance, and the comparison between shallow and deep networks is dependent on the data features being explored."
    },
    {
        "Section_Num": "IV",
        "Section": "IV Conclusion",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "References",
        "Section": "References",
        "Text": "eatures l , localized approximation sigmoidal sparse+smooth sigmoidal smooth+manifold relu , hierarchical+smooth sigmoidal hierarchical piecewise smooth relu finite radial+smooth relu log() , sparse (frequency) analytic log() table i powers of deep nets in approximation (within accuracy ) such advantages in approximation. in this subsection, we use the covering number, which is widely used in learning theory , , , , , to measure the capacity of hn,l,,r and then unify the comparison within the same framework to show the outperformance of deep nets. let b be a banach space and v be a subset of b. denote by n(, v, b) the -covering number of v under the metric of b, which is the minimal number of elements in an -net of v . if b = l(id), we denote n(, v ) := n(, v, l(id)) for brevity. our purpose is a tight bound for covering numbers of hn,l,, r. to this end, we need the following assumption. assumption for arbitrary t r and every k {, . . ., l}, assume |k(t) k(t)| c|t t| () and |k(t)| c(|t| + ) () for some c, c to be detailed, () shows the lipchitz continuous prop- erty of k and () exhibits the linear increasing condition of k. these assumptions have been utilized in , , to quantify covering numbers of neural networks with different structures. we can see that almost all widely used activation functions such as the logistic function, hyperbolic tangent sigmoidal function (t) = (tanh(t) + ) with tanh(t) = (et )/(et + ), arctan sigmoidal function (t) = arctan(t) + , gompertz function (t) = eaebt with a, b > , relu (t) = max{t, }, and gaussian function (t) = et satisfy assumption with this assumption, we present our rst main result in the following theorem, whose proof can be found in appendix a. theorem let hn,l,,r be dened by (). under assump- tion , there holds n \u0000, hn,l,,r} \u0001 (crdmax)(l+)n n, where dmax := maxl dand c is a constant depending only on c, c and d. for satisfying assumption , it was deduced in , that log n(, hn,,,r) = o \u0012 n log cr \u0013 , () where c is a constant independent of or n. from theorem , we can derive log n(, hn,l,,r) = o \u0012 ln log cr \u0013 () for some c independent of , l, d, d, . . . , dl or n. compar- ing () with (), we nd that, up to a logarithmic factor, deep nets do not essentially enlarge the capacity of shallow nets, provided that they have same number of free parameters and the depth of deep nets is at most log n. noting that the depths of deep nets in table i all satisfy this constraint, theorem shows that to realize various data features presented in table i, deep nets can improve the performance of shallow nets without imposing additional capacity costs. therefore, theorem together with table i yields the reason why deep nets perform much better than shallow nets in some complex learning tasks such as image processing and computer vision. recently, presented a tight vc dimension bounds for piecewise linear neural networks. in particular, they proved that v cdim(sgn(h{d,...,dl,})) = o(ln log n), () where v cdim(v ) denotes the vc dimension of the set v and sgn(v ) := {x sgn(f(x)) : f v }, where sgn(f(x)) = if f(x) and sgn(f(x)) = otherwise. using the standard approach in , we can derive log n(, hn,l,,r) = o \u0012 ln log cr \u0013 () provided that = = = l are piecewise linear, where c is a constant independent of , l, d, . . . , dl or n. comparing (), there is an additional l in our analysis. the reason is that we focus on all activation functions satisfying () rather than piecewise activation functions. it should be also mentioned that similar covering number estimates for deep nets with tree structures has been studied in , , . we highlight that different structures yield essentially non trivial approaches. in fact, due to tree structures, the approach in , , is just to decouple layers by using the boundedness and liptchiz property of activation functions. however, in estimating covering number of deep nets with arbitrarily xed structure, we need a novel matrix- vector transformation technique, as presented in appendix a. iii. necessity of the depth previous studies showed that, to realize some complex data features, deep nets can improve the performance of shallow nets without additional capacity costs. in this section, we study in a different direction to prove that, to realize some simple data features, deep nets are not essentially better than shallow nets. a. limitations of deep nets approximation smoothness or regularity is a widely used feature that has been adopted in a vast literature , , , , , , . to present the approximation result, we at rst introduce the following denition. denition let c > and r = s+v with s n := {}n and < v we say a function f : id r is (r, c)- smooth if f is s times differentiable and for every j n, j = , . . . , d with + +d = s, its s th partial derivative satises the lipschitz condition sf x . . . xd d (x) sf x . . . xd d (x) cx xv, () where x, x id and xdenotes the euclidean norm of x rd. denote by lip(r,c) the set of all (r, c)-smooth functions dened on id. approximating smooth functions is a classical topic in neural networks approximation. it is well known that the approximation rate can be as fast as o(nr/d) for neural networks with n free parameters. in particular, the jackson- type error estimate dist(lip(r,c), h{n,,,r, lp(id)) c nr d () has been established for shallow nets with analytic activation functions, where dist(u, v, lp(id)) := sup fu dist(f, v, lp(id)) := sup fu inf gv f glp(id) denotes the deviations of u from v in lp(id) for u, v lp(id). similar results has been derived in with deep nets with two hidden layers and a sigmoidal activation function. recently, derived an error estimate taking the form of dist(lip(r,c), hn,l,,r, lp(id)) c nr d log n () for deep nets with l = log n and relu activation functions. we would like to point out that, for shallow nets with relu activation functions, estimates () holds only for < r , which is also considered as the approximation bottleneck of shallow nets. the paper showed that deepening the networks can overcome this bottleneck for shallow nets. however, it should be mentioned from () that for other activation functions except the relu activation functions, such a bottleneck does not exist. thus, the paper indeed conduct a nice analysis on the necessity of deepening relu nets. however, their established results can not illustrate the necessity of depth. in the following theorem that will be proved in appendix c, we show that deep nets cannot be essentially better than shallow nets in realizing the smoothness feature. theorem let p , l n. then dist(lip(r,c), hn,l,,r, l(id)) cr d , () where c is a constant depending only on c, c, c, d and r. combining the estimates () and (), and noting fl(id) cd,pflp(id) with cd,p a constant depending only on d and p, we see that, when l is not too large, deep nets cannot essentially improve (a) approximation by shallow nets (b) approximation by dfcns fig. comparison between deep and shallow nets the approximation rate if one only considers the smoothness feature. when l is too large, it follows from theorem that we will need additional capacity cost for deep nets to improve the approximation ability of shallow nets. in other words, the smoothness feature is not sufcient to judge whether the depth of neural networks is necessary. b. remarks and discussions limitations of the approximation capabilities of shallow nets were rstly studied in in terms of providing lower bounds of approximation of smooth functions in the minimax sense. recently, highlighted that there exists a probabilistic measure, under which, all smooth functions cannot be ap- proximated by shallow nets very well with high condence. in another two interesting papers , , limitations of shallow nets were presented in terms of establishing lower bound of approximating functions with some variation restrictions. however, due to these results, it is still not clear whether the depth of neural networks is necessary, if only the smoothness information is given. theorem goes further along this direction and presents a negative answer. in theorem , to realize smoothness features, deep nets perform almost the same as shallow nets. this result veries the common consensus that deep learning outperforms shallow learning in some difcult learning tasks , but not always. moreover, our result also implies that whether deep nets can help to improve the performance of the existing learning schemes depends on what features for data we are exploring. combing our work with , , , , , , , , we can illustrate the comparison between shallow and deep nets in figure we declare that theorem only presents limitations of deep nets in realizing smooth features. as shown in figure , if more features are explored, we believe that the approximation rate of deep nets can break through the lower bound presented in (). iv. conclusion in this paper, we study the advantages and limitations of deep nets in realizing different data features. our results showed that, in realizing some complex data features such as the rotation invariance, manifold structure, hierarchical struc- ture, sparseness, deep nets can improve the performance of shallow nets without additional capacity costs. we also exhibit that for some simple data features like the smoothness, deep nets performs essentially similar as shallow nets. appendix a: proof of theorem for l, let w f,w be the set of d d matrices with xed structures and total f,w free parameters and b f,b be the set of f,b dimensional vectors with xed structures and total f,b free parameters. denote wf,w := {w w f,w : |w i,j| r} and bf,b := { b b f,b : |bi| r}. for x id, let h = {x} and dene iteratively h= { h(x) = (w h(x) + b) : () h h, wwf,w, b bf,b}, = , , . . ., l. for each h= (h , . . . , hd ) h, dene h,d:= maxidhi l(id). the following lemma devotes to the uniform bound of functional vectors in h. in our analysis, we always assume that the activation functions satisfy assumption with uniform constants c and c moreover, we also suppose that r and c lemma for each = , , . . . , l and h h, if satises (), then there holds h,d \u0000c( + d+)r \u0001d d () proof: for arbitrary = , . . . , l, it follows from () that h,d= (w h(x) + b) ,d = max id z id (w i h(x) + bi ) dx c max id z id d x j= w i,j hj (x) + bi + dx c max id d x j= |w i,j | z id |hj (x)|dx + |bi |d + cd (cd h,d + cd)r + cd, where w i denotes the i row of the matrix w, w i,j denotes the (i, j)-element of w, b= (b , . . . , bd ) and h h noting h,d = maxid r i |xi|dxi = , we then have h,d \u0000c( + d+)r \u0001d d this nishes the proof of lemma our second lemma aims at deriving covering number of some matrix and vector with xed free parameters. lemma for arbitrary > and l, we have n(, wf,w, ) \u0012ddr \u0013f,w , and n(, bf,b, m ) \u0012r \u0013f,b , where w := pd i= pd j= |w i,j | denotes the -norm of the matrix w. proof: for arbitrary dd matrix, we can rewrite it as a d d dimensional vector as {w, . . . , wdd}. with- out loss of generality, we assume that the rst f,w elements of the d d dimensional vector are free parameters. let efi be the -cover nets of {wi : |wi| r}, that is, for each |wi| r, there is a w i efi such that |wi w i| , i = , . . . , f,w. then, for arbitrary w, w wf,w with w, w the matri- ces corresponding to the vector (w, w, . . . , wf,w, . . . ) and (w , w , . . . , w f,w, . . . ) respectively, there holds w w = d x i= d x j= |w i,j w i,j| = f,w x i= |wi w i| + dd x i=f,w+ |wi w i|. if the reminder dd f,w are xed constants, we have pdd i=f,w+ |wi w i| = if the weight matrix is generated by the other two ways, which implies some elements in the remainder df,w terms sharing the same values as some elements in the rst f,w terms, then we have dd x i=f,w+ |wi w i| (dd f,w) max if,w |wi w i|. both cases yield w w dd hence f,w -covers for sets {wi : |wi| r} with i = , . . . , f,w constitute a dd cover for wf,w, which together with |efi| r , i = , , . . ., f,w implies n(, wf,w, ) \u0012ddr \u0013f,w , where |e| denotes the cardinality of the set e. this completes the rst estimate. the second estimates can be derived by using the similar approach. with these, we completes the proof of lemma based on the previous lemmas, we can derive the following iterative estimates for the covering number associated with the afne mapping (w h + b). lemma if satises assumption for each = , , . . ., l, then n(, h, ,d) (c r)fdf f n (c r) d , h, ,d ! , holds for = , . . . , l and n(, h, ,d) \u0012c rd \u0013f . where d= d d, f= f,w + f,b and c = cc( + d+). proof: for each = , , . . . , l, let e,w and e,b be - cover nets of wf,w and bf,b respectively. for = , . . . , l, let e,h be the -cover nets for h therefore, for each h h, wwf,w and b bf,b, there exist h e,h, w e,w, b e,b such that h h ,d , = , . . . , l () and ww , b b d , = , . . . , l () then, for arbitrary h hand = , , . . ., l, there holds h (w h + b ),d () (w h + b) (w h + b),d + (w h + b) (w h + b),d + (w h + b) (w h + b ),d. due to (), we get from lemma that (w h + b) (w h + b),d max id z id |(w i h(x) + bi ) (w i h(x) + bi )|dx c d x i= z id |(w i w i ) h(x)|dx c h,d d x i= d x j= |w ij w ij | c \u0000c( + d+)r \u0001 d dww for = , , . . . , l, we have from () that (w h + b) (w h + b),d max id z id |(w i h(x) + bi ) (w i h (x) + bi )|dx c max id z id |w i ( h(x) h (x))|dx c d x i= d x j= |w ij | h h ,d cddr h h ,d and (w h + b) (w h + b ),d = max id z id |(w i h (x) + bi ) (w i h (x) + b i )|dx c max id z id |bi b i |dx dc b b d . for = , . . . , l, plugging the above three estimates into (), we then get from () and () that h (w h + b ),d c(c( + d+)r)d. . . d this implies that { (w h + b ) : w e,w, h e,h, b e,b} is a c(c(+d+)r)d. . . d net of h. this together with lemma implies n \u0010 c(c( + d+)r)d. . . d, h, ,d \u0011 \u0012ddr \u0013f,w+f,b n \u0010 , h, ,d \u0011 . scaling to c(c(+d+)r)d...d , we then have n(, h, ,d) ((c r))fdf f n (c r) d , h, ,d ! , where c = cc( + d+) and d:= d. . . d this proves lemma for = , . . . , l. if = , then for arbitrary h h, we have h (w x + b ),d (wx + b) (w x + b),d + (w x + b) (w x + b ),d () the same approach as above yields that { (w x + b ) : w e,w, b e,b} is a cd+-net of h using lemma again, we obtain n(cd+, h, ,d) \u0012dr \u0013f,w+f,b . scaling to /cd+, we get n(, h, ,d) \u0012c rd \u0013f . this completes the proof of lemma with the help of the above two lemmas, we are in a position to prove theorem proof of theorem : let a fl,a be the set of dl- dimensional vectors with xed structures and totally fl,a free parameters. denote afl,a := { a a fl,a : |ai| r}. assume that el,a is an -cover of the set afl,a under the metric of dl . then, for arbitrary a rdl and hl hl there is a ael,a and h l el,h such that a a dl , and hl h l,dl . note that a hl a h ll(id) a hl a hll(id) + a hl a h ll(id). () moreover, lemma shows a hl a hll(id) a a dl hl,dl \u0000c( + d+)r \u0001l dl d a a dl \u0000c( + d+)r \u0001l dl d and a hl a h ll(id) dlr hl h l,dl dlr. plugging the above estimates into (), we have a hl a h ll(id) (c r)l dl, where c = c( + d+). since lemma implies n (c r)l dl , afl,a, dl ! (c r)(l+)fl,a dfl,a l fl,a, there holds n \u0000, hn,l,,r, l(id) \u0001 (c r)(l+)fl,a dfl,a l fl,a n (c r)l dl , hl, ,dl ! . () we then use lemma to estimate the second part of the above term. let b:= (max{c , c }r)d d+, = , , . . . , l , bl := (max{c , c }r)ld l, bl+ := (max{c , c }r)l+d l. () then, the rst estimate of lemma shows n(, h, ,d) bf f n \u0012 b , h, ,d \u0013 , = l, . . . , using the above inequality iteratively with = l, l, . . ., , we obtain n (c r)l dl , hl, ,dl ! l y = bf ! \u0010 pl = f \u0011 n ql = b , h, ,d ! bfl l (blbl)fl (bl b)f = l y = bf ! l y = b p j= fj ! pl = f n ql = b , h, ,d ! but the second estimate in lemma and the denition of b yield n ql = b , h, ,d ! b l y = b !f f then, n (c r)l dl , hl, ,dl ! l y = bf ! l y = b p j= fj ! pl = f () inserting the above estimate into (), we have n \u0000, hn,l,,r}, l(id) \u0001 bfl,a l+ l y = b f+p j= fj pl = ffl,a. it follows from () that max l+ bbl+dl. then, n \u0000, hn,l,,r}, l(id) \u0001 (bl+dl)fl,a+(l+) pl = fn (bl+dl)(l+)nn. this together with () yields n \u0000, hn,l,,r}, l(id) \u0001 \u0000(cr)l+d l \u0001(l+)n n, where c = max{c , c } this completes the proof of theorem by noting dl dl+ max. appendix b: covering numbers and approximation the main tool in our analysis is a relation between cov- ering numbers and lower bounds of approximation, which is presented in the following theorem. theorem let n n and v l(id). for arbitrary > , if n(, v ) c cn !n () with , c, c > , then dist(lip(r,c), v, l(id)) c(n log(n + ))r/d, () where c := dd/ h ( + + r/d) \u0010 log( c + dd/( + + r/d + c)) + \u0011ir d . we postpone the proof of theorem to the end of this section. theorem shows that to approximate functions in lip(r,c), the capacity of the approximations, measured by the covering number, plays a crucial role. to present the limitations of deep nets, theorem implies that we only need to estimate their covering numbers. we highlight that theorem is motivated by , in which a relation between the so called pseudo dimensions and lower bounds of approxi- mation is established. however, estimating pseudo dimensions of classes of functions is not so easy, even for shallow nets . to prove theorem , we need the following four technical lemmas. at rst, we introduce the denition of the -packing number (see , ) by m(, v, b) = max{m : f, . . . , fm b, fi fjb , i = j}. we also denote m(, v ) := m(, v, l(id)). the following lemma which was proved in establishes a relation between n(, v ) and m(, v ). lemma for arbitrary > and v l(id), there holds m(, v ) n(, v ) m(, v ). for arbitrary n n, denote e(n )d := { = (, . . . , (n )d) : i {, }, i (n )d}. the following lemma can be found in (see also[, claim ]). lemma for arbitrary n n, there exists a set g(n )d e(n )d with |g(n )d| (n )d/ such that for any v, v g(n )d with v = v, there holds vv (n )d/, where v = p(n )d i= |vi| for v = (v, . . . , v(n )d) and |g(n )d| denotes the cardinality of g(n )d. dene g : r r such that supp(g) [/ d, / d]d, g(x) = for x [/( d), /( d)]d and g lip(r,cv), where supp(g) denotes the support of g. par- tition id by (n )d sub cubes {ak}(n )d k= of side length /n and with centers {k}(n )d k= . for arbitrary x id, dene gk(x) := (n )rg(n (x k)) () and fg(n)d := (n )d x k= kgk(x) : = (, . . . , (n )d) g(n )d . () the following lemma shows that fg(n)d lip(r,c) lemma for arbitrary n n, we have fg(n)d lip(r,c), where g(n )d is dened in lemma proof: let = (, , d). denote by f ( )(x) = sf x . . . xd d (x) for every j n, j = , . . . , d with + +d = s. since n (xk)n (xk)= n kk, k = k, () n (x k) and n (x k) do not belong to the set (/ d, / d)d simultaneously. then it follows from supp(g) [/ d, / d]d that for arbitrary x id, there is at most one k {, , . . ., (n )d} such that gk(x) = , g( ) k (x) = , that is, gk(x) = , g( ) k (x) = , if x ak with k = k. () if x, x ak for some k {, . . ., (n )d}, then gk(x) = for k = k so, for each f fg(n)d , we get from |k| = , () and g lip(r,cv) with r = s + v and < v that |f ( )(x) f ( )(x)| = (n )d x k= k[g( ) k (x) g( ) k (x)] = |g( ) k (x) g( ) k (x)| = (n )r+s [g( )(n (x k) g( )(n (x k)] cvx xv cx xv. if x ak but x ak for some k, k {, . . ., (n )d} with k = k, we can choose z ak and z ak such that z, z are on the segment between x and x, where a denotes the boundary of the sub cube a. then x z+ x zx x. due to the fact that supp(g) [/ d, / d]d, g is smooth on rd and (), we get g( ) k (z) = g( ) k (z) = () so, g lip(r,cv) with < v and jensens inequality yield |f ( )(x) f ( )(x)| = (n )d x k= k[g( ) k (x) g( ) k (x)] g( ) k (x) + g( ) k (x) = g( ) k (x) g( ) k (z) + g( ) k (x) g( ) k (z) (n )dr h |g( )(n (x k)) g( )(n (z k))| + |g( )(n (x k)) g( )(n (z k))| i cv \u0014x zv + x zv \u0015 cv \u0014x z + x z \u0015v cx xv. both assertions yield f lip(r,c) and proves lemma the last lemma describes the geometry of fg(n)d . lemma let n n and g(n )d be dened in lemma for any f = f fg(n)d , there holds f fl(id) dd/(n )r. () proof: for arbitrary f, f fg(n)d with f = f, it follows from () that there exist , g(n )d with = such that f fl(id) = z id (n )d x k= (k k)gk(x) dx. () since gk(x) = for x ak, k = , , . . ., (n )d, we get from (), (), () and g lip(r,cv) that f fl(id) = (n )d x k= z ak (n )d x k= (k k)gk(x) dx = (n )d x k= z ak |(k k)gk(x)| dx = (n )r (n )d x k= |k k| z ak |g(n (x k))| dx.() for each k = , , . . ., (n )d, when x runs over ak, n (x k) runs over a cube s centered at k and with side length then, g(x) = for x [/( d), /( d)]d yields z ak |g(n (x k))| dx () = z ak |g(n (x k))| d(x k) (n )d z [/( d),/( d)]d |g(x)|dx ( dn )d. but lemma with , g(n )d shows (n )d x k= |k k| (n )d/ () hence, for arbitrary f, f fg(n)d, inserting () and () into (), we obtain ffl(id) (n )r( dn )d(n )d/ dd/(n )r. () this completes the proof of lemma by the help of the above four lemmas, we are in a position to prove theorem proof of theorem : for arbitrary > , denote = dist(fg(n)d , v, l(id)) + . () for any f fg(n)d, dene a function pf v such that f pfl(id) . () due to (), there are more than one pf satisfying (). dene tg(n)d := {pf : f fg(n)d } v . for arbitrary f, f fg(n)d with f = f, write f = pf and f = pf then f f l(id) = pf pfl(id) = pf f + f f + f pfl(id) f fl(id) pf fl(id) pf fl(id), which together with () and () shows f f l(id) dd/(n )r () we now claim > dd/(n )r for n satisfying (n )d = ( + + r/d)n log( c + dd/( + + r/d + c) + n) m ,() with adenoting the smallest integer not smaller than the positive number a. to prove the claim, suppose, to the con- trary, that dd/(n )r, () then () implies f f l(id) dd/(n )r. this shows that f = f implies f = f . so it follows from lemma that |tg(n)d | = |fg(n)d| = |g(n )d| (n )d/ fixing = dd/(n )r, we then have m(, v ) (n )d/ on the other hand, since tg(n)d v , it follows from () and lemma that m(, v ) n(/, v ) c cn !n = c \u0010 cndd/(n )r\u0011n . combining the above two inequalities, we get (n )d/ c \u0010 cndd/(n )r\u0011n . () this together with () shows ( + + r/d)n log( c + dd/( + + r/d + c) + n) < log( c) + n log( cdd/) () + n log n + rn d log(( + + r/d)) + rn d log n + rn d log log( c + dd/( + + r/d + c) + n). since the righthand of the above inequality is smaller than ( + + r/d)n log( c + dd/( + + r/d + c) + n), it leads to a contradiction. this proves the claim > dd/(n )r for n satisfying (). noting for arbitrary u , log(n+u) log u+log(n+) (log u+) log(n+), we have > dd/(n )r c(n log(n + ))r/d. but () with = / shows dist(fg(n)d , v, l(id)) = > c(n log(n + ))r/d. therefore, it follows from lemma that dist(lip(r,c), v, l(id)) dist(fg(n)d, v, l(id)) c(n log(n + ))r/d. this completes the proof of theorem appendix c: proof of theorem combining theorem and theorem , we can prove theorem as follows. proof of theorem : it sufces to prove theorem for p = , since dflp(id) fl(id) for arbitrary f lp(id) and p due to theorem , () in theorem is satised with v = hn,l,,r, c = , = and c = (crdmax)(l+)l. hence, it follows from theorem that dist(lip(r,c), hn,l,,r}, l(id)) c r d . where c = dd/ h ( + r/d) \u0010 log( + dd/( + r/d + (crdmax)(l+)l) + \u0011ir d . since + dd/( + r/d + (crdmax)(l+)l) (dd/crdmax)(l+)l, and log(dd/crdmax) (log(dd/c)+) log(rdmax), we have c c r/d where c := \u0002 ( + r/d)(log(dd/c + )) \u0003r d . therefore, dist(lip(r,c), hn,l,,r, l(id)) c r d r d cr d with c = r/d c this completes the proof of theorem acknowledgement the research was supported by the national natu- ral science foundation of china [grant nos. , , , ]. lei shi is also supported by the program of shanghai subject chief scientist (project no.xd). references h. adeli and a. panakkat. a probabilistic neural network for earthquake magnitude prediction. neural networks, vol. , pp. -, m. bianchini and f. scarselli. on the complexity of neural network classiers: a comparison between shallow and deep architectures, ieee. trans. neural netw. & learn. sys., vol. , pp. -, c. k. chui, x. li and h. n. mhaskar. neural networks for lozalized approximation. math. comput., vol. , pp. -, c. k. chui, x. li and h. n. mhaskar. limitations of the approximation capabilities of neural networks with one hidden layer. adv. comput. math., vol. , pp. -, c. k. chui, s. b. lin and d. x. zhou. construction of neural networks for realization of localized deep learning. front. appl. math. statis., vol. , no. , c. k. chui, s. b. lin and d. x. zhou, deep neural networks for rotation invariance approximation and learning. submitted to analysis and applications. o. delalleau and y. bengio. shallow vs. deep sum product networks, nips, -, i. goodfellow, y. bengio and a. courville. deep learning. mit press, l. gy orfy, m. kohler, a. krzyzak and h. walk. a distribution free theory of nonparametric regression. springer, berlin, b. hanin. universal function approximation by deep neural nets with bounded width and relu activations. arxiv preprint arxiv:, n. harvey, c. liaw and a. mehrabian. nearly tight vc dimension bounds for piecewise linear neural networks. conference on learning theory. : - g. e. hinton, s. osindero and y. w. teh. a fast learning algorithm for deep belief networks. neural comput., vol. , pp. -, v. e. ismailov. on the approximation by neural networks with bounded number of neurons in hidden layers. j. math. anal. appl., vol. , pp. -, m. kohler and a. krzyzak. nonparametric regression based on hierar- chical interaction models. ieee trans. inf. theory, vol. , pp. - , v. n. konovalov, d. leviatan and v. e. maiorov. approximation by polynomials and ridge functions of classes of s monotone radial functions. j. approx. theory, vol. , pp. -, v. n. konovalov, d. leviatan and v. e. maiorov. approximation of sobolev classes by polynomials and ridge functions. j. approx. theory, vol. , pp. -, v. k urkov a and m. sanguineti. estimates of covering numbers of convex sets with slowly decaying orthogonal subsets. discrete appl. math., vol. , pp. -, v. k urkov a and m. sanguineti. probabilistic lower bounds for approxi- mation by shallow perceptron networks. neural networks, vol. , pp. -, v. k urkov a. constructive lower bounds on model complexity of shallow perceptron networks. neural comput. appl., doi: /s-- - a. krizhevsky, i. sutskever and g. e. hinton. imagenet classication with deep convolutional neural networks. nips, -, y. lecun, y. bengio and g. hinton. deep learning. nature, vol. , no. , pp. -, h. w. lin, m. tegmark and d. rolnick. why does deep and cheap learning works so well? j. stat. phys., vol. , pp. -, s. b. lin, j.zeng and x. chang, learning rates for classication with gaussian kernels. neural comput., vol. , pp. -, s. b. lin. limitations of shallow nets approximation. neural networks, vol. , pp. -, s. b. lin. generalization and expressivity for deep nets. ieee trans. neural netw. learn. syst., in press. t. lin and h. zha. riemannian manifold learning. ieee trans. pattern anal. mach. intel., vol. , pp. -, g. g. lorentz, m. z. golitchek and y. makovoz. constructive approx- imation, advanced problems. springer verlag, new york, v. maiorov and a. pinkus. lower bounds for approximation by mlp neural networks. neurocomputing, vol. , pp. -, v. maiorov and j. ratsaby. on the degree of approximation by manifolds of nite pseudo dimension. constr. approx., vol. , pp. -, v. maiorov and r. meir. on the near optimality of the stochastic approximation of smooth functions by neural networks. adv. comput. math., vol. , pp. -, v. maiorov. pseudo dimension and entropy of manifolds formed by afne invariant dictionary. adv. comput. math., vol. , pp. -, y. makovoz. random approximants and neural networks. j. approx. theory, vol. , pp. -, b. mccane and l. szymanski. deep radial kernel networks: approxi- mating radially symmetric functions with deep networks. arxiv preprint arxiv:, l. meylan and s. susstrunk. high dynamic range image rendering with a retinex based adaptive lter. ieee trans. image proc., vol. , pp. -, h. mhaskar, q. liao and t. poggio. learning real and boolean functions: when is deep better than shallow. arxiv preprint arxiv:, h. n. mhaskar and t. poggio. deep vs. shallow networks: an approx- imation theory perspective, anal. appl., vol. , pp. -, g. mont ufar, r. pascanu, k. cho and y. bengio. on the number of linear regions of deep nerual networks. nips, , - p. petersen and f. voigtlaender. optimal aproximation of piecewise smooth functions using deep relu neural networks, neural networks, vol. , pp. -, t. poggio, h. mhaskar, l. rosasco, b. miranda and q. liao. why and when can deep but not shallow networks avoid the curse of dimension- ality: a review. intern. j. auto. comput., doi: /s-- -, d. rolnick and m. tegmark. the power of deeper networks for expressing natural functions. arxiv:v, i. safran and o. shamir. dept width tradeoffs in approximating natu- ral functions with neural networks. arxiv reprint arxiv:v, c. satriano, y. m. wu, a. zollo and h. kanamori. earthquake early warning: concepts, methods and physical grounds. soil dynamics earth. engineer., vol. , pp. -, l. shi, y. l. feng and d. x. zhou. concentration estimates for learning with l regularizer and data dependent hypothesis spaces. appl. comput. harmon. anal., vol. , pp. -, l. shi. learning theory estimates for coefcient based regularized regression. appl. comput. harmon. anal., vol. , pp. -, u. shaham, a. cloninger and r. r. coifman, provable approximation properties for deep neural networks. appl. comput. harmon. anal., to appear. k. vikraman. a deep neural network to identify foreshocks in real time, arxiv preprint arxiv:, z. wang, a. c. bovik, h. r. sheikh and e. p. simoncelli. image quality assessment: from error visibility to structural similarity. ieee trans. image process., vol. , pp. -, j. wright, y. ma, j. mairal, g. sapiro, t. s. huang and s. yan. sparse representation for computer vision and pattern recognition. proc. ieee, vol. , pp. -, d. yarotsky. error bounds for aproximations with deep relu networks. neural networks, vol. , pp. -, g. b. ye and d. x. zhou. learning and approximation by gaussians on riemannian manifolds. adv. comput. math., vol. , pp. -, d. x. zhou. the covering number in learning theory. j. complex., vol. , pp. -, d. x. zhou. capacity of reproducing kernel spaces in learning theory. ieee trans. inf. theory, vol. , pp. -, d. x. zhou. deep distributed convolutional neural networks: universal- ity. anal. appl., in press, d. x. zhou. universality of deep convolutional neural networks. arxiv:, ",
        "Subsections": [],
        "Groundtruth": "The section discusses the estimation of covering numbers for deep neural networks to measure their capacity in approximating functions. The analysis reveals that deep neural networks may not necessarily improve approximation abilities compared to shallow networks for simple data features like smoothness. The text discusses assumptions and provides theorems and proofs to support the comparison between shallow and deep networks in terms of approximation performance. It also discusses the necessity of network depth for different data features and compares the advantages and limitations of deep neural networks in image processing and computer vision tasks. The text mentions the use of various activation functions and their impact on network capacity and performance in approximation tasks."
    },
    {
        "Section_Num": "NA",
        "Section": "NA",
        "Text": "arxiv:v jan realizing data features by deep nets zheng chu guo, lei shi, and shao bo lin abstractthis paper considers the power of deep neural networks (deep nets for short) in realizing data features. based on rened covering number estimates, we nd that, to realize some complex data features, deep nets can improve the performances of shallow neural networks (shallow nets for short) without requiring additional capacity costs. this veries the advantage of deep nets in realizing complex features. on the other hand, to realize some simple data feature like the smoothness, we prove that, up to a logarithmic factor, the approximation rate of deep nets is asymptotically identical to that of shallow nets, provided that the depth is xed. this exhibits a limitation of deep nets in realizing simple features. index termsneural networks, approximation rates, deep nets, covering numbers, data feature. i. introduction ",
        "Subsections": [],
        "Groundtruth": "This paper explores the ability of deep neural networks (deep nets) to realize data features. Through refined covering number estimates, it is found that deep nets can enhance the performance of shallow neural networks in achieving complex data features without requiring additional capacity costs. However, for simpler data features like smoothness, deep nets demonstrate similar approximation rates to shallow networks when depth is fixed. This highlights the advantage of deep nets for complex features and the limitation for simple features. Key terms: neural networks, approximation rates, deep nets, covering numbers, data feature."
    },
    {
        "Section_Num": "1",
        "Section": "1 Introduction",
        "Text": "the traditional approach to proving decay of correlations and statistical limit laws for deterministic dynamical systems, following and continuing with young , involves symbolic coding. in particular, by quotienting along sta- ble leaves one passes from an invertible dynamical system to a one sided shift. decay of correlations is then a consequence of the contracting properties of the associated transfer operator. in addition, nagaev perturbation arguments and the mar- tingale approximation method of gordin are available in this setting, leading to numerous statistical limit laws. these results on decay of correlations and statistical limit laws are then readily passed back to the original dynamical system. a downside to this approach is that geometric and smooth structures associated to the underlying dynamical system are typically destroyed by symbolic coding. in recent department of mathematics, faireld university, faireld, ct , usa. email: mde- mers@faireld.edu mathematics institute, university of warwick, coventry, cv al, uk. email: i.melbourne@warwick.ac.uk department of mathematics, university of houston, houston tx -, usa. email: nicol@math.uh.edu years, a method proposed by and developed extensively by numerous authors (for recent articles with up to date references see ) uses anisotropic banach spaces of distributions to study the underlying dynamical system directly. in particular, the method does not involve quotienting along stable manifolds. this leads to results on rates of decay of correlations and also to various statistical limit laws via nagaev perturbation arguments, see especially gou ezel . however, so far gordins martingale approximation argument has been absent from the anisotropic banach space framework. this is the topic of the current paper. the utility of such an approach is illustrated by the following example. example the landmark result of young established exponential decay of correlations for the collision map corresponding to planar periodic dispersing billiards with nite horizon. the method, which involves symbolic coding, also yields the central limit theorem (clt) for h older observables, recovering results of . turning to the corresponding ow, known as the nite horizon planar periodic lorentz gas, the clt follows straightforwardly from the result for billiards . however, decay of correlations for the lorentz gas and the clt for the time one map of the lorentz gas are much harder. superpolynomial decay of correlations was established for suciently regular observables in (see also ) using symbolic coding and dolgopyat type estimates . this method also yields the clt for the time one map , but again only for suciently regular observables. here, regular means smooth along the ow direction, so this excludes many physically relevant observables such as velocity. the rate of decay of correlations was improved to subexponential decay and nally in a recent major breakthrough to exponential decay . both references handle h older observables, suggesting that statistical limit laws such as the clt for the time one map should hold for general h older observables. currently the nagaev method is unavailable for lorentz gases, and as a conse- quence the clt for the time one map was previously unavailable except for a re- stricted class of observables. we show that the gordin approach is applicable and hence the clt and related limit laws are indeed satised by h older observables for these examples. in particular, observables such as velocity are covered for the rst time. in the remainder of the introduction, we describe some of the limit laws that follow from the methods in this paper. for deniteness, we focus on example . let x be the three dimensional phase space corresponding to a nite horizon planar periodic lorentz gas, with invariant volume measure , and let t : x x be the time one map of the lorentz ow. let : x r be a h older observable with mean zero and dene the birkhosum n = pn j= t j. it follows from that we can dene = lim nn z x n d = x n= z x t n d. by , typically > (the case = is of innite codimension). we obtain the following results. clt: n/n d n(, ) as n . that is lim n(x x : n/n(x) c) = ()/ z c ey/() dy for all c r. weak invariance principle (wip): dene wn(t) = n/nt for t = , n, n, . . . and linearly interpolate to obtain wn c. then wn w w where w denotes brownian motion with variance moment estimates: for every p there exists cp > such that |n|p cpn/ consequently, limnnp/np p = e|y |p where y =d n(, ). homogenization: now suppose that : x rk. we continue to suppose that is c for some (, ] and that r x d = consider the fast slow system x(n + ) = x(n) + a(x(n)) + b(x(n))(y(n)), y(n + ) = ty(n), () where x() = rd and y() is drawn randomly from (x, ). we suppose that a : rd rd lies in c+ and b : rd rdk lies in c+. solve () to obtain x(n) = + n x j= a(x(j)) + n x j= b(x(j))(y(j)), y(n) = t ny(), and let x(t) = x(). this denes a random process on the probability space (x, ) depending on y() x. then x w z as , where z satises an it o stochastic dierential equation dz = a(z)dt + b(z) dw, z() = , where w is a k dimensional brownian motion with covariance matrix and a(x) = a(x) + d x = k x ,= e b x (x)b(x). () here, b is the th column of b and the matrices , e rkk are given by = x n= z x t n d, e = x n= z x t n d. the remainder of this paper is organized as follows. in section , we recall back- ground material on martingale coboundary decompositions and statistical limit laws. in section , we state an abstract theorem on obtaining martingale coboundary de- compositions for invertible systems with stable directions. in section , we apply our results to the time one map of the lorentz gas. in what follows, d denotes convergence in distribution while w denotes weak convergence. ",
        "Subsections": [],
        "Groundtruth": "The traditional approach to proving decay of correlations and statistical limit laws for deterministic dynamical systems involves symbolic coding and quotienting along stable leaves to analyze contracting properties. However, this can destroy geometric and smooth structures. A new method using anisotropic Banach spaces directly studies the dynamical system without symbolic coding, yielding results on decay of correlations and statistical limit laws. This paper focuses on incorporating Gordin's martingale approximation argument into the anisotropic Banach space framework. The approach is illustrated with examples from planar periodic dispersing billiards and Lorentz gases. The paper establishes the central limit theorem for Hӧlder observables, including non-smooth observables like velocity. The utility of the approach is demonstrated through various statistical limit laws, such as the central limit theorem, weak invariance principle, and moment estimates. The paper also discusses properties like homogenization for fast-slow systems."
    },
    {
        "Section_Num": "2",
        "Section": "2 Martingale approximations",
        "Text": "in this section, we review the approach going back to gordin . this method yields martingale approximations for observables of dynamical systems leading to various limit theorems. related references include . let (x, ) be a probability space, and let t : x x be an invertible ergodic measure preserving transformation. let f be a sub--algebra of the underlying -algebra on x such that t f f consider an observable l(x) with r x d = denition we say that admits a martingale coboundary decomposition if = m + t , where m, l(x), m is f measurable, and e = the conditions on m in denition mean that {m t n : n z} is a sequence of martingale dierences with respect to the ltration {t nf : n z}. proposition let lp(x) for some p suppose that p n |e|p < , p n e t n p < . () then admits a martingale coboundary decomposition with m, lp(x). proof this is a standard argument . we give the details for completeness. by (), = p n(e t n) + p n e converges in lp(x). dene m = + t lp(x). then m = p n=(gn gn t) = p n=(gn+ gn t), () where gn = e. clearly, gn = e is f measurable. also, gn t is measurable with respect to t f f hence m is f measurable. next, note that gn t = e t = e. hence e = e = e|t f] = e, where we used that t f f substituting into (), we obtain e = as required. most observables in this paper are real valued, but occasionally in this section we consider observables with values in rk. we write l(x, rk) to denote vector valued observables and write l(x) instead of l(x, r). central limit theorem and invariance principles corollary assume that l(x) and conditions () hold with p = then the clt and wip hold with = r x m d = limnn|n| proof this is a standard application of martingale limit theorems . somewhat surprisingly, by the results of , if l(x) and conditions () hold for p = , then automatically m l(x) even though proposition only gives m, l(x). this suces for the clt. related references for this phenomenon whereby m has extra regularity include . in particular, the following result holds: theorem assume that l(x) and conditions () hold with p = then the clt and wip hold. proof the clt and wip in reverse time (as n ) is an immediate consequence of . passing from reverse time to forward time is standard (see for example ). now let be vector valued with values in rk. dene c` adl` ag processes wn in rk and wn rkk: wn(t) = n/ x j<nt t j, w n (t) = n x i<j<nt t i t j. proposition (iterated wip) suppose that t is mixing. assume that l(x, rk) and conditions () hold with p = then (i) the series = p n= r x t n d, e = p n= r x t n d, con- verge. (ii) (wn, wn) w (w, w), where w is a k dimensional brownian motion with covariance matrix and w(t) = r t w dw + et. proof by proposition , admits a martingale coboundary decomposition with m, l(x, rk), so the result holds by . moments for optimal moment estimates, the following projective version of conditions () are better suited: p n n/|e|p < , p n n/ et n p < . () proposition assume lp(x) and conditions () hold for some p > then maxkn |k| p = o(n/). if in addition n/n d y for some lp random variable y , then limnnq/|n|q q = e|y |q for all q < p. proof let an = pn j= t j. then for r , r x k= k/|e(ak|f)|p r x k= k/ k x j= |e( t j|f)|p = r x j= r x k=log j k/|e( t j|f)|p r x j= j/|e( t j|f)|p. by condition (), p k= k/|e(ak|f)|p < . similarly, p k= k/|ak e(ak|t kf)|p < . recalling that t f f, it follows from that maxkr |ak| p r/ for general n choose r so that r < n r. then max kn |ak| p max kr |ak| p r/ (n)/ finally, k = (an ank) t n so max kn |k| p = max kn |an ank| p max kn |ak| p n/, proving the rst statement. the second statement is an immediate consequence of the rst, see for example [, lemma (e)]. now let be vector valued with values in rk and dene s n = p i<j<n t i t j. proposition assume that lp(x, rk) and conditions () hold for some p then maxkn |s k | p/ = o(n). proof by proposition , we have a martingale coboundary decomposition = m + t with m, lp(x, rk). write s n = x i<j<n m t i t j + x j<n ( t j ) t j = in + jn we use the notation a b to denote a const.b, where the constant is independent of the other parameters present. where in = p i<j<n m t i m t j and jn = x i<n m t i ( t n t i+) + x j<n ( t j ) t j. now, max kn |jk| x i<n |m| t i (|| t n + || t i+) + x j<n (|| t j + ||)|| t j. hence maxkn |jk| p/ n \u0000|m|p||p + ||p||p \u0001 . next, we recall the identity ik = in ink t k (m n m nk t k)(m nk t k), k n, where m n = pn i= m t i. set m, n = x in m t i, i n = x j<in m t im t j. then m nk t k = m, nk t n and ink t k = i nk t n for all k n. hence ik = \u0010 i n i nk (m, n m, nk)m, nk \u0011 t n and so max kn |ik| p/ max kn |i k | p/ + max kn |m, k | p max kn |m, k | p. () now i k = k x i= xi where xi = m t i\u0010 i x j= m t j\u0011 = m t im, i since {mt n; n } is a sequence of lp martingale dierences, {xi; i } is a se- quence of lp/ martingale dierences. by the inequalities of doob and burkholder , max kn |i k | p/ |(pn i=x i )/| p/ = |pn i=x i |p/ (the implied constant depends only on p and is in particular independent of n hence, using that p , max kn |i k | p/ pn i=|x i |p/ = pn i=|xi| p/ |m| p pn i=|m, i| p. applying burkholder once more, maxkn |m, k | p n/|m|p ; in particular maxkn |i k | p/ n|m|p|m|p. substituting these estimates into () yields maxkn |ik| p/ n|m|p|m|p and the result follows. remark there is an error in due to an inaccurate appli- cation of a (correct) result of . (the argument in is ne for nonuniformly expanding maps but false for nonuniformly hyperbolic maps since the observable is not adapted to the ltration for the martingale this error was repeated in the rst version of the current paper and was spotted by the referee. as pointed out to us by the referee, the reference can be used for the ordinary moments n and this argument is now employed in the proof of proposition . (indeed, proposition is an improvement on the previous result [, eq. ()] since it is no longer required that l(x) however, it remains an interesting open problem to obtain optimal control of the iterated moments sn. homogenization as shown in , rough path theory yields homogenization of fast slow systems () provided the iterated wip and suitable iterated moment estimates hold. the iterated moment estimates have been relaxed in . we now apply these results to the fast slow system (). dene the c` adl` ag process x and the stochastic process z as in the introduction. we continue to assume that a c+ and b c+ for some > theorem suppose that t is mixing. assume that lp(x, rk) and condi- tions () hold with p = then x w z as proof the iterated wip holds by proposition . by , it now suces to show that maxkn |k| q = o(n/) and maxkn |sk| q = o(n) for some q > this and more follows from propositions and . remark the standard wip and moments are insucient to determine the limiting stochastic process z. by rough path theory the iterated process wn provides the extra information required to determine limiting stochastic integrals, and thereby the modied drift term (). the iterated moment estimate s n provides the required tightness. note that wn and s n involve summation over i < j. the behaviour of their symmetrized versions (incorporating i > j terms, equivalently i j terms) follows immediately from the ordinary wip and moment estimate, and hence provides no extra information. (indeed the symmetrized version of w n is w n w n which converges weakly to w w ",
        "Subsections": [],
        "Groundtruth": "This section discusses martingale approximations for observables of dynamical systems, leading to various limit theorems. It presents conditions for martingale coboundary decomposition, central limit theorem, invariance principles, and iterated weak invariance principle. The text also addresses optimal moment estimates, iterated moments, homogenization of fast-slow systems, and the application of rough path theory. The results provide insights into obtaining limiting stochastic processes and determining modified drift terms in such systems."
    },
    {
        "Section_Num": "3",
        "Section": "3 Main abstract theorem",
        "Text": "let t : x x be an invertible ergodic measure preserving transformation on a probability space (x, ). we suppose that x is covered by a collection ws of disjoint measurable subsets, called local stable leaves, such that tw s(x) w s(tx) for all x x, where w s(x) is the partition element containing x. let f denote the -algebra generated by ws. note that w s(y) t w s(x) for all y t w s(x), so t w s(x) is a union of elements of ws. hence t f f we denote by l(f) the set of functions in l(x) that are f measurable. theorem let l(x) be a mean zero observable. assume that there exists > and c > such that for all n , (a) | r x t n d| c||n for all l(f). (b) r x diam((t nw s)) d cn. then the conditions in () are satised for all p < , and the conditions in () are satised for all p < proof this is a standard argument. we again give the details for completeness. let = |e|p sgn e = t n, where = |e|p sgn e l(f), and ||||p . then |e|p p = |e|p p = z x e d = z x e d = z x d = z x t n d. by assumption (a), |e|p p = z x t n d c||n c||p n, and the rst part of conditions () and () follows by taking pth roots and using the restriction on p. next, using the pointwise estimate |e | diam((t nw s)) and as- sumption (b), |e t n|p p = |e |p p | diam((t nw s))|p p (||)p| diam((t nw s))| pc||p n. the second part of conditions () and () follows. in the remainder of this section, we show that the conditions in theorem are satised in many standard situations. (the verications below are not needed for our main example in section verifying condition (b) in theorem suppose that t : x x and ws are as above. let y x be a positive measure subset that is a union of local stable leaves in ws. dene the rst return time r : y z+ and rst return map f : y y , r(y) = inf{n : t ny y }, f(y) = t r(y)y. let hn be the random variable on x given by hn(x) = #{ j n : t jx y }. lemma let : x r be measurable. suppose that (y y : r(y) > n) = o(n(+)) for some > and that there are constants c , (, ) such that | diam((t nw s))| chn(x) for all w s ws, n then condition (b) in theorem holds. proof we have z x diam((t nw s)) d c n+ x k= k z x {hn=k}d c n+ x k= k z y {hn=k}r d. if y y {hn = k}, then pk j= rf j > n, and so rf j > n k for some j = , . . . , k hence z y {hn=k}r d k x j= z y {rf jn k }r d. it follows from the tail assumption on r that there is a constant c > such that (y y : r(y) > n) cn(+) and r y {r>n}r d cn. write r = {rn}r + {r>n}r. then z y {rf jn k }r d z y n{rf jn k }d + z y {r>n}r d = n(r n k) + z y {r>n}r d ck+n + cn ck+n. therefore, r y {hn=k}r d ck+n, and z x diam((t nw s)) d ccn x k= kk+ = o(n), as required. verifying condition (a) in theorem for completeness, we show that theorem includes examples that t within the chernov markarian zhang setup (in the summable decay of correlations regime, so > ) for h older mean zero observables : x r. in particular, we recover limit theorems that have been obtained previously for such invertible examples . since there are no new results here, we only sketch the construction from . remark when treating examples falling within the chernov markarian zhang setup, a signicant (over)simplication is to suppose that there is exponential (or rapid) contraction of stable leaves under the underlying dynamics. for billiards with subexponential decay of correlations, such a condition fails since on average stable directions contract as slowly as unstable directions expand. in general, one should assume that there is an inducing set (called y below) such that expansion and con- traction occurs only on visits to y . this general point of view is the one adopted here, as codied by the random variable hn in lemma . it is part of the setup that x is a metric space and t : x x is the canonical billiard map corresponding to the rst collision with the boundary of the billiard table. it is assumed (and for many classes of billiards explicitly constructed) that there is a set y x and a rst return map f = t r : y y such that f is uniformly hyperbolic and the return time has tail bounds satisfying (r > n) = o(n(+)), where we assume that > (see ). moreover, y is modelled by a young tower with exponential tails . a standard argument (see for example [, theorem ]) shows that t : x x is modelled by a young tower f : with polynomial tails , with tail rate o(n(+)) for all < in particular, there is a measure preserving semiconjugacy : x, so we can work with f : instead of t : x x and observables = : r where : x r is h older. the nal part of the set up that we require is that is covered by stable leaves ws satisfying t(w(x)) w(tx), for all x , where w(x) is the element of ws containing x. due to the uniform hyperbolicity of f = t r, the contraction condition in lemma holds . hence f : satises condition (b) of theorem and it remains to verify condition (a). let f : denote the quotient (one sided) young tower obtained by quoti- enting along stable leaves. consider observables : r that are lipschitz with respect to a symbolic metric on , with lipschitz norm . by , there is a constant c > such that z f n d z d z d c | |n, () for all : r lipschitz, l( ), n (the dependence on and | | is not stated explicitly in but follows by a standard argument using the uniform boundedness principle. alternatively, see for a direct argument returning to the two sided tower f : and the lifted observable = : r, it follows for instance from that there exists a choice (depending only on the h older exponent of ) of symbolic metric on and a sequence of observables l(), , such that (i) is f measurable and hence projects down to an observable : r. (ii) sup l < . (iii) lim| f | = here, f is the -algebra generated by ws and l is the transfer operator correspond- ing to f : . let l(f) with projection l( ). following [, proof of corol- lary ], r f n d= r f f +n d= i + i + i, where i = z ( f ) f +n d, i = z d z d, i = z f +n d z d z d. now |i| | f |||. also, i = r ( f ) d r d, so |i| | f ||| by (iii), limij = for j = , by (i), i = z f +n d z d z d = z l f n d z l d z d , so by () and (ii), |i| cl | |n ||n. together, these estimates establish condition (a) in theorem . ",
        "Subsections": [],
        "Groundtruth": "The main abstract theorem presented discusses conditions under which certain functions satisfy given criteria in a measure-preserving transformation setting. The theorem involves conditions (a) and (b) and provides a standard proof for completeness. The text further delves into verifying these conditions in various scenarios, including examples from the Chernov Markarian Zhang setup. The importance of considering stable leaf contraction under dynamics is highlighted. The technical presentation covers aspects such as first return maps, measurable sets, and mean zero observables in detail, emphasizing the applicability of the theorem in specific setups such as billiards systems."
    },
    {
        "Section_Num": "4",
        "Section": "4 Application to Lorentz gases",
        "Text": "",
        "Subsections": [
            {
                "Section_Num": "NA",
                "Section": "NA",
                "Text": "in this section, we use the results of to show that the hypotheses of theorem (with > arbitrarily large) are satised for the time one map corresponding to a nite horizon planar periodic lorentz gas for all h older observables . hence the results of section hold for all p , establishing the results listed in the introduction. setting and main result for lorentz gases ",
                "Subsections": [],
                "Groundtruth": "The results presented in this section demonstrate that the hypotheses of a theorem hold for the time one map of a finite horizon planar periodic Lorentz gas for all Hölder observables. This implies that the results previously introduced in the section apply to all situations, confirming the established outcomes for Lorentz gases."
            },
            {
                "Section_Num": "4_1",
                "Section": "4.1 Setting and main result for Lorentz gases",
                "Text": "let t = r/z denote the two torus, and let bi t, i = , . . . , d, denote open convex sets such that their closures are pairwise disjoint and their boundaries are c curves with strictly positive curvature. we refer to the sets bi as scatterers. the billiard ow t is dened by the motion of a point particle in q = t \\ sd i= bi undergoing elastic collisions at the boundaries of the scatterers and moving at con- stant velocity with unit speed between collisions. hence t is dened on the three dimensional phase space x = q s, s = / , where indicates that and are identied. between collisions, t(x, x, ) = (x + t cos , x + t sin , ), while at collisions the point (x, ) becomes (x, +) where and + are the pre- and post collisions angles, respectively. dening x = x/ , where we identify (x, ) (x, +) at collisions, we obtain a continuous ow t : x x let m = sd i= bi . the billiard map f : m m is the discrete- time map which maps one collision to the next. parametrizing each bi by an ar- clength coordinate r (oriented clockwise) and letting denote the angle that the post collision velocity vector makes with the normal to the scatterer (directed in- wards in q), we obtain the standard coordinates (r, ) on m. for x x, dene the collision time (x) to be the rst time t > that t(x) m. since the closures of the scatterers are disjoint, there exists min > such that (x) min for all x m. in addition, we assume that the billiard has nite horizon so that there exists max < such that (x) max for all x x. it is well known (see ) that the ow preserves the contact form = cos dx + sin dx, so that the contact volume is d = dx ddx we denote by the normalized lebesgue measure on x, which by the preceding calculation is preserved by the ow. the main result of this section is the following. theorem let t be the time one map corresponding to a nite horizon lorentz gas as described above, and let : x r be a mean zero h older observable. then conditions (a) and (b) of theorem hold with n replaced by ecn for some c > as a consequence, conditions () and () hold for all p , and all the results described in section apply in this setting. we remark that the observable is assumed to be h older continuous only on x, not x thus is allowed to be discontinuous at the boundary of x, i.e. at collisions. in particular, theorem applies to the velocity. ",
                "Subsections": [],
                "Groundtruth": "The text discusses Lorentz gases in a two-torus setting, with open convex scatterers that the point particle undergoes elastic collisions with. The billiard flow is defined by the particle moving at a constant unit speed and undergoing collisions at scatterer boundaries. The text introduces the billiard map and collision time, assuming finite horizon and contact form preservation. The main result states that certain conditions hold for Lorentz gases with mean zero Holder observables, leading to implications for all p-values. The results also apply when the observable is Holder continuous only on x."
            },
            {
                "Section_Num": "4_2",
                "Section": "4.2 Proof of Theorem ??",
                "Text": " the remainder of this section is devoted to the proof of theorem , which consists of verifying the conditions of theorem . first we recall some of the essential properties and main constructions used in . hyperbolicity and singularities the singularities for both the collision map and the ow are created by tangential collisions with the scatterers. let s = {(r, ) m : = }. away from the set s = s f s (resp. s = s fs) the map f (resp. f ) is uniformly hyperbolic: letting = + minkmin, () where kmin denotes the minimum curvature of the scatterers, there exist stable cs and unstable cu cones in the tangent space of m such that stable and unstable vectors in these cones undergo uniform expansion and contraction at an exponential rate given by . flowing cs backward and cu forward between collisions allows us to dene two families of stable cs and unstable cu cones for the ow that lie in the kernel of the contact form. (hence they are at two dimensional cones in the tangent space of the ow; see for an explicit denition of these cones let p denote the projections from x onto m under the forward and backward ow. then cu is continuous on x away from the surface s = {x x : p +(x) s}, and cs is continuous on x away from the surface s+ = {x x : p (x) s}. to maintain control of distortion, we dene the standard homogeneity strips hk = \b (r, ) m : k (k+) , k k, for some k which is determined to ensure a one step expansion condition. a similar set of homogeneity strips hk, k k, is dened for near following , we dene a set of admissible stable curves as for the ow. a c curve w belongs to as if the tangent vector at each point of w belongs to cs, and w has curvature bounded by b and length |w| bounded by here, > is chosen to satisfy a complexity bound (see ) and b is chosen large enough that the family as is invariant under t, t (once long pieces are subdivided according to the length ). we call w as homogeneous if p +(w) lies in a single homogeneity strip. we dene ws to be the family of maximal c connected homogeneous stable manifolds for the ow. note that ws forms a partition of x (mod -measure ). moreover, each element of ws (up to subdivision due to the length ) belongs to as. when we dene a homogeneous stable manifold w ws, we take into account cuts introduced at the boundary of the extended singularity set, which includes the boundaries of the homogeneity strips. thus p +(tw) lies in a single homogeneity strip for all t let f denote the sigma algebra generated by elements of ws. since ws forms a partition of x, it follows that f comprises countable unions of elements of ws. due to our denition of cs, if w as, then p +(w) is a stable curve for the map; and if w ws, then p +(w) is a local homogeneous stable manifold for the map. norms and banach spaces with the class of admissible stable curves dened, we can now describe the banach spaces used to prove decay of correlations in . let (, ]. for w as, let c(w) denote the closure of c functions in the holder norm dened by ||c(w ) = sup xw |(x)| + sup x,xw x=x |(x) (x)| dw(x, x), where dw is arclength distance along w. dene the weak norm of c(x) by ||w = sup w as sup c(w ) ||c(w ) z w dmw, where mw denotes arclength measure on w. the weak space bw is dened as the completion of the set { c(x) : ||w < }. the strong norm b is dened as in . the space b is similarly dened as the completion of a class of smooth functions on x in the b norm. since we do not need the precise denition of b here, we omit its denition; however, the following lemma summarizes some of the important properties of these spaces. lemma () we have the inclusions c(x) c(x) b bw (c(x)), where the rst two inclusions are injective. moreover, | |w b c| |c(x) and the unit ball of b is compactly embedded in bw. when we refer to functions c(x) as elements of b or bw, we identify with the measure d. with this identication, the two denitions of lt given in the next section are reconciled. the following lemma is central to our verication of condition (a) in theorem , and is a strengthening of . let c(ws) denote those functions which are in c(w) for all w ws with ||c(ws) = supw ws ||c(w ) nite. lemma there exists c > such that for bw and c(ws), |()| c||w||c(ws). again, due to our identication, when c(x), we intend () = r x d. lemma is proved at the end of this section. transfer operator we dene the transfer operator lt, for t , by lt = t, for c(x). this can be extended to any element of bw, and more generally a distribution of order by lt() = ( t), for all c(as), (c(as)). by , the map (t, ) lt from [, ) b to b is jointly continuous, so {lt}t is a semi group of bounded operators on b. dene the generator of the semi group by z = limt lt t for c(x). while z is not a bounded operator on b, the strong continuity of lt implies that z is closed with domain dense in b. indeed, by the domain of z contains all c(x) c(x) such that c(x) where denotes the ow direction, and there is a constant c > such that zb c||c(x) for all such . () condition (b) of theorem recall that t and f denote the time one map for the ow and the collision map, respectively. by the nite horizon condition, any w ws must undergo k n/maxcollisions after n iterates by t. by [, lemmas and ], diam(t nw) = |nw| c|f k(p +(w))| c(k)|p +(w)| cn/max|w|, where > is the hyperbolicity constant dened in (). we have used here that the lengths of p +(w) and w are bounded multiples of one another (indeed the jacobian of this map is c , see ). let : x r be c. then diam((t nw)) || diam(t nw) cn/max. hence condition (b) holds with n replaced by n/max. condition (a) of theorem by , z has a spectral gap on b and, using results of , lt admits the following decomposition: there exists > , a nite rank projector : b b and a family of bounded operators pt on b satisfying pt = pt = , and a matrix b z : (b) (b) with eigenvalues , z, . . . , zn c satisfying re zj < for j = , . . . , n, such that lt = pt + et b z for all t () moreover, there exists c > such that for all in dom(z) b, |pt|w cetzb for all t () now suppose c(x) c(x) is of mean zero and c(ws). by (), r x t d = r x lt d = r x pt d + r x et b z d. hence by lemma , z x t d c n |pt|w + |et b z|w o ||c(ws). () letting denote the projector corresponding to the simple eigenvalue , we see that = r d = since is the conformal probability measure with respect to lt. hence by lemma , |et b z|w = |et b z( )|w cet||w cet||c(x). by () and (), |pt|w cetzb cet||c(x). substituting these estimates in (), | r x t n d| = | r x n d| cen||c(x)||c(ws) for all n the result extends to c(x) as in by a standard mollication argument. (exponential contraction persists with a rate dependent on in particular, there are constants c, c > such that | r x t n d| cecn||c(x)||c(ws) for all n , for all c(x), c(ws). let k(ws) denote the set of bounded functions on x that are constant on el- ements of ws, and let | |c(ws) = supw ws | |c(w ). note that these functions are f measurable. moreover, k(ws) c(ws) and ||c(ws) = ||c(ws) for k(ws). hence | r x t n d| cecn||c(x)||c(ws) for all n , for all c(x), k(ws). finally, let c(x), l(f). recall that l(f) is the set of functions in l() which are f measurable, so there exists a pointwise representative in the equivalence class of in l() that is constant on local stable manifolds and such that sup || = ||. in particular, k(ws) with ||c(ws) = ||and r x | | d = hence | r x t n d| = | r x t n d| cecn||c(x)||. hence condition (a) holds with n replaced by ecn. as promised, we end this section by proving lemma . proof of lemma by density of c(x) in bw, it suces to prove the lemma for c(x) and c(ws). the normalized lebesgue measure on x projects to the measure = (|q|) cos drd on m; this is the unique smooth invariant probability measure for the billiard map f. let w s denote the set of maximal connected homogeneous stable manifolds for f. note that p +(ws) = w s. indexing elements of w s, we write w s = {v}, which denes a (mod ) partition of m. we disintegrate into conditional measures on v, , and a factor measure on . indeed, the conditional measures are smooth on each v, and we can write d = d m d(), where m is arclength measure along v (in m), and | log |c (v) c, | |c(v) c|v|, () for some c > depending only on the table q (see ). the exponent comes from the denition of the homogeneity strips. this is the standard decom- position of into a proper standard family (see ). we further subdivide = sd i= i, where i is the index set corresponding to each component mi = bi [ , ] of m. write x = sd i= xi where xi = {x x : p +(x) mi}. on each xi, we represent lebesgue measure as d = c cos dr d ds, where c is a normalizing constant, (r, ) range over mi, and s ranges from to the maximum free ight time under the backwards ow, which we denote by ti max. next, for each i, the ow surface v = {x xi : p +(x) v} is smoothly foliated by elements of ws, which are simply ow translates of one another. for each s and v, let w,s = t(s)v, where t(s, z) is dened for z v so that w,s lies in the kernel of , i.e. it is an element of ws. note that for s < , some points in v may not have lifted oof m. for such small times, w,s denotes only those points that have lifted oof m. similarly, for s > min, some part of t(s)v may have collided with a scatterer. for such times, w,s only denotes those points which have not yet undergone a collision. thus s s w,s = v . using this decomposition, we may represent lebesgue measure on each xi by d(x) = (x) dmw,s(x) d() ds, where is smooth along each w,s, satisfying analogous bounds to (), since the contact form is con xi and the projection p + is suciently smooth (see [, lemma ]), so that the arclength of w,s varies smoothly with that of v. standard families in are standard pairs dened on local unstable manifolds, while here we use local stable manifolds. the decompositions of have equivalent properties due to the symmetry of the map f under time reversal. using the fact that each w,s ws can be subdivided into at most c elements of as, we are ready to estimate z x d d x i= z xi d x i z ti z i z w,s dmw,s d() ds x i z ti z i c ||w||c(w,s)||c(w,s) d() ds c max||w||c(ws) z |v|d() . this last integral is nite by since our decomposition of consti- tutes a proper standard family, yielding the desired estimate for (). for completeness, we nish by proving . for x v, let rs(x) denote the distance measured along v from x to the nearest endpoint of v. by [, theorem ], there exists c > such that sup > (x m : rs(x) < ) c . we claim this quantity provides an upper bound on the relevant integral. to see this, we use the decomposition () to write, c = sup > (x m : rs(x) < ) = sup > z (rs(x) < ) d() sup > z c |v| |v {rs < }| d() sup > c z {:|v|>} |v| d() = c z |v| d() , where we have used the fact that |v| > for -a.e. , and the bound |v {rs(x) < }| = if |v| > (one can also prove a reverse inequality, but we do not need this here acknowledgements we are very grateful to the referees for several helpful sug- gestions, especially with regard to the issue mentioned in remark . this research resulted from a research in pairs on techniques des martingales et espaces de banach anisotropes (martingale techniques and anisotropic banach spaces) at cirm, luminy, during august md was supported in part by nsf grant dms im was supported in part by european advanced grant stochexthomog (erc adg ). mn was supported in part by nsf grant dms references v. ara ujo, i. melbourne and p. varandas. rapid mixing for the lorenz attractor and statistical limit laws for their time- maps. comm. math. phys. () v. baladi. the quest for the ultimate anisotropic banach space. j. stat. phys. () v. baladi, m. f. demers and c. liverani. exponential decay of correlations for nite horizon sinai billiard ows. invent. math. () p. b alint and s. gou ezel. limit theorems in the stadium billiard. comm. math. phys. () p. b alint and i. melbourne. statistical properties for ows with unbounded roof function, including the lorenz attractor. j. stat. phys. () m. blank, g. keller and c. liverani. ruelle perron frobenius spectrum for anosov maps. nonlinearity () r. bowen. equilibrium states and the ergodic theory of anosov dieomor- phisms. lecture notes in math. , springer, berlin, l. a. bunimovich, y. g. sina and n. i. chernov. statistical properties of two- dimensional hyperbolic billiards. uspekhi mat. nauk () d. l. burkholder. distribution function inequalities for martingales. ann. prob- ability () o. butterley. a note on operator semigroups associated to chaotic ows. ergodic theory dynam. systems () n. chernov. a stretched exponential bound on time correlations for billiard ows. j. stat. phys. () n. chernov and r. markarian. chaotic billiards. mathematical surveys and monographs , american mathematical society, providence, ri, n. chernov and h.-k. zhang. billiards with polynomial mixing rates. nonlin- earity () n. chernov and h.-k. zhang. a family of chaotic billiards with variable mixing rates. stochastics and dynamics () i. chevyrev, p. k. friz, a. korepanov, i. melbourne and h. zhang. multiscale systems, homogenization, and rough paths. probability and analysis in inter- acting physical systems: in honor of s.r.s. varadhan, berlin, august, (p. friz et al., ed, springer proceedings in mathematics & statistics , , p. i. chevyrev, p. k. friz, a. korepanov, i. melbourne and h. zhang. determin- istic homogenization under optimal moment assumptions for fast slow systems. part preprint, j. dedecker, f. merlev` ede and f. p` ene. empirical central limit theorems for ergodic automorphisms of the torus. alea lat. am. j. probab. math. stat. () j. dedecker and e. rio. on the functional central limit theorem for stationary processes. ann. inst. h. poincar e probab. statist. () m. f. demers. a gentle introduction to anisotropic banach spaces. chaos soli- tons fractals () d. dolgopyat. prevalence of rapid mixing in hyperbolic ows. ergodic theory dynam. systems () p. k. friz and m. hairer. a course on rough paths. universitext, springer, cham, m. i. gordin. the central limit theorem for stationary processes. soviet math. dokl. () s. gou ezel. almost sure invariance principle for dynamical systems by spectral methods. ann. probab. () s. gou ezel. limit theorems in dynamical systems using the spectral method. hy- perbolic dynamics, uctuations and large deviations. proc. sympos. pure math. , amer. math. soc., providence, ri, , pp. h. hennion and l. herv e. limit theorems for markov chains and stochastic properties of dynamical systems by quasi compactness. lecture notes in math. , springer, berlin, c. c. heyde. on the central limit theorem and iterated logarithm law for sta- tionary processes. bull. austral. math. soc. () d. kelly and i. melbourne. smooth approximation of stochastic dierential equa- tions. ann. probab. () d. kelly and i. melbourne. homogenization for deterministic fast slow systems with multidimensional multiplicative noise. j. funct. anal. () c. kipnis and s. r. s. varadhan. central limit theorem for additive functionals of reversible markov processes and applications to simple exclusions. comm. math. phys. () a. korepanov, z. kosloand i. melbourne. explicit coupling argument for nonuniformly hyperbolic transformations. proc. roy. soc. edinburgh a () c. liverani. central limit theorem for deterministic systems. international con- ference on dynamical systems (f. ledrappier, j. lewowicz and s. newhouse, eds, pitman research notes in math. , longman group ltd, harlow, , pp. t. j. lyons. dierential equations driven by rough signals. rev. mat. iberoamer- icana () r. markarian. billiards with polynomial decay of correlations. ergodic theory dynam. systems () m. maxwell and m. woodroofe. central limit theorems for additive functionals of markov chains. ann. probab. () i. melbourne. rapid decay of correlations for nonuniformly hyperbolic ows. trans. amer. math. soc. () i. melbourne. superpolynomial and polynomial mixing for semiows and ows. nonlinearity () rr i. melbourne and m. nicol. almost sure invariance principle for nonuniformly hyperbolic systems. comm. math. phys. () i. melbourne and m. nicol. large deviations for nonuniformly hyperbolic sys- tems. trans. amer. math. soc. () i. melbourne and a. t or ok. central limit theorems and invariance principles for time one maps of hyperbolic ows. comm. math. phys. () i. melbourne and a. t or ok. statistical limit theorems for suspension ows. israel j. math. () i. melbourne and a. t or ok. convergence of moments for axiom a and nonuni- formly hyperbolic ows. ergodic theory dynam. systems () i. melbourne and p. varandas. a note on statistical properties for nonuniformly hyperbolic systems with slow contraction and expansion. stoch. dyn. () , pages. m. peligrad and s. utev. a new maximal inequality and invariance principle for stationary sequences. ann. probab. () d. ruelle. thermodynamic formalism. encyclopedia of math. and its applica- tions , addison wesley, massachusetts, y. g. sina . gibbs measures in ergodic theory. russ. math. surv. () m. tyran kami nska. an invariance principle for maps with polynomial decay of correlations. comm. math. phys. () m. viana. stochastic dynamics of deterministic systems. col. bras. de matem atica, d. voln y. approximating martingales and the central limit theorem for strictly stationary processes. stochastic process. appl. () d. voln y. a nonadapted version of the invariance principle of peligrad and utev. c. r. math. acad. sci. paris () l.-s. young. statistical properties of dynamical systems with some hyperbolicity. ann. of math. () l.-s. young. recurrence times and rates of mixing. israel j. math. () ",
                "Subsections": [],
                "Groundtruth": "The section focuses on proving a theorem related to the conditions outlined in the text. It discusses the hyperbolicity and singularities in the context of collision maps and flows. Stable and unstable cones related to expansion and contraction rates are defined. Admissible stable curves and Banach spaces are introduced to analyze decay of correlations. The text also covers the transfer operator, spectral decomposition, and conditions (a) and (b) of the theorem. Lemmas and proofs are provided to support the analysis, ensuring exponential contraction rates and spectral properties. Acknowledgements, references, and a detailed proof of a lemma are included, emphasizing contributions and background research. The discussion involves various mathematical concepts, spaces, and techniques to validate the theorem and its conditions."
            }
        ],
        "Groundtruth": ""
    },
    {
        "Section_Num": "NA",
        "Section": "NA",
        "Text": "arxiv:v jun martingale approximations and anisotropic banach spaces with an application to the time one map of a lorentz gas mark demers ian melbourne matthew nicol december revised february abstract in this paper, we show how the gordin martingale approximation method ts into the anisotropic banach space framework. in particular, for the time- one map of a nite horizon planar periodic lorentz gas, we prove that h older observables satisfy statistical limit laws such as the central limit theorem and associated invariance principles. previously, these properties were known only for a restricted class of observables, excluding for instance velocity. introduction ",
        "Subsections": [],
        "Groundtruth": "The text discusses martingale approximations and anisotropic Banach spaces in the context of the time one map of a Lorentz gas. The authors demonstrate the integration of the Gordin martingale approximation method into the anisotropic Banach space framework, showing that Hӧlder observables exhibit statistical limit laws like the central limit theorem and associated invariance principles. This expands the known properties to a broader class of observables, including velocity, for the specified system."
    },
    {
        "Section_Num": "I",
        "Section": "I Introduction and related work",
        "Text": "unlike their fourth generation counterparts, g networks will not only transport data, but also process them. network, computing, and memory resources controlled by mobile net- work operators (mnos), will concurrently support multiple services, under the network slicing paradigm , . it is universally expected that vertical industries (e.g., auto- motive or media companies) specify the requirements of their services, i.e., which computation must be performed and the associated target key performance indicators (kpis). mnos, on the other hand, have to manage their network so as to ensure that all target kpis are met at the lowest cost for themselves, a problem known as service orchestration , . our purpose in this paper is to study a different model of interaction between vertical industries (henceforth verticals) and mnos, whereby verticals provide not only the target kpis but also an estimation of their expected trafc patterns. the reason for this change is that service orchestration is greatly simplied if the evolution of the demand to serve is known , or it can be reliably predicted , and verticals are in a better position than mnos to make such a prediction. indeed, unlike verticals, mnos cannot access, for technical and legal reasons, detailed, application layer information on the trafc owing through their network. it follows that, since network slices are tailored around a single type of service, the service specic predictions that verticals can make may be more useful than predictions made by mnos. our rst task is therefore to compare the accuracy of the predictions that mnos and verticals can make based on the information they can access. to this end, we leverage a real- world, large scale, crowd sourced trace, containing mobility i.e., services with the same kpis. and trafc information about over , users in the los angeles area. thanks to its crowd sourced nature, the trace contains a superset of the information available to verticals and mnos; therefore, we can consider a state of the art prediction technique, feed it the data available to mnos and verticals, and check which of them yields the most accurate result. beyond the accuracy of predictions, we are interested in the effect of prediction errors on the resulting orchestra- tion decisions. specically, we are interested in two adverse consequences of inaccurate predictions, namely, (i) unused capacity, when the demand is overestimated and the network slice is over provisioned, and (ii) scale up events, when the capabilities of an under provisioned slice must be swiftly improved to face a higher than predicted demand. our task is to establish which of these events is more common and how the predictions obtained by verticals and mnos affect them. finally, we compare both alternatives against a scenario where verticals and mnos share not only the trafc prediction but the input information they use to make them. this serves as a useful benchmark,a lthough it would be realistic only in very specic scenarios, e.g., when the mno also acts as a vertical and provides value added services such as video calls or streaming. the rest of this paper is organized as follows. sec. ii presents the real world dataset we use for the forecasts, while sec. iii describes the techniques we adopt. after presenting the metrics of interest and numerical results in sec. iv, we conclude the paper in sec. v. ",
        "Subsections": [],
        "Groundtruth": "In the text, it is discussed that upcoming 5th generation mobile networks will not only transport data but also process it. Mobile network operators (MNOs) will manage network resources to support various services simultaneously under the network slicing paradigm. Vertical industries are expected to specify service requirements, and MNOs must ensure target performance indicators are met efficiently. The text proposes a model where verticals provide traffic patterns along with performance indicators to simplify service orchestration. The accuracy of predictions by MNOs and verticals is compared using a real-world dataset, with a focus on the impact of prediction errors on network resource allocation. The text concludes by comparing scenarios where verticals and MNOs share prediction information, highlighting potential benefits in certain situations."
    },
    {
        "Section_Num": "II",
        "Section": "II A real-world dataset",
        "Text": "for our analysis, we use a real world, crowd sourced dataset collected from the wefi app . wefi provides its users with location specic information on the available wi fi networks, and such information is crowd sourced from the users them- selves. specically, the app tracks: the current time (with a one hour granularity) and loca- tion; the mobile operator and cell the user is associated with (if any); the ssid and bssid of the wi fi network she is con- nected to (if any); arxiv:v jan table i the wefi trace. metric value covered area km collection time march number of records million unique users , unique cells , unique bssids , unique apps , total trafc tbyte coverage % (wefi estimate) fig. the area covered by the wefi dataset. colors reect the average download rate considering all applications; warmer colors correspond to a higher trafc demand. the amount of data (uplink and downlink) used by the currently active application, and the identity of the appli- cation itself. new records in the trace are created every time any of the above pieces of information changes, e.g., the user switches between apps. the features of the trace are summarized in tab. i. fig. depicts the area covered by the trace greater los angeles and the trafc density therein. we can observe a higher trafc demand in the most densely populated zones, e.g., downtown los angeles, and a lower demand in rural or wilderness areas. also notice, in the far east and north of the map, the edwards and twentynine palms military bases, with a much higher trafc than the surrounding rural areas. importantly, unlike similar datasets collected by mobile operators , , the wefi trace includes information on different mobile operators and technologies (including wi- fi), as well as different applications an aspect that makes the trace especially well suited to study g networks. indeed, the network slicing paradigm is predicated on tailoring slices to individual applications; in this context, knowledge on application specic trafc patterns is much more useful than information on global demand uctuations. ",
        "Subsections": [],
        "Groundtruth": "A real-world crowd-sourced dataset collected from the wefi app is used for analysis. The dataset includes details such as current time and location, mobile operator and cell association, Wi-Fi network details, data usage by applications, and traffic density in Greater Los Angeles. This dataset stands out as it covers information on various mobile operators, technologies (including Wi-Fi), and applications, making it ideal for studying 5G networks and network slicing tailored to individual applications. The dataset offers insights into application-specific traffic patterns, which are more valuable for this purpose compared to generic demand fluctuations."
    },
    {
        "Section_Num": "III",
        "Section": "III Forecasting technique",
        "Text": "here we briey describe the data available to mnos and to verticals, the prediction technique that we apply, and the output (i.e., the predictions) that mnos and verticals can obtain. input and output data. the information available to mnos, i.e., in machine learning jargon, the features their forecast is based upon, include, for each cell and time period: the total demand (by all users, for all apps); the number of users covered by the cell (regardless their activity). in the case of the wefi trace, time periods correspond to one- hour time intervals. notice that, due to technical and legal reasons, mnos have no knowledge of what individual users do, i.e., which app(s) they use. verticals, on the other hand, only have information about their own service on the positive side, they know the identity of their users and their ne grained location in the case of the wefi trace, a m tile. for each time period and tile, verticals can thus keep track of: the trafc demand for their service; the number of users of their service; their trafc history, e.g., the amount of data they down- loaded in the past. for both mnos and verticals, the quantity to predict is the total demand of a specic app, i.e., the trafc the network slice will process. as it is commonplace in machine learning, we split our dataset into a training set, including the rst three weeks of the trace, and a testing set, containing the last one. prediction technique. the prediction task under study belongs to the class of time series forecasting problems. input data are multi variate, i.e., the forecast must be based on the evolution of multiple quantities in the case of mnos, for example, the trafc demand of each individual cell. this rules out the use of traditional approaches like the holt winters method adopted in , which requires uni variate time series. we therefore turn to neural networks, namely, long short- term memory (lstm) networks, introduced in and, since then, successfully applied in a variety of elds, from computer vision to voice recognition. specically, we use the implementation from googles tensorflow library, accessed through the keras high level front end. ",
        "Subsections": [],
        "Groundtruth": "The text describes the data available to Mobile Network Operators (MNOs) and verticals, the prediction technique applied, and the output predictions. MNOs use information such as total demand and number of users at specific time intervals, while verticals have data on user identity and traffic history. Both aim to predict the total demand for a specific app. The prediction task is a time series forecasting problem with multivariate input data, leading to the use of neural networks, specifically Long Short-Term Memory (LSTM) networks implemented in TensorFlow through Keras."
    },
    {
        "Section_Num": "IV",
        "Section": "IV Numerical results",
        "Text": "this section shows the predicted behavior of the traf- c demand, describes the performance metrics we consider (sec. iv a), and discusses the results we obtain (sec. iv b). fig. shows the actual trafc (black line), as well as the trafc predicted: using the data available to mnos (blue lines); using the data available to verticals (yellow lines); using both (green lines). a. performance metrics as mentioned in the introduction, we are interested in two main aspects, namely, the prediction accuracy and the quality of orchestration decisions. quantifying the prediction accuracy for simplicity, we assume that a service corresponds to an app, and that each vertical only provides one service. (a) (b) (c) fig. actual and predicted trafc for youtube (a), facebook (b), and netix (c). fig. prediction error (rmse) for different apps and scenarios. fig. unused capabilities and scale up events for different apps and scenarios. is fairly straightforward; specically, we resort to the well- known rmse (reduced mean square error) metric, dened as: rmse = v u u t n n x j= (yj yj), () where n is the number of time periods the forecast extends across (seven days, i.e., one hour periods in our case), while yj and yj are, respectively, the actual and predicted trafc at the jth time period. note that all metrics are computed separately for each app. with regard to orchestration decisions, there are two adverse effects we seek to minimize. the rst is unused capabilities, i.e., network slices that are over provisioned with respect to the actual trafc demand. we can quantify unused capabilities as: w = n x j= max (, yj yj) . () that is, for each time period, we consider the difference between the predicted demand (according to which the slice is dimensioned) and the actual one; if positive, such a difference is representative of the amount of unused capabilities. the second adverse effect is represented by scale up events, i.e., under provisioned slices whose capabilities have to be swiftly extended to cope with unforeseen increases in trafc. as discussed in , such events have the potential to decrease the qos/qoe of all services supported by the mno. the quantity u expresses the number of time periods in which such events happen: u = n x j= [yj> yj]. () b. results each of the plots in fig. refers to one of the three most used apps in the trace: youtube, facebook, and netix. as one might expect, the trafc demand exhibits clear weekly and daily patterns, e.g., morning and evening peaks. however, the magnitude of trafc peaks is not consistent throughout all days, e.g., see the peaks around periods and in fig. (a). this feature of trafc patterns makes prediction harder; indeed, we can observe that these higher than usual peaks are never properly predicted, regardless of the scenario. fig. shows the prediction error (rmse, as dened in ()) associated with different services and scenarios. a rst fact to notice is that the prediction error is, in general, fairly small: a testament to the effectiveness of the lstm prediction technique, as well as to the overall regularity of the trafc demand. it is perhaps even more interesting to observe that the rmse changes signicantly across apps; comparing fig. to fig. , we can conclude that more numerous and irregular peaks are associated with a higher prediction error. the effect of shifting the task of trafc prediction from the mno (rst group of bars in fig. ) to the vertical (second group of bars) is also inconsistent across services. the vertical seems to be better than the mno at predicting youtube, but the opposite is true for facebook; as for netix, no signicant difference can be observed. the third group of bars refers to the benchmark scenario where mnos and verticals share their information and jointly predict the demand: in this case, the resulting error is close to the lowest of the errors yielded by the other two scenarios. fig. focusses on the impact of the trafc prediction on orchestration decisions. the x axis therein shows the total unused capabilities, i.e., the w metric dened in (), while the y axis shows the frequency of scale up events, i.e., the u- metric dened in (). each dot corresponds to a combination of app (identied by its color) and scenario (identied by the marker used), e.g., the red square corresponds to the vertical making predictions about youtube trafc. a rst observation we can make is that scale up events are much more common than unused capabilities. indeed, scale- up events happen in around % of time periods (i.e., roughly twice per day), while unused capabilities account for less than % of the total. this is unwelcome news, since scale up events are, in most real world cases, a more serious issue than unused capabilities, and far more likely to result in a violation of target kpis and/or higher costs for the mno . this seems to disagree with the low rmse values summarized in fig. ; however, it is worth recalling that any underestimation of the demand, no matter how slight, leads to a scale up event. for the same reason, the netix app (green markers) is associated with almost the same number of scale up events as facebook and youtube, in spite of the much lower rmse. even more interestingly, there is a remarkably consistent relationship between the different scenarios. mno predictions yield the highest number of scale up events and the lowest amount of unused capabilities; moving to vertical and joint predictions has the effect of reducing the scale up events in exchange for a small increase in unused capabilities. it is important to remark how this happens for all apps, in spite of the different levels of prediction accuracy (fig. ) they are associated with. in other words, while involving the verticals in trafc prediction does not necessarily improve the predictions accuracy per se, it does yield better orchestration decisions, with a healthier balance between scale up events and unused resources. ",
        "Subsections": [],
        "Groundtruth": "In the section on Numerical Results, the text presents the predicted traffic demand with performance metrics focusing on prediction accuracy and orchestration decisions. Actual traffic and predictions using different data sources are compared for various apps. The Root Mean Square Error (RMSE) metric is used to evaluate prediction accuracy. Results show clear weekly and daily traffic patterns, with inconsistencies in peak magnitudes affecting predictions. The impact of different prediction scenarios on orchestration decisions is discussed, highlighting the trade-off between unused capabilities and scale-up events. Involving verticals in traffic prediction leads to a better balance between scale-up events and unused resources, improving orchestration decisions despite varying prediction accuracies for different apps."
    },
    {
        "Section_Num": "V",
        "Section": "V Conclusion and Future Work",
        "Text": "we considered the orchestration problem in g networks based on network slicing. after remarking that good orches- tration decisions depend on accurate trafc predictions, we investigated whether verticals are in a better position than mnos to make such predictions. using a real world, large- scale, crowd sourced trace, we found that, while the prediction error is not consistent throughout different apps, involving the verticals in the prediction leads to a slightly higher amount of unused capacity and, more importantly, to a lower frequency of scale up events. a rst direction to extend our work is considering additional prediction techniques, including generalizations of the holt- winters method. furthermore, we can couple the predictions with actual orchestration algorithms, including state of the art approaches taken from the literature as well as purpose built ones. ",
        "Subsections": [],
        "Groundtruth": "The text discusses the orchestration problem in 5G networks with network slicing, highlighting the importance of accurate traffic predictions for good orchestration decisions. A study compared the ability of verticals and mobile network operators (MNOs) to make traffic predictions, finding that involving verticals led to slightly higher unused capacity and fewer scale-up events. Future work includes exploring additional prediction techniques and integrating predictions with orchestration algorithms from existing literature or purpose-built ones."
    },
    {
        "Section_Num": "References",
        "Section": "References",
        "Text": "] h. zhang, n. liu, x. chu, k. long, a.-h. aghvami, and v. c. leung, network slicing based g and future mobile networks: mobility, resource management, and challenges, ieee comm. mag., p. rost, c. mannweiler, d. s. michalopoulos, c. sartori, v. sciancale- pore, n. sastry, o. holland, s. tayade, b. han, d. bega et al., network slicing to enable scalability and exibility in g mobile networks, ieee comm. mag., x. foukas, g. patounas, a. elmokash, and m. k. marina, network slicing in g: survey and challenges, ieee comm. mag., s. vassilaras, l. gkatzikis, n. liakopoulos, i. n. stiakogiannakis, m. qi, l. shi, l. liu, m. debbah, and g. s. paschos, the algorithmic aspects of network slicing, ieee comm. mag., x. li, j. mangues bafalluy, i. pascual, g. landi, f. moscatelli, k. an- tevski, c. j. bernardos, l. valcarenghi, b. martini, c. f. chiasserini et al., service orchestration and federation for verticals, in ieee wcnc workshops, s. agarwal, f. malandrino, c.-f. chiasserini, and s. de, joint vnf placement and cpu allocation in g, in ieee infocom, v. sciancalepore, k. samdanis, x. costa perez, d. bega, m. gramaglia, and a. banchs, mobile trafc forecasting for maximizing g network slicing resource utilization, in ieee infocom, wefi. https://we.com. m. z. shaq, l. ji, a. x. liu, j. pang, and j. wang, characterizing geospatial dynamics of application usage in a g cellular data network, in ieee infocom, p. di francesco, f. malandrino, t. k. forde, and l. a. dasilva, a sharing- and competition aware framework for cellular network evolution planning, ieee transactions on cognitive communications and networking, s. hochreiter and j. schmidhuber, long short term memory, neural computation, tensorflow: an open source machine learning framework for everyone. https://www.tensorow.org. keras: the python deep learning library. https://keras.io. v. sciancalepore, f. z. yousaf, and x. costa perez, z torch: an automated nfv orchestration and monitoring solution, ieee trans- actions on network and service management, acknowledgment this work is supported by the european commission through the h projects g transformer (project id ) and g eve (project id ). ",
        "Subsections": [],
        "Groundtruth": "The text discusses various aspects of network slicing in 5G and future mobile networks, including mobility, resource management, challenges, scalability, flexibility, survey, algorithms, service orchestration, verticals, VNF placement, CPU allocation, traffic forecasting, resource utilization, geospatial dynamics of application usage, sharing and competition-aware framework for network evolution planning. It also mentions machine learning frameworks like Tensorflow and Keras, as well as an automated NFV orchestration and monitoring solution called Z Torch. The work is supported by the European Commission through the projects G-Transformer and G-EVE."
    },
    {
        "Section_Num": "NA",
        "Section": "NA",
        "Text": "g trafc forecasting: if verticals and mobile operators cooperate francesco malandrino cnr ieiit, politecnico di torino, cnit email: francesco.malandrino@ieiit.cnr.it carla fabiana chiasserini politecnico di torino, cnr ieiit, cnit email: chiasserini@polito.it abstractin g research, it is traditionally assumed that ver- tical industries (a.k.a verticals) set the performance requirements for the services they want to offer to mobile users, and the mobile operators alone are in charge of orchestrating their resources so as to meet such requirements. motivated by the observation that successful orchestration requires reliable trafc predictions, in this paper we investigate the effects of having the verticals, instead of the mobile operators, performing such predictions. leveraging a real world, large scale, crowd sourced trace, we nd that involving the verticals in the prediction process reduces the prediction errors and improves the quality of the resulting orchestration decisions. i. introduction and related work ",
        "Subsections": [],
        "Groundtruth": "The text discusses a collaborative approach between vertical industries and mobile operators for traffic forecasting. Traditionally, mobile operators set performance requirements and manage resources to meet them. However, involving verticals in traffic predictions can reduce errors and enhance orchestration decision quality. Real-world data shows this collaboration improves forecasting accuracy and overall performance."
    }
]