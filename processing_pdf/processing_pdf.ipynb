{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ensure that the imported .py file will get auto imported and updated whenever there is a change\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install PyMuPDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbxlvBEhC-kO",
        "outputId": "faebd136-d3f6-4d9b-aa34-09c270578605"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Claud\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Claud\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Claud\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import nltk, os, re\n",
        "from nltk.tokenize import word_tokenize #Used to extract words from documents\n",
        "from nltk.stem import WordNetLemmatizer #Used to lemmatize words\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import processing_pdf\n",
        "pd.options.display.max_colwidth = 1000\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_V2cOEPzSe5U"
      },
      "outputs": [],
      "source": [
        "\n",
        "project_path = os.path.dirname(os.getcwd())\n",
        "project_data_path = project_path + \"/data/paper_pdf\"\n",
        "project_processed_data_path = project_path + \"/processed\"\n",
        "if not os.path.exists(project_data_path):\n",
        "    os.makedirs(project_data_path)\n",
        "if not os.path.exists(project_processed_data_path):\n",
        "    os.makedirs(project_processed_data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Claud\\Documents\\PDF_conversion/data/paper_pdf/An Empirical Survey on Long Document Summarization.pdf\n",
            "Auto generated table of content:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[[1, 'Abstract', 1],\n",
              " [1, '1 Introduction', 1],\n",
              " [1, '2 Fundamentals of Long Document Summarization', 3],\n",
              " [2, '2.1 Length of Document', 3],\n",
              " [2, '2.2 Breadth of Content', 4],\n",
              " [2, '2.3 Degree of Coherence', 4],\n",
              " [1, '3 Datasets', 4],\n",
              " [2, '3.1 Corpora', 5],\n",
              " [2, '3.2 Data Metrics', 5],\n",
              " [2, '3.3 Intrinsic Characteristics of Datasets', 6],\n",
              " [2, '3.4 Fine-grained Analysis on ArXiv', 9],\n",
              " [1, '4 Models', 10],\n",
              " [2, '4.1 Overview', 10],\n",
              " [2, '4.2 Main Architecture and its Mechanisms', 12],\n",
              " [2, '4.3 Mechanisms of Transformer-based Architectures', 14],\n",
              " [2, '4.4 Summary of Trends in Long Document Summarization Systems', 17],\n",
              " [1, '5 Multi-dimensional Analysis of Long Document Summarizers', 18],\n",
              " [2, '5.1 Implementation', 19],\n",
              " [2, '5.2 Results and Analysis', 20],\n",
              " [2, '5.3 Limitation of Experiment', 22],\n",
              " [1, '6 Metrics', 23],\n",
              " [2, '6.1 Relevance', 23],\n",
              " [2, '6.2 Factual Consistency', 24],\n",
              " [2, '6.3 Conciseness and Semantic Coherence', 25],\n",
              " [2, '6.4 Research Efforts on Metrics in the Long Document Domain', 25],\n",
              " [1, '7 Applications', 26],\n",
              " [1, '8 General Challenges and Future Directions', 26],\n",
              " [2, '8.1 Neural Models and Long Sequence Reasoning', 26],\n",
              " [2, '8.2 Summarizer with Automatic Discourse Parsers/Annotator', 27],\n",
              " [2, '8.3 End-to-end Neural Summarizer with Content Selection Mechanism', 27],\n",
              " [2, '8.4 Quality and Diversity of Benchmark Dataset', 27],\n",
              " [2, '8.5 Practicality of Summarization Metrics', 28],\n",
              " [1, '9 Conclusion', 28],\n",
              " [1, 'References', 28],\n",
              " [1, '10 Supplementary materials', 36],\n",
              " [2, '10.1 Long Document Summarization Systems', 36],\n",
              " [2, '10.2 Graph-based Ranking Algorithm in Experimental Section', 36],\n",
              " [2,\n",
              "  '10.3 BERT NSP - Assessing Semantic Coherence of Candidate Summaries',\n",
              "  37],\n",
              " [2, '10.4 Textual Entailment as Factual Consistency Metric', 38],\n",
              " [2, '10.5 Metric-related ACL main conference research papers', 38]]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pdf_file =  \"An Empirical Survey on Long Document Summarization.pdf\"\n",
        "# pdf_file = \"1901.00009v1.pdf\"\n",
        "# pdf_file = \"1901.00002v1.pdf\"\n",
        "# pdf_file = \"1901.00936v3.pdf\"\n",
        "# pdf_file = \"an image is worth 16 by 16 words.pdf\"\n",
        "\n",
        "doc, total_text, total_pages = processing_pdf.open_file(project_data_path + \"/\" + pdf_file)\n",
        "\n",
        "# Use processing_pdf.auto_find_toc(), which will clean up the original toc\n",
        "# table_of_content = doc.get_toc()\n",
        "table_of_content = processing_pdf.auto_find_toc(doc)\n",
        "display(table_of_content)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "starting looking for all the sections according to the provided section title info...\n",
            "save the dataframe to c:\\Users\\Claud\\Documents\\PDF_conversion/processed/An Empirical Survey on Long Document Summarization.csv\n",
            "save the dataframe to c:\\Users\\Claud\\Documents\\PDF_conversion/processed/An Empirical Survey on Long Document Summarization_meta.csv\n",
            "save the dataframe to c:\\Users\\Claud\\Documents\\PDF_conversion/processed/An Empirical Survey on Long Document Summarization.json\n",
            "\n",
            "Title: An Empirical Survey on Long Document Summarization:Datasets, Models and Metrics\n",
            "\n",
            "Authors:HUAN YEE KOH, Monash University, AustraliaJIAXIN JU, Monash University, AustraliaMING LIU∗, Deakin University, AustraliaSHIRUI PAN∗†, Monash University, Australia\n",
            "\n",
            "Other info: \n",
            "\n",
            "Abstract:Long documents such as academic articles and business reports have been the standard format to detail outimportant issues and complicated subjects that require extra attention. An automatic summarization systemthat can effectively condense long documents into short and concise texts to encapsulate the most importantinformation would thus be significant in aiding the reader’s comprehension. Recently, with the advent of neuralarchitectures, significant research efforts have been made to advance automatic text summarization systems,and numerous studies on the challenges of extending these systems to the long document domain have emerged.In this survey, we provide a comprehensive overview of the research on long document summarization anda systematic evaluation across the three principal components of its research setting: benchmark datasets,summarization models, and evaluation metrics. For each component, we organize the literature within thecontext of long document summarization and conduct an empirical analysis to broaden the perspective oncurrent research progress. The empirical analysis includes a study on the intrinsic characteristics of benchmarkdatasets, a multi-dimensional analysis of summarization models, and a review of the summarization evaluationmetrics. Based on the overall findings, we conclude by proposing possible directions for future exploration inthis rapidly growing field. CCS Concepts: • Information systems →Summarization; • Computing methodologies →Informationextraction. Additional Key Words and Phrases: document summarization, datasets, neural networks, language models,Transformer\n"
          ]
        }
      ],
      "source": [
        "#uncomment this list to customize table-of-content\n",
        "# table_of_content = [[1, 'I. INTRODUCTION', 1],\n",
        "#  [1, 'II. SFC BASED ON IPV6 SEGMENT ROUTING', 2],\n",
        "#  [1, 'III. DESIGN OF THE SRV6 PROXY', 4],\n",
        "#  [2, 'A. General Concepts and State-of-the-art', 4],\n",
        "#  [2, 'B. SRNKv1', 5],\n",
        "#  [2, 'C. SRNKv2', 7],\n",
        "#  [2, 'D. Implementation of other SR proxy types', 8],\n",
        "#  [1, 'IV. TESTING ENVIRONMENT', 8],\n",
        "#  [1, 'V. PERFORMANCE ANALYSIS', 9],\n",
        "#  [1, 'VII. CONCLUSIONS', 11]]\n",
        "\n",
        "# separate content into sections\n",
        "processing_pdf.clear_processed_folder(project_processed_data_path)\n",
        "title, authors, other_info, abstract = processing_pdf.find_meta_data(doc, table_of_content)\n",
        "df_meta = pd.DataFrame([title, abstract]).T\n",
        "df_meta.columns = [\"Title\", \"Abstract\"]\n",
        "ds, json_dict = processing_pdf.separate_content(total_text, table_of_content)\n",
        "processing_pdf.save_dataframe(ds, df_meta, json_dict, project_processed_data_path,  pdf_file.rsplit(\".\", 1)[0])\n",
        "# extract images\n",
        "processing_pdf.find_images(doc, table_of_content, total_pages, project_processed_data_path)\n",
        "\n",
        "# display(ds)\n",
        "print(f\"\\nTitle: {title}\")\n",
        "print(f\"\\nAuthors:{authors}\")\n",
        "print(f\"\\nOther info: {other_info}\")\n",
        "print(f\"\\nAbstract:{abstract}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# open json file\n",
        "with open(project_processed_data_path + \"/\" + pdf_file.rsplit(\".\", -1)[0] + \".json\") as f:\n",
        "    data = json.load(f)\n",
        "    for item in data:\n",
        "        display(item)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
