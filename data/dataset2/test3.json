[
  {
    "Section_Num": "Abstract",
    "Section": "Abstract",
    "Text": "arxiv:190 00144v2 20 dec 2020 continuity of the mackey-higson bijection by alexandre afgoustidis & anne-marie aubert abstrac ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "1",
    "Section": "1. Introduction",
    "Text": "let g be the group of real points of a connected reductive algebraic group defined over   we will refer to this correspondence as the mackey-higson bijectio  for real groups, the homeomorphic pieces are defined through david vogan's theory of lowest k-type  the pieces are stitched together differently in both duals; but taking k-theory somehow blurs out that fact: the connes-kasparov isomorphism can actually be obtained in a rather elementary way from the topological properties of the mackey-higson bijection let us further comment on property we should mention that most of the available information on the mackey bijection has been obtained through the existence of a continuous family tbelong to of groups interpolating between g and g0: see the properties we shall need are essentially algebraic, often boiling down to a careful analysis of lowest k-types, and do not rely on deformation theor  we close this introduction with a few remarks on the possibility that analogues of properties and above may hold true for the admissible dua  furthermore, both admissible duals are stratified according to lowest k-type theor  eyal subag conjectured in that the same property holds for arbitrary   he verified this in for g = sl using the algebraic framework of furthermore, it seems that subag's conjecture may be the final word on the algebraic analogue of property abov  as a result, we shall remain entirely confined within the tempered duals her  acknowledgment ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "2",
    "Section": "2. Topology of the tempered dual",
    "Text": " harish-chandra decompositio  let us first recall how the general shape of harish-chandra's plancherel formula yields an infinite direct product decomposition of m  the presentation is modeled on the p-adic case, more precisely, on schneider and zink's tempered version of the bernstein decomposition of the category of admissible representations let us now fix a discrete pair for all this, original sources include ; a convenient reference is we refer to for the parallel with the non-archimedean cas  connected components of the tempered dual e   although it is well-knownwe will sketch a proo  proo  that is indeed a closed subset we will assume that our pair has that propert  see for instance the claim easily follow  vogan's lowest-k-type picture for the decompositio  see for connected g, and for a class of groups that includes the current on  closure of a subset of e   proo  we can identify the unitary unramified characters of ma with the unitary characters of   all representations in b are irreducible and tempere  we shall need a consequence of the above results, taken fromthat says what happens when one considers the continuous parameters for a convergent sequence of irreducible tempered representation ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "3",
    "Section": "3. Topology of the motion group dual and remarks on the Mackey-Higson bijection",
    "Text": " the fact that the statement is correct is a reformulation of parts of the construction of the correspondence in proo  to see how this translates into theorem  3 in our case, let us first give a statement closer in spirit to this leads to the statement in theorem  ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "4",
    "Section": "4. Continuity of the Mackey-Higson bijection",
    "Text": " preliminaries on the reductive sid  we now take up the setting of notations   by corollary   strata in the weyl chambe  these do not depend on   given observation   mackey parameter  write vlseq for that representatio  proo  proo  thus the latter function must be zer  continuity of the mackey-higson bijection 11   verification of baggett's criterio  by the argument already used for lemma   given baggett's criterion   proo  the lemma follow ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "References",
    "Section": "References",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "Abstract",
    "Section": "Abstract",
    "Text": "arxiv:190 00145v3 19 jan 2022 on the various notions of poincar e duality pair john   klein, lizhen qin, and yang su abstrac  we establish a number of foundational results on poincar e spaces which result in several application  one application settles an old conjecture of   wall in the affrmativ  we also prove a relative version of a result of gottlieb about poincar e duality and fibration  contents",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "1",
    "Section": "1. Introduction",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "2",
    "Section": "2. Preliminaries",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "3",
    "Section": "3. The Thom Isomorphism",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "4",
    "Section": "4. Poincar\u00e9 Duality",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "5",
    "Section": "5. Doubling",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "6",
    "Section": "6. Finite Coverings",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "7",
    "Section": "7. Fibrations",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "8",
    "Section": "8. Historical Remarks",
    "Text": "27 appendix   skew-commutativity of cup products 28 appendix   proof of theorem 3",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "Appendix",
    "Section": "Appendix A. Skew-Commutativity of Cup Products",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "Appendix",
    "Section": "Appendix B. The K\u00fcnneth Theorems",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "Appendix",
    "Section": "Appendix C. Proof of Theorem 3.6",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "References",
    "Section": "References",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "Abstract",
    "Section": "Abstract",
    "Text": "cherenkov light imaging in astroparticle physics   a selection of detection principles is discussed and corresponding experiments are presented together with breakthrough-results they achieve  some future developments are highlighted",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "1",
    "Section": "1 Introduction",
    "Text": "in 2018, we commemorate the 60th anniversary of the award of the nobel prize to pavel alexeyewich cherenkov, ilya mikhailovich frank and igor yevgenyevich tamm for the discovery and the interpretation of the cherenkov effect the impact of this discovery on astroparticle physics is enormous and persisten  the geometry of cherenkov emission allows for reconstructing the particle trajectory, provided suffciently many cherenkov photons are measured with good spatial and time resolution, and they can be separated from background ligh  the recorded cherenkov intensity furthermore can serve as a proxy for the particle energ  usually, the detectors need to be shielded from ambient light and employ photo-sensors that are sensitive to single photons with nanosecond time resolutio  email address: katz@physi uni-erlange de photomultiplier tubes and, more recently, silicon photomultipliers are the standard sensor type  they are provided by specialised companies who cooperate with the experiments in developing and optimising sensors according to the respective specific needs in the following, the detection principles of different types of cherenkov experiments in astroparticle physics are presented together with selected technical details and outstanding result ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "2",
    "Section": "2 Ground-based gamma-ray detectors",
    "Text": "while the atmosphere is transparent to electromagnetic radiation in the radio and optical regimes, it is not for x-rays and gamma-ray  gamma-rays below 20 gev are only accessible to satellite experiment  at significantly higher energies, satellite instruments rapidly lose sensitivity to the steeply decreasing gamma-ray flux due to their limited collection area, and groundbased observations take over imaging air cherenkov telescopes require clear, preferentially moon-less nights and sites with negligible light pollution and an elevation of typically 2 k  they are pointing instruments with a field of view of a few degrees in diamete  alternatively, timing arrays at higher altitude can directly measure the gammainduced particle cascad  they cover a significant fraction of the sky, albeit with a higher energy threshold than iacts and inferior sensitivity at energies below about 50 te ",
    "Subsections": [
      {
        "Section_Num": "2_1",
        "Section": "2.1 Detection principle of Cherenkov telescopes",
        "Text": "upon entering the atmosphere, gamma-rays with gev energies and above initiate electromagnetic cascades, extending longitudinally over several kilometres and having their maximum typically at a height of 10 km above sea leve 00146v1 1 jan 2019 along the cascad  since all relevant particles in the cascade propagate with a speed very close to that of light, the cherenkov radiation arrives at ground in a flash of only a few nanoseconds duration and can thus be separated from the night-sky backgroun  it is observed with one or several iacts that have a camera made of photomultiplier or sipm pixels in their focal plan  an important parameter, governing   the ability for large-area sky scans, is the field of view of the camera each camera pixel corresponds to a certain solid angle of the light arrival directio  a telescope pointed to a gamma-ray source thus sees the start of the cascade close to its centre, from where it propagates outwar  an example of a cascade observed by all five   telescopes is shown in fi  the gamma-ray direction and energy are reconstructed from the recorded light pattern and intensit  typical resolutions of   atmospheric cascades induced by cosmic-ray protons or heavier nuclei are three orders of magnitude more frequent than gamma-rays, but can be effciently suppressed using the topology of the camera pictures telescope  each hexagonal pixel corresponds to one photomultiplie  the colour code indicates the measured light intensit  picture provided by the   collaboratio  the sensitivities of current and future gamma-ray telescopes are compared in fi ",
        "Subsections": [],
        "Groundtruth": ""
      },
      {
        "Section_Num": "2_2",
        "Section": "2.2 Current Cherenkov telescopes",
        "Text": " the main figure 2: differential flux sensitivity of the current and the future ground-based iact  also shown are the corresponding sensitivities for the timing array haw  see sections  2- 4 for more details on these instrument  the green, dashdotted lines indicate the sensitivity of the satellite instrument fermilat for two different directions of observatio  see sections  2- 4 for more details on these instrument  picture provided by the cta collaboratio  instrument is shown in fi  they have established gamma-ray astronomy as a major field of observational astrophysics and provided a wealth of scientific information on high-energy processes in the univers  see and references therein for detail  fact has demonstrated that sipms can take very high rates, enabling observations even in full-moon night ",
        "Subsections": [],
        "Groundtruth": ""
      },
      {
        "Section_Num": "2_3",
        "Section": "2.3 Cherenkov Telescope Array",
        "Text": " cta will be installed in two sites, one in the northern hemisphere on la palma and the other in the southern hemisphere close to the eso paranal observatory in chil  telescope system in namibi  picture provided by the   collaboratio  table 1: main parameters of the currently operational iacts and references to their web page  the second magic telescope came in operation 2009, the large   telescope in 201  magic veritas site namibia la palma, arizona, us spain altitude   the first lst was recently inaugurated in la palm  cta is expected to start operation in 2022 with partial arrays and to be completed in 202  the cta sensitivity promises major progress in gamma-ray astronomy and high-energy astrophysics once cta will take dat  the major scientific targets of cta are cosmic particle acceleration, probing extreme environments such as close to neutron stars and black holes, and fundamental physics such as investigations into the nature of dark matte  cta will be operated as an observatory, with some key science projects reserved for the cta collaboratio  also, the dark matter programme and the exploitation of cta data beyond gamma-rays are key science project ",
        "Subsections": [],
        "Groundtruth": ""
      },
      {
        "Section_Num": "2_4",
        "Section": "2.4 Timing arrays",
        "Text": " water tanks in which through-going charged particles generate cherenkov light detected by pmt  from measuring the arrival time of the shower front as a function of the horizontal position, the direction of the incoming particle can be determine  the intensity of the shower and the size of its footprint on ground yield an energy estimat  similarly to iacts, leptonic and hadronic showers are separated using the different event topologies and muon content in the detector arra  figure 4: photograph of the hawc detector array in mexic  picture provided by the hawc collaboratio  the currently most sensitive detector of this type is hawc near puebla in mexico, at an altitude of 4100   hawc consists of 300 water tanks covering   a photograph of hawc is shown in fi  hawc reaches an angular resolution of about   it is particularly well suited to observe very high-energy gamma-ray emission from extended object  as an example, a recent measurement of the tev 3 gamma-ray flux from the vicinity of two pulsars has strongly constrained the hypothesis that positrons from these/such pulsars is responsible for the unexpectedly high flux of cosmic-ray positrons at earth",
        "Subsections": [],
        "Groundtruth": ""
      }
    ],
    "Groundtruth": ""
  },
  {
    "Section_Num": "3",
    "Section": "3 Neutrino telescopes",
    "Text": " the basic principle of neutrino telescopes is to observe cherenkov light from charged secondary particles emerging from neutrino reactions and passing a detector volume filled with a transparent dielectric medium and observed by an arrangement of pmts for the low-energy regimedetectors are installed in deep-underground caverns and the pmts cover a large percentage of the detector volume outer surfac  for high energiesnaturally abundant volumes of water or ice are instrumented with three-dimensional arrays of pmts",
    "Subsections": [
      {
        "Section_Num": "3_1",
        "Section": "3.1 Low-energy neutrino detectors",
        "Text": " the energy ranges indicate the typical observation windows of cherenkov detector  the physics questions addressed through these measurements are neutrino oscillations, the processes in the sun and in supernovae, and searches for relic neutrinos from unresolved supernovae and for possible sterile neutrino  two detectors have provided outstanding results: superkamiokande in japan and the sudbury neutrino observatory in canad  sk is installed in a cavern of a mine, with an overburden of 1000 m of roc  the outermost 18 ktons are used as veto layer against incoming charged particle  the inner volume is observed by more than 11000 20-inch pmts, the veto layer by about 1900 8-inch pmt  sk is operational since 1996, with a period of reduced sensitivity in 2001-2006 as the consequence of an accident destroying more than 50% of the large pmt  sk has played a crucial role in the discovery and precision investigation of neutrino oscillation  while one piece of evidence came from confirming the solar neutrino deficit and muon-like event  the upper plots are for visible energy below   the high-energy muons are combined with partially contained events the error bars are the measured data, the red dotted lines show the prediction without oscillations and with the best-fit oscillation scenario, respectivel  plot taken from solar neutrinos were measured than expected by the solar standard model), the breakthrough was the observation of oscillations of atmospheric neutrinos in 199  the final confirmation that the solar neutrino deficit is a neutrino flavour transition effect had to await the sno results see for a summary of recent sk result  sno is located below an overburden of 2100 m of rock in a deep mine in ontario, canad  it uses a cavity with a diameter of 22 m and a height of 34   the core of the detector is an acrylic vessel filled with 1 kton of heavy water and observed by more than 9000 8-inch pmt  the remaining volume of the cavern is filled with normal waterserving as a veto volume against incoming charged particle  the experiment started data taking in 1999 and was operated until 200  currently a follow-up experimental phase is in preparation, employing a liquid-scintillator filling doped with tellurium for the search for neutrino-less double beta deca  the gamma radiation comptonscatters on electrons which then generate cherenkov ligh  the overall neutrino flux is found to be consistent with the model expectation, but to consist only to about a third of electron neutrinos this finding solved the puzzle of the solar neutrino deficit and established a consistent standard description of neutrino oscillation  the grey band shows the sk resul  point and error contours are the combined result from the cc and nc measurement  the dashed lines represent the prediction by the standard solar mode  plot taken from in 2015, the nobel prize in physics was awarded to takaaki kajita and arthur   mcdonald for the discovery of neutrino oscillations, which shows that neutrinos have mas  note that the nobel committee selected the pictures reproduced in fig  5 and 6 for the corresponding announcemen ",
        "Subsections": [],
        "Groundtruth": ""
      },
      {
        "Section_Num": "3_2",
        "Section": "3.2 Deep-ice and deep-water neutrino telescopes",
        "Text": "for neutrino energies beyond some 100 gev, the detectors discussed in the previous subsection lack sensitivity, simply because the target volume is too small to yield suffcient event rates and also because events at such energies are too large to be contained in the detector volum  they are constructed by deploying arrays of vertical structures carrying pmts to the deep ice of the south pole or the deep water of the mediterranean sea or the lake baika  the water/ice layer above the sensors completely shields the dayligh  the pmts are included in pressureresistant glass spheres which also house the voltage supplies, the front-end electronics and calibration instruments the strings are connected to surface/shore by cables for data transport, operation control and electrical power suppl  particle-induced events from neutrinos or atmospheric muons are recognised and reconstructed using the space-time pattern of cherenkov photons recorded by the optical module  see for more detail  figure 7: schematic of the icecube detecto  picture provided by the icecube collaboratio  note that the universe is transparent to neutrinos of all energies, whereas the reach of electromagnetic radiation is severely constrained for energies exceeding some 10 tev due to gamma-ray interaction with ubiquitous radiation field  the currently most sensitive high-energy neutrino telescope is icecube at the south pole, operational in full configuration since 201  a sub-volume is instrumented more densely than the rest to detect neutrinos with energies down to 10 gev the icetop cherenkov surface array serves for cosmic-ray studies and also provides some veto capability against muons and neutrinos from air shower  a schematic of icecube is shown in fi  main systematics come from the inhomogeneity of the optical ice properties and from light scattering, which blurs the space-time pattern of cherenkov photon  this detection became possible by relating an icecube neutrino alert with electromagnetic observations, and was confirmed by archival icecube dat  see for these and further icecube result  the icecube detector will be further developed and extende  as a first step, 7 additional strings with newly developed optical modules and calibration devices will be added to the deep core regio  this project has currently been approved by the us national science foundationand deployment is expected in 2022/2  if all works well, icecube-gen2 could be installed 2025-203  see for detail  complementing icecube in the field of view and in the major systematic uncertainties, the antares neutrino telescope in the mediterranean sea offthe french shore near toulon is operational in full configuration since 200  the strings are connected to a junction box on the sea bed and from there by an electro-optical cable to shor  all pmt hits exceeding a signal height corresponding to  3 photo-electrons are read out and sent to shore, where the data are filtered by an online computer cluste  antares instruments a water volume of about  015 km3 and is thus intrinsically significantly less sensitive than icecub  main instrumental systematics are due to the inhomogeneity of detector and deep-sea environment in time, and due to background light from bioluminescenc  see for a selection of important antares result  antares has proven the feasibility of deep-sea neutrino detection and has paved the way towards the next-generation neutrino telescope in the mediterranean sea, km3net the km3net  0 detector will consist of two installations, arca1 offthe eastern sicilian shore and orca2 close to the antares sit  the prime objective of arca is neutrino astronomy in an energy range beyond a few te  picture provided by the km3net collaboratio  will be better than in antares orca will use the same basic detector technology as arca, but be much more densely instrumented orca will focus on neutrino oscillation physics with atmospheric neutrinos in the energy range of a few gev to a few 10 gev, and in particular on measuring the neutrino mass orderin  an option to investigate cp violation by directing a neutrino beam from protvino to orca is under discussio  a number of new technical developments has been achieved for km3net that improve functionality, cost-effectiveness, risk mitigation and construction tim  amongst them are equipressure vertical cablesa new deployment strategyand a multi-pmt digital optical module the construction of both km3net detectors has begun and is expected to be completed 2021/2  a further extension with four more arca blocks is envisioned but not yet negotiate  see for expected km3net sensitivities, in particular to a diffuse cosmic neutrino flux as observed by icecube and to the neutrino mass orderin  each string carries 36 optical modules equipped with 10-inch, downward-looking pmt  currently three clusters are operational, and two more are to be deployed per yea  first results from gvd have been reported at the neutrino conference 2018 ",
        "Subsections": [],
        "Groundtruth": ""
      },
      {
        "Section_Num": "3_3",
        "Section": "3.3 Other neutrino detectors",
        "Text": " a first study of this option has been performed with magic ; other experimental approaches to detect tau neutrinos through the same mechanism are discussed in",
        "Subsections": [],
        "Groundtruth": ""
      }
    ],
    "Groundtruth": ""
  },
  {
    "Section_Num": "4",
    "Section": "4 Cosmic-ray and hybrid detectors",
    "Text": " corresponding hybrid infrastructures are typically targeting cosmic rays, in several cases in combination with gamma-ray and/or neutrino measurement  figure 9: photograph of a water cherenkov detector and a building housing six fluorescence telescopes of the auger infrastructur  picture provided by the auger collaboratio  auger targets cosmic rays with energies of about 101 5-1021 ev and searches for gamma-rays and neutrinos in the same energy interva  the water detectors yield a measurement of the footprint of an air shower, from which the energy can be inferred, and act as timing arrays for the direction measuremen  they have a duty cycle of close to 100%.  the fluorescence telescopes measure the light intensity along the shower and, in stereoscopic observations, determine shower position and directio  they thus provide an independent determination of energy and direction, which is cross-calibrated against and combined with the array measuremen  even though the fluorescence telescopes can only be operated in clear, moon-less nights and thus have a limited duty cycle, their contribution to the control of systematics and thus to the resulting experimental precision is essentia  figure 10: cosmic-ray energy spectrum beyond 101 5 ev measured by auge  the power-law behaviour below and above the ankle is indicated by the dashed line 2 ev the spectrum steeply decrease  picture from auger started operation with a partial installation in 2001 and has achieved major breakthroughs, amongst others a precise measurement of the cosmic ray spectrum above 101 5 ev indicating a clear cut-offbeyond 101 2 evand the first detection of a dipole anisotropy of the arrival direction of cosmic rays at highest energies see for a summary of these and further recent result  auger is currently upgraded to augerprime with new fast electronics and in particular with scintillator detectors on top of the cherenkov tanks to improve the discrimination between hadronic and leptonic shower components on groun  a different type of hybrid detector is nevoda comparatively small ground-level installation in moscow, russi  it is unique in combining a   the combination of muon tracking and cherenkov measurements offers promising options to cross-check simulations and calibrate   acceptances of optical module  also, the simultaneous measurement of muon directions and energies can be interesting for cosmic-ray studie  see for more detail ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "5",
    "Section": "5 Conclusion and Outlook",
    "Text": "cherenkov detectors play a crucial role in gamma-ray, neutrino and cosmic-ray astroparticle physic  many of the recent breakthrough-results would not have been possible without them, and they are essential for the future experiments being constructed or planne ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "Abstract",
    "Section": "Abstract",
    "Text": "vojnovic@ls a a zhou3@ls a uk the problem of assigning ranking scores to items based on observed comparison data,   one of the most popular statistical models of ranking outcomes is the bradley-terry model for paired comparisonsand its extensions to choice and full ranking outcome  the problem of computing ranking scores under the bradley-terry models amounts to estimation of model parameter  we establish tight characterizations of the convergence rate for the mm algorithm, and show that it is essentially equivalent to that of a gradient descent algorith  for the maximum likelihood estimation, the convergence is shown to be linear with the rate crucially determined by the algebraic connectivity of the matrix of item pair co-occurrences in observed comparison dat  for the bayesian inference, the convergence rate is also shown to be linear, with the rate determined by a parameter of the prior distribution in a way that can make the convergence arbitrarily slow for small values of this paramete  we propose a simple modification of the classical mm algorithm that avoids the observed slow convergence issue and accelerates the convergenc  the key component of the accelerated mm algorithm is a parameter rescaling performed at each iteration step that is carefully chosen based on theoretical analysis and characterisation of the convergence rate",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "1",
    "Section": "1 Introduction",
    "Text": " given a set of items, the goal is to infer ranking scores of items or an ordering of items based on observed data that contains partial orderings of item  a common scenario is that of paired comparisons,   an iterative optimization algorithm for the maximum likelihood parameter estimation of the bradley-terry model has been known since the original work of zermelo lange et a  showed that this algorithm belongs to the class of mm optimization algorithm  here mm refers to either minorize-maximization or majorizeminimization, depending on whether the optimization problem is maximization or minimization of an objective functio  lange provided a book on mm algorithms and hunter and lange provided a tutoria  mairal established some convergence results for incremental mm algorithm  in a seminal paper, hunter derived mm algorithms for generalized bradley-terry models as well as sufficient conditions for their convergence to ml estimators using the framework of mm optimization algorithm  for the bradley-terry model of paired comparisons, a necessary and sufficient condition for the existence of a ml estimator is that the directed graph whose vertices correspond to items and edges represent outcomes of paired comparisons is connecte  in other words, the set of items cannot be partitioned in two sets such that none of the items in one partition won against an item in other partitio  a bayesian inference method for generalized bradley-terry models was proposed by caron and doucetshowing that classical mm algorithms can be reinterpreted as special instances of expectation-maximization algorithms associated with suitably defined latent variables and proposed some original extension  this amounts to mm algorithms for maximum a posteriori probability parameter estimation, for a specific family of prior distribution  the first package uses a fisher scoring algorithmwhile the latter two use mm algorithms while the conditions for convergence of mm algorithms for generalized bradley-terry models are well understood, to the best of our knowledge, not much is known about their convergence rates for either ml or map estimatio  in this paper, we close this gap by providing tight characterizations of convergence rate  our results identify key properties of input data that determine the convergence rate, and in the case of map estimation, how the convergence rate depends on prior distribution parameter  our results show that mm algorithms, commonly used for map estimation for generalized bradley-terry models, can have a slow convergence for some prior distribution hajek et a borkar et a chen et a  note that the question about statistical estimation accuracy and computation complexity tradeoff is out of the scope of our paper, and this was studied in the above cited paper  the focus of our work is on convergence properties of first-order iterative optimization methods for parameter estimation of bradleyterry model  here first-order refers to optimization methods that are restricted to value oracle access to gradients of the optimization objective function, thus not allowing access to second-order properties such as values of the hessian matri  specifically, we are interested in convergence properties of first-order methods for ml and map estimation objective  it is noteworthy that some recently proposed algorithms show empirically faster convergence rate than mm,  g, negahban et a maystre and grossglauseragarwal et a but it is hard to apply them for the map estimation objectiv  we thus restrict our attention to mm and gradient descent algorithms which are able to solve both mle and map optimization problem  a preliminary version of our paper was published in vojnovic et a which contains results on convergence rates of gradient descent and mm algorithms in the present paper, we extend our prior work by proposing new accelerated algorithms and establishing their theoretical guarantees as well as demonstrating their efficiency through numerical evaluations summary of our contributions we present tight characterizations of the rate of convergence of gradient descent and mm algorithms for ml and map estimation for generalized bradley-terry model  our results show that both gradient descent and mm algorithms have linear convergence with convergence rates differing only in constant factor  an iterative optimization algorithm that has linear convergence is generally considered to be fast in the space of first-order optimization algorithms, and many first-order algorithms cannot guarantee a linear convergenc  we provide explicit bounds on convergence rates that provide insights into which properties of observed comparison data play a key role for the rate of convergenc  intuitively, a quantifies how well is the graph of paired comparisons connecte  here a is the fiedler valuec  the fiedler value of a matrix of paired comparison counts is known to play a key role in determining the mle accuracy,  g, hajek et a shah et a  this is different from the problem of characterizing the number of iterations needed for an iterative optimization algorithm to compute a ml or a map parameter estimate satisfying an error tolerance condition, which is studied in this pape  this bound is shown to be tight for some input data instance  we observe that the convergence time for the map estimation problem can be arbitrarily large for small enough parameter beta, where small values of parameter beta correspond to less informative prior distribution  our results identify a slow rate of convergence issue for gradient descent and mm algorithms for the map estimation proble  while the map estimation alleviates the issue of the non-existence of a ml estimator when the graph of paired comparisons is disconnected, it can have a much slower convergence than ml when the graph of paired comparisons is connecte  this acceleration method resolves the slow convergence issue of classic mm algorithm for the map estimation for generalized bradleyterry model  the acceleration method normalizes the parameter vector estimate in each iteration of the gradient descent or mm algorithm using a particular normalization that ensures that the value of the objective function is non decreasing along the sequence of estimated parameter vectors and that the objective function satisfies certain smoothness and strong convexity properties that ensure high convergence rat  this amounts to a slight modification of the classical mm algorithm that resolves the identified convergence issu  this acceleration method is derived by using a theoretical framework that may be of general interes  this framework can be applied to different statistical models of ranking data and prior distributions for bayesian inference of parameters of these model  we present numerical evaluation of the convergence time of different iterative optimization algorithms using input data comparisons from a collection of real-world dataset  these results demonstrate the extent of the slow convergence issue of the existing mm algorithm for map estimation and show significant speed ups achieved by our accelerated mm algorith  our theoretical results are established by using the framework of convex optimization analysis and spectral theory of matrice  in particular, the convergence rate bounds are obtained by using concepts of smooth and strongly convex function  this approach transforms the parameter estimator in each iteration so that certain conditions are preserved for the gradient vector and the hessian matrix of the objective functio  for generalized bradley-terry models, this transformation turns out to be simple, yielding a practical algorith  organization of the paper in section 2, we present problem formulation and some background materia  section 4 presents our accelerated algorithms for map estimatio  section 5 contains our numerical result  we conclude in section  ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "2",
    "Section": "2 Problem formulation",
    "Text": " the bradley-terry model of paired comparisons was studied by many,   all our convergence results are for the model with parameter   our results apply to all these different model  in the main body of the paper, we focus only on the bradley-terry model for paired comparisons in order to keep the presentation simpl  the maximum likelihood optimisation problem is a convex optimization proble  note, however, that the objective function is not a strictly concave function as adding a common constant to each element of the parameter vector keeps the value of the objective function unchange  map estimation problem an alternative objective is obtained by using a bayesian inference framework, which amounts to finding a maximum a posteriori estimate of the parameter vector under a given prior distributio  mm algorithms the mm algorithm for minimizing a function f is defined by minimizing a surrogate function that majorizes   majorization surrogate functions are used for minimization of convex functions, and minorization surrogate functions are used for maximization of concave function  our goal in this paper is to characterize the rate of convergence of mm algorithms for generalized bradley-terry model  we also derive convergence rates for gradient descent algorithm  it is natural to consider gradient descent algorithms as they belong to the class of first-order optimization methods intuitively, the rate of convergence of an iterative algorithm quantifies how fast the value of the objective function converges to the optimum value with the number of iteration  background on convex analysis we define some basic concepts from convex analysis that we will use throughout the pape  a proof of the last claim can be found in lemma  4 in bubeck",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "3",
    "Section": "3 Convergence rates",
    "Text": "in this section, we present results on the rate of convergence for gradient descent and mm algorithms for ml and map estimation for the bradley-terry model of paired comparison  these results are then used to derive convergence rate bounds for the bradley-terry mode  these extensions are established by following the same main steps as for the bradley-terry model of paired comparison  the differences lie in the characterization of the strong-convexity and smoothness parameter  the results provide characterizations of the convergence rates that are equivalent to those for the bradley-terry model of paired comparisons up to constant factor  we provide details in section  ",
    "Subsections": [
      {
        "Section_Num": "3_1",
        "Section": "3.1 General convergence theorems",
        "Text": " this result is due to nesterov and a simple proof can be found in boyd and vandenberghechapter   this result follows from the following theore  the proof of theorem  2 is based on separating the gap between the objective function value at an iteration and the optimum function value in two components: one due to using a surrogate function and other due to a virtual gradient descent updat  the remaining arguments are similar to those of the proof of theorem   it is worth noting that a different set of sufficient conditions for linear convergence of mm algorithms were found in proposition  7 in mairal from theorems  ",
        "Subsections": [],
        "Groundtruth": ""
      },
      {
        "Section_Num": "3_2",
        "Section": "3.2 Maximum likelihood estimation",
        "Text": "we consider the rate of convergence for the ml parameter estimation for the bradley-terry model of paired comparison  this estimation problem amounts to finding a parameter vector that minimizes the negative loglikelihood function, with the log-likelihood function given in recall that m denotes the matrix of the item-pair co-occurrence counts and lm denotes the associated laplacian matri  by lemma   by the gershgorin circle theorem,   this is a tight characterization up to constant factor  in the context of paired comparisons, d has an intuitive interpretation as the maximum number of observed paired comparisons involving an ite  the proof of lemma   conditions and are satisfied by negative log-likelihood functions for generalized bradley-terry model  furthermore, by lemma   combining these facts with theorem   the result in corollary   we next consider the classic mm algorithm for the ml estimation problem, which uses the surrogate function in this surrogate function satisfies the following property: lemma   by theorem   from corollaries   the only difference is the value of constant   hence, both gradient descent and mm algorithm have a linear convergence, and the convergence time bound maximum a posteriori probability estimation we next consider the maximum a posteriori probability estimation proble  we first note that the negative log-a posteriori probability function has the following propertie  this has important implications on the rate of convergence which we discuss nex  by theorem   the result in corollary   vojnovic, yun and zhou: accelerated mm algorithms for ranking scores inference from comparison data 10 this bound can be arbitrarily large by taking parameter beta to be small enoug  in section  4, we will show a simple instance for which this bound is tigh  hence, the convergence time for map estimation can be arbitrarily slow, and much slower than for the ml cas  we next consider the mm algorith  by theorem   from corollaries   we next establish tightness of this rate of convergence by showing that it is achieved for a simple instanc ",
        "Subsections": [],
        "Groundtruth": ""
      },
      {
        "Section_Num": "3_4",
        "Section": "3.4 Tightness of the rate of convergence",
        "Text": "consider an instance with two items, which are compared m time  note that the smaller the value of beta, the slower the convergence for map and the mm algorithm for map can be slower for several orders of magnitude than for m  this establishes the tightness of the rate of convergence bound in corollary  ",
        "Subsections": [],
        "Groundtruth": ""
      },
      {
        "Section_Num": "3_5",
        "Section": "3.5 A simple illustrative numerical example",
        "Text": "we illustrate the rate of convergence for a simple example, using randomly generated observations of paired comparison  both algorithms have the convergence time increasing by decreasing the value of beta for strictly positive values of this paramete ",
        "Subsections": [],
        "Groundtruth": ""
      }
    ],
    "Groundtruth": ""
  },
  {
    "Section_Num": "4",
    "Section": "4 Accelerated MAP inference",
    "Text": "in this section, we present a new accelerated algorithm for gradient descent and mm algorithms for map estimatio  the key element is a transformation of the parameter vector estimate in each iteration of an iterative optimization algorithm that ensures monotonic improvement of the optimization objective along the sequence of parameter vector estimates and ensures certain second-order properties of the objective function hold along the sequence of parameter vector estimate  we first introduce transformed versions of gradient descent and mm algorithm ",
    "Subsections": [
      {
        "Section_Num": "4_1",
        "Section": "4.1 General convergence theorems",
        "Text": " condition is a standard smoothness condition imposed on x we have the following two theorem  the theorem establishes the same rate of convergence as for the gradient descent algorithm in theorem   the last theorem establishes the same rate of convergence as for classic mm algorithm in theorem   we next present a lemma which will be instrumental in showing that the pl condition in holds for the map estimation proble  note that pd is the projection matrix onto the space orthogonal to vector  ",
        "Subsections": [],
        "Groundtruth": ""
      },
      {
        "Section_Num": "4_2",
        "Section": "4.2 Convergence rate for the BT model",
        "Text": "in this section, we apply the framework developed in the previous section to characterize the convergence rate for the map parameter estimation of the bradley-terry model of paired comparison  this will allow us to apply theorems   condition holds because, in lemma   condition can be shown to hold by lemma  1 as follow  the assumptions of lemma   these two conditions are shown to hold in the following lemm  from theorem   from theorem   condition holds by lemma   conditions and hold by and in lemma  2 correspond to the bounds for the ml estimation in corollaries   from corollaries   this algorithm first performs the standard mm update in e  this can be interpreted as fixing the scale of parameters to a carefully chosen scale that is dependent on the choice of the prior distributio  note that the scaling factor c cannot be arbitrarily fixed while still preserving good convergence propertie  we demonstrate this in section 5 through numerical example  it turns out that the rescaling in algorithm 1 is roughly of the same order as the random rescaling suggested in caron and doucet therein, the authors suggested using independent identically distributed rescaling factors across different iteration steps with distribution gamm  the mode of this rescaling factor is /bet  our results show that it suffices to use a simple deterministic rescaling factor to ensure linear convergenc  our numerical example revisited we ran the accelerated mm algorithm for our numerical example and obtained the results shown in figure  ",
        "Subsections": [],
        "Groundtruth": ""
      }
    ],
    "Groundtruth": ""
  },
  {
    "Section_Num": "5",
    "Section": "5 Numerical results",
    "Text": "in this section we present evaluation of convergence times of mm algorithms for different generalized bradley-terry models for a collection of real-world dataset  our goal is to provide empirical validation of table 1 dataset propertie 338 some of the hypotheses derived from our theoretical analysi  the code and datasets for reproducing our experiments are available online at: acceleratedbradleyterr ",
    "Subsections": [
      {
        "Section_Num": "5_1",
        "Section": "5.1 Datasets",
        "Text": "we consider three datasets, which vary in the type of data, size and sparsit  the three datasets are described as follow  the dataset was collected through an online web service by the mit media lab as part of the placepulse project rich et a  this service presents the user with a pair of images and asks to select one that better expresses a given metric, or select non  the dataset contains 1,048,576 observations and covers 17 metric  we used this dataset to evaluate convergence of mm algorithms for the bradley-terry model of paired comparison  we did this for each of the three aforementioned metric  chess this dataset contains game-by-game results for 65,030 matches among 8,631 chess player  the plots in the top row indicate that the algorithm converge  the plots in the bottom row indicate linear convergenc  from the plots, we also observe that the convergence is slower for smaller values of parameter bet  chess ratings competition sonas this dataset has a large degree of sparsit  we used this dataset to evaluate convergence of the raokupper model of paired comparisons with tie  nascar this dataset contains auto racing competition result  each observation is for an auto race, consisting of the ranking of drivers in increasing order of their race finish time  the dataset is available from a web page maintained by hunter this dataset was previously used for evaluation of mm algorithms for the plackett-luce ranking model by hunter and more recently by caron and doucet we used this dataset to evaluate convergence times of mm algorithms for the plackettluce ranking mode  we summarise some key statistics for each dataset in table   for full gifgif and chess datasets, we can split the items into two groups such that at least one item in one group is not compared with any item in the other group,   in this case, there exists no ml estimate, while an map estimate always exist  in order to consider cases when an mle exists, we consider sampled datasets by restricting to the set of items such that the algebraic connectivity for this subset of items is strictly positiv  this subsampling was done by selecting the largest connected component of item ",
        "Subsections": [],
        "Groundtruth": ""
      },
      {
        "Section_Num": "5_2",
        "Section": "5.2 Experimental results",
        "Text": "we evaluated the convergence time defined as the number of iterations that an algorithm takes until a convergence criteria is satisfie  we use the standard convergence criteria based on the difference of successive parameter vector estimate  in our experiments, we also evaluated the convergence time measured in real processor time unit  we noted that they validate all the observations derived from the convergence times measured in the number of iterations, and hence we do not further discuss the  this corresponds to fixing the mode of the gamma prior marginal distributions to value   note that the case beta = 0 corresponds to ml estimatio  before discussing numerical convergence time results, we first show results validating that the mm algorithm converges and that this convergence is linea  this is shown in figure 4 for gifgif a dataset for three different values of parameter bet  for space reasons, we only include results for this datase  we observe that in all cases the log-a posteriori probability monotonically increases with the number of iterations, thus validating convergenc  we next discuss our numerical results for convergence time evaluated for the mm algorithm and accelerated mm algorithm for different datasets and choice of parameter  our numerical results are shown in table   for the values of beta considered, this increase can be for as much as two orders of magnitud  the results indicate that the log-a posteriori probability is not guaranteed to monotonically increase with the number of iteration  we also observe that a significant reduction of the convergence time can be achieved by the accelerated mm algorith  this reduction can be for as much as order 10% of the convergence time of the mm algorithm without acceleratio  these empirical results validate our theoretical result  for chess datasets, all the observations derived by using the gifgif datasets remain to hol  again, all the observations made for gifgif and chess datasets remain to hol  it is noteworthy that the mm algorithm for ml estimation converges much faster than for map estimation for sufficiently small values of parameter bet  for the cases considered, this can be for as much as three orders of magnitud  similarly, the accelerated mm algorithm converges much faster than the classical mm algorith  we present the results for the gifgif dataset  we observe that our acceleration method can converge much faster, and that there are cases for which the alternative change of scale results in no convergence within a bound on the maximum number of iteration ",
        "Subsections": [],
        "Groundtruth": ""
      }
    ],
    "Groundtruth": ""
  },
  {
    "Section_Num": "6",
    "Section": "6 Further discussion",
    "Text": " this bound is shown to be tigh  our results identify a slow convergence issue for gradient descent and mm algorithms for the map estimation problem, which occurs for small values of parameter bet  the small values of parameter beta correspond to more vague prior distribution  our results identify a discontinuity of the convergence time at =which corresponds to ml estimatio  the proposed acceleration method for the map estimation problem resolves the slow convergence issue, and yields a convergence time that is bounded by the best of what can be achieved for the ml and map estimation problem  our results provide insights into how the observed comparison data affect the rate of convergence of gradient descent and mm algorithm  the two key parameters affecting the rate of convergence are d and   we observe that when each distinct pair is compared the same number of times,   we further consider the case of random design matrices where each distinct pair of items is either compared once or not compared at all, and this is according to independent bernoulli random variables with parameter p across all distinct pairs of item  we can derive an upper bound for the convergence time, which depends only on some simple properties of the graph associated with matrix  4 in merris",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "7",
    "Section": "7 Proofs and Additional Results",
    "Text": " proof of theorem  ",
    "Subsections": [
      {
        "Section_Num": "7_2",
        "Section": "7.2 Proof of Theorem 3.2",
        "Text": " now, by the same arguments as in the proof of theorem   comparison of theorem  7 in mairal theorem   let x be the output of the mm algorithm for input  2 can be tighter than the rate of convergence bound derived from theorem   to show this consider the bradley-terry model for which we have shown in lemma   the convergence rate bound of theorem  2 is tighter than the convergence rate bound of theorem   since by lemma  ",
        "Subsections": [],
        "Groundtruth": ""
      },
      {
        "Section_Num": "7_4",
        "Section": "7.4 Proof of Lemma 3.1",
        "Text": " it remains to show that and hol ",
        "Subsections": [],
        "Groundtruth": ""
      },
      {
        "Section_Num": "7_5",
        "Section": "7.5 Proof of Lemma 3.3",
        "Text": "let y be an arbitrary vector in  ",
        "Subsections": [],
        "Groundtruth": ""
      },
      {
        "Section_Num": "7_7",
        "Section": "7.7 Proof of Lemma 3.4",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
      },
      {
        "Section_Num": "7_8",
        "Section": "7.8 The asymptote in Section 3.4",
        "Text": "we consider the case of two items, compared m time  the limit point of s as t goes to infinity is 2/bet  note that /beta is the mode of gamm  note that a goes to 0 as t goes to infinit ",
        "Subsections": [],
        "Groundtruth": ""
      },
      {
        "Section_Num": "7_9",
        "Section": "7.9 Proof of Theorem 4.1",
        "Text": " by the same steps as those in the proof of theorem  ",
        "Subsections": [],
        "Groundtruth": ""
      },
      {
        "Section_Num": "7_10",
        "Section": "7.10 Proof of Lemma 4.1",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
      },
      {
        "Section_Num": "7_11",
        "Section": "7.11 Proof of Lemma 4.2",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
      },
      {
        "Section_Num": "7_12",
        "Section": "7.12 Proof of Lemma 7.1",
        "Text": " this can be noted as follow ",
        "Subsections": [],
        "Groundtruth": ""
      },
      {
        "Section_Num": "7_13",
        "Section": "7.13 Proof of Lemma 7.5",
        "Text": " by theorem  ",
        "Subsections": [],
        "Groundtruth": ""
      }
    ],
    "Groundtruth": ""
  },
  {
    "Section_Num": "Abstract",
    "Section": "Abstract",
    "Text": "arxiv:190 00151v1 1 jan 2019 sterile neutrin  a short introductio  naumov1,* 1joint institute for nuclear research abstrac  we briefly review existing anomalies and the oscillation parameters that best describe these dat  we discuss in more detail how sterile neutrinos can be observed, as well as the consequences of its possible existenc  the current status of searches for a sterile neutrino state is also briefly reviewe ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "1",
    "Section": "1 Introduction",
    "Text": " therefore, there is not much sense in using the flavor neutrino fields, as they are not fundamental objects of the s  respectively, they can be abandoned in any consideratio  in e  in a two-neutrino model the oscillation probability is strictly periodic as a function of l/  the plane-wave model of neutrino oscillation being used elsewhere is not self-consistent and leads to a number of paradoxes a consistent model adopts wave-packet  the oscillation probability in e  the interference term in e  real analyses of neutrino oscillation data use a three-neutrino mode ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "2",
    "Section": "2 Anomalies in neutrino data",
    "Text": " there are two groups of the corresponding anomalies seen as appearance and disappearance of neutrino ",
    "Subsections": [
      {
        "Section_Num": "2_1",
        "Section": "2.1 Appearance and disappearance data",
        "Text": "appearance data include lsnd and miniboone observation  the mean neutrino energy was about 30 me  the statistical significance of the excess was about three standard deviation 8 standard deviations we note that none of these anomalies show a distinct l/p oscillation dependence predicted by e ",
        "Subsections": [],
        "Groundtruth": ""
      },
      {
        "Section_Num": "2_2",
        "Section": "2.2 Interpretation of anomalies within the hypothesis of neutrino oscillation",
        "Text": " and these are essential, minimally required ingredients to review the concept of sterile neutrin ",
        "Subsections": [],
        "Groundtruth": ""
      }
    ],
    "Groundtruth": ""
  },
  {
    "Section_Num": "3",
    "Section": "3 Concept of Sterile Neutrino",
    "Text": "1 masses of fermions in the sm the main trick to create a sterile neutrino state is to add to the sm a neutrino field without adding the fourth left-handed fields of lepton  in order to explain it, let us recall how the masses of fermions appear in the s  we simplify our consideration by examining only the dirac-type particles in order to keep the analysis simple and clea  all fermions in the sm are massless because the corresponding mass-term is not gauge invarian  the masses of fermions are acquired via the so-called yukawa interactions of a scalar field with two fields of fermion  originally, this interaction was proposed to explain an attractive potential of two nucleons interacting with each other by exchange of a scalar massive field the pio  this is not the sterile field one needs in interpretations of anomalies in neutrino data mentioned in se  x11 both terms in e  must be diagonalized in order to be interpreted as mass-term  as a result, the fields with definite masses and belonging to different sul doublets mix in their interactions with w boson as shown in e  this matrix should not be attributed, as done often in the literature, to neutrino field  instead, v is a mixing matrix of both charged leptons and neutrino  the reviewed mass-generation mechanism used fields for three charged leptons and three neutrino ",
    "Subsections": [
      {
        "Section_Num": "3_2",
        "Section": "3.2 Four right-handed neutrinos and three left-handed doublets",
        "Text": "the sterile neutrino emerges if there are four right-handed neutrino and still three left-handed doublet  assuming that this new field interacts with the higgs and left-handed neutrino fields in yukawa interactions, one arrives at non-diagonal terms like in e  equations and show that the interaction amplitudes of a sterile neutrino state with w and z are both vanishin  as mentioned in section 1 use of states with definite momentum is an approximation which fails in describing neutrino oscillation phenomeno  a more consistent approach is based on using the wave-packet model which necessarily imposes a non-zero incoherency in the production of states with different masse  the term suppressing the coherence of neutrino states is the second term in e  the first term in e  a similar calculation can be applied to all other possible final state  thus, according to e  let us now add two right-handed neutrino fields and a yukawa interaction term which should be diagonalized, similar to considerations with three generations of lepton  instead of e  the flavor part of the lagrangian given by e  and let us briefly summarize key elements of the concept of sterile neutrino  assume a disparity between the numbers of neutrino fields interacting and noninteracting directly with w and z bosons in the s  assume a mechanism of mixing active and inert neutrino fields in a generally nondiagonal mass-ter  an example of such mechanism is the yukawa interactio  other mechanisms also exis  the sterile neutrino field emerges after the diagonalization of the mass-ter  the sterile field is a superposition of at least four massive neutrino fields with nearly zero interaction amplitude with w and   this amplitude corresponds to a coherent superposition of all four neutrino state  the widths of w and z bosons decaying into any possible combination of neutrino and anti-neutrino states is proportional to the number of active neutrino fields",
        "Subsections": [],
        "Groundtruth": ""
      },
      {
        "Section_Num": "3_3",
        "Section": "3.3 How sterile neutrino state can be observed",
        "Text": "in neutrino oscillation as a deficit of the event rate and as l/p pattern in both appearance and disappearance channel  remarkably, these effects are expected for both charged and neutral current  this is in contrast to neutrino oscillation without the sterile neutrino, in which only charged currents display an oscillatory pattern and the event rates in neutral currents remains unchange  in the latter case, the oscillation to a sterile neutrino state is impossible and no event rate deficit due to sterile neutrinos can be expecte  the sterile neutrino state affects big-bang-nucleosynthesis because a larger number of neutrino species means a faster expansion rate of the univers  there are many other ways in which the sterile neutrino impacts the evolution of the univers  this contribution is determined by p i m ",
        "Subsections": [],
        "Groundtruth": ""
      },
      {
        "Section_Num": "3_4",
        "Section": "3.4 Loss of coherence for sterile neutrino",
        "Text": "the argument of the second exponential in e  is responsible for the coherency of neutrino states at production and detectio  consider neutrinos from pion decay  this reaction is important for atmospheric and accelerator neutrino  for a three neutrino mode  is very close to unity, which means no significant suppression of the interference ter  a posteriori, this calculation confirms the assumption of neutrino coherence made in the plane wave mode  is approximately equal to and the second exponential in e  takes values  8, sizably affecting the oscillation patter 5 ev2 the oscillation amplitude is suppressed by 50%.  correspondingly, an erroneous statement about the mixing angle of sterile neutrinos could be drawn in a plane wave model in which these suppressions are ignore  a careful analysis of neutrino oscillation data requires a wave packet mode  thus, the interference terms vanish",
        "Subsections": [],
        "Groundtruth": ""
      },
      {
        "Section_Num": "3_5",
        "Section": "3.5 Confusions in terminology",
        "Text": "one might be confused by the use of the same terminology sterile neutrino with different meaning by physicists working with neutrino oscillations and by cosmologist ",
        "Subsections": [],
        "Groundtruth": ""
      },
      {
        "Section_Num": "3_6",
        "Section": "3.6 Current status and perspectives",
        "Text": "a world-wide research program is carried out examining the possible existence of a sterile neutrino stat  here we very briefly review the current status suggesting an interested reader to follow dedicated reviews in 2018 there are several hints in favor of the existence of sterile neutrinos, and there is a bulk of data excluding possible parameter space for this still hypothetical particl 14 was addressed by a number of experiment  a model with sterile neutrinos suggesting equal deficit for any nuclear isotope is excluded by daya bay data at   the compatibility of appearance and disappearance datasets is less than   therefore, the sterile neutrino interpretation of lsnd and miniboone anomalies is unlikel  cosmology provides other strong constraints on the existence of sterile neutrin  an additional relativistic degree of freedom is disfavored by constraints from big bang nucleosynthesis and from recombination epoch",
        "Subsections": [],
        "Groundtruth": ""
      }
    ],
    "Groundtruth": ""
  },
  {
    "Section_Num": "4",
    "Section": "4 Summary",
    "Text": "we reviewed main concepts of sterile neutrinos - a yet hypothetical particle, coined to resolve some anomalies in neutrino dat  in the framework of an extension of the sm, a sterile neutrino field is a superposition of fields of massive neutrino  the corresponding interaction amplitude of the sterile neutrino state vanishes if states of massive neutrinos are coheren  this elegant theoretical construction appears when there is a disparity between active and inert numbers of neutrino fields in an extension of the s  on the other hand there is a bulk of accelerator and atmospheric data strongly disfavoring lsnd and miniboone anomalies as due to sterile neutrino oscillatio  still, the allowed parameter space is within the sensitivity region of currently running experiments",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "5",
    "Section": "5 Acknowledgments",
    "Text": "we sincerely thank     naumov and   kullenberg for reading the manuscript and making important comment ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "Abstract",
    "Section": "Abstract",
    "Text": "arxiv:190 00152v1 1 jan 2019 on fusible rings   we answer in negative two of questions posed in we also establish a new characterization of semiprime left goldie rings by showing that a semiprime ring r is left goldie iffit is regular left fusible and has finite left goldie dimensio ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "1",
    "Section": "1. Introduction",
    "Text": "throughout this paper, r denotes an associative ring with unit  elements which are not left zero-divisor are called left regula  ghashghaei and mcgovern in introduced and investigated fusible ring  our ring r is called left fusible if every nonzero element of r is left fusibl  let us point out that our notation differs from the one introduced in left zero divisors are right zero divisors in the meaning of thus our left fusible rings are right fusible in the language of clearly both domains and clean rings are examples of fusible rings it was proved in that polynomial rings and matrix rings over left fusible rings are left fusibl  the aim of the paper is to continue a study of fusible and related ring  in particular, we answer in negative questions and posed in we also introduce regular left fusible rings, a class which is slightly wider than that of left fusible rings and discuss relations between those two classe  as a result a new description of semiprime left goldie rings is given in theorem   we also prove that the regular left fusible ring property lifts to matrix rings for rings having left quotient ring  some questions are formulate  2000 mathematics subject classificatio  16n20, 16u6  key words and phrase  fusible ring, the classical quotient ring, goldie ring ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "2",
    "Section": "2. Results",
    "Text": "we begin with the following definitio  it is also clear that every left fusible ring is regular left fusible the following example shows that the class of left fusible rings is strictly smaller than the one of regular left fusible ring  bythe set of all left zero divisors of r is equal to x  in particular, a difference of any two left zero divisors is a left zero divisor as wel  thus r is not left fusibl  thus r is a regular left fusible rin  similarly, one can see that r is not right fusible but it is regular right fusibl  we will use the above example to answer question of before doing so let us recall some notion  a ring r is a left  -baer if the left annihilators of principal left ideals are generated as left ideals by an idempoten  birkenmeier et a  in proved that a ring r is an abelian left   ring if and only if r is a reduced  -baer rin  it was proved in that every abelian left  -baer rings do not have to be left fusible question of asks whether every abelian  -baer ring is fusibl  the following example shows that the answer to this question is negativ -baer rin  this can be checked directly or one can apply a classical result of herstein bythe set of all nilpotent elements of r is equal to xr  thus r is an abelian  -baer rin  we have already seen in example  2 that r is neither left nor right fusibl 2 also suggests that although abelian  -baer rings need not be fusible but maybe they have to be regular fusibl  we are unable to answer the following question: question   is every abelian left  -baer ring regular left fusible? on fusible rings 3 now we will move to some basic properties of regular left fusible ring  proo  it is well known that singl is a two-sided ideal of   the following lemma generalizes every regular left fusible ring r is left nonsingula  proo  since singl is a two-sided ideal, sa belong to sing  thus, by lemma   writing ql we will mean that r possesses a classical left quotient ring which is equal to q  proo  let r be a rin  if r is left fusible, then so is s-1  if r is regular left fusible, then ql is left fusibl  if s-1r is regular left fusible, then r is regular left fusibl  suppose every regular element of r is invertibl  then r is regular left fusible iff r is left fusibl  proo  let s-1r belong to s-1  suppose r is regular left fusibl  by lemma   by lemma   this implies that the element tr is left fusible in r and hence r is regular left fusibl  let r belong to   suppose r is regular left fusibl  then, by lemma   this yields the proof of suppose r has the left quotient ring q  the following conditions are equivalent: r is regular left fusibl  ql is left fusibl  ql is regular left fusibl  inthe authors observed that the statement of proposition  8 holds when r is a commutative ring and asked whether the reverse implication hold  in the context of corollary   let r be a regular left fusible ring having the classical left quotient rin  thus the above question has a negative answer without the assumption that r possesses the classical left quotient rin  using the same arguments as in and making use of the fact that in a commutative ring r an element r is nilpotent if and only if the ideal rr is nilpotent we get the following modification of every commutative regular fusible ring is reduce  inthe authors showed that a commutative ring with only finitely many minimal prime ideals is fusible if and only if it is reduce  using this characterization we obtain a positive, partial answer to question   let r be a commutative ring with only finitely many minimal prime ideal  the following conditions are equivalent: r is fusibl  r is regular fusibl  r is reduce  proo 11 andrespectivel  the ring from this example is also not regular fusibl  we do not know of any example of a commutative ring which is regular fusible but not fusibl  inthe following proposition was prove  we offer a short direct argumen  every unit-regular ring is left fusibl  proo  this shows that r is regular left fusibl  r is regular left fusible and has finite left goldie dimension proo  suppose r is left goldi  then, by goldie's theorem, ql is a semisimple artinian rin  in particular it has finite left goldie dimension and it is a unit-regular rin  thus, by proposition  8, ql is left fusibl  now corollary  9 yields that r is regular left fusibl  let r be as in then, by lemma   byevery right complemented ring is left fusibl  asks whether a right complemented ring have always possesses a right classical quotient ring? since any domain is complemented on both sides and there are domains having no a classical right quotient ring, the answer to this question is no in genera  however the above theorem gives immediately the following: corollary   every right complemented ring of finite left goldie dimension has a classical left quotient rin  proo  as observed inevery right complemented ring is reduced, so it is semiprim  now the thesis is a consequence of the left version of theorem   suppose that for any finite set r1,srn are left fusibl  then the matrix ring mn is regular left fusibl  suppose r is a regular left fusible ring having the classical left quotient rin  then the matrix ring mn is regular left fusibl  proo  by corollary  16 shows that mn) is regular left fusibl  now the thesis is a direct consequence of proposition   therefore we have: example   let r be the ring from example   we were unable to answer the following: question   is the matrix ring mn over a regular left fusible ring r itself regular left fusible? we close the paper with an observation that regular left fusible property behaves well under polynomial extension  essentially the same proof as in gives the following proposition  ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "References",
    "Section": "References",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "Abstract",
    "Section": "Abstract",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "Occupation",
    "Section": "Occupation time statistics of a gas of interacting diffusing particles",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "Abstract",
    "Section": "Abstract",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "Hexagonal",
    "Section": "Hexagonal MASnI3 exhibiting strong absorption of ultraviolet photons",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "Abstract",
    "Section": "Abstract",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "1",
    "Section": "1 \u0412\u0432\u0435\u0434\u0435\u043d\u0438\u0435.",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "2",
    "Section": "2 \u041f\u043e\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0430 \u0437\u0430\u0434\u0430\u0447\u0438.",
    "Text": "",
    "Subsections": [
      {
        "Section_Num": "2_2",
        "Section": "2.2 \u041f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0439 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c.",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
      }
    ],
    "Groundtruth": ""
  },
  {
    "Section_Num": "3",
    "Section": "3 \u041f\u0430\u0440\u0430\u043b\u043b\u0435\u043b\u044c\u043d\u044b\u0439 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u043f\u043e\u0438\u0441\u043a\u0430 \u0434\u0438\u0441\u0441\u043e\u043d\u0430\u043d\u0441\u043e\u0432 PhiDD",
    "Text": "",
    "Subsections": [
      {
        "Section_Num": "3_2",
        "Section": "3.2 \u0420\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
      }
    ],
    "Groundtruth": ""
  },
  {
    "Section_Num": "4",
    "Section": "4 \u0412\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u044b.",
    "Text": "",
    "Subsections": [
      {
        "Section_Num": "4_2",
        "Section": "4.2 \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u043e\u0432",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
      }
    ],
    "Groundtruth": ""
  },
  {
    "Section_Num": "5",
    "Section": "5 \u0417\u0430\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0435.",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "Abstract",
    "Section": "Abstract",
    "Text": "arxiv:190 00156v2 2 nov 2023 cluster editing with vertex splitting faisal   sullivan petra wolf abstract cluster editing, also known as correlation clustering, is a well-studied graph modification proble e, a graph consisting of a disjoint union of clique  however, in real-world networks, clusters are often overlappin  those corresponding to work, school, or neighborhoo  other strong motivations come from biological network analysis and from language network  trying to cluster words with similar usage in the latter can be confounded by homonyms, that is, words with multiple meanings like ba  in this paper, we introduce a new variant of cluster editing whereby a vertex can be split into two or more vertice  we call the new problem cluster editing with vertex splitting and we initiate the study of i  we show that it is np-complete and fixed-parameter tractable when parameterized by the total number k of allowed vertexsplitting and edge-editing operation  in particular, we obtain an o-time algorithm and a 6k-vertex kerne  1 introduction cluster editing is defined as follow  over the last decade, cluster editing has been well studied from both theoretical and practical perspectives in general, clustering results in a partitioning of the input grap  hence, it forces each vertex to be in one and only one cluste  this can be a limitation when the entity represented by a vertex plays a role in multiple cluster  this situation is recorded in work on gene regulatory networkswhere enumeration of maximal cliques was considered a viable alternative to clusterin  moreover, such vertices can effectively hide clique-like structures and also greatly increase the computational time required to obtain an optimal solution in this paper, we introduce a new variant, which we call cluster editing with vertex splitting in an attempt to allow for such overlapping clusters  we show the new problem to be np-complete and investigate its parameterized complexit  we obtain a polynomial kernel using the notion of a critical cliques as introduced by lin et a  and applied to cluster editing by guoabukhzam@la ed lb, emmanue arrighi@gmai com, matthia bentert@ui no, pa drange@ui no, judit egan@cd ed au, serg gaspers@uns ed au, alexi shaw@studen uns ed au, petershaw@ojla a cn, sullivan@c uta edu, and mail@wolf ne  1preliminary versions of parts of this paper have been presented at isco 2018 and ipec 2023 kernel in linear tim  section 7 presents a fixed-parameter tractable algorithm and we conclude in section 8 with some open problems and future direction  all logarithms in this paper use 2 as their bas  we use standard graph-theoretic notation and refer the reader to the textbook by diestel for commonly used definition  all graphs in this work are simple, unweighted, and undirecte  a cluster graph is a graph in which the vertex set of each connected component induces a cliqu  equivalently, a graph is a cluster graph if and only if the graph does not have p3 as an induced subgrap  an example of the splitting operation is given in figure 1 and we call the two new vertices v1 and v2 copies of the original vertex v cluster editing with vertex splitting is then defined as follow  2 the graph w6 one of the three ways of transforming w6 into two k5's using six operation  cluster editing with vertex splitting is np-complet  proo  since containment in np is obviouswe focus on the np-hardness and present a reduction from 3-sa  therein, we will use two gadgets, a variable gadget and a clause gadge  the variable gadget is a wheel graph with two center vertice  an example of this graph is depicted on the left side of figure   the clause gadget is a crown graph as depicted in figure   we arbitrarily assign each of the three vertices of degree two in hj to one literal in c  finally, we connect the variable and clause gadgets as follow  if a variable xi appears in a clause cj, then let u be the vertex in hj assigned to xi let the vertices on the outer cycle of gi be v1, v2, for each variable xi, we will partition gi into k5's as follow  let a be the value such that gi is isomorphic to w6  moreover, we split the two center vertices 2a -1 time  in total, we use 8a -2 modifications to transform gi into a collection of k5'  since each clause contains exactly three literals and we add six vertices for each variable appearance, the sum of lengths of cycles in all variable gadgets combined is 18  hence, in all variable gadgets combined, we perform 24m -2n modification  next, we modify the crown graph  to this end, let cj be a clause and let hj be the constructed clause gadge  since beta is a satisfying assignment, at least one variable appearing in cj satisfies i  the good solution is the only solution with three operations that creates at least one isolated verte  multiple such variables exist, then we pick one arbitraril  let xi be the selected variable and let u be the vertex in hj assigned to x  we first turn hj into a k4 and an isolated vertex by removing the two edges incident to u in hj and add the missing edge between the two vertices assigned to different variable  finally, we look at the edges between variable gadgets and clause gadget  hence, we use 3+2+6 = 11 modifications for each claus  for the other direction, suppose the constructed instance of cluster editing with vertex splitting is a yes-instanc  we first show that 24m -2n modifications are necessary to transform all variable gadgets into cluster graphs and that this bound can only be achieved if each time exactly three consecutive vertices on the cycles are contained in the same k  to this end, consider any variable gadget g  by construction, gi is isomorphic to w6a for some integer   by the counting argument from above, we show that at least 8a -2 modifications are necessar  note first that some edge in the cycle has to be removed or some vertex on the cycle has to be split as otherwise any solution would contain a clique with all vertices in the cycle and this would require at least 18a2 -9a > 8a -2 edge additions we next analyze how many modifications are necessary to separate b vertices from the outer cycle into a cliqu  we require at least two modifications for the center vertices and one operation to separate the cycle on the other end note that it is always preferable to delete an edge on the outer cycle and not split one of the two 4 incident edges as splitting a vertex increases the number of vertices on the cycle and thus invokes a higher overall cos  next, we analyze the clause gadget and the edges between the different gadget  we start with the latte  let u be a vertex in a clause gadget hj with incident edges to some variable gadge  the only way to not use at least three operations to deal with the three edges is if u is an isolated vertex or if the three neighbors do not have two more neighbors in the current solutio  in the former case, we can add the two edges between u and the two centers of the respective variable gadgets to build a k  in the latter case, we have used at least three operations more in the variable gadget than intended since each vertex in a variable gadget is only adjacent to at most one vertex in a clause gadget, this cannot lead to an overall reduction in the number of operations and we can therefore ignore this latter cas  we are now in a position to argue that at least eleven modifications are necessary for each clause gadge  first, note that at least three operations are required to transform a crown into a cluster grap  possible ways of achieving this are depicted in figure   in each of these possibilities, at most one vertex becomes an isolated verte  to make two vertices independent, at least four operations are required and for three isolated vertices, at least five operations are require  as shown above, at least two operations are required for each isolated vertex with edges to variable gadgets and at least three operations are required for non-isolated vertices with edges to variable gadget  thus, at least eleven operations are required for each clause gadget and eleven operations are suffcient if and only if the three vertices incident to one of the vertices in hj belong to the same k5 in the variable gadge  note that we never set a variable to both true and false in this wa  we set all remaining variables arbitrarily to true or fals  thus, the constructed instance is equivalent to the original instance of 3-sa  since the reduction can clearly be computed in polynomial time, this concludes the proof for the np-hardnes  in contrast to the reduction for cluster editingour reduction does not produce instances with constant maximum degre  we instead observe that in our reduction, the maximum degree of the produced instances depends only on the maximum number of times a variable appears in a claus  cluster editing with vertex splitting remains np-hard on graphs with maximum degree 24",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "Introduction",
    "Section": "Introduction",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "Preliminaries",
    "Section": "Preliminaries",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "NP-hardness",
    "Section": "NP-hardness",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "The",
    "Section": "The Edit-Sequence Approach",
    "Text": " we mention that removing edges can also be moved to the front or to the back and that we do not use the fact that we want to reach a cluster graph at any poin  the statement therefore holds for any graph-modification problem which only adds edges, deletes edges, and splits vertice  we show first that all vertex splittings can be moved to the back of the edit sequenc  proo  note that this is possible as v is adjacent to w after performing the edit sequence we show next that moving edge additions to the front results in an equivalent edit sequenc  proo  note that if ei+1 adds the edge that ei removed, then replacing these two operations with do-nothing operations results in the same grap  this concludes the proo  we can easily deduce the following theorem from the two above lemmat  if ei is a do-nothing operation for some i belong tothen the edit sequence is equivalent and shorte  if the application of lemma 2 introduces a do-nothing operation, then we again remove i  we refer to an edit sequence satisfying the statement of theorem 2 as an edit sequence in standard form",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "Critical",
    "Section": "Critical Cliques",
    "Text": "originally introduced by lin et a critical cliques provide a useful tool in understanding the clique structure in graph  a critical clique is a subset of vertices c that is maximal with the properties that   c is a clique   hence, each vertex v appears in exactly one critical cliqu  the following lemma is adapted from lemma 1 by guo with a careful restatement in the context of our new proble  let be a yes-instance of cluster editing with vertex splittin  proo  in the latter case, there also exists such a pair of vertices where one of the two vertices is r  let w be the other verte  we find a new optimal solution by removing all operations involving w and copying all operations including ri and replacing ri by w in the cop  for the sake of notational convenience, we 2we mention in passing that we claimed a slightly stronger lemma in a previous version of this pape  firbas et a  observed that the stronger version does not hold and conjectured that this weaker version is tru  we confirm this her  the center cannot be a copy of ri as each copy of w is only adjacent to the corresponding copy of ri and vice vers  hence, we have another induced p3 where we replace the two copies of w by their respective copies of r  so assume that x contains only the jth copy of   since these two vertices are adjacent, one of the two vertices is the center of   however, all other vertices have either both or none of the two vertices as neighbor  this contradicts that x exist  in the following two sections, we will use lemma 3 to develop a linear-vertex kernel and a 2o-time algorithm",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "A",
    "Section": "A 6k-vertex kernel ",
    "Text": "in this section, we prove a problem kernel for cluster editing with vertex splitting with at most 6k vertices based on the critical clique lemma and a similar kernel for cluster editing by guo we start with the simplest on  reduction rule   remove all isolated clique  reduction rule 1 is saf  proo  we next bound the size of each critical clique in terms of the sizes of adjacent critical clique  reduction rule   reduction rule 2 is saf  proo  moreover when splitting another vertexthen we treat each copy of w the same as the corresponding copy of r  note that each copy of w is adjacent to the respective copy of ri and not adjacent to any other copy of ri 4a reduction rule is safe if its application results in an equivalent instanc  8 one copy of vertices in   we next show that a =   proo  we follow an approach similar to that taken by guo let us now consider   assume towards a contradiction that this number is more than 4  the above lemma can be converted into the following reduction rul  note that a c4 is a cycle on four vertices and a c4 cannot be transformed into a cluster graph with a single operatio  reduction rule   based on the previous three reduction rules, it is easy to derive a problem kernel with 6k vertices in linear tim  one can compute a kernel with at most 6k vertices and 4k critical cliques for cluster editing with vertex splitting in linear tim  proo  computing the critical clique quotient graph c of g takes linear time applying reduction rule 1 exhaustively also takes linear time and this removes all isolated nodes in   applying reduction rule 2 exhaustively takes linear time as shown nex  first, we can sort all critical cliques by their size in linear time using bucket sor  applying reduction rule 2 to a critical clique c takes deg tim  by the handshaking lemma, this procedure takes time linear in the number of edges in c which is upper-bounded by the number of edges in the input grap  note that if we iterate over the critical cliques in increasing order of size, then an application of reduction rule 2 can never affect previously considered critical clique  this is due to the fact that an application of reduction rule 2 does only depend on adjacent critical clique  hence, applying reduction rule 2 exhaustively takes linear tim  otherwise, we apply reduction rule   this concludes the proo  we leave it as an open problem whether the size of the kernel can be improved and mention that there is a 2k-vertex kernel for cluster editing ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "An",
    "Section": "An FPT algorithm",
    "Text": "the result in theorem 3 implies that cluster editing with vertex splitting is fixedparameter tractabl  by lemma 3, we can assume that all vertices in the same critical clique belong to the same clique in a solutio  this is, however, far from optimal as shown nex  cluster editing with vertex splitting can be solved in o tim  proo  next, we compute the kernel from theorem 3 in linear tim  note that c contains at most 4k vertice  note that there are at most 4k belong to o4k) such coloring  the idea behind the coloring is the followin  it remains to compute the best solution corresponding to each possible set of guesse  to this end, we first iterate over each pair of vertices and add an edge between them if this edge does not already exist and we guessed that there is a clique si which contains a copy of each of the two vertice  moreover, we remove an existing edge between them if we guessed that the two vertices do not appear in a common cliqu  finally, we perform all split operation  therein, we iteratively split one vertex v into two vertices u1 and u2 where u1 will be the vertex in some clique si and u2 might be split further in the futur  the vertex u1 is adjacent to all vertices that are guessed to belong to s  the vertex u2 is adjacent to all vertices that u was adjacent to, except for vertices that are adjacent to u1 and not guessed to also belong to some other clique sj which u2 belongs t  since our algorithm basically performs an exhaustive search, it will find an optimal solutio  it only remains to analyze the running tim  we first compute the kernel in o tim  we should note in passing that, while the constants in the running time of our algorithm can probably be improved, a completely new approach is required if one wants a single-exponentialtime algorith  this is due to the fact that the number of possible partitions of o critical cliques into clusters grows super-exponentially even if no vertex-splitting operations are allowe  8 conclusion by allowing a vertex to split into two vertices, we extend the notion of cluster editing in an attempt to better model clustering problems where a data element may have roles in more than 11 one cluste  on the other hand, we give a 6k-vertex kernel and an o-time algorith  this leaves the following gap  open problem   while we do understand the parameterized complexity of cluster editing with vertex splitting with respect to k reasonably well, there are still a lot of open questions regarding structural parameters of the input grap  future work may also consider a bound on the number of allowed edge edits incident to each vertex as used by komusiewicz and uhlmann and abu-khzam moreover, one might also study the approximability of cluster editing with vertex splitting as the trivial constant-factor approximation of cluster editing does not carry over if we allow vertex splittin  in case cluster editing with vertex splitting turns out to be hard to approximate, one might then continue with studying fpt-approximation and approximation kernels we note that the results in section 4 are directly applicable to these settings as wel  especially bicluster editing has received significant attention recently to the best of our knowledge, nothing is known about bicluster editing with vertex splittin  we should also note that there are two natural versions in the bipartite case and both of them seem worth studyin  the two versions differ in whether or not one requires that all copies of a split vertex lie on the same side of a bipartition in a solutio  on the one hand, the additional requirement makes sense if the data is inherently bipartit  this happens for example if each vertex represents either a researcher or a paper and an edge represents an authorshi  on the other hand, if edges reflect something like a seller-buyer relationship, then it is plausible that a person both sells and buy  finally, we believe that it also makes sense to study a variant of the vertex-splitting operation where the neighborhood of the two newly introduced vertices are a partition of the neighborhood of the split vertex rather than a coverin  this operation is called exclusive vertex splitting in the literature, and can be seen to be closely related to the clique partitioning proble  acknowledgments matthias bentert is supported by the european research council under the european union's horizon 2020 research and innovation programme serge gaspers is the recipient of an australian research council future fellowship and acknowledges support under the arc's discovery projects funding scheme alexis shaw is the recipient of an australian government research training program scholarshi ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "Conclusion",
    "Section": "Conclusion",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "Abstract",
    "Section": "Abstract",
    "Text": "arxiv:190  we study the discovery potential for this fcnh signal and physics background from dominant processes with realistic acceptance cuts as well as tagging and mistagging effciencie  pacs numbers: 1 fr, 1 15mm, 1 ec, 1 ha *e-mail address: rishab jain@o edu e-mail address: chun kao@ou",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "I",
    "Section": "I Introduction",
    "Text": "the standard model has been very successful in explaining almost all experimental data to date, culminating in the discovery of the the long awaited standard higgs boson at the cern large hadron collider the most important experimental goals of the lhc, future high energy hadron colliders, and the international linear collider are to study the higgs properties and to search for new physics beyond the standard model including additional higgs bosons and flavor changing neutral higgs interaction  in the standard model there is one higgs doublet, which generates masses for both vector bosons and fermion  there is no explanation for the large differences among yukawa couplings of fermions with the higgs boso  in addition, there are no flavor changing neutral currents mediated by gauge interactions or by higgs interactions at the tree leve  the top quark is the most massive elementary particle ever discovere  the fact that the higgs boson is lighter than the top quark makes it possible for the top quark to decay into the higgs boson along with a charm quark kinematicall  if this decay mode is detected in the near future, it would indicate a large tree-level coupling or a significant enhancement from beyond sm loop effect  a general two higgs doublet model usually contains flavor changing neutral higgs interactions if there is no discrete symmetry to turn offtree-level fcnc in 1991, it was pointed out that top-charm fcnh coupling could be prominent if the yukawa couplings of fermions and the higgs boson are comparable to the geometric mean of their mass a special two higgs doublet model for the top quark might provide a reasonable explanation why the top quark is much more massive than other elementary fermion  in the t2hdm, top quark is the only elementary fermion acquiring its mass from a special higgs doublet with a large vacuum expectation value since the up and charm quarks couple to another higgs doubletthere are fcnh interactions among the up-type quark  the down type quarks have the same interactions as those in the s  the lhc has become a top quark factor  thus, the lhc will provide great opportunities to study electroweak symmetry breaking as well as other important properties of the top quark and the higgs boso  most atlas and cms measurements of the 125 gev higgs boson are consistent with expectations for the standard mode  the branching fractions of the standard higgs boson are presented in table i in a general two higgs doublet model, let us consider the light higgs scalar as the sm higgs boson in the alignment limit table i: branching fractions and partial decay widths of the light cp-even higgs boson of a general two higgs doublet model in the alignment limit094 w-loop and fermion loop 22 as show in table   the lhc limits for the branching ratios can be translated to a limit on the flavor changing yukawa coupling by a simple rescalin  we have evaluated production rates with full tree-level matrix elements including breit-wigner resonances for both the signal and the physics backgroun  in addition, we optimize the acceptance cuts to effectively reduce the background with realistic b-tagging and mistagging effciencie  section ii shows the production cross sections for the higgs signal and the dominant background, as well as our strategy to determine the reconstructed masses for the top quark and the higgs boso  realistic acceptance cuts are discussed in section ii  our optimistic conclusions are drawn in section  ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "II",
    "Section": "II The Higgs Signal and Physics Background",
    "Text": "in this section we present the cross section for the fcnh higgs signal in pp collisions as well as for the dominant physics background processe ",
    "Subsections": [
      {
        "Section_Num": "A",
        "Section": "A The Higgs Signal in Top Decay",
        "Text": "applying the lagrangian in e  using mt = 17  for typical values of parameters cos =   it is the effective coupling of the fcnh yukawa couplin 0901with mt = 17  this choice of scale leads to a k factor of approximately  8 for top quark pair productio  we have used the computer program top++ to evaluate higher order correction  in addition, we have checked the tree-level signal cross section with narrow width approximatio  in every event, we require that there should be one b jet and three light jets in addition, there are two leptons and neutrinos, which will be lead to missing transverse energy",
        "Subsections": [],
        "Groundtruth": ""
      },
      {
        "Section_Num": "B",
        "Section": "B The Physics Background",
        "Text": " we evaluate the cross section of physics background in pp collisions with proper tagging and mistagging effciencie ",
        "Subsections": [],
        "Groundtruth": ""
      },
      {
        "Section_Num": "C",
        "Section": "C Mass Reconstruction",
        "Text": " applying suitable cuts on the cluster transverse mass mt as well as mt, we can greatly reduce the physics background and enhance the statistical significance for the higgs signa  fi  7 in our analysis, we assume that the fcnh signal comes from top quark pair production with one top quark decaying into a charm quark and a higgs boson while the other decays hadronically in every event, there is one tagged b-jet and three light jet  we present the invariant mass distributions for mj1j2 and mbj1j2 in fi 20 *m 20*m  where m*is the value of invariant mass or cluster transverse mass with a peak of the distributio  these distributions provide powerful selection tools to remove physics background while maintaining the higgs signa ",
        "Subsections": [],
        "Groundtruth": ""
      }
    ],
    "Groundtruth": ""
  },
  {
    "Section_Num": "III",
    "Section": "III Realistic Acceptance Cuts",
    "Text": "to study the discovery potential of this charming fcnh signal from top decays at the lhc, we have applied realistic basic cuts listed in e  and tagging effciencies for b-jet  these selection requirements remove more than 90% of the total backgroun  based on the atlas and the cms specifications we model these effects by 9 fi  the current atlas-limit is shown as a black dash vertical lin 01, for charged leptons with individual terms added in quadratur ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "IV",
    "Section": "IV Discovery Potential at the LHC",
    "Text": " here we have kept cos =   later we will vary it from  2 for discovery contour  cross sections for dominant background processes are presented in table ii  to estimate the discovery potential at the lhc we include curves that correspond to the minimal cross section of signal required by our discovery criterion described in the followin  5: similar to fi 1 for the lhc and future hadron collider  the parameter n specifies the level or probability of discover  if the background has fewer than 25 events for a given luminosity, we employ the poisson distribution and require that the poisson probability for the sm background to fluctuate to this level is less than  87 * 10-7,   an equivalent probability to a 5-sigma fluctuation with gaussian statistic 1fb-1 and higher luminosities for the future hl lhc  all tagging effciencies and k factors discussed above are include  our analysis suggests an improvement in the reach of atlas at a luminosity of 3000 fb-1, which gets better at higher energies,   6, in the parameter plane of we have chosen l = 300 and 3000 fb-  high energy lhc with high luminosity is quite promising as it nearly covers the entire parameter space that we have used in our analysi  the shaded region above this curve is excluded at 95% c ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "V",
    "Section": "V Conclusions",
    "Text": "it is a generic possibility of particle theories beyond the standard model to have contributions to tree-level fcnh interactions, especially for the third generation quark  these contributions arise naturally in models with additional higgs doublets, such as the special two higgs doublet model for the top quarkor a general 2hd  the shaded region above this curve is excluded at 95% c  limit, the light higgs boson resembles the standard higgs boson, and it has a mass below the top mas  we investigated the prospects for discovering such a decay at the lhc, focusing on the channel where tt are pair produced and subsequently decay, one haronically and the other through the fcnh mod  the primary background for this signal is a ttjj with both top quarks decaying leptonicall  nonetheless, by taking advantage of the available kinematic information, we can reconstruct the resonances of the signal and reject much of the backgroun  we look forward to being guided by more new experimental results as we explore interesting physics of electroweak symmetry breaking and fcnh interaction  acknowledgments we are grateful to kai-feng jack chen for beneficial discussion  thanks the institute of physics at the academia sinica and the high energy physics group at national taiwan university for excellent hospitality, where part of the research was complete  this research was supported in part by the   department of energ  aad et a  let 1016/ physlet  chatrchyan et a  let  b 716, 30 doi:1 1016/ physlet    aguilar-saavedra, acta phy  polo  petrarca and   soddu, phy  let  hewett and   soni, phy  re  glashow and   weinberg, phy  re  doi:1 1103/physrev  kane and   dawson, fron  phy  hou, phy  let  doi:1  cheng and   sher, phy  re  doi:1 1103/physrev  das and   kao, phy  let  davidson and   haber, phy  re  mahmoudi and   stal, phy  re  dawson and   ellis, nuc  phy  doi:1  kidonakis, phy  re  d 82, 114030 doi:1 1103/physrev  ahrens,   pecjak and   yang, phy  let  b 703, 135 doi:1 1016/ physlet  mangano,   mitov and   nason, phy  let  b 710, 612 doi:1 1016/ physlet  fiedler and   mitov, phy  re  let 1103/physrevlet  aaboud et a  let 1016/ physlet 1016/ physlet    sirunyan et a jhep 1709, 051 doi:1  heinemeyer et a 5170/cern-2013-004 arxiv:130  aad et a  phy  the atlas and cms collaborations, atlas-conf-2015-04  gunion and   haber, phy  re  d 67, 075019 doi:1 1103/physrev  carena,   shah and   wagner, jhep 1404, 015 doi:1  hou and   sayre, phy  let  b 716, 225 doi:1 1016/ physlet  gupta and   soni, jhep 1410, 057 doi:1 1007/jhep10057 14 ].    sirunyan et a jhep 1806, 102 doi:1  kao and   kohda, phy  let  b 725, 378 doi:1 1016/ physlet  aaboud et a jhep 1710, 129 doi:1  chala and   spannowsky, eu  phy    thomas and   walker, phy  re  d 86, 075002 doi:1 1103/physrev  cms collaborationcms-pas-hig-13-03  khachatryan et a  re  d 90, 112013 doi:1 1103/physrev  aaboud et a  re 1103/physrev  the atlas collaboration, atlas-phys-pub-2013-01  rossi and   skands, arxiv:131  toma e al doi:1 1016/ nuclphysbp  zimmermann, icfa beam dy  newslet  shiltsev, doi:1 18429/jacow-napac2016-tupob07 arxiv:170  kohda and   mccoy, phy  let  b 751, 135 doi:1 1016/ physlet  mattelaer and   stelzer, jhep 1106, 128 doi:1  li and   mawatari, eu  phy  c 56, 435 doi:1  murayama and   watanabe helas : helicity amplitud subroutines for feynman diagram evaluations arxiv:080  dulat et al, phy  re 1103/physrev  gao et al, phy  re 1103/physrev  the atlas collaborationatlas-conf-2018-00  scodellaroarxiv:170  barger and   han and   zhang, phy  re  let 1103/physrevlet  kao and   sayre, phy  let  b 722, 324 doi:1 1016/ physlet  cms collaborationcms-pas-hig-15-00  the atlas collaboration, atlas-phys-pub-2013-004 the cms collaboration, arxiv:1607",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "Acknowledgments",
    "Section": " Acknowledgments",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "References",
    "Section": " References",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "Abstract",
    "Section": "Abstract",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "Hard",
    "Section": "Hard X-ray spectroscopy of the itinerant magnets RFe4Sb12 (R=Na, K, Ca, Sr, Ba)",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "Abstract",
    "Section": "Abstract",
    "Text": "arxiv:190 00160v4 1 feb 2019 puzzles of the cosmic ray anisotropy *  machavariani 1 and a",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "1",
    "Section": "1 Introduction",
    "Text": "one of the most important and simultaneously most diffcult studies of the origin of cosmic rays is the study of their anisotrop  the diffculty is due to the extremely low level of the anisotropy and the steeply falling energy spectrum of c  in their seminal book 'origin of cosmic rays' issued in 1964  ginzburg and    more than 20 years later  ginzburg and his co-authors on the basis of data obtained by linsley   could show that the amplitude and phase of the anisotropy are not constant, but vary with the energy this variability is the consequence of the non-uniformity of the spatial distribution of cr sources and properties of the interstellar medium sources of different locations and ages contribute in different energy regions and magnetic fields of different strength and orientations tend to isotropise the arrival directions of cr particle  one of the most interesting regions of the cr energy spectrum is sub-pev and pev region where the well-known 'knee' is observe  the study of this region is diffcult because of the very small intensity of particles and low anisotropy leve  that is why there are many unsolved problems and puzzles here which still wait for their solutio  below we describe only three of the lebedev",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "2",
    "Section": "2 Puzzle 1: an inverse anisotropy",
    "Text": "due to cr scattering on the chaotic magnetic fields their motion in the galaxy is like a diffusion from regions of higher to lower cr densit  since the solar system is located at about 8 kpc from the galactic center then the main source of cr - supernova remnants and pulsars are concentrated in the inner galax  the cr density is higher towards the galactic center and their gradient is directed from the inner to outer galax  hence we have to expect some anisotropy with the maximum flux from the inner galax  the expected and observed situation with the first harmonic of the anisotropy in the sub-tev region is shown in figure   ihe upper panel shows the amplitudes and it is seen that they do not exceed the value of 10-  the lower panel presents measurements of the phase in equatorial coordinate  if transferred to galactic coordinates they occupy the region delimited by two dashed line  cr come preferentially from the outer galax  we call this phenomenon an 'inverse anisotropy'.  one of the possible explanations of an inverse anisotropy is to assume that it is a local phenomenon caused by a spatial orientation of the magnetic field or the ism density fluctuatio  the sun is located in the local bubble at the inner edge of the orion spiral ar  the strength of the magnetic field and of the ism density in the arm is higher than in the interarm region where the local bubble is locate  the diffusion in the nearby outer side of the galaxy is slower than locall  cr moving from the inner galaxy meet like a wall, a part of them reflect from it, accumulate in a number and create an inverse gradient there might be an alternative explanation recent measurements of cr at subtev - sub-pev energies demonstrated the irregular non-power law shape of the energy spectru  the observed irregularities point to the possible role of local source  however, to be responsible for an observed inverse anisotropy these local sources should be located mostly in the outer galax  we should underline that these explanations are only a possible assumption with many internal uncertaintie  however, we should emphasize that the inverse anisotropy is most likely a local phenomenon, which is caused by the reflection of cr from a nearby region of higher ism density or the dominance of some local sources in the outer galax ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "3",
    "Section": "3 Puzzle 2: peculiarity of the amplitude and phase",
    "Text": " thick line above loge =   here and in the text below the energy e is in ge  the full line in the upper panel relates to calculations with our model described in section 3 in what follows we shall endeavour to build a model which can reproduce these features with the minimum number of assumption  this model contains three basic ingredients: the galactic disk, the halo and the single source the details of the model suggested to explain all these peculiar features are given in here we give just its main features and they are illustrated in figure   we assume that the anisotropy appears only in the vicinity of the sources,   in the dis  the anisotropy of cr in the halo is postulated as being ah = 0 and the isotropy of cr re-entrant from the halo back into the disk dilutes the anisotropy of cr produced and trapped in the dis  in this treatment we just consider the first harmoni  later work will deal with higher multipole  we calculate the amplitude of the first harmonic for the case where only disk and halo contribute to cr as a = adid id + ih the result is shown in figure 2b by the dotted lin  the rise of the amplitude at energies above 100 gev is due to the rise of the diffusion coeffcient in the expression for ad mentioned abov 5 and the subsequent rise of the amplitude above this dip however, this idea alone is not enough to reproduce the experimental data and here the examination of the phase of the first harmonic could hel  in figure 2b it is seen that after the moderate decrease in the energy interval  1 - 100 tev the phase suddenly changes to its opposit  we consider this change seriously and propose that cr from the single source have a phase opposite to that of the background at lower energie  this is a necessary complementary requirement in our mode  a comparison with the experimental data is shown also in figure 1a by the full lin  it is seen that after minimum in the dip the amplitude of the anisotropy starts rising again and it is caused by the rising contribution of the single source, which has the opposite phas  we think that the described model with three basic ingredients: disk, halo and single source is reasonable and is worthy of discussio  the new features advocated here are: the dominance of the halo component in the sub-pev regio  it means that the cr which we observe and study in spite of being ourselves inside the disk come mostly from the hal  it is disputable but helps to understand the low anisotropy, small radial 5 gradient of cr intensity and small level of irregularities in the regular power law energy spectru  it is a very reasonable question and this work gives the answe  the single source causes the stronger decrease and the dip in the amplitude of the dipole anisotropy at sub-pev energie  it is also seen in the change of the phase of the anisotropy at the same energie  it means that the single source should deliver cr from the direction opposite to the direction of cr from the background and it is a new assumption in the single source scenari  above the dip energy the amplitude starts to rise again with the opposite phase, as expecte  the dip in this model appears as the result of subtraction of two bigger value  its position and shape are extremely sensitive to the choice of parameters participating in expression the phase of the first harmonic in the pev region, where the contribution of the single source is big enough, could help to locate it on the sk  the present experimental data have a too big spread to make a conclusio  we understand that this scenario raises more questions than gives answer ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "4",
    "Section": "4 Puzzle 3: the nature of the Single Source",
    "Text": "the existence of the single source has been proposed by us to explain the puzzling sharpness of the knee in the size spectrum of extensive air showers the physical basis of this proposal is the evident non-uniformity of the spatial and temporal distributions of sn explosions and subsequent sn  as 6 a result one sn could explode not very long ago and close to the solar syste  its contribution to the cr intensity is rather high and it gives rise to a small peak above the background from other snr - it is our single sourc  to search for the single source authors used the so called difference meto  it is different from the traditional study of the cr intensit  its main idea is that the difference between properties of eas coming from the direction of the single source and from the opposite direction should be maximu  the method is stable against random experimental errors and allows to separate anomalies connected with the laboratory coordinate system from anomalies in the celestial coordinate syste  the method allows to study the whole celestial sphere including regions outside of the line of sight of the experimental installatio  to search for an anisotropy authors used experimental data obtained with the gamma experimental eas array and have taken the eas age parameter as a characteristic of the eas propert  the result of the search is given in figure   the closest source to this location is cluster vela and therefore it is a good candidate for the role of the single source, which is responsible for the knee in the cr spectrum and the minimum in the amplitude of the anisotrop  however, a firm conclusion can be drawn only if it will be confirmed by the analysis of 7 data from other eas array  an alternative explanation is the influence of the regular magnetic field in the area surrounding the earth especially taking into account the nearby spiral arm the only point we should like to stress is that the effect of the regular magnetic field in pev and sub-pev energies should be small compared with the effect of the single source in order not to destroy the diffusive character of cr propagatio  additional asguments in favour of the dominance of the single source in the formation of the maximum difference in age distributions of eas coming from the opposite directions could be the vicinity of vela source and the 'younger' age of eas coming from the direction of the maximum since magnetic fields do not change the energy spectrum of eas and their age  in the section 6 of our paper we speculated that a minor shift of the maximum difference from vela source seen in figure 3 could be due to the influence of this small regular magnetic fiel ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "5",
    "Section": "5 Conclusion",
    "Text": "we mentioned only 3 puzzles existing in the pev and sub-pev energy regio ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "Abstract",
    "Section": "Abstract",
    "Text": "arxiv:190  in this paper, we give precise description for the lowest lowest two-sided cell c0 and the left cells in it for a weighted coxeter group of rank   then we show conjectures p1 -p15 and e p hold for c0 and do some calculation for the based ring of c ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "0",
    "Section": "0. Introduction",
    "Text": "let be a weighted coxeter grou lusztig conjectured that the maximal weight value of the longest elements of the finite parabolic subgroups of w is a bound for this property is referred as boundness of a weighted coxeter group =   in this paper, we give precise description for c0 and the left cells in i  then we can show conjectures p1 -p15 and e p hold for c  at last, we do some calculation for the based ring of c ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "1",
    "Section": "1. Preliminaries",
    "Text": " we call |s| the rank of and denote it by ran  the neutral element of w will be denoted by   then is also a coxeter group, called a parabolic subgroup of key words and phrase  let be a coxeter grou  then we call a weighted coxeter grou  let be a weighted coxeter group and z be the ring of laurent polynomials in an indeterminate v with integer coeffcient  obviously, te is the multiplicative unit of   we can get the following facts by easy computatio lusztig gave the following conjecture inlusztig proved this conjecture when w is an affne weyl grou shi and  yang proved this conjecture when w has complete coxeter grap  these polynomials px,y are called kazhdan-lusztig polynomial  the equicalence classes are called left cells, right cells and two-sided cells of   now we assume conjecture  5 holds and w has a lowest two-sided cell c  ini proved conjecture  5 in this cas  then theorem  1 has the following corollarie  in particular, txwjty = txwj  proo  see proo  by the proofs inwe have the following table  it is obvious when w is a finite coxeter grou  we can get this inequality by when w is an affne weyl group and by when w has complete coxeter grap  now we only consider other case",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "2",
    "Section": "2. The boundness of (W,S,L)",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "3",
    "Section": "3. The lowest two-side cell c0",
    "Text": "now we fix an element wj belong to m and let c0 be the two-sided cell of w containing w  ini proved the following consequenc  the two-sided cell c0 is the lowest two-sided cell of   by corollary  4, we can prove the following proposition using the same method used in now we can give precise description for c0 and the left cells in i  proo  by proposition   this conclusion follow  therefore, it is a disjoint unio  we have the following table for weighted coxeter groups of rank   when the number of left cells in c0 has various possibilities, it depends on the weight function   proo  it is clear when w is a finite coxeter grou  see when w is an affne weyl grou  10 jianwei gao proo  by proposition   by theorem 3",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "4",
    "Section": "4. The based ring of c0",
    "Text": " moreover, we have cw = excpwf  let w1, w2, w3 belong to c  we have the following decomposition as in so we assume x indecomposable at firs  then we have the following propositio  let be a weight coxeter group of rank 3 but not an affne weyl grou  to prove this proposition, we need two lemma  proo  we claim that x2 belong to u  by theorem   proo  now we prove proposition   finite coxeter groups don't have any indecomposable element  if w has complete coxeter graph, see it is much easier than computing cxc  by lemma   by lemma   we have completed the proof of proposition 4",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "References",
    "Section": "References",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "Abstract",
    "Section": "Abstract",
    "Text": "arxiv:190 00164v1 1 jan 2019 a field-size independent code construction for groupcast index coding problems mahesh babu vaddi and  a  the side-information hypergraph becomes a side-information graph for a special class of groupcast index coding problems known as unicast index coding problem  the computation of minrank is an np-hard proble  there exists a low rank matrix completion method and clique cover method to find suboptimal solutions to the index coding problem represented by a side-information grap  however, both the methods are np-har  the number of computations required to find the minrank depends on the number of edges present in the side-information grap  in this paper, we define the notion of minrank-critical edges in a side-information graph and derive some properties of minrank, which identifies minranknon-critical edge  using these properties we present a method for reduction of the given minrank computation problem into a smaller proble  also, we give an heuristic algorithm to find a clique cover of the side-information graph by using some binary operations on the adjacency matrix of the side-information grap  we also give a method to convert a groupcast index coding problem into a single unicast index coding proble  combining all these results, we construct index codes for groupcast index coding problem  the construction technique is independent of field size and hence can be used to construct index codes over binary fiel  in some cases the constructed index codes are better than the best known in the literature both in terms of the length of the code and the minimum field size require ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "I",
    "Section": "I Introduction",
    "Text": " the transmitter can take cognizance of the side-information of the receivers and broadcast coded messages, called the index code, over a noiseless channe  the objective is to minimize the number of coded transmissions, called the length of the index code, such that each receiver can decode its demanded message using its side-information and the coded message  an index coding problem with no restrictions on wantset and side-information is called a groupcast index coding proble  let ek = belong to fk q then, the minrankq over fq is defined as min{rankfq of its sideinformation hypergrap  however, finding the minrank for any arbitrary side-information hypergraph is np-hard there exists a low rank matrix completion method to find the rank of a binary matrix which is also np-hard an index coding problem is unicast if the demand sets of the receivers are disjoin  an index coding problem is called single unicast if the demand sets of the receivers are disjoint and every receiver wants only one messag  any unicast index problem can be equivalently reduced to a single unicast index coding problem in a single unicast index coding problem, the number of messages is equal to the number of receiver  in g, there exists an edge from xi to xj if the receiver wanting xi knows x  let rkq denote the rank of this matrix over f  in a side-information graph, if receiver ri knows xj and receiver rj knows xi, then the vertices xi and xj in the side-information graph are connected with an undirected edg  the undirected edges in the side-information graph contribute towards cliques in the side-information grap  all the receivers which want a message symbol in a clique can be satisfied by one index code symbol which is the xor of all message symbols present in the cliqu  all the receivers which wants a message symbol in a cycle of length k can be satisfied by k -1 index code symbol  as finding the minrank of a side-information graph is nphard, many researchers have proposed heuristic methods to solve the minrank proble  birk et a  proposed least difference greedy clique cover algorithm to find the cliques in the side-information grap  ldg algorithm works by computing all the possible distances between the rows of the fitting matri  kwak et a  proposed extended least difference greedy clique cover algorithm to find the cliques in the side-information grap  eldg algorithm works by computing all possible distances between the rows and columns of the fitting matri  eldg algorithm also gives a method to find directed cycles of length thre  awais e  al proposed an algorithm to piggyback a message which is sparsely connected to cycles of length k on the given graph   note that an undirected edge from xi to xj implies that there exists two directed edges, one from xi to xj and the other from xj and x  the th entry of the matrix an gives the number of paths of length n from the vertex xi to xj these properties of adjacency matrix were used in to give an algorithm to piggyback a message which is sparsely connected to cycles of length n on the given side-information graph   index coding is motivated by wireless broadcasting applications where the side-information may be a random quantit  the size of cliques and cycles in random graphs were extensively studied in the literatur  grimmett e  a  being the size of the cliques is small ), heuristic algorithms may be very useful to give a solution to the index coding proble  in this paper, we give a heuristic approach to find the clique cover and a heuristic method to convert the groupcast index coding problem into a unicast index coding proble  the index coding problem g is critical if every edge e is critica  tahmasbi et a  studied critical graphs and analyzed some properties of critical graphs with respect to capacity regio  in this paper, we analyze properties of minrank by defining the notion of minrank-critical edge  hence, identification of every minranknon-critical edge can reduce the number of computations required to compute the minrank by hal  tehrani e  al in proposed a partition multicast technique to address the groupcast index coding proble  in the partition multicast, one divides the messages into partitions and consider each partition as a partial cliqu  the messages are partitioned in such a way that the sum of savings of all partitions are maximize  however, the proposed partition multicast technique is suboptimal and nphard and the required field size depends on the number of messages in the partition and the number of messages known to each receiver in the partitio  throughout we assume a finite field with characteristic 2 and use the xor operation for convenienc  however the results are easily extendable to finite fields with any characteristi    contributions the main contributions of this paper are summarized as follow  partition multicast index codes is the best known in the literature for groupcast index coding problems and they do not exist for all field  we give instances of groupcast index coding problem where the length of index code obtained by using proposed method is less than that of partition multicas  we define the notion of minrank-critical edges in a side-information graph and derive some properties of minrank, which identify minrank-non-critical edges in a side-information grap  by using the properties of minrank, we give an algorithm to convert the given minrank computation problem into a smaller proble  we give a heuristic algorithm to find a clique cover of the side-information grap  we also give a sub-optimal method to convert a groupcast index coding problem into a single unicast index coding proble  the remaining part of this paper is organized as follow  in section ii, we derive some properties of the minrank of a sideinformation graph and give a method to reduce the complexity of the minrank computation proble  in section iii, we give a method to construct index codes for groupcast index coding problems which works over every finite fiel  we conclude the paper in section i  in the appendix we give a heuristic algorithm to find the clique cover of side-information grap  i  properties of minrank of a side-information graph in this section, we derive some properties of the minrank of the index coding proble  by using the derived properties, we provide a method to identify minrank-non-critical edges of a side-information grap  as the number of computations required to find exact value of the minrank is exponential in the number of edges present in the side-information graph, identification of every minrank-non-critical edge can reduce the number of computations required to compute the minrank by hal  let g be the side-information graph of an suicp with k message  let g be the side-information graph after removing all the incoming and outgoing edges associated with a vertex xk for any xk belong to v proo  let ak be the fitting matrix of g  the matrix ak is a * matrix which can be obtained from a by removing the row and column corresponding to x  thus the minrank of gk can not be greater than the minrank of   the graph g is the union of gk and the isolated vertex x  the minrank of the isolated vertex is one and the minrank of a union of disjoint subgraphs is equal to the sum of the minrank of the subgraph  thus the minrank of g is atmost one greater than the minrank of   consider the side-information graph g in fi  g1 g2 g3 fi  let a13 be the fitting matrix of g1  the matrix a13 can be obtained from a by removing the rows and columns corresponding to the vertices in v thus the minrank of g13 can not be greater than the minrank of   but the graph g13 is the disjoint union of the graphs g1 and g  this along with completes the proo  consider the graph g in fi  g1 g2 xk fi  consider the case when the minrank of g is m -  that is, the row and the column corresponding to the vertex xk in the fitting matrix of g are in the span of rows and columns corresponding to the remaining vertices of the graph respectivel  let lj be the row corresponding to xj in the fitting matrix of g for any j belong to let lk be in the span of li1, li2,ltt and lk such that the ones in the diagonal positions gets cancele  this is a contradiction to our assumption that there exists no cycle through x  proo  let the minrank of gk be m -  let g be the graph after removing all the incoming and outgoing edges from xk in   hence, removing all incoming and outgoing edges from xk does not reduce the minrank of g and these edges are minranknon-critica  hence the presence of xk in any directed cycle can be obtained from pk i=1 a  hence by computing pk i=1 ai, one can identify all the vertices which are not present in any cycl  consider the side-information graph g given in fi  the adjacency matrix a of g and p7 k=0 ak are shown belo  the element in p7 k=0 ak is zero indicates that the vertex x3 is not present in any directed cycl  let a3 be the matrix after deleting the third row and third column of   we have minrank=minrank+1 and we can compute the minrank of a by computing the minrank of a  x1 x2 x5 x3 x4 x7 x6 fi  then, the minrank of gc is atmost one greater than the minrank of   proo  the minrank of g1 is one as it is a cliqu  the last inequality in the above equation follows from the fact that g2 is a subgraph of   this completes the proo  let g be a side-information grap  the three examples given below illustrate definition   consider the side-information graph g1 given in fi  in g1, an undirected edge represents a clique of size tw  the vertex x1 is present in a cycle which comprises of vertices only from the set vr similarly, the vertex x2 is present in a cycle which comprises of vertices only from the set vr the vertex x3 is not present in any cycle comprising of vertices only from the set v  x1 x3 x2 x7 x6 x4 x5 x7 x7 x7 x2 x8 x9 x10 x11 fi  consider the side-information graph g2 given in fi  the graph in fi  5 is same as that of the graph in fi  4, except one edge from x3 to x5 is remove  consider the side-information graph g3 given in fi  the graph in fi  6 is same as that of the graph in fi  4, except the direction of one edge from x6 to x7 is reverse  the vertex x1 x3 x2 x7 x6 x4 x5 x9 x8 x11 x10 fi  x1 x3 x2 x7 x6 x4 x5 x9 x8 x11 x10 fi  let g be a side-information grap  then all the edges between ci and cj are minrank-non-critica  proo  let g3 be the induced graph of vr in   hence, the vertices in g1 are connected to g3 via g  figure 7 is useful to illustrate   hence, we have minrank=  this completes the proo  xk xk0 g1 g2 g3 ci n fxkg cj n fxk0g vr fi  note that any vertex is also a trivial clique of size on  then, minrank = minran  proo  let ci and cj be two cycle-free clique  consider the side-information graph g3 given in fi  x1 x3 x2 x7 x6 x4 x5 x9 x8 x11 x10 fi  8 theorem 3 reduces the minrank computation problem into a smaller problem in terms of number of edges construction i given in next subsection reduces the minrank computation problem into a smaller problem in terms of both the number of vertices and the number of edge    a heuristic method to reduce the minrank computation problem in the following three steps, we give a heuristic approach to reduce the minrank computation problem into a smaller proble  we refer the following three steps as construction i in the rest of the pape  note that any vertex is also a trivial clique of size on xi|ci|} be the vertices in the ith clique for i belong to else, leave the vertex in ci as it i  the reduction of minrank computation problem by using theorem 3 and construction i is summarized belo  the necessary and sufficient conditions that the side-information graph g need to satisfy such that the minrank of g is equal to the minrank of gr needs further investigatio  init was shown that mais lower bounds the minrank of   let g be a side-information grap  proo  we have minrank = minran  this completes the proo  let g be the side-information graph of a single unicast ic  let gr be the graph obtained from g by using construction   an index code c for the icp represented by gr can be used as an index code for the icp represented by g after replacing yi with the xor of vertices present in ci for i belong to proo  let xj belong to ci for some i belong to from construction i, receiver rj knows all the messages corresponding to out neighbourhood of yi in g  hence, rj computes yi from c and from yi it computes x  consider the index coding problem represented by the side-information graph g given in fi  the reduced side-information graph gr is given in fi  in this example, the minrank of the graph g is four and the minrank of graph gr is fou  x1 x3 x2 x7 x6 x4 x5 fi  9 y1 y4 x6 x7 fi  10: reduced side-information of g given in fi  consider the index coding problem represented by the side-information graph g given in fi  from construction i, the reduced side-information graph gr is given in fi  in this example, the minrank of the graph g is three and the minrank of graph gr is thre  however, the reduction method given in construction i may not necessarily keep the rank sam  for some graphs, the reduction procedure given in construction i x1 x3 x2 x7 x6 x4 x5 fi  11 y1 y4 x6 x7 fi  12: reduced side-information of g given in fi  11 may increase the minrank of reduced side-information grap  example 8 given below is useful to understand thi  consider the index coding problem represented by the side-information graph g given in fi  from construction i, the reduced sideinformation graph gr is given in fi  x1 x3 x2 x7 x6 x4 x5 x9 x8 x11 x10 x11 x12 x13 fi  note that finding a clique cover of a graph is an np-hard proble  there exist various heuristic algorithms to find clique cover  in the appendix, we give a heuristic algorithm to find the cliques by using binary operations on the adjacency matri  there exist polynomial time cycle detection algorithms to check the cycle among a given set of vertices hence, given two cliques, one can y1 y4 x6 x9 x8 x11 x10 x11 x12 x13 x7 fi  14: reduced side-information of g given in fi  13 find whether two cliques are cycle-free or not in polynomial tim  ii  code construction for groupcast index coding problems in this section, we give a method to convert a groupcast index coding problem into a single unicast index coding proble  this method, along with the other techniques given in this paper leads to a construction of index code for groupcast index coding proble    let wk be the want-set and kk be the side-information of receiver rk for k belong to in groupcast index coding problem, there are no restrictions on want-set and side-information of each receive  consider a groupcast index coding problem with k messages and m receiver  then, any index code for this single unicast icp is also an index code for the groupcast ic  proo  let c be the index code for the converted single unicast index coding proble  steps to construct index code for groupcast index coding problems in the following four steps, we give a heuristic approach to construct an index code for groupcast index coding problem  we refer the following four steps as construction ii in the rest of the pape  construction ii step   convert the given groupcast index coding problem into a single unicast index coding problem by using theorem   find the clique cover by using algorithm   reduce the given minrank problem into a smaller problem by using construction   find the cycle cover in the reduced minrank problem by using any cycle cover algorith  construct the index code by using the clique cover and cycle cover found in step 2 and step   the other method that can be used to construct index codes for groupcast problems is partition multicast however, the partition multicast is np-hard and requires higher field siz  the field size required in partition multicast depends on the number of messages in a partition and the number of messages known to each receiver in the partitio  whereas, construction ii can be used to construct index code in polynomial time and this method is independent of field siz  note that both partition multicast and construction ii are suboptimal in the length of index cod  consider a groupcast index coding problem with seven messages and ten receivers as given in table   the single unicast index coding problem corresponding to the groupcast index coding problem obtained from theorem 4 is given in table i  cliques in this single unicast index coding problem can be found by using algorithm 1 and this single unicast index coding problem can be converted into a reduced index coding problem by using construction   the side-information graph of single unicast icp given in table ii and its reduced side-information graph are shown in fi  15 and fi  the minrank of gr and hence the minrank of g is fou  some of the groupcast index coding problems in which the length of the index code given by construction ii is less than the length obtained from partition multicast are given in table ii  for the groupcast index coding problem given in   n ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "II",
    "Section": "II Properties of minrank of a side-information graph",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "III",
    "Section": "III Code construction for groupcast index coding problems",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "IV",
    "Section": "IV conclusion and discussions",
    "Text": "in this paper, we give a method to find some of the minranknon-critical edges in the side-information grap  we give a simple heuristic method to find the clique cover by using binary operations on adjacency matri  we presented a method to address groupcast index coding problem  it is interesting to analyze more properties of minrank and design algorithms to compute the minrank of a side-information graph in a more efficient wa  bose national fellowship to   sundar raja ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "References",
    "Section": "References",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "Appendix",
    "Section": "Appendix",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "Abstract",
    "Section": "Abstract",
    "Text": "arxiv:190 com marcos  br abstract in this paper, we are concerned with a kirchhoffproblem in the presence of a strongly-singular term perturbed by a discontinuous nonlinearity of the heaviside type in the setting of orliczsobolev spac  *carlos alberto santos acknowledges the support of capes/brazil proc",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "1",
    "Section": "1 Introduction",
    "Text": " our approach is based on the existence 3 of positive solution to the problemwhich provides a non-empty effective domain for the energy functional associated to and enable us to apply techniques of the generalized gradient in clarke sense to get a multiplicity resul  besides this, we prove qualitative results about these three solution  to our knowledge, both the results of equivalent conditions and qualitative information on solutions are new in literatur  as our main results will be obtained via variational methods, we need to introduce the energy functional associated to problem another diffculty exploited in this work is the presence of a more general quasilinear operator, which may be even nonhomogeneou  to deal with this situation, we approach the problem in orlicz-sobolev space settin  for more information about the orlicz and orlicz-sobolev spaces, we referand before stating the main results, let us clarify what we mean by a solution of about multiplicity, our main result can be stated as follow  as a consequence of theorem   this is the content of the next corollar 1 assumeand hol  in particular, as a consequence of theorem  1, we have the followin 2 extends the principal result in faraci e  this paper is organized as follow  in section 2, we present some preliminary knowledge on the orlicz-sobolev spaces and some results of non-smooth analysis related to our proble  the section 3 is reserved to prove theorem  ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "2",
    "Section": "2 Non-smooth analysis for locally Lipschitz functional",
    "Text": "in this section, we are going to remember some facts related to non-smooth analysi 2, via ricceri's theorem next, we present a summary proof of the other item  so, it follows from theorem   the conclusion of the proof is a direct consequence of theorem  1 in and classical riesz theorem for orlicz spaces, see for instance to end the proof, it follows from proposition   this ends the proo  then, i is a coercive functiona  proof first, by the assumption and lemma   this ends the proo  then i satisfies the conditio  so, by lemmas   this ends the proo  below, let us connect the existence of solution to problem with existence of two local minima to the functional   hence, we obtain from proposition   this ends the proo  by applying corollary 2",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "3",
    "Section": "3 Proof of Theorem ??",
    "Text": "1 before starting the proof of theorem  1, let us prove the two below lemma  proof of so, it follows from lemma   this ends the proof of i).  17 let us prove ii).  this ends the proof of item ii).  now, we are ready to prove iii).  we know from lemmas   let us prove this ends the proof of lemm 1-.  this proves i).  let us prove ii).  20 this ends the proof of lemm  proof of theorem  1-conclusio  the item - are consequences of lemmas   similar arguments work when u is a local maximum for   below, let us consider two case  on the other hand, it follows from and lemma   hold  this ends the proof of i).  so, it follows from lemma   as we already know from lemma   finely, by applying lemma   this ends the proo ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "4",
    "Section": "4 Proof of Theorem ??",
    "Text": " to provewe borrow ideas from yijingwho treated this situation in the context of homogeneous operator  the is immediatel  we will end this section ensuring that the discontinuity of the nonlinearity f may be attaine  proof take u belong to   so, it follows from and lemma   in particular, n is unbounded as wel  this ends the proo  by using similar ideas as done yijing for the homogeneous case, we are able to prove the below lemma in the context of non-local and non-homogeneous operato  to complete our basics tools to prove theorem  2, let us prove the below lemma that is interesting itsel  this is impossible and so the proof of lemma  3 is don  proof of theorem  2-conclusio  we begin proving the first implicatio  so, it follows from lemmas  2 that n is a nonempty complete metric spac  moreover, by lemmas   hence, it follows from the same arguments as used to prove lemma   by corollary   so, by theorem  1 we know that each one of these critical point is a solution for the problem that satisfy the qualitative properties claime  these ends the proof of the equivalence  below, let us prove the items iv) and we are going to prove iv) firs  let u = ua be a solution of problem this ends the proof of this ite  let ua be a solution of problem proof of corollary   let us construct a such on  this finishes the proo ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "Abstract",
    "Section": "Abstract",
    "Text": "com, swarnendughos cs rs,nibara das@jadavpuruniversit in dep  handwritten character recognition is a typical example of such task that has also attracted attentio  cnn architectures such as lenet and alexnet have become very prominent over the last two decades however the spatial invariance of the different kernels has been a prominent issue till no  with the introduction of capsule networks, kernels can work together in consensus with one another with the help of dynamic routing, that combines individual opinions of multiple groups of kernels called capsules to employ equivariance among kernel  in the current work, we have implemented capsule network on handwritten indic digits and character datasets to show its superiority over networks like lene  furthermore, it has also been shown that they can boost the performance of other networks like lenet and alexnet",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "I",
    "Section": "I Introduction",
    "Text": "it has been two decades since the first convolutional neural networks was introduced in 1998 for handwritten digit classification proble  since then computer vision has matured a lot in terms of both the complexity of the architectures as well as the difficulty of the challenges they addres  many works have been introduced in later years to address challenges like object recognition however through all these years one principal issue was yet to be addresse  convolutional neural networks by its nature employ invariance of features against their spatial positio  as the kernels that represent specific features are convolved throughout the entire image, the amount activation is position invarian  the activations across different kernels do not communicate with each other and hence their outputs are spatially invarian  we have intuitively developed the skill to analyze relative positions of various parts of an objec  to learn these relations the capsule networks were proposed in short capsule networks consist of a group of kernels that work together and pass information to next layers through a mutual agreement that is achieved by dynamic routing of information during the forward pas  in our experiments, our goal is to analyze the performance of these networks for some indic digit and character dataset  there have been many works in indic datasets using cnns beforein our experiments, the performance of the capsule networks are compared with respect to lenet and alexne  to show that the capsule network learns unique concepts, we have combined it with other networks to show a boost in performanc  in the next section, a refresher is provided as to the basics of a simple cn  in section 3 it is shown how the capsule network evolves over the simple cnn along with explanations regarding its internal mechanism ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "II",
    "Section": "II CNN Refresher",
    "Text": "convolutional neural networks are typically designed as a series of 2-d convolution and pooling operations along with non-linear activations in the middle followed by a fully connected network for classificatio  a kernel convolution on such an input should be of a shape cin *kh *k  note that the depth of the kernel is equal to the input number of channel  the height hout and width wout are dependent on factors like input height hin, input width win, stride of the kernel and the padding of the inpu  convolutions are typically followed by non-linear activations such as a sigmoid, tanh, or a rectified linear unit  pooling operations normally take a small region of the input and compresses it to a single value by taking either maximum or average of the corresponding activation  this reduces the size of the activation map  hence when kernels convolve over this tensor, it actually corresponds to a larger area in the original imag  after a series of convolutions, activations and pooling, we obtain a tensor signifying the extracted features of the imag  the total number of neurons in this layer nf c is arxiv:190 00166v1 1 jan 2019 fi  a schematic diagram of a sample convolutional network(pooling layers omitted to keep the architecture analogous with respect to fig   at the end of the fully connected network we get a vector of size nclass*1 that corresponds to the output laye  a loss such as mean-square error, or cross entropy, or negative log likelihood is computed which is then back-propagated to update the weights using optimizers like stochastic gradient descent or adaptive moment  a schematic diagram is shown in fi 1, with a typical convolution operation followed by a fully connected layer to perform classificatio  layers such as pooling and non-linearities are not shown to keep simplicity and to keep the diagram analogous to fi ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "III",
    "Section": "III The Capsule Network",
    "Text": "the primary concern with cnns are that the different kernels work independentl  if two kernels are trained to activate for two specific parts of an object they will generate the same amount activations irrespective of the relative positions of the objec  capsule networks brings a factor of agreement between kernels in the equatio  subsequent layers receive higher activations when kernels corresponding to different parts of the object agree with the general consensu  the capsule network proposed in consist of two different capsule layer  a primary capsule layer that groups convolutions to work together as a capsule uni  this is followed by a digit capsule layer that is obtained by calculating agreement among different capsules through dynamic routin  a schematic diagram of capsule network is provided in fi  the diagram does not represent the actual architecture proposed in the original workrather it demonstrates a primary and digit capsule laye  the diagram drawn is kept analogous to a typical cnn shown in fi 1 to highlight the major difference    primary capsules the capsule network starts with typical convolution layer that converts the input image into a block of activation  this tensor is fed as an input into the primary capsule laye  kh and kw are the height and width of the kerne  the capsule network replaced the output layers with a digit capsule laye  each class is represented by a capsule of dimension dd  hence we get a digit capsule block of shape nclass * dd  by calculating the l2 norm of each row we get our output layer of shape nclass *   the values of digit capsules are calculated by dynamic routing between primary capsule  dynamic routing the dynamic routing is computed to obtain the digit capsules from the primary capsule  two different types of weights are required to perform dynamic routin  firstly we need the weights to calculate individual opinions of every capsul  these weights, w dc are normally trained using back-propagatio  if i belong to is the index of the primary capsules of dp c dimension and j belong to is the index of the digit capsules of ddc dimensio  so for each capsule i we get an individual digit capsule block of shape nclass * dd  the second type of weight can be called the routing weights the routing weights are used to combine these individual digit capsules to form the final digit capsule  fi  a schematic diagram of capsule network demonstrating primary and digit capsule layer  these routing weights are updated on during the forward pass based on how much the individual digit capsules agree with the combined on  during each forward pass the routing weights are first initialized as zero  the coupling coefficients are used to combine the individual digit capsules and form the combined digit capsul  the more the value of the agreement the more preference is awarded to the corresponding capsule i in the next routing iteratio v  equations 2-4 are repeated for a specific number of routing iterations to perform iterative dynamic routing of opinions of primary capsules to form the digit capsul  loss function the loss function used for capsule networks is a marginal loss for the existence of a digi  the upper and lower bounds m+ and m-are set to   regularization proper regularization of a network is essential to stop models from over-fitting the dat  in case of capsule networks a parallel decoder network is connected with the obtained digit capsules as its inpu  the decoder tries to reconstruct the input imag  a reconstruction loss is also minimized along with margin loss so that the network does not over-fit the training se  however the reconstruction loss is scaled down by a factor of  0005 so that the margin loss is not dominate ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "IV",
    "Section": "IV Experimentations and Results",
    "Text": "our experiments focus on the implementation of capsule networks for handwritten indic digits and character database  the results have been compared with other famous cnn architectures like lenet and alexne  while lenet was built for smaller problems like digit classification, alexnet was intended for much more complicated data like the imagene  in total 7 models have been tested on 5 dataset  firstly, the basic lenet, alexnet and capsule network was teste  second set of experiments involved an ensemble of two of the three networks using a probabilistic averagin  finally all the three networks were combined by averaging the output probability distributio  all the results are tabulated in table   fi  test accuracy vs number of training epochs for character dataset    datasets we have used five datasets for our experiment  firstly we have indic handwritten digit databases 1 in three scripts that is bangla, devanagari and telug  these are a typical 10 class problems to primarily challenge the performance of lene  subsequently, the character databases namely, bangla basic characters and bangla compound characters give us a 50 class and a 199 class problem to deal wit  the description of the datasets are given belo  all the datasets were split into train and test set in the ratio 2:  the accuracies provided are with respect to the best model in terms of training accurac  architecture and hyperparameters the capsule network has been used as it has been proposed in the performance is compared with respect to lenet and alexne  the specifics of the capsule network architecture is provided in table   the lenet was primarily built for mnist digit classification with only around 61k trainable parameter  the alexnet has around 57 million trainable parameters so that it can tackle harder problem  like lenet, the capsule network was also proposed for mnist digit classification, however it is much more robus  it has around  2 million parameters out of which around 11k parameters are trained on the runtime by dynamic routin  all the provided statistics is with respect to a single channel input of native input size and a 10 class outpu  all networks are optimized with adam optimizer with an initial learning rate of  001, eps of 1e-08 and beta values as   the experiments were carried out using a nvidia quadro p5000 with 2560 cuda cores and 16 gb of vra  1https://cod googl  result and analysis the result of the experiments have been tabulated in table   it can be clearly seen that capsule networks surpasses lenet in case of every dataset use  alexnet being almost 7 times larger network as compared to capsule networks performs better than capsule network  however the difference in performance is much more visible in case of the character datasets with much higher number of classes as compared to digit  lenet fail poorly for the character dataset  capsule network proved to be much more robust against complex data with higher number of classe  upon combination we can see that combining lenet with alexnet is detrimental in nature with respect to alexnet alone for every datase  however combining capsule networks have always shown a positive effec  this proves that capsule network are capable of extracting some information that even alexnet fails to obtai  for most datasets the best performance was achieved by combining alexnet with capsule networks except for telugu digits, where combination of all three networks proved to be the bes  furthermore we have analyzed the rise of test accuracy with every epoch of trainin  it can be seen that the capsule networks have the steepest slope signifying that they have the fastest learning curv  finally in table 3 we have compared the obtained result against some state of the art works performed on the dataset  in terms of computational complexity, the extra computational overhead is during the dynamic routing phas  during each iteration np c *nclass number of coupling coefficients must be calculate  further an weighted sum over np c dimension is needed to compute the combined digit capsul  finally np c *nclass number of routing weights must be tuned using agreement of individual and combined digit capsule 62 table iii comparitive study against other state of the art approaches dataset our approach accuracy other approaches accuracy bangla digits alexnet + capsnet 9 75 basu et al67 roy et a 08 roy et a 45 devanagari digits alexnet + capsnet 9 60 das et a 44 roy et a 50 telugu digits alexnet + capsnet + lenet 9 80 sarkhel et a 50 roy et a 20 bangla basic characters alexnet + capsnet 9 20 sarkhel et a 53 bhattacharya et a 15 bangla compound characters alexnet + capsnet 9 40 roy et a 33 pal et a 12 sarkhel et a 64 but as evident from fi  iv it also learns much faster as compared to lenet and alexne ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "V",
    "Section": "V Conclusion",
    "Text": "in our current work we have implemented the capsule networks on handwritten indic digits and character database  we have shown that capsule networks are much superior and robust compared to the lenet architectur  we have also seen that capsule networks can act as a booster when combined with other networks like lenet and alexne  the best performance was achieved by combining alexnet with capsule networks for most of the dataset  only in case of telugu dataset, combination of all three networks worked the bes  from the results it can be concluded that even with 7 times more parameters that capsule networks, the alexnet failed to capture some information that the capsule network learn  thus it was able to improve the performance of alexne  finally it has also been seen the capsule network converge much faster that lenet or alexne  in terms of pros and cons, the use of capsule networks can be beneficial for learning with much lesser number of features and also as improvement technique for other bigger network  the problem with capsule network is its slow iterative process and limitation to single layer routin  that reveals many avenues of researc  acknowledgment this work is partially supported by the project order n ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "References",
    "Section": "References",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "Abstract",
    "Section": "Abstract",
    "Text": " we review the well-known peebles-vilenkin quintessential inflation model and discuss its possible improvements in agreement with the recent observation  contents",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "Contents",
    "Section": " Contents",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "1",
    "Section": "1 Introduction",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "2",
    "Section": "2 The original model",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "3",
    "Section": "3 Evolution from kination to matter-radiation equality",
    "Text": "",
    "Subsections": [
      {
        "Section_Num": "3_1",
        "Section": "3.1 Decay before the end of kination",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
      },
      {
        "Section_Num": "3_2",
        "Section": "3.2 Decay after the end of kination",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
      }
    ],
    "Groundtruth": ""
  },
  {
    "Section_Num": "4",
    "Section": "4 Evolution from the matter-radiation equality",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "5",
    "Section": "5 Exponential quintessence potential",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "6",
    "Section": "6 Concluding remarks",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "References",
    "Section": " References",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "Abstract",
    "Section": "Abstract",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "Optimal",
    "Section": "Optimal Object Placement using a Virtual Axis",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "Abstract",
    "Section": "Abstract",
    "Text": "arxiv:190  in this note, an upper bound for the sum of fractional parts of certain smooth functions is establishe  such sums arise naturally in numerous problems of analytic number theor  the main feature is here an improvement of the main term due to the use of weyl's bound for exponential sums and a device used by popo ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "1",
    "Section": "1. Introduction and main result",
    "Text": " the problem is henceforth reduced to estimating exponential sums, for which several methods have been developed by weyl, van der corput and vinogrado  for monomial functions,   primary 11l07; secondary 11l15, 11j5  key words and phrase  weyl's and van der corput's exponential sums, fractional par  unfortunately, as often in exponential sums estimates, the secondary terms remain too weak to be really efficients in practic  nevertheless, the result below seems to be new and we think that it may be of interes  define dk := 2k+2 ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "2",
    "Section": "2. Technical lemmas",
    "Text": " proo  this is proof",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "3",
    "Section": "3. Proof of Theorem ??",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "4",
    "Section": "4. Extension to integer points close to smooth curves",
    "Text": " define dk := 2k+2  this must be compared to the existing results of the theor ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "References",
    "Section": "References",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "Abstract",
    "Section": "Abstract",
    "Text": "ed cn, jingdw@microsof com abstract cross-platform recommendation aims to improve recommendation accuracy through associating information from different platform  existing crossplatform recommendation approaches assume all cross-platform information to be consistent with each other and can be aligne  in this paper, we propose a cross-platform association model for cross-platform video recommendation,   the proposed dca model employs a partially-connected multi-modal autoencoder, which is capable of explicitly capturing platform-specific information, as well as utilizing nonlinear mapping functions to handle granularity difference  we then present a crossplatform video recommendation approach based on the proposed dca mode  extensive experiments for our cross-platform recommendation framework on real-world dataset demonstrate that the proposed dca model significantly outperform existing cross-platform recommendation methods in terms of various evaluation metric ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "1",
    "Section": "1 Introduction",
    "Text": "recommender systems are playing an important role in our lif  with the emergence of various online services, people are now getting used to engaging on different platforms simultaneously in order to meet their increasing diverse information needs the complementary information from various platforms jointly reflects user interests and preferences, providing us with a great opportunity to tackle the data sparsity problem and improve the recommendation accuracy through associating information across *corresponding author platform  in order to improve user experience and increase user adoption, many online service providers release new features to encourage cross-platform data association  for example, google+ encourages users to share their homepages on other platforms, enabling different accounts of the same person to be linked togethe  to date, quite a number of various platforms tend to associate with each other, which provides chances for cross-platform information transfer and association analysis possibl  existing cross-platform recommendation works try to improve their cross-platform performances mainly by investigating the ablity of cross-platform associatio  yan et a  propose a user-centric topic association framework to map cross-platform data in a common latent space they then introduce a predefined micro-level metric to adaptively weight data while doing the data integration man et a  propose an embedding and mapping framework, emcdr, to represent and associate users across different platforms however, existing works on cross-platform video recommendation ignore the inconsistencies in crossplatform association and differences in semantic granularities, two challenging phenomena discovered in this pape  our discovery demonstrates that the inconsistency is mainly caused by the platform-specific disparity,   the second challenge is that different platforms may have different semantic granularitie  take twitter and youtube as an example, tags on twitter are mainly hot event topics, which are generally finer-grained than video categories on youtub  as such, a good crossplatform video recommendation model based on the correlated information across platforms should have the ability to reveal characteristics at different semantic granularitie  to address these challenges, we propose the disparitypreserved deep cross-platform association model for cross-platform recommendation, which employs a partiallyconnected multi-modal autoencoder to explicitly capture and preserve platform-specific disparities in latent representaarxiv:190 00171v2 12 nov 2019 tio  besides, we also introduce nonlinear mapping function, which is superior to their linear competitors in terms of handling granularity differenc  to validate the advantages of our proposed dca model, we further design a cross-platform video recommendation approach based on the proposed dca model, which is capable of automatically targeting and transferring useful cross-platform information from a comprehensive semantic leve ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "2",
    "Section": "2 Related Work",
    "Text": "existing cross-platform recommendation works can be categorized in two way  categorization by what to associat  one option is to take advantage of different platform characteristics towards collaborative application  particularly, qian et a  propose a generic cross-domain collaborative learning framework based on nonparametric bayesian dictionary learning model for cross-domain data analysis min et a  propose a cross-platform multi-modal topic model which is capable of differentiating topics and aligning modalities an alternative option is to associate in a user-centric way, which focuses on integrating multiple sources of overlapped user's activitie  in particaular, xptrans optimally bridges different platforms through exploiting the information from a small number of overlapped crowd  yan et a  propose an overlapped user-centric topic association framework based on latent attribute sparse coding, and prove that bridging information from different platforms in common latent space outperforms explicit matrix-oriented transfer man et a  propose an embedding and mapping framework, emcdr, where user representations on different platforms are first learnt through matrix factorization and then mapped via multi-layer perceptron although the user-centric works above are based on different premises, they share the same core idea that all cross-platform information is consistent and should be aligne  orange and purple dots represent two groups of aligned users across platform  concentrated areas are circled with dashed line  problem by data selectio  to be concrete, lu et a  find that selecting consistent auxiliary data is important for crossdomain collaborative filtering they propose a novel criterion to assesses the degree of consistency, and embed it into a boosting framework to selectively transfer knowledg  yan et a  divide users into three groups and introduce a predefined micro-level user-specific metric to adaptively weight data while integrating information across platforms our proposed dca model associates information from different platforms in the user-centric wa  categorization by entire model structur  one group of methods build the model in a unified framework, where the former two works adopt matrix factorization and the latter two employ the probabilistic model  another group of works adopt a two-step framewor  these works first map users from different platforms into corresponding latent spaces for representations, and then associate these representation  our proposed model in this work utilizes the two-step structure in a topic-based wa ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "3",
    "Section": "3 Measurement and Observation",
    "Text": "198 table 1: average euclidean distance from the center of user groups clustered on twitter v  rando  user groups clustered on twitter no longer concentrate on youtubeindicating cross-platform user interest disparit  closer two dots are in distance, the more similar their corresponding users are in topic spac  we observe from figure 1 that only a part of users who are similar on twitter still stay similar on youtube and the rest actually becomes quite differen  in addition to visualizing the interest representations, table 1 further confirms the existence of the inconsistency phenomenon through a measurement study on casiacrossos  i) for each clustered user group on twitter and youtube, we calculate the average euclidean distance from the group center to group member  ii) we do the same thing for randomly sampled groups on both platforms and use the calculated average euclidean distances as normalizers to obtain the concentration ratio such that a larger concentration ratio indicates a less clustered patter  we close this section by giving a conclusion on the existence of inconsistency in cross-platform association: users' interests on different platforms may be diverse and inconsisten 1 problem formulation we first introduce the problem of user-centric cross-platform associatio  the goal is to find the association among these representationsso that cross-platform applications can be carried out2 disparity-preserved deep cross-platform association multi-modal autoencoder is an extension to basic autoencode  it is trained to reconstruct multiple inputs from a unified hidden layer when given multiple modalities of the same entity, thus is able to discover correlations across modalitie  it has been widely applied to various multi-modal tasks since being proposed hong et a  extend the concept of multimodal to multi-source in this paper, we treat representations of the same user on different platforms as multiple modalities of his or her unified latent representation, and modify the structure of multi-modal autoencoder to conduct disparity-preserved cross-platform associatio  xi is the multi-modal input layer, specifically the representations ui of the user u on different platform  h is the hidden layer,  e, the derived unified user latent representatio e, the reconstructions of input  g is the activation functio  the second term is regularizer to avoid overfittin  the third term is a sparsity constraint to encourage sparse latent representation  we minimize the loss function and train the parameters through backpropagatio  multi-modal autoencoder takes representations of the same user on different platforms as inputs, mapping them to a unified hidden layer h and expecting accurate reconstruction  it learns mapping functions from representations on different platforms to the unified representation and then vice versa, which automatically completes the process of cross-platform associatio  moreover, it is able to handle the granularity difference through introduction of nonlinearit  we note that all multi-modal inputs should be given for the multi-modal autoencoder to work properl  ut and uy are representations of a same user on twitter and youtube respectivel  in latent representations, ht and hy are platform-specific parts preserving the disparities, and hc is the common part associating different platform  equall  however, such an approach does not scale well as we will need an exponential number of model  ngiam et a  propose to train bi-modal autoencoder using augmented but noisy data with additional examples that have only one single modality as inputs in this paper, we augment data with average values of different platforms, and still expect the network to reconstruct real representations for all modalitie  as aforementioned, platform-specific disparity may cause inconsistencies in cross-platform associatio  we need to envisage it and avoid messing up the associatio  to achieve this goal, we divide the hidden layer into three parts and cut off certain links, through which the shared and platformspecific parts are explicitly captured and preserve  as is shown in figure 2, we assume that twitter representations are derived from ht and hc without hy while youtube representations are derived from hy and hc without ht the model is able to automatically map representations of the same user on different platforms to a unified representation while preserving the information in platform-specific parts at the same tim  it does not require cross-platform data to be thoroughly consistent by allowing the existences of inconsistencie  thanks to the usage of shared common structure, cross-platform association becomes clearer and tighte  in addition, most traditional linear methods associate representations on different platforms by utilizing a linear transfer matrix, which is formulated in e  and e  the linear combination limits the power in modeling complex relations across different semantic granularitie  our modified multi-modal autoencoder naturally introduces nonlinear activation function g, possessing great advantages in handling the challenging granularity differences over linear method  we choose sigmoid as the activation function in this paper and other activation functions such as relu or tanh can also be use ",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "4",
    "Section": "4 DCA: Disparity-preserved Deep Cross-platform Association",
    "Text": "",
    "Subsections": [],
    "Groundtruth": ""
  },
  {
    "Section_Num": "5",
    "Section": "5 Cross-platform Video Recommendation",
    "Text": "in this section, we present the cross-platform video recommendation approach based on the proposed dca mode ",
    "Subsections": [
      {
        "Section_Num": "5_1",
        "Section": "5.1 Problem Formulation",
        "Text": "we first formulate the cross-platform video recommendation proble  the goal of crossplatform video recommendation is to recommend videos for those aligned users on video platform through making use of the cross-platform informatio  the capability of dca model in tackling the cross-platform inconsistency issue through explicitly capturing shared information and preserving platform disparities enables the cross-platform recommendation method to automatically concentrate on useful information and discard useless on  moreover, nonlinear mapping functions in our proposed approach enhance the method's ability to conduct association across platforms at a deeper and more comprehensive semantic leve  in order to apply our proposed dca model for the associations between twitter and youtube, we first obtain user representations by latent dirichlet allocation on twitter, we regard each user's tweeting data together as a document in latent dirichlet allocation then each user u belong to u can be represented by a topic distribution ut we remark that v and uy are in the same youtube topic space, which enables similarity matching for recommendatio  figure 3: cross-platform video recommendation framework based on the proposed disparity-preserved deep cross-platform association model then the representation pairs of the same user as well as the augmented examples are fed to the dca model to learn cross-platform associatio  because of the input augmenting, the trained model is able to predict user representations on youtube when only information from twitter is availabl  we finally adopt euclidean similarities between the representations of the predicted user and candidate videos as the properness measur  the recommendation results can be obtained according to the rank of candidate video ",
        "Subsections": [],
        "Groundtruth": ""
      }
    ],
    "Groundtruth": ""
  },
  {
    "Section_Num": "6",
    "Section": "6 Experiments",
    "Text": "in this section, we carry out extensive experiments on a real-world cross-platform dataset and compare our proposed method with several state-of-the-art algorithms to show the advantages of the proposed approac ",
    "Subsections": [
      {
        "Section_Num": "6_1",
        "Section": "6.1 Dataset",
        "Text": "casia-crossosn1 is a cross-network user dataset with account linkages between youtube and twitter, created by institute of automation, chinese academy of science  it contains 11,687 aligned users across platforms and 2,280,129 youtube video  on youtube, three kinds of user behaviors as well as videos' rich metadata are collecte  on twitter, only users' representations extracted from their tweets via standard topic modeling process are provided because of privacy concern  we filter aligned users and youtube videos by keeping users who interacted with at least 3 videos and videos which were consumed by at least 3 user  1casia-crossosn dataset: homepage/myan/datase html",
        "Subsections": [],
        "Groundtruth": ""
      },
      {
        "Section_Num": "6_2",
        "Section": "6.2 Experimental Settings",
        "Text": "on twitter, casia-crossosn offers a 60-dimensional topical distribution for each use  on youtube, we resort to perplexity to determine the number of topic  mlp-based nonlinear mapping man et a  employ mlp to cope with the latent space matching problem mlp is a flexible nonlinear transformatio 3 evaluation of cross-platform association as for the proposed model in this paper, multi-modal autoencoder with l1-norm but without disparity-preserved structure and the dca model are examine  we perform evaluations in two scenarios: infer youtube from twitte  given users with their twitter representation utto estimate their youtube representation uy infer twitter from youtube in tur  the results show that both disparity preserving and nonlinearity contribute to better cross-platform association performanc  for model parameters, in lr model as e  in la model as e  in the dca model as e  we then carefully fine-tune by a combined line-search strateg  specifically for all randomly initialized autoencoder model, we conduct 6 experiments and take the average resul  performances are shown in figure 4 several observations can be made: la and mlp achieve better result than lr, indicating that the shared latent structure and nonlinear mapping function both benefit the associatio  ma outperforms la and mlp through combining the two strategie  compared with ma, dca further improves the performance, demonstrating the contribution of the disparitypreserved structur  we also notice that compared with nonlinear modelsthe optimal parameters of the linear models vary greatly between two scenario  we think it is the granularity differences that cause the parameter gap  non-linear mappings can naturally tackle this issue, and have more advantages over linear one  in total, the proposed dca model outperforms la by reducing relatively   it greatly improves the performance by preserving the disparities and introducing nonlinearity, validating that envisaging the inconsistency caused by platform-specific disparity and granularity difference jointly contributes to better cross-platform association performanc 4 evaluation of cross-platform video recommendation then we evaluate the proposed dca-based cross-platform video recommendation method for new youtube users, following the above infer youtube from twitter scenari  for each test user u, we randomly select as many as groundtruth interacted videos as additional candidate  model precision recall f-score lr   the results show that the proposed dca-based cross-platform video recommendation method outperforms other approache  evaluation metrics the evaluation metrics are calculated by examining whether the recommended videos are included in u's groundtruth video set v  final results are averaged over all test user  the proposed dca-based method improves over existing cross-platform association-based recommendation approache ",
        "Subsections": [],
        "Groundtruth": ""
      }
    ],
    "Groundtruth": ""
  },
  {
    "Section_Num": "7",
    "Section": "7 Conclusion",
    "Text": "in this paper, we discover the existence of inconsistency in cross-platform recommendatio  we propose the dca model, which tackles the inconsistency issue, as well as the granularity difference proble  we further present a crossplatform video recommendation method based on the proposed dca mode  extensive experiments demonstrate the superiority of the dca model and the dca-based crossplatform recommendation approach over several state-of-theart method  acknowledgments this research is supported by national program on key basic research project n  2015cb352300, national natural science foundation of china major project n  u1611461, china postdoctoral science foundation n  bx201700136 and shenzhen nanshan district ling-hang team grant under n lhtd2017000 ",
    "Subsections": [],
    "Groundtruth": ""
  }
]
