[{"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190 00230v2 4 dec 2019 the weil algebra of a double lie algebroid eckhard meinrenken and jeffrey pike abstrac  we show that vb-algebroid structures on d are equivalent to horizontal or vertical differentials on two of the weil algebras and a gerstenhaber bracket on the thir  furthermore, mackenzie's definition of a double lie algebroid is equivalent to compatibilities between two such structures on any one of the three weil algebra  contents", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1. Introduction", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2. Double vector bundles", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3. Double-linear functions", "Text": " the weil algebra w 16 5", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4. The Weil algebra W(D)", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5. Linear and core sections of A D", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "6", "Section": "6. Poisson double vector bundles", "Text": " double lie algebroids 37   applications, connections with other work 42 appendix   splitting of double vector bundles 48", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "7", "Section": "7. Relationships between brackets, differentials, and pairings", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "8", "Section": "8. Double Lie algebroids", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "9", "Section": "9. Applications, connections with other work", "Text": "42 appendix   splitting of double vector bundles 48", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Appendix", "Section": "Appendix A. Splitting of double vector bundles", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "References", "Section": "References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190 ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1. Introduction", "Text": "an important class of finite-dimensional representations for affne lie algebras are the kirillov-reshetikhin modules, which are characterized by their drinfel'd polynomials kr modules have been well-studied and have many interesting propertie  for example, their characters are solutions of the q-system an open problem is to determine a uniform model for kr crystal  this has been achieved for br,1 by using kashiwara's construction of projecting an extremal level-zero module/crystal this was done explicitly by naito and sagaki using lakshmibai-seshadri paths the construction of kashiwara was also shown to partially extend to general br,s in nonexceptional affne types kr crystals are connected with mathematical physic  we refer the reader to for more detail  another important property of kr crystals is that 2010 mathematics subject classificatio  05e10, 17b3  key words and phrase  was partially supported by the nserc discovery grant of her postdoc supervisor michael lau at universit e lava  was partially supported by the australian research council grant dp17010264  1 they are perfecta technical condition that allows highest weight crystals to be modeled using a semi-infinite tensor product known as the kyoto path model we achieve this by considering the decomposition of the classical highest weight crystal b into a6 highest weight crystals, which is multiplicity fre  the novelty of our approach is doing a further levi decomposition and reconstructing the affne action to a type a7 crystal rather than through the classical decompositio  we note that the local energy function is given by as a potential application of our results, the combinatorial r-matrix allows us to study soliton cellular automata of b7,1 using different techniques from this paper is organized as follow  in section 2, we give the necessary backgroun  in section 3, we give our main result  acknowledgment  the authors thank the referee for useful comments on our manuscrip ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2. Background", "Text": " let w0 be the weyl group of g ", "Subsections": [{"Section_Num": "2_1", "Section": "2.1. Crystals", "Text": " the dynkin diagram of type e 7 the definition of an abstract crystal given in this paper is sometimes called a regular or seminormal abstract crystal in the literatur g, for the more general definitio  for abstract uq-crystals b1, b2, our tensor product convention followswhich is opposite to that of kashiwara let b1 and b2 be two abstract uq-crystal  an abstract crystal b is a uq-crystal if b is the crystal basis of some uq-modul ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2_2", "Section": "2.2. Minuscule crystals", "Text": "we say a highest weight uq-crystal b is minuscule if w0 acts transitively on   let b be a minuscule representatio ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2_3", "Section": "2.3. Type An crystals", "Text": "in this section, we consider the lie algebra of type an, which is sln+  in terms of the crystal graph, we simply remove all n-colored edge  note that the decomposition of proposition  4 is multiplicity-fre ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3. Results", "Text": "in this section, we give our main result  see figure 2 for the crystal b with this labeling conventio  multiplicity freeness of   we first prove that when we decompose the e7 crystals b into the levi subalgebra of type a6, we obtain a multiplicity free decompositio  this can be seen by a direct computation using proposition   we remark by reversing the arrows and adding loops to every vertex inwe obtain the composition graph g  the crystal b in type e  therefore, from and the signature rule, we have the followin  let mi denote the number of occurrences of xi in t belong to   moreover, this decomposition is multiplicity fre  proo  the first claim follows immediately from lemma  1 and relabeling the fundamental weight  therefore, there is a unique m1, ", "Subsections": [{"Section_Num": "3_2", "Section": "3.2. Reconstructing the A7 crystals", "Text": "in this section, we continue to use the notation of proposition   hence, we obtain the followin  from proposition   let b7,s denote the corresponding crysta  in order to show this is the combinatorial structure of kr crystal, we need the following uniqueness theore 2 instead of we can then decompose the crystal b into i0,2-crystals according to proposition  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3_3", "Section": "3.3. Combinatorial isomorphism", "Text": " thus it would remain to sort these elements to obtain an honest component of   we could have arrived at this refined isomorphism by using the a6 decomposition of   recall that jeu-de-taquin is a crystal isomorphis ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3_4", "Section": "3.4. Diagram automorphism", "Text": "we can construct the type e6 crystal decomposition as follow  the computation is similar to similarly, by reversing the arrows and adding loops at every vertex, we obtain the composition graph g  then r is an i0,7-highest weight elemen  note that the decomposition into e6 crystals from proposition  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3_5", "Section": "3.5. Combinatorial R-matrix", "Text": " the fact that the decomposition into e7 crystals is multiplicity free is exactly the same reasoning as in the proof of proposition   since the combinatorial r-matrix must map classical components to classical components, we have the followin fundamental_weights for k in range): return ch  one way to construct the a7 highest weight elements would be to use the a6 highest weight elements, which we can compute from the composition graph in figure 3 12 as a step towards proving conjecture   proo  this is a finite computation that can be done,  letter f_strin  subcrysta value ==   valuelele ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4. Conjectures for B1s", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Appendix", "Section": "Appendix A. SageMath code for composition graphs", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "References", "Section": "References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190 org january 25, 2019 abstract reinforcement learning algorithms allow agents to learn skills and strategies to perform complex tasks without detailed instructions or expensive labelled training example  that is, rl agents can learn, as we lear  given the importance of learning in our intelligence, rl has been thought to be one of the key components to general artificial intelligence, and recent breakthroughs in deep reinforcement learning suggest that neural networks are natural platforms for rl agent  to deploy rl into a wider range of applications, it is imperative to develop explainable nn-based rl agent  here, we propose a method to derive a secondary comprehensible agent from a nn-based rl agent, whose decision-makings are based on simple rule  our empirical evaluation of this secondary agent's performance supports the possibility of building a comprehensible and transparent agent using a nn-based rl agen ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "reinforcement learninginspired by our brain's reward-based learning, allows artificial agents to learn a wide range of tasks without detailed instructions or labeled training sets which are necessary for supervised learning given that rl agents' learning resembles our process of learning and that learning is essential to our intelligence, it seems natural to assume that rl is one of the key components to brain-like intelligent agents or general artificial intelligenc  for instance, the alphago's strategies employed during the match with sedol lee exhibited efficiency leading to victories, but the exact reasons behind its moves are unknow  alphago demonstrated that incomprehensible decision-making can still be effective, but its effectiveness does not mean that it cannot be fault  a loss in one go match out of 100 matches is insignificant, but crashing a car once out of 100 drives is perilous and unacceptabl  if we comprehend the exact internal mechanisms of rl agents, we can correct their mistakes without negative impact on their performanc  that is, 'transparent' agents with comprehensible internal decision-making processes are necessary to safely deploy rl agents into high stake problem  two earlier studies showed that the decision-making processes of rl agents can be translated into humanreadable description  this proposal is based on two idea  second, the secondary agent can perform general tasks, if it takes advantage of trained rl agent  in this study, we propose a quasi-symbolic agent as a secondary agent and compare its performance to rl agents' performanc  after learning the values of transitions, qs agents identify the most valuable state-transitions and search for a sequence of actions to reach one of the hub state  if qs agent cannot find a proper action plan to reach a hub state, they choose the best action by comparing the values of immediate transition  our results show that qs agents' performance is comparable to rl agents' performanc  while our experiments are conducted in a simple environment, the results indicate that comprehensible agents with transparent decision-making process can be derived from rl agent ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Related work", "Text": "while deep learning spreads its influence, it remains unclear whether dl can safely be applied to high stake decision problems this means that despite extensive testings we cannot fully predict their action  thus, deploying dl agents to high stake problems may lead to critical failures; such failures have already been reported to deploy dl agents into high stake decisions, it is imperative to understand the exact process of their decision  several methods have been proposed to analyze the internal mechanisms of dl agentsand olah et a  presented an intriguing way to synergistically use them to gain insights into dl agents' decision-makin  also, multiple studies sought the post-hoc interpretability of dl agents, which we could use to predict and prevent potential failure  specifically, human interpretable descriptions can be automatically generated by secondary agentsand representative examples or image parts can be identified by algorithms such as sensitivity analysis the majority of studies have focused on feedforward dl, but a few studies pursued the explainability of r  specifically, hayes and shah showed that a secondary network could be trained to provide user-interpretable descriptions, and waa et a  further showed that user-interpretable descriptions could be contrasive; that is, their methods can explain why rl agents would prefer one option to another although such post-hoc interpretability can be used to evaluate the quality/reliability of rl agents, it does not provide a way to fix rl agents' mistake  however, with transparent agents, we can correct their mistakes selectivel  in our study, we propose a potential approach, which can help us obtain transparent rl agent ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Network Structure", "Text": "qs agents interact with rl agents and make plans for future actions by utilizing the model of environment all these three networks/agents are constructed using the 'pytorch', an open-source machine learning toolkit below, we discuss qs, rl and env network in detail ", "Subsections": [{"Section_Num": "3_1", "Section": "3.1 The structure of reference RL agent ", "Text": "we used an actor-critic model as a reference rl agent the gradient ascent of e is estimated as shown in e  the initial learning rate is  2 the structure of quasi-symbolic agent qs agent consists of matching and value networks, which are both single layer network  the matching and value networks are sequentially connected that is, the output node of matching network is directly connected to the output node of value networ  the number of matching nodes is identical to the number of value nodesand the connections between matching and value networks are one-to-on  the matching network memorizes input vectors by imprinting normalized inputs to synaptic weights converging onto the output node oi, as shown in e  2; hi the synaptic input to o  with normalized inputs stored in synaptic weights wij, the outputs of matching nodes represent cosine similarities between the current input and stored input 97, unless stated otherwis  once a new node is added to the matching network, a new node is also added to the value network, to keep one-to-one mapping the strength of the connection between these two newly established units is determined by the reward obtained by rl agents with the selected actio  when the current input is one of the previously stored onesthe maximally activated node is identified, and the connection to the corresponding value network node is updated by adding the reward induced by the input vector the sizes of the matching and value networks are not fixe  instead, they are determined during the training of qs agent  the more diverse inputs are, the bigger matching networks becom  for instance, if all inputs are identical, there is only one matching nod ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3_3", "Section": "3.3 Env network", "Text": "the env network models the environmen  thus, it receives the state vector s and action a as inputs and returns the next state in our experiments, we set the hidden layer of the env network to have 300 node  the env network is trained using the mean squared error in each episode of rl training, the error is accumulate  the initial learning rate is 0", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Results", "Text": "in this section, we discuss the operating principles of qs agents including the interplay between qs and rl agent  then, we present our experiments which were conducted to evaluate qs agents' performance in solving lunar-lander problem compared to rl agent 1 training and operating rules of quasi-symbolic agents unlike rl agents, qs agents do not work alon  the matching and value networks in qs agents are updated by using rl agents' behaviors during trainin  the matching network first determines the novelty of the current transition vector; this novelty detection is done by inspecting synaptic inputs to matching nodes when a novel transition vector is introduced, a new output node is added to the matching and value networksand the connection between matching and value networks is established it should be noted that only one matching node is allowed to be active and used to assess the value of the transitio  in brief, the matching network memorizes transition vectors observed during training, and the value network stores the amount of rewards induced by the observed transition  due to the one-to-one connections between matching and value networks, the synaptic weights allow us to identify the most valuable transitions observed during the training period, which are referred to as hub states hereafte  after identifying the hub states, qs agents search for an action plan to reach one of the hub state  in doing so, qs agents utilize the env network and the trained rl agent to predict future states to make an action pla  at each state, rl agent provides a possible action, and env network returns the next state in response to the suggested actio  employing them recursivelyqs agents can predict the future state  during this planning, at each time step, qs agents examine whether a transition vector is one of the hub states or no  if qs agents do expect to reach one of the hub states, they stop planning and execute the current pla  the maximum length of the action plan is 10 time-step, unless stated otherwis  if no hub state cannot be reached within the predefined maximal time-step, qs agents start over and make a new pla  for each state, qs agents are allowed to make a total of 5 different plan  if they cannot find a path to the hub states in all five plans, they inspect the values of the first transition in the five plans and choose the best immediate action according to the reward estimated by the value networ  in this case, rather than taking a sequence of actions, qs agents execute a single action onl  figures 2a and b show the total amount of rewards given to the rl agent and the error function of the env network during 5000 episode  after 1000 episodes, the speed of improvement is reduce  similarly, the error of env network is reduced most rapidly in the first 1000 episodes, and then the speed of error-reduction slows dow  the rl agent in this study uses stochastic policy to choose actionsand thus its behaviors depend on the random seed forwarded to the pytorc  to avoid potential biases based on their stochastic behaviors, we constructed 10 independent qs and rl agents by forwarding distinct random seeds to pytorch and calculated the average reward for both agent  figures 3a and b show the average amount of qs and rl agents for all 100 instantiations of environmen  x-axis represents the identity of environment, and yaxis represents the reward averaged over 10 independently constructed agent  rewards are estimated after training the rl agent in 1000 episodes and 5000 episodes as shown in the figure, the performance of qs agents with 10 time-step action plan are comparable to that of rl agents then, we varied the parameters to examine how they affect qs agents' performanc  first, we tested the effects of action plans' length  as shown in fi 05 and estimated the amount of reward  as shown in fi  4b, qs agents' performance improve  as shown in fi  5b, qs agents obtained less rewards", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Discussion", "Text": "in this study, we propose qs agents to develop transparent rl agent  the newly proposed qs agents have two operating units, matching and value network  with these two units, qs agents evaluate actions suggested by rl agents and choose the most probable choic  to select the most probable action, qs agents search for a path to reach one of the hub states by utilizing the env network which models the environmen  our results suggest that this future plan ensures qs agents' good performanc  first, the two operating units of qs agents have simple structures, which can be analyzed easil  the value network simply accumulates rewards given after the transition of states into synaptic weights, and it returns these stored values depending on inputs the matching network identifies the old inputwhich is the closest to the current input by using the cosine similarity in addition, as only a single matching node is allowed to be active, it is clear that the matching network's operation can be easily analyze  second, qs agents rely on a simple set of rules to select the best actions based on suggestions made by r  with both simple inner mechanisms and operating rules, qs agents have transparent decision-making proces  moreover, it should be noted that the output nodes of the matching network are working independently from each othe  that is, the matching nodes can be removed and added without interfering with other matching nodes' operation  this property makes manual modification of qs agents' actions possibl  if a new piece of evidence finds a particular statetransition to be unacceptable, it can be remove  similarly, the synaptic weights of value networks and hub states can also be changed, if necessar  therefore, qs agents can be continuously and incrementally improved to avoid mistake ", "Subsections": [{"Section_Num": "5_1", "Section": "5.1 Potential variations of QS agents", "Text": "to address the possibility of deriving a comprehensible secondary agent from rl agents, we sought generic algorithms to be applied to general rl problems, but qs agents and their operations can be customized in domain-specific way  below, we list a few potential variations of our generic qs agent  however, the actual values of transitions can be estimated differentl  for instance, instead of using the immediately obtained reward, we can use the total rewards from the transition to the end of an episode for evaluating state transition  however, state variables do not have equivalent values in agents' behavior  for example, the coordinates and velocities included in the state vectors of lunar-lander do have different importance in terms of agents dynamic  if we deal with coordinates and velocities distinctively, qs agents may evaluate the state-transitions more effectivel  in doing so, qs agents need to utilize multiple matching and value networks, and the actual value can be estimated with a linear summation of outputs of multiple value node  for instance, an agent's horizontal move can be either bad or good depending on the current stat  if both state and transition vectors are used, qs agent may have better estimation of agents' action  however, pfc does not work alone and is known to be connected to other brain area  the two main areas that have strong interactions with pfc are hippocampus and anterior cingulate cortex notably, complementary learning system theory suggests central roles of the interplay between pfc and hippocampus in our ability to learn continuously based on our results that qs agents can evaluate rl agents' decisions, we propose that one of acc functions is to evaluate possible actions suggested by pfc and choose the best one depending on contex ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190 00189v5 8 nov 2019 transition densities of reflecting brownian motions on lipschitz domains kouhei matsuura abstrac  in this paper, we study the continuity of the transition density of the reflecting brownian motion on a general lipschitz domai  we also provide local estimates for the densit  applying the estimates, we prove that the surface measure on the domain is in the local kato class of the reflecting brownian motio ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1. Introduction", "Text": " we denote by d the closure of d in r  we denote by h1 the first order sobolev space on d with the neumann boundary conditio  we call x the reflecting brownian motion on   we denote by pt the transition probability of   see for detail  35k08, 47d0  key words and phrase  it is also known that implies see for detail  although the framework of is seemingly wide, there are many domains on which does not hol  an essential reason why the the sobolev type inequality fails on dh is a presence of a cusp at infinit  hence, dh is not a lipschitz domains in the sense of relatively recently, gyrya and saloff-coste prove in that heat kernels of the rbms on inner uniform domains are continuou  however, dh is not an inner uniform domai  it seems that there is no preceding results which prove the continuity of the heat kernels of rbms on horn-shaped domains like as d  in this paper, we obtain the continuity of the heat kernel of the rbm on a general lipschitz domai  for the proof, it is important to show that part processes of the rbm are identified with part processes of rbms on bounded lipschitz domains this kind of argument is found inwhere the author essentially uses the theory of sobolev extension domain  we use the theorem of the spectral synthesis, and the proof of lemma  1 is much simpler than that of combining lemma   we apply the estimates to prove that the surface measure on the boundary of a lipschitz domain is in the local kato class of the rbm on it to classify measures in this way is important in the transformation theory of the markov processe  see and for the transformation theory and its application  the local estimates will also be used in to study the lp-spectral independence of neumann laplacians on horn-shaped domain  notatio  throughout this paper, we adopt the following notatio  the d-dimensional lebesgue measure is denoted by m or d ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2. Main results", "Text": " we denote by d the closure of d in r  see for the definition of bounded lipschitz domain  it is shown in that ) becomes a regular dirichlet form on l  by theorem   furthermore, x is conservative by takeda's tes  see for the proo  we are ready to state our main result  4 kouhei matsuura theorem   pu t also possesses a continuous versio ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3. Preliminaries", "Text": " here, in is the closure of in in r  kn is an open subset of   we denote by xn = the part process of x on k  bythe dirichlet form of xn is regular on l  the dirichlet form is regular on l  proo  a monotone class argument completes the proo  the proof of lemma  1 is much simpler than that ofwhere the author uses the theory of extension domain  it follows from lemma   in particular, each pn t becomes a compact operator on l  6 kouhei matsuura lemma   the principal eigenfunction can be taken to be positive on k  proo  proo  by lemma   by lemma   we shall show that t*is finit  let h be the upper half-plane of   since the convergence is monotone and non-increasing, we complete the proof by dini's theorem", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4. Proof of Theorem ??", "Text": " using lemma   we shall give a proof of theorem   proof of theorem   by lemma  2, we complete the proo  the third condition has already shown in theorem   we define ft : bet  it follows from lemma   we write ex for the expectation with respect to the probability measure p  by theorem   monotone convergence theorem and lemma   by lemma 3", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5. Proof of Theorem ??", "Text": " thus, by lemma   by repeating the same arguments as in lemma  4, we obtain the next lemm  proof of corollary   therefore, we may assume that u is unbounde  by using and repeating the same argument as in the proof of theorem  3we complete the proof", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "6", "Section": "6. Application", "Text": " let hd-1 be the -dimensional hausdorffmeasure on r  we call l the boundary local time of   proo  by theorem   see for the proo  acknowledgement the author would like to thank professor naotaka kajino for his helpful comments on the proof of lemma  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "References", "Section": "References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190  in health care applications, high reliable communication links between the health care provider and the desired destination in the human body play a central role in designing end-toend telemedicine syste  in the advanced health care applications,   drug delivery, molecular communication becomes a major building block in bio-nano-medical application  in this paper, an e2e communication link consisting of the electromagnetic and the molecular link is investigate  this paradigm is crucial when the body is a part of the communication syste  based on the quality of service metrics, we present a closed-form expression for the e2e ber of the combination of molecular and wireless electromagnetic communication  next, we formulate an optimization problem with the aim of minimizing the e2e ber of the system to achieve the optimal symbol duration for ec and dmc regarding the imposing delivery time from telemedicine service  the proposed problem is solved by an iterative algorithm based on the bisection metho  eduard   jorswieck is with department of electrical engineering and information technology, tu dresden, german  numerical results show that the proposed method obtains the minimum e2e bit error probability by selecting an appropriate symbol duration of electromagnetic and molecular communication ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "I", "Section": "I Introduction", "Text": "in our modern era, the amount of the required health care services is rapidly increasin  these high expectancies are going to overtake a proportional increase in health system infrastructures and professional personnel telemedicine, which can be defined as the implementation of telecommunication technologies to provide medical services, has been introduced as a promising solution for the increasing shortages of the traditional medicin  telemedicine allows the measurement of biological and vital signals regardless of the borders and distance  in the telemedicine paradigm, the biological signals are obtained through variety of the biological sensing technologies, from the simple pulse oxiometer and temperature sensors to more sophisticated electrophysiological and electrocardiology devices the gathered signals must be post-processed, and eventually transmitted to the associated health care provide  the exterior communication hardwares are placed in order to form the pathway from the on-body devices to at a distant hospital or physicians statio  one of the advanced applications of the telemedicine is the drug delivery which can be controlled via end-to-end communication link  the drug delivery is an engineered method to deliver drugs to their targeted locations while minimizing the undesired side effect  one of the most important drug delivery applications is gene therap  the utilization of the drug delivery in the gene therapy allows to convey of the desired genetic information to the patient's organism the main challenge in the gene therapy is minimizing the risk of in vivo toxicity and prolonging the lifespan of the payloa  also, the gene expression is short-lived due to the degradation of the plasmid in the nucleus the e2e communication scenario plays a central and fundamental role in designing the telemedicine system between the health care provider and the human bod  the applicability of the telemedicine crucially depends on the reliability of e2e communication links inspired by nature, one of the best solutions for inner body communication is to use chemical signals for carrying the information inside the human body to nanomachines through nanonetworksthis communication paradigm, which is called molecular communicationhas several advantages in comparison to electromagnetic based and acoustic wave based communicatio  the advantages includes but not limited to low energy consumption, the biocompatible characteristics, and the existence of the biological receptors to serve as antenna recently, diffusion-based molecular communication has been received a significant attention among various mc propagation scenarios such as walk-way or flow-based however, the major challenge of dmc is its high limited range of the communication the intermediate nanomachines, which are serving as relays, are deployed to overcome this issu  the performance of relay-assisted dmc is investigated in in-to-on body wireless communication is the ec between nanomachines inside the body and the wearable device on the surface of the body skin the main propagation environments of electromagnetic waves are inside and around the human bod  and ultimately, off-body wireless communication carries information from gateway transceivers to health care provider  the time is divided into multiple slots with equal duration  at each time slot, a message is transmitted from inner nanomachine to the distant health care provide  in addition, the time slots are divided into several symbol durations for conveying the information in each lin  the inner nanomachine sends its massage to the relay then the relay sends the received massage to the receiver nanomachine and ultimately through this sequence the massage is received by the health care syste  due to the fact that no buffer is considered in the nanomachines, ec and dmc communications should be in serial  therefore, ber and the delay of ec and dmc communication affects the e2e-ber and the total dela  it implies that the combination of ec and dmc must be considered as an integrated communication lin  in addition, some telemedicine services are imposing the limited delivery time of the command from the health care provider to the end nanomachin  it results in a compromise between the performance of ec and dm  then, we formulate the optimization problem to determine the optimal symbol durations in ec and dmc and solve it by using the bisection algorith  it is important to emphasis that the main contribution of this paper is that what happen if we combine the ec and dmc as an integrated e2e-telemedicine communication and illustrate the trade off between the physical layer parameters such as symbol durations in ec and dmc which based 1 it means as soon as the packet arrives at an intermediate node, it is relayed over the next link towards the destinatio  we also study the effect of the system parameters including the drift velocity and the detection threshold of dmc receiver on the performance of the e2e syste  the rest of paper is organized as follows: in section ii, the system model is describe  in section iii, the channel model of each communication is formulated, and the optimization problem is formulated and solved in section i ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "II", "Section": "II system model", "Text": "we assume the e2e e-health communication includes molecular and electromagnetic wireless communicatio  in this e2e e-health system, the information is exchanged between the health care provider and nanomachines/receptors in the body environmen  the considered e2e ehealth system is shown in fi  one should note that all inner-body communications are dmc and in2on-body, on-body, and off-body communications are e  fi  2 illustrates the symbol time duration and the dedicated time intervals of each communication type in the proposed system mode    inner-body communication we use a generic nanotransmitter as the transmitter node, which can be reused as often as necessar  these types of transmitters are not natural and produced artificiallythe transmitter which resides in a liquid communication mediumconstitutes the transmitted signal by encoding the information onto the special type of the messenger molecules, and releases them to environmen  due to the very small sizes of messenger moleculestheir propagation in this medium is governed by brownian motion we use a relay-assisted dmc system, because such relay node can potentially improve the reliability and performance of a communication link -.  the relay node resides in the environment can be an artificial nanomachine or biological one 1: overview of e2e e-health communicatio  fi  2: symbol durations in the considered dmc and ec system  these are a key for designing a nanomachine, which acts as relay nod  fi  3 demonstrates the relay-assisted diffusion-based molecular communication system employed in this articl  as could be seen in fi  the assisting relay node is transmitting and receiving in the fullduplex fashionand also we assume ts,dmc is divided into two intervals of equal duratio  we adopt ook modulation due to the fact that it is the most efficient binary modulation scheme in terms of molecular reception in dmc the use of two different types of molecules guarantees nearly interference free communicatio  wireless communications links ec between nanomachines inside the body and the wearable device on the body skin or at most 20 mm away from it, is called in2on-body communication the wearable device communicates with node t electromagnetically in its own time interva  the communication between the wearable device on the body skin and the gateway on the body or at most 20 cm away from it, is called on-body communication the gateway connects the on-body part to the off-body part via ec, in its own time interva  3: relay-assisted diffusion-based molecular communication syste  the time slo  further details of ec channel model and the corresponding ber are described in the following sectio  the main parameters and notification used throughout this paper is listed in table  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "III", "Section": "III E2E bit error performance ", "Text": "to derive the closed-form e2e-ber, each type of communication described in the previous section is modeled and ultimately, e2e-ber is calculated by concatenating the    molecular communication channel model and ber we assume a diffusive environment,   there exists three nodes namely the transmitter node, relay node, and the receiver node in the introduced schem  the transceivers are placed in the 3-dimensional diffusive environment where the drift velocity is also attende  therefore, the diffusion of the molecules are coalesced with the drift velocity of the environment to propagate them toward the receive  to attain the cumulative distribution function of  as stated init could be approximated by using simpson's rul  we can write the cdf given in as f = phi  node r detects the type a molecules that are transmitted by node   in the receiver of node r, maximum-a-posterior probability rule is employed for detection of the transmitted molecule in this model, the communication occurs with no error if xd = xt hold  3in this paper, the intersymbol interference is ignore  the analysis of this interference is out of the scope and is relegated as future work  a similar approach can be employed to extend the second term of in2on body communication channel model and ber our goal in this subsection is to model the statistical ber of the in2on-body channe  considering the fact that the snr is log-normally distributed, the aber in does not have the analytical closed-form solution168e- 144e- 002e- 206x2x13 on-body communication channel model and ber the path loss of the link between the wearable device and the gateway is a function of the distance, the part of the body that the wearable device is located, and the motion of the human body the best fitting distribution for these scenarios has been found to be the log-normal distribution off-body communication channel model and ber rayleigh fading channel with additive white gaussian noise is assumed for off-body communication lin  e2e communication channel model and ber in order to compute the e2e-ber, we use the fact that bers of the above-mentioned communication links are independent, due to their independent physical medium ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "IV", "Section": "IV symbol duration ratio optimization problem", "Text": " the important question is that how ts is chosen? in fact, some telemedicine services are imposing the limited delivery time of the command from the health care provider to the end nanomachin  for example the gene therapy drug delivery allows us to transfer the desired genetic information to the patient's organism in this e2etelemedicine case, it is important to convey the genetic information with minimum risk and at a specified time due to performing the desired chemical reactions consequently, the value of ts is imposed by telemedicine services regarding the type of drug delivery proces  consequently, the value of ts is imposed by telemedicine services regarding the type of drug delivery proces  by fixing ts, this predefined delay time is divided into ec and dmc which leads to a compromise between the performance of the ec and dm  once again, we emphasize that ts is fix and predetermined via telemedicine servicesfor example, in case of utilizing dmc in drug delivery, the time of releasing the drugs into the intended location is controlled by the drug releasing mechanism our target is finding the symbol duration ratio in a way that hold  next, we formulate an optimization algorithm for finding the optimum time slot partitioning to achieve the best performance in e2e communicatio  the objective is to minimize the e2e ber of the syste  we also assume ts,ec is divided into three intervals of equal duration associated to in2on-body, on-body and off-body communication link  one should note that the objective function in is not a convex function, and hence, the optimization problem is not conve  one method to solve the quasiconvex optimization problem relies on the representation of the sublevel sets of a quasiconvex function via a family of convex inequalities, as described in step 2: solve the convex feasibility problem otherwise, go back to step   the bisection optimization solution is a function of the dmc parameters and snr of ec where snr of the ec is estimated at the beginning of the transmission as a transmission routine procedure, and dmc parameters are pre-determined and fixe  therefore, the bisection algorithm is run in the ec transmission side which does not have computational complexitie  on the other hand, from the computational point of view, this method is a simple and robust root-finding mathematical algorithm which is of low complexity as investigated in23 table iv: in2on body communication link parameter  variable deep tissue near surface pl 47", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "V", "Section": "V Numerical Results", "Text": "in this section, we present the numerical results to evaluate the error probability performance of the proposed e2e communication syste  we also show how the system parameters affect the performanc  we consider a diffusive environment like blood, where the molecules propagated through i  4without loss of generality, no channel coding is considered for ec and dm  the system parameters used in the analysis and simulations are given in table ii  choosing an appropriate value for threshold at the destination node can improve the performance of such dmc systemtherefore, the performance of the error probability in dmc is analyzed in fi  in this figure, the ber of dmc as a function of threshold at the receiver for different values of relay location is show  it shows that by changing the location of node r, the ber is also change  however, the closer the relay node to the destination node, the less the ber performance in dm  5: the molecular ber performance as a function of different values of gy for different values of symbol duration threshold, the ber decreases from around 10-9 to around 10-  in addition, the ber in dmc as a function of gy for different values of symbol duration in dmc is shown in fi  it is also shows that ber can be improved by locating the relay node at the middle of the transmitter and the destination nod  the ber in dmc is also analyzed as a function of the drift velocity in y axis of the environment for different values of wx and ts,dmc in fi  it is also shown that by increasing ts,dmc from  2ms to   it is due to the fact that in 3-d model of environment, increasing symbol duration can not necessarily increase the reception probability ,4 ms fi  6: the molecular ber performance as a function of different values of wy for different values of wx and symbol duration the e2e ber as a function of symbol duration in dmc is analyzed in fi 327 ms there is a trade-off between ec and dm  however, the minimum e2e ber is reached when the trade-off between dmc and ec is achieve  the regions of dmc dominance and trade-off between ec and dmc are shown in fi  it is understood that by increasing the drift velocity, the minimum value of e2e ber is reache  however, in this case, the symbol duration is increase  therefore, the molecules need more times to arrive at the relay and destination node  7: the e2e ber performance as a function of different values of symbol duratio 531 ms fi ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "VI", "Section": "VI Conclusion", "Text": "in this paper, we investigated the e2e communication link consisting of the electromagnetic and molecular communicatio  first, we derived a closed-form expression for the e2e bit error probability of concatenation of molecular and wireless electromagnetic communication  then, we formulated the optimization problem that aims at minimizing the e2e bit error probability of the system to determine the optimal symbol durations for both molecular and wireless electromagnetic communication  the results reveal that an adaptive system must be considered to achieve the minimum bit error rate and optimal performance for the e2e syste ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "References", "Section": "References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": " abstract we study the effective interactions of the fermionic, scalar and vector dark matter with leptons and neutral electroweak gauge bosons induced by the higher dimensional effective twist-2 tensor operator  the thermally averaged indirect dm pair annihilation cross-section and the spin-independent dm - free and/ or bound electron scatterinng cross-section are observed to be consistent with the respective experimental dat +d, 1 -a, 1 de", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "com be-mail: sukant dutta@gmai com popular proposition for dm theories are weakly interacting dark matter particles features of dm interactions can be determined from the direct and indirect detection experiments apart from their direct searches in the present and proposed colliders the peaks in e+ e-channel are also observed in atic and ppb-bets balloon experiments at around 1 tev and 500 gev respectivel 4 tev favoring the lepto-philic dm annihilation cross-section of the order of 10-26cm3/  the excess in e+ e-can be either due to astrophysical events like high energy emission from the pulsars or resulting from dm pair annihilation in our galactic neighborhood preferably to e+ e-channe  various uv complete new physics extensions of sm have been proposed essentially to solve the gauge hierarxiv:190  these models naturally provide the dm candidates or wimps, whose mass-scales are close to that of the electro-weak physic  however, the direct detection experiments have shrunk the parameter space of the simplified and popular models where the wimps are made to interact with the visible world via neutral scalars and/ or gauge boson  the model independent dm-sm particles interactions have also been studied in the bottom-up effective lagrangian approach where the mediator of dmsm interactions are believed to be much heavier than the mass-scale of the lighter degrees of freedom say, in our case sm and dm particles the nature of these interactions are encapsulated in a set of coeffcients corresponding to limited number of lorentz and gauge invariant higher dimensional effective operators constructed with the light degrees of freedo  the constrained parameters space from various experimental data then essentially maps and direct towards the viable uv complete theoretical model  similarly, analysis for dm-gauge boson effective couplings at lhc have been done by the authors in reference gross and wilczek analyzed the second rank twist operators appearing in the operator-product expansion of two weak currents along with the renormalizationgroup equations of their coeffcients for asymptotically free gauge theories in the context of deep inelastic leptonhadron scatterin  later authors of reference analysed the effective dm-nucleon scattering induced by twist-2 quark operators in the context of super-symmetric models where the majorana dm particle neutralino being lsp was assumed to be much lighter than that of the squark masse  this was followed by series of papers where the authors have calculated the one loop effect of dm-nucleon scattering induced by the twist-2 quarks and gluonic operators for the fermion, vector and scalar dm respectivel  the hadronic matrix elements induced by twist-2 operators can however be identified with the second moment of parton distribution functions and thus can be constrained from the available pdf  this in turn constrain the coeffcients of such higher dimensional effective operators to estimate the dm-nucleon scattering cross-sections for a suggested dm and squark mass rang  indirect and direct detection experiment  the constraints from the lep on the coeffcients of the effective lepto-philic and uy gauge boson b-philic operators and the sensitivity analysis of these coeffcients at the proposed ilc are discussed in section   finally we summarize in section   these contact interactions for example can be motivated from the super-symmetric models where spin-independent neutralino-lepton interactions are facilitated by the exchange of heavy sleptons and/ or higgses, assuming majorana neutralino to be the ls  is the scalar operato  the constraints on the four fermionic scalar dm operators has been extensively studied in the literature therefore it is worthwhile to probe the effect of these tensor operators in the dm pair annihilation and dm-electron or dm-nucleon scattering  further the formalism can be extended to include the twist-2 type-2 interactions of spin 0 and spin 1 dm particles with sm charged leptons and electro-weak gauge bosons using equation we analyse all such operators emerging from twist-2 interactions with sm charged leptons and neutral electroweak gauge bosons in this stud  there are other lepto-philic dm tensor operators which can have significant effect on dm phenomenology like dipole moments et  as shown in references since we are interested in operators which contribute to the spin independent non-relativistic dm nucleon scattering process, we drop all the operators which are suppressed by the velocity of dm and/or nucleo  the spinindependent leading interactions in super-weak coupling expansion of the fermionic, vector and scalar dm operators are retained for analysis as explicitly shown in references for the case of fermionic, vector and scalar dm operators interacting with quarks and gluon  in  = lspin 1/2 dm ef  in  + lspin 0 dm ef  in  + lspin 1 dm ef  in  the lorentz structure of the dm operators characterize the nature of dm pair annihilation and hence its contribution to the relic density it is to be noted that the partial wave analysis of the annihilation processes induced by fermionic dm operators given in and remain same for both the majorana type and/ or dirac type because of the contact effective interaction  we are now equipped to analyse and constrain these effective interactions from the dm phenomenolog  the respective shaded regions depict the allowed parameter spac  contours in figure 1a are depicted assuming the universal lepton flavor couplings of effective dm - sm lepton interaction ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Effective interactions of lepto-philic & U(1)Y gauge Boson B-philic DM", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 DM Phenomenology", "Text": "0022 by wmap and planck collaborations respectivel  to numerically compute the current dm relic density we need to calculate the thermally averaged dm annihilation cross section  since the freeze-out for thermal relics occurs when the massive particle is non-relativistic   we have computed the thermal-averaged annihilation cross-section in the appendix for the dirac fermion, a real scalar and real vector dm candidate  they are worked out in appendix   to compute relic density numerically, we have used maddm and madgraph we have generated the input model file required by madgraph using feynruleswhich calculates all the required couplings and feynman rules by using the full lagrangian given in equation these points are also the allowed upper limit on the cut-offfor a given dm mass and thus the shaded region enclosed by the corresponding solid line is the cosmologically allowed parameter region of the respective operato  the lower value of the cut-offcorresponding to the same dm mass will lead to only partial contribution to relic density and therefore may survive in model where more than one type of dm particles are allowed and/ or switching more than one type of effective operators simultaneousl  we observe that the sensitivity of the cut-offfor fermionic type-1 operator increases with the varying dm mas  among the type-2 operators, scalar dm cut-offis found to be the most sensitiv  electron, positron and photo  indirect experiments which in general are either ground-based or satellite borne particle detectors are sensitive to these characteristic fluxes of light sm particle  for example fermilat is a space borne experiment designed to measure the tracks of electron-positron pairs which are produced when gamma-rays interact with the detector materialwhile hess is the ground based cherenkov telescope geared to detect the gamma ray spectrum in this section we calculate the thermally averaged annihilation cross sections for fermionic, real scalar and real vector boson dm candidates to pair of charged leptons and photon  the analytical expressions for these cross-sections corresponding to the lepto-philic and uy gauge boson b-philic operators are given in equations - and - respectivel  we have used 220 km/s as the average velocity of the d  the thermal averaged dm annihilation cross-section is computed numerically for a given set of parameter which satisfy the relic density constraint from the planck data as depicted in figure ??.  the variation of the annihilation cross-section with the dm mass are depicted in figures 2a and 2b respectivel  the solid lines in figures 2a and 2b are essentially the lower bound on the allowed annihilation cross-section satisfying the relic 6 figure 2a shows the contours for the fermionic, scalar and vector dm interacting via type-1 or 2 twist operators assuming the universal lepton flavor couplings with the d  density constraints for a given dm mas  thus the null experimental results for the given mass range translate into the lower limits on the cut-offfor the respective operator  the dm annihilation cross-section to the pair of electrons induced by the electro-philic dm are identical to those depicted in figure 2b for the respective operator  figure 2c show the annihilation cross-sections depict in solid line for each case satisfying the relic density and thereby giving the lower bound on the cosmologically allowed annihilation cross-sectio  we compare our results with 7 the exclusion plots from dama at 90%   for the case of dmelectron scattering are also shown bounds at 90%   are shown for xenon100 from inelastic dm-atom scattering that obtained from the null observation at hess for dm mass 100 gev and above, which gives the upper lower bounds on the respective b-philic operator 3 dm-electron scattering direct detection experiments look for the scattering of nucleon or atom by dm particle  these experiments are designed to measure the recoil momentum of the nucleons or atoms of the detector materia  these scattering can be broadly classified as dm-electron scattering, dm-atom scattering, and dm-nucleus scatterin  in this article, we restrict our study for those directdetection processes which are realised at the tree level interactions of the lepto-philic dm operators with the free and bound electron 119 for a given dm mas  these results are then compared with the null results of dama/libra at 90% confidence level for dm-electron scattering and xenon100 at 90% confidence level for inelastic dm-atom scatterin  it is important to note that electro-philic dm -free electron scattering cross-sections corresponding to the respective operators computed using the upper bound on the cut-offobtained for a given dm mass from relic density constraints as shown in figure 1b and unity coupling strength, will be slightly higher than those shown in the figure   the inelastic dm - electron differential scattering cross-section   4 collider sensitivity of effective operators  1 lep constraints on the effective operators we investigate the constraints on the lepto-philic and b-philic effective operators from the existing results and observations from lep dat 9 gev and an integrated luminosity of 67 9 gev b-philic fi 9 gev and an integrated luminosity of 67 032 pb obtained from combined analysis of delphi and l3 the enclosed shaded region corresponding to each solid line are forbidden by lep observatio  sured cross-section from the combined analysis for the said process is found to be031 pb,008 pb and032 pb respectively the respective shaded regions in figure 4 are disallowed by the combined lep analysi  we believe the enhancement in the centre of mass energy and luminosity will enable both the detectors to probe the sensitivity of the lepto-philic twist operator 1 fb-1 which agrees with sm predictions within the systematic and statistical uncertaint  the events containing fake photons coming from electrons and jets are also included in the analysi 07 table 1: accelerator parameters as per technical design report the analysis for the background and the signal processes corresponding to the accelerator parameters as conceived in the technical design report for ilc and given in table 1 is performed by simulating sm backgrounds and the dm signatures using madgraph and the model file generated by feynrules therefore, we restrict our analysis for the dm pair production with associated with mono-photon  to study the shape profile and its mass dependence we generate the normalized one dimensional distribution for the sm background processes and signals for the fermionic, real scalar and real vector dm candidates, keeping the respective effective coupling constant to be unity and rest to zer  shaded rosy-brown and dark khaki histograms depict the normalized differential distributions   repeating the same exercise for the b-philic operators, we depict the shape profile of the normalised differential distributions   the differential distributions for the background sm processesses   the shape of normalized distributions are comparatively more sensitive   dm masses in case of the b-philic operator  this suggests that for the bphilic operators induced interactions, imposition of dm mass dependent dynamical cut can minimize the background and enhance the significanc  we consider only one effective operator at time with the fixed coupling constant of unity and adopted a conservative value for the systematic error to be 1%.  we simulate the two-dimension differential distributions using the collider parameters as given in table 1 and choosing the basic selection cut ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Collider sensitivity of effective operators", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Summary and Outlook", "Text": " each figure contain four contours corresponding to the twist-2 type-1 fermionic and type-2 fermionic, scalar and vector operators respectivel  16 gev at pamelaexcess in flux of electrons/positrons around 400-500 gev at atic and ppb-bets balloon experiments and exclusion of quark channels by ams-02 data hints toward the existence of non-baryonic d  this implies that the direct detection experiments have to be sensitive on the recoil momentum of the atom or an electron in dm - atom and/ or dm - electron scattering respectively due to suppressed loop-level interactions of dm with the quarks in the nucleo  characterization for such lepto-philic and electro-weak gauge boson b-philic dm particles are likely to be diffcult and challenging at the lhc and therefore it becomes imperative to probe the sensitivity of the associated dm pair production channels at the proposed lepton collider il  we have listed a minimal set of the twist-2 operators corresponding to lepto-philic and uy gauge boson tensor currents in section 2 which couples to the tensor currents generated by the bi-linears of the dm field  data although the contribution of the loops are suppressed but they need to bee investigated for the complete study of the twist-2 operator  on superimposing inelastic dm - atom scattering cross-section from dama we have analysed the bound state effects of the electron and derived the analytical expressions for the event rate are drawn   we hope this study will be useful in studying the physics potential of the ilc in context to dark matter searche  acknowledgements hb and sd thank mihoko nojiri and mamta dahiya for discussions and suggestions throughout the wor  hb acknowledges the csir-jrf fellowshi  hb and sd acknowledge the partial financial support from the csir grant n  03/ 15/ emr-i  sd thanks the theory division, kek, for an excellent hospitality where this problem was conceive  to compute the same we express the relative velocity of dm pair in the laboratory frame |v| in terms of c", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Appendix", "Section": "Appendix A: Thermal averaged annihilation cross-sections", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": " finally, we prove the tate and mumford-tate conjectures for those surfaces s that lie in connected components of the gieseker moduli space that contain a product-quotient surfac ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": " the results of this paper are inspired by the following two observations:   such a hodge structure is said to be of k3 typ  tra denotes the transcendental part of a hodge structure, that is, the orthogonal complement of the hodge classe ) these observations lead to the following question 2 question   the strategy of the proof is the same as for morrison's result mentioned abov  in these cases we give an affrmative answer to question b in theorem   and question   however, we give suffcient conditions for a positive answer to this questio  we summarise these results in the following theorem6 structure of this tex  it is important to stress that the classification of these surfaces is not yet complet  hence we present the state of the art up to no  we shall pay particular attention to those surfaces which are product-quotients recalling definitions, important properties and its associated group theoretical dat  furthermore, we recall what is known about their moduli spac  in theorem   indeed, for those surface which are product-quotients we are able to find an algebraic k3 partne  finally in the last section we see how the results obtained can be used to prove that the tate and mumford-tate conjectures hold for these surface 18 proves the last point of theorem   the first author was supported by the netherlands organisation for scientific research under project n 207 and by the deutsche forschungs gemeinschaft under graduiertenkolleg 1821 the second author was partially supported by miur prin 2015 geometry of algebraic varieties and also by gnsaga of inda  we give an overview of the current state of the ar  since we are dealing with irregular surfaces,   by hodge theory, alb is an abelian variet  the first case is completely understood: we have a classification theorem, see the second case is still ope  by the degree deg of the albanese map is a topological invarian  in table 1 we summarize the state of the art of the classification using k2 s and deg as main invariants", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 On the classification of surfaces with pg = q = 2", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Hodge-theoretic K3 partners", "Text": "12 we give an affrmative answer to this questio  in particular, the weight of v is always eve  in this paper we use the notation v only for tate twists of the hodge structur  assume that l is unimodula  proof this is a slightly weaker form of theorem   8/25 proof by proposition   by construction it is fibrewise a primitive embedding and a hodge isometry on the transcendental lattice  then h2 new has rank 14 -k2   proof finally, b2 is the sum of the ranks of h2 new and h  in other words, we have a positive answer to question a of the introductio  proof the result follows from corollary 3", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Algebraic K3 partners", "Text": "12 provides an affrmative answer to question a, the hodge conjecture predicts a positive answer to question b as wel  in theorem  8 we show that this is indeed the case for certain surfaces that are product-quotients1 outline of this sectio  it is organised as follow  first we recall some facts about chow motives of surface  in theorem   10/25 this proof relies on a case-by-case computation, for which we refer to a magma-script of which we tabulate the outpu  the final part of this section illustrates this proof by discussing one of the cases in detail, as an exampl 2 chow motives of surface  let mrat denote the category of chow motives over   we recall that mrat is an additive, q-linear, pseudoabelian category this leads to a satisfactory theory of motives of quotient varieties, as is explained in we denote with chi ) the i-th chow group of a smooth projective variety x for algebraic surfaces there is in fact the following theorem, which strengthens the decomposition of the chow motiv  statement and proof are copied from theorem   here ch2 aj denotes the kernel of the abel-jacobi ma  proof the further splitting into an algebraic and transcendental component is proposition   11/25 proof the rest of this section is dedicated to its proo 16 for an explicit computation that illustrates the general argument of this proo  following proposition   for the purpose of this theorem we are interested in the remaining term z  this definition does not depend on the chosen integer   we will now describe the so-called isotypical decomposition of the abelian variety a with group action by   the decomposition is the isotypical decomposition mentioned abov 2 of for detail  recall that the schur index of wi is the degree mi of di over its centr  note that, whereas is uniquely determined, is no  however, the dimension and the isogeny class of the abelian varieties bi is independent of choice qr} where each qi has corresponding monodromy g  the tuple is called the generating vector for the action we now copy equation   see for detail  we may calculate the dimension of the bi's for each class of surfaces in the statement, either by hand or using the magma script as in the result of this computation is given in table   it is a coincidence that in the table all the characters that appear are actually self-dual and defined over   we will complete the proof by a case distinctio  in other words, we are in the situtation of remark  15 the case g = q  in this case we cannot use the methods employed so far to prove that the jacobian is isogenous to a product of elliptic curves as in all the other case  therefore, we can try to decompose the jacobian of c using this larger grou 2 on page 65 of are fulfille  hence the restriction of this character to q8 is the only two dimensional irreducible representation, which is self dua  this concludes the proof of theorem  8 and in particular the computations performed by the magma script, we will now study one example in detai  this example will occupy us for the next few page  as group we take g = v  therefore, by the riemann-hurwitz formula the ci are curves of genus   we proceed by calculating the hurwitz character relative to the first quotient now we use e  the k3 partner x will turn out to be the minimal resolution of kummer surface associated with l1 * l  observe that li is isogenous to p).  we now go back to the surface s = /v  let us go further and build an algebraic k3 partner of   in this way we get a singular kummer surfac  according to the propostion above and shioda and inose the minimal resolution x of the singularities of km is a k3 surface whose transcendental part of h2 is isomorphic to the transcendental part of h ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Motivated K3 partners", "Text": " then we will recall three conjectures that 18/25 are connected in the following sense: if two of the conjectures hold, then so does the thir  once we we have all the machinery in place we turn our attention to the proof of the tate and mumford-tate conjectures for the surfaces mentioned abov  it is naturally endowed with a pure hodge structure of weight  3 mumford-tate group  we denote the mumford-tate group of v with gm  alternatively, gmt may be defined using the tannakian formalis  it is the algebraic group over q associated with the tannakian subcategory of q-hs generated by v an element of v is called a tate class if it is invariant under an open subgroup of ga  this follows from9 motivated cycle  every algebraic cycle is motivated, and under the lefschetz standard conjecture the converse holds as wel  the set of motivated cycles naturally forms a graded q-algebr  observe that hmot = l i hi mo  this gives contravariant functors hmot and hi mot from the category of smooth projective varieties over k to mot  every classical cohomology theory of smooth projective varieties over k factors via mot  proof we denote the category of abelian motives over k with abmot  proof proof we shall write gmt for gmt).  we give a partial answer to this question in the following result  in particular, the motive h2 mot is abelia  proof the main idea of the proof is as follows: using proposition   we then use theorem  13 to prove that h2 mottra new is isomorphic to h2 mottra ne  finally, this isomorphism spreads out to the other fibres via theorem   we now make this sketch precis  fix a point b belong to   by proposition   recall from example  12 that h2 mot is an abelian motiv  also note that h2 mot is abelian by assumptio  by theorem   in particular, the motive h2 mottra new is abelia  the term h2 mottra old is abelian, because it is the part coming from the albanese surface, whose motive is abelian by definitio  assume that s lies in one of the connected components of the gieseker moduli space of surfaces of general type that contain a surface that is isogenous to a product of curve  then the tate and mumford-tate conjectures are true for   proof we first prove the mumford-tate conjecture for   let a be the albanese variety of   by theorem   hence the motive hmot is an object in the tannakian subcategory of motk generated by hmot and hmo  this follows from the main result of recall that the hodge conjecture is true for s, by the lefschetz- theore  therefore the tate conjecture for s is true, since it follows from the conjunction of the hodge conjecture and the mumford-tate conjectur  those gmt-invariant classes are precisely hodge classes, and by the lefschetz- theorem we know that they are in the image of the cycle class map", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "jordan frame beyond scalar-tensor theories fethi   the generic theory in this family contains higher order time derivative terms in the jordan frame action which is indicative of ill-posednes  however, we show that equations of motion can always be reduced to a second-order-in-time form as long as the original einstein frame formulation is well pose  the inverse transformation from the jordan frame back to the einstein frame is not possible for all field values in all theories, but we obtain a fully invertible transformation for vector-tensor theories by a redefinition of the vector fiel  our main motivation is a better understanding of spontaneous scalarization and its generalizations, however our conclusions are applicable to a wide class of theorie  jordan frame has been traditionally used for certain calculations in scalar-tensor theories of gravitation, and our results will help researchers generalize these results, enabling comparison to observational dat ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "I", "Section": "I Introduction", "Text": "scalar-tensor theories have been among the most popular alternatives to general relativityand also had a large impact on cosmology these theories commonly posit that gravitation is governed by scalar degrees of freedom in addition to the usual metric of general relativity, but their phenomenology can be very diverse otherwise due to various different coupling terms in their action  an important feature of stts is the freedom to choose the fundamental field variables while formulating them,   the first is the jordan frame where the fundamental metric field of the theory couples minimally to matter degrees of freedom, and the second is the einstein frame where the metric is such that the metric action is in the einstein-hilbert form, hence identical to that of gr einstein and jordan frames have been investigated in great detail in the literature which has shown their equivalence in many casesrevealed that one frame can be more useful for analyzing specific problems such as approximation schemesand even led to the discovery of previously overlooked stts the aim of this work is generalizing the analysis of the relationship between these two frames to theories that contain higher spin fields such as vectors instead of scalars, or less common conformal scaling functions a such as those that depend on field derivative  our main motivation is the recently investigated phenomenon of spontaneous tensorization which is a generalization of spontaneous scalarization in the scalar tensor theories introduced by damour and esposito-far` ese in def theories, the scalar fields spontaneously grow to large values from arbitrarily small perturbations near neutron stars due to a tachyonic instabilit  the desirable controlled spontaneous growth in def theories is not a direct results of the scalar nature of the coupling, or the tachyonic nature of the instabilit  any field that carries an instability, such as a ghost on a vector field, in principle can lead to similar spontaneous growth which is called spontaneous tensorization overall, the theory of def is but one member of a large family of theories with similar observational signatures, all of which can be potentially tested with gravitational waves in the near future all spontaneous tensorization theories have been formulated in the einstein frame for reasons we will discuss, and he main theme of this study is their properties in the jordan fram  despite our motivation, we will not specify our coupling terms to those that incite spontaneous growth, hence our results are genera  we will use the terminology of einstein and jordan frames in a generalized sense, the former is always the one where the gravitational action is in the einstein-hilbert form, and the latter is always the one where matter fields couple to the metric minimall  in se  ii we present the tranformation between einstein and jordan frames in the quintessential stt of brans and dicke whose conformal matter coupling structure is kept in all other theories we are intereste  in se  iii we obtain the jordan frame for a vectortensor theor  in se  iv we go back to scalar fields, but this time study derivative coupling  we demonstrate the existence of higher derivative terms in the jordan frame, commonly indicative of ill-posedness, and present the results from the existing literature which resolve this proble  we also discuss the invertibility of the frame transformation  in se  v we analyze the most general spontaneous tensorization theory which also has potentially dangerous higher derivative terms in the jordan fram  we address this problem by showing that the equations of motion have at most second order time derivative  in arxiv:190  i  e  the first term is exactly that of gr as we desire  the second problem of having a non-canonical scalar action can also be addressed by using our freedom to redefine the scalar fiel  the opposite is sometimes employed   in all the alternative nature of this action is in its matter coupling, the first two terms simply represent a scalar field living under g  the price to have these familiar action terms is the nonminimal matter couplin  ii  jordan frame in theories of spontaneous vectorization a close examination of the def theory in e  we keep all vector-related quantities in the lower index to explicitly see the the inverse metric term  it is clear from this presentation why einstein frame is more suitable to generalize stts to vectors or other field  both the metric and the scalar field actions are in their standard forms, hence one keeps the einstein-hilbert action and replaces the action of the scalar with the standard action for a vector field to obtain a vector-tensor theor 2 whereas, it is hard to see how to change the unusual scalar field terms in the jordan frame of e  1 to those of vector  in e  then our only choice is the function a  however, intrinsic mass of vectors or scalars do not affect the current discussio  3 to spontaneously growing vector fields in the vicinity of neutron stars in analogy to spontaneous scalarizatio  the implicit definition in e  we will discuss this issue in more detail in the coming sections, but we can address it for this specific theory by utilizing the freedom to redefine the vector fiel  we will see that this is not always the case in other generalizations of stt  the action for the vector field is not the standard one, but this is expected in the jordan frame as in e  we should note that the actions in e  8 and e  15 both satisfy our definition of the jordan frame since they both have minimal matter couplin  this is reminiscent of einstein-maxwell-scalar theories, especially the ones that feature a newly discovered type of spontaneous scalarization 15 in somewhat closer resemblance to e  i  jordan frame for ghost-based spontaneous scalarization a second avenue to generalize spontaneous growth of def is using a different instability, as opposed to using a different fiel  the resulting theory leads to scalarization of neutron stars,   this theory is named ghost-based spontaneous scalarization since it can be shown that small perturbations around the scalar field vacuum behave like 4 a ghost, but this instability is suppressed as the field grow  using e  even though this jordan frame formulation looks quite similar to that of the vector field case in e  first, e  this is peculiar, since we do not expect the nature of the theory to change radically from one frame to the other, and there are no higher time derivative terms in the einstein frame formulation in e  these equations indeed have up to fourth order time derivatives indicative of ill-posednes  inserting these identities back ensures that e  20 contains at most second time derivatives non-invertibility of the frame transformation is not a new problem, and is present in the simplest stts such as bd as wel  in e  field variables cannot be transformed to the jordan frame otherwise due to e  nevertheless, we would still want to have an invertible transformation between frames for as much of the configuration space as possibl  a proper analysis requires tools from the theory of partial differential equations, and we hope mathematical physics can provide some insight for this problem which has not been addressed in the gravitational physics literature to the best of our knowledg  jordan frame for generic spontaneous tensorization we have seen that spontaneous growth can be generalized from scalars to vectors, or from a tachyonbased mechanism to a ghost-based mechanis  this approach can be continued to various other fields such as spinorsother mechanisms such as spontaneous growth through the higgs mechanismor any combination of the  all these form the family of spontaneous tensorization theorie  how generic are the issues of higher time derivatives and invertibility of frame transformations that we encountered in se  both cases are potentially problemati  hence, changing from the einstein frameto the jordan frame does not introduce ill-posednes  note that our starting action e  thus, our results continue to hold for the spontaneous growth of massive scalars or vectors, or theories where ghost-based and tachyon-based instabilities are present at the same tim  these cover all examples of spontaneous tensorization in the literature all these theories, aside from spontaneous vectorization in se  iii, also contain conformal scaling functions which contain derivative  hence, the resolution of the higher time derivative problem we outlined is central to their viability in the jordan frame as physical theorie  invertibility of the frame transformation in relation to the existence of a solution to e  iv are open in this generic case as wel  however, we remind that such problems are present even in the def theory", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "II", "Section": "II Changing the frame in Scalar-Tensor Theories", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "III", "Section": "III Jordan Frame in Theories of Spontaneous Vectorization", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "IV", "Section": "IV Jordan frame for ghost-based spontaneous scalarization", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "V", "Section": "V Jordan frame for generic spontaneous tensorization", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "VI", "Section": "VI Conclusions", "Text": "we studied the jordan frame formulation of generalizations of stts, where the scalar is replaced with other fields, and couplings can depend on derivative  our motivation came from the specific class of theories that feature spontaneous tensorizatio  these are most naturally defined in the einstein frame where the action for the additional field to the metric is in the canonical for  however, our results can be applied to any generalization of stts that is based on a conformal scaling of the metric in the matter action by some function of a dynamical field and its first derivative  the first case we examined is the vector-tensor theory obtained by replacing the scalar in the def theory by a vector where the conformal scaling function ax depends on the norm of the vector fiel  moreover, interesting connections can be observed to the recently discovered spontaneous growth in einstein-maxwell-scalar theories jordan frame of ghost-based spontaneous scalarization where the conformal scaling depends on the derivatives of the scalar field presents challenge  the equations of motion indeed contain up to fourth order time derivatives, but it can be shown that such terms cancel each other to render the equations second order in tim  we could not find a formulation where this problem is resolved, but noted that this is the case even in the def theory where the scalar field is restricted to be positive in the jordan fram  it is important to understand the meaning of the field values in the jordan frame where the transformation back to the einstein frame is not defined, which we leave to future studie  we finally showed that a generic spontaneous tensorization theory contains higher time derivative terms in its formulation, much like ghost-based spontaneous scalarizatio  the equations of motion are again ultimately rendered second order in time, even though they naively contain fourth time derivatives, demonstrating that the jordan frame formulation does not introduce illposednes  we should add at this point that the fact that there are only first derivative terms in the einstein frame action does not guarantee well-posednes  a theory can be rendered unphysical by other factors such as indefinitely growing fields such as ghosts, even if the equations of motion have no more than two time derivative  the einstein frame formulation of spontaneous tensorization theories are not known to be completely free of such undesirable featuresbut our work here shows that transferring to the jordan frame at least does not add new sources of ill-posednes  certain calculations on stts have been performed using the jordan frame such as the gravitational wave memory for the def theory consequently, we believe this study will enable researchers to extend similar work to spontaneous tensorization in genera  possibility of nearfuture testing is a basic appeal of spontaneous scalarization and tensorizatio  calculations of specific observational signs will enable the gravity community to compare the predictions of these theories to actual observations, and understand the differences between individual theories in the spontaneous tensorization famil  acknowledgments this project started with a question posed by leonardo gualtieri to whom we are gratefu  the author is supported by grant n  117f295 of the scientific and technological research council of turkey we would like to acknowledge networking and travel support by the cost action ca1610  fujii and   maeda, the scalar-tensor theory of gravitation flanagan, clas  quantum gra  damour and   esposito-farese, clas quan gra  du and   nishizawa, phy  re  lehner, phy  re  zumalac arregui and   garca-bellido, phy  re  d89, 064046130  damour and   esposito-far` ese, phy  re  let  ligo scientific collaboration and virgo collaboration,   abbott et al, phy  re  let  abbott et al, phy  re  let  re  d96, 064009170  re  d97, 024008171  brans and   dicke, phy  re  beltran jimenez,   delvas froes, and   mota, phy  let  b725, 212121      re  let  re  d98, 044011180  re  d98, 044013180 ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Acknowledgments", "Section": " Acknowledgments", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "References", "Section": " References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190 00195v1 1 jan 2019 some remarks on the pointwise sparse domination andrei   lerner and sheldy ombrosi abstrac  we obtain an improved version of the pointwise sparse domination principle established by the first author in this allows us to determine nearly minimal assumptions on a singular integral operator t for which it admits a sparse dominatio ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1. Introduction", "Text": "sparse bounds for different operators have been a recent and active topic in harmonic analysi  localization and sparseness are two main ingredients which make sparse bounds especially effective in quantitative weighted norm inequalitie  the literature about sparse bounds is too extensive to be given here in more or less adequate for  we mention only that sparse bounds for calder on-zygmund operators can be found in also, there are several general sparse domination principles 2010 mathematics subject classificatio  42b20, 42b2  key words and phrase    lerner is supported by isf grant n  ombrosi is supported by conicet pip 11220130100329co, argentin  lerner and sheldy ombrosi theorem a in this paper we improve the above result by means of weakening the two main hypothese  we call this the wq propert  our main result is the followin 1 provides a more convenient tool compared to theorem   indeed, there is no need now to work with the grand maximal truncated operator mt, which typically requires some additional effor  the fact that we do not require the weak type of t in theorem   the paper is organized as follow  in section 2 we present a proof of theorem   we also show separately how this proof looks in the model case of calder on-zygmund operator  in section 3 we discuss some variations and extensions of theorem   a sparse t1-type result is presented in section   finally, in section 5 we collect different examples of operators admitting the pointwise sparse dominatio  we show how theorem  1 simplifies sparse bounds for these operator ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2. Proof of Theorem ??", "Text": "1 in this section we prove theorem   the proof is a variation of the proof of theorem a, with an additional twis  although some parts of both proofs are almost identical, we provide a complete proof for reader's convenienc  first, we separate a common ingredient of both proofs in the following lemm  proo  next, in the same way cover 9q0 3q0, and so o  the union of resulting cubes, including q0, will satisfy the desired propert  having such a partition, apply to each qj instead of   we obtain a 1 2-sparse family fqj such that for  1, we show that it especially elementary in the model case of calder on-zygmund operator  let t be a calder on-zygmund operato  this result is well known its proof in is based on theorem   the proof given below illustrates in a simplified form the main idea behind the proof of theorem   proof of theorem   fix a cube   from this and fromfor   it remains to apply lemma   in the proof of theorem   proof of theorem   from this and fromfor  1, completes the proof", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3. Some variations of Theorem ??", "Text": "1 we mention here some simple but useful variations/extensions of theorem   let us start with the following, a slightly more precise version of theorem   pointwise sparse domination 7 theorem   let f be a compactly supported function from l  the advantage of theorem  1 compared to theorem   this advantage will be used in theorem   for example, the corresponding variant of theorem  1 can be stated as follow 1/  for an application of theorem   we also note that theorem  1 can be easily extended to a multilinear cas  ina multilinear extension of theorem a was obtaine  our multilinear variant of theorem  1 improves this result exactly in the same way as theorem  1 improves theorem   we point out the necessary changes in the proof compared to the proof of theorem   the rest of the proof is identically the sam  note that in theorem   the only place where the sublinearity of t in theorems  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4. A sparse T1-type theorem", "Text": " on the other hand, many results in the theory of singular integrals hold under stronger assumptions on   recall that k is called standard 10 andrei   it is still unknown what are the minimal regularity conditions on k for which the t1 theorem hold  in particular, it is unknown whether this condition can be relaxed to the classical dini conditio  similarly, one can ask about the minimal assumptions on t yielding the pointwise sparse dominatio  our result in this direction is the followin  we collect several standard fact  the proof of the following lemma is almost the same as the standard proof of the weak type of   let k*belong to h  proo  fix a cube   therefore, by lemma   applying theorem  1 completes the proo  this along with theorem  1 easily implies the followin  then t has a bounded extension that maps l2 to itself if and only if condition hold  proo  the necessity part of this statement is obviou  thus, we only need to show the suffciency par  by theorem   having in mind corollary  3 can be further improved to the minimal assumption k belong to h ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5. Examples", "Text": "in this section we give several examples of operators admitting the pointwise sparse dominatio  note that most of the sparse bounds mentioned below are know  but here we provide a unified and simplified approach to these results based on theorem  1 and its variant  then, as it was mentioned above, hold  this result in a slightly different form can be found in 14 andrei   lerner and sheldy ombrosi example   the corresponding result can be found in therefore, by theorem   first, ta is of weak type this result was obtained in seewhere a more refined result with a specific k is obtaine ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "References", "Section": "References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "0 accounting for incompleteness due to transit multiplicity in kepler planet occurrence rates jon   christiansen2, and bradley   hansen1 1mani   the kepler pipeline typically detects planets in order of descending signal strength we find that the detectability of transits experiences an additional   we provide a method for determining the transit probability for multiple-planet systems by marginalizing over the empirical kepler datase  furthermore, because detection effciency appears to be a function of detection order, we discuss the sorting statistics that affect the radius and period distributions of each detection orde  our occurrence rate dataset includes radius measurement updates from the california kepler surveygaia dr2, and asteroseismolog  our population model is consistent with the results of burke et a but now includes an improved estimate of the multiplicity distributio  from our obtained model parameters, we find that only  6% of solar-like gk dwarfs harbor one plane  this excess is smaller than prior studies and can be well modeled with a modified poisson distribution, suggesting that the kepler dichotomy can be accounted for by including the effects of multiplicity on detection effcienc  using our modified poisson model we expect the average number of planets is  86+/- 18 planets per gk dwarf within the radius and period parameter space of kepler", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "the kepler mission has revolutionized our understanding of the frequencies and properties of planets around sun-like star  with the final data release dr25, providing all of the data up until the failure of two reaction wheelsthe primary phase of the project has offcially conclude  there have been many attempts to quantify the frequency of planetary systems and the properties of the planets themselveswith a special attention given to attempting to characterize the frequency of planets with earth-like propertie ucl ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2  Stellar Selection", "Text": "using the final release of kepler data which includes q1q17, we select a stellar sample for use in creating a detection efficiency map that accounts for kepler completenes  we use the stellar parameters provided by mathur et a  with improved radius values derived from gaia dr2 the updates from gaia dr2 have yet to provide updated corresponding mass value  thus we must still utilize the kepler dr25 stellar mass parameters it is also important for completeness mapping that each star has a stellar radius and mass measurement availabl  null values for either of these fields result in omission we also place requirements on the duty cycle and the time length of the light curve these are fduty >   the fduty limit requires that 60% of dataspan has been collecte  this ensures that a significant portion of the light curve is filled, while still including stars lost in the q4 ccd loss time-varying noise measurements have been provided in the dr25 dataset through a value known as cdpp this parameter has been calculated for every field star over 14 different time periods:   by requiring stars to have a cdpp 5h < 1000 ppm, we minimize the inclusion of stellar and instrumental fluctuations from this we produce a stellar sample of 86,605 solar-like star ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Planet Selection", "Text": "when available we utilize the updated planetary parameters provided by the california kepler survey and the asteroseismic updates provided by van eylen et a  one of the main advantages for the inclusion of these updates is the improved planet radius measurement  through private communication, it was indicated that this early release of gaia data may contain some planet radius outlier  to combat this issue, we test the radius values against the kepler dr25 catalo  overall, 19 planets exceed this outlier limit all period measurements are drawn from the light curves; thus, improved measurements from gaia and cks have no effect on the inferred period measurement  we use the periods provided in the kepler dr25 catalog  both the cks and kepler dr25 provide flags for false positive  we include data from both confirmed and candidate planets in dr25 and cksf p = false in the cks updat  to further avoid contamination from false positives, we only include planets with periods periods beyond 500 days have been noted to be highly contaminated by false positives because they barely meet the three transit limit of the pipeline our period and radii range exceeds the conservative cutoffs adopted by many previous studies, but is necessary when exploring the effects of multiplicit  often planetary systems span the entire range of the kepler parameter space, thus the inclusion of nearly all the planets is needed for an accurate calculatio  there exist 3 multi-planet systems where one planet within the system fall beyond the range of this stud  we only select the planets from these system that lie within our radius and period cut  the smoothed recovery fraction at each mes bi  the vertical lines represent the uncertainty in each bin under the assumption of a binary distributio  the bin values are plotted at the center of each bi  in providing a stronger statistical argumen  furthermore, if we include the planets that lay beyond our radius and period cuts, our analysis we will artificially inflate the number of inferred planets within this rang  the accuracy of the kepler detection order can be affected by systems with existing false positive  when removing these data points, we manually ensure that the detection order only reflects the order in which valid kois are detecte  it should be noted that these false positives do create cuts in the data, similar to that of a planet and therefore affect the detection orde  however, without reordering these systems we artificially inflate our multiplicity calculation in section   higher multiplicities are especially sensitive to mild increases as their detection probabilities are very lo  since most false positives provide relatively weak signals, only 14 systems experience this artificial re-orderin  after making the discussed cuts we find that the highest detection order existing in the parameter space is   this means that the highest system multiplicity we consider in this study is a 7 planet syste  we find 3062 kois meet the indicated period and radius requirement  it has been suggested that gas giants eject companion planets while migrating inward their large hill radius forces the planets to become unstable as the hill radius ratio falls below 1  these hot jupiters create an independent population of single planet systems if it forms via a distinct channel, this population has the ability to skew the inferred distribution of the model for the generic underlying populatio  further evidence of this independent population was discussed by johansen et a 1 jupiter mass are dynamically unstable on short timescale  we find that 120 of these single hot jupiters exist in the dataset, leaving us with 2942 kois that fit all the parameter requirements describe  our final catalog of planets and their corresponding parameters can be found online", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Injection Recovery", "Text": "here we shall discuss how we can account for the detection effciency as a function of detection orde  christiansen injected artificial planet signals into the calibrated pixels of each of the kepler field stars and processed the altered light curves with the standard detection pipelin  this allows the recovery fraction to be assessed, producing a probability function based on transit mes therefore planet detection order was not considere  however, many of the target stars are known to host real kois, and these signals will remain in the christiansen analysi  this provides an opportunity to consider the effects of detection order on recover  here we define detection order by the variable m, where m=1 indicates the first planet discovered in the system likewise, planet m=2 and m=3 corresponds to the second and third planets found by the kepler pipelin  we split the data from christiansen into injection with a the break at 200 days was selected by testing different value  beyond 200 days, we find that the distributions begin to change significantl  to focus on the 2 system ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Effects of mutual inclination", "Text": "here, we shall discuss how the effects of mutual inclination are handled within our mode  the initial recovery study was performed without consideration of higher multiplicity planet  thus, there was no accounting for mutual inclinatio  the artificial planets were injected with a random impact parameter from 0 to   to understand the effects of mutual inclination on detection effciency we look at the difference of impact parameters for recovered planet system  since an existing koi is required for this test, we only look at systems with known planet  however, larger mutual inclinations can cause certain planets to geometrically avoid transit completel ", "Subsections": [{"Section_Num": "5_1", "Section": "5.1 Transit Probability", "Text": "analytic models of transit probability have been found for double transit systems as a function of mutual inclination however, larger multiplicity systems are more difficult and require semi-analytic models in order to simplify our calculation, we simulate various semi-major axis to stellar radius ratios and look at 106 lines of sight to predict the probability of transi  to determine the period population we need a function for m transit probability at some semi-major axis value in order to create a function for probability of transit in addition to m -1 other transits, it is essential that we know the distributions of exoplanet period  clearly, this argument is circular in natur  we deal with this issue by using a non-uniform method of sampling from the empirical period populatio  to establish the desired detection order, the required number of planets are drawn from the empirical kepler period dat  the periods of the additional two planets are redrawn at each line of sigh  this is the same as saying we marginalized the additional two planets over the kepler period populatio  in order to properly account for the transit probability of higher detection orders, we need to know the unbiased underlying populations of period  this is done to account for the geometric bias against the detection of longer period planet  this mild distribution was found by looking at the impact parameter ratios within kepler system  once all orbits have been selected, the number of lines of sight where all planets transit is divided by 106 to establish the transit probabilit  each line of sight is drawn uniformly over sin and the nodes of each orbit are also drawn uniformly over si  for nodes between planets within the same system, we sample uniformly over si  we note that equation 2 is only valid for circular orbit  consideration of eccentric orbits is presented in section   to avoid the creation of unstable systems, we check the planet separations if any separation is < 10% the semi-major axis of the outer planet we resample the entire syste  this process is repeated until no separations fall below the 10% threshol  although mutual hill radius would provide a better measure of stability, our metric requires no assumptions about the mass of the planet  furthermore, we find that changing this threshold makes little difference to the probabilities calculated, indicating that stability accounting has little effect statisticall  the results of this simulation can be seen in figure   it is worth noting that equation 2 does not account for grazing transit  using a uniform distribution of r values from  2% to the overall transit probabilit  however, this uniform distribution is weighted far more heavily towards large planets than the underlying planet radius distribution, thus we expect the true correction to be much smalle  to properly account for grazing transit one must have some understanding of the underlying radius populatio  any attempt to do so here would add more uncertainty to the calculation and provide a very minimal correctio  thus, we ignore such complications her ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "6", "Section": "6 Detection Efficiency Grid", "Text": "to represent the kepler survey detection effciency a grid is created in period and radius spac  both log10p and log10r are divided into 100 bins, creating 10,000 regions of the parameter spac  for every region we uniformly sampled in log space for period and radius, all 86,605 stars are assigned m planets based on the detection order of interes  for example, in the detection grid for the first transiting planetthe probability of detecting at least one planet is calculated at each bi  similarly for m=2, the probability of detecting at least one planet at each bin in addition to finding another planet in some other arbitrary bi  the probability for transit of high multiplicity systems using the fang & margot mutual inclination mode  a machine-readable version of this data is available onlin  each region is calculated using these planetary assignments and the procedures provided in the next sections this process is then repeated for each of the 10,000 region  this procedure is similar to that of burke et a  and traubbut now with 7 different detection order grid  in our base model we assume all planets have perfectly circular orbits and consider the effects of eccentricity in section   this assumption of little or no eccentricity is reasonable for the typical multiple systems sampled by kepler, where non-circular orbits would result in unstable system architectur  the detection effciency maps for m=1:4 exoplanet discovery order  the color map is representative of log1  the fading of color across detection order shows the decreasing detection probabilit  a machine-readable version of this data is available onlin  where p is the orbital period of the plane  the expected number of transits can be found with ntr = dataspan p where dataspan is the span of the data within the kepler surve  because of various shut downs and data downloads throughout the kepler mission, it is possible that some of the transits may have been misse  to account for the probability of the transit occurring in the window of the kepler mission we adopt the window function provided by burke et a  since most targets have a duty = almost all of our sample have data throughout the full data set span of 145  the mean dataspan for this study is 142  other studies have used various way to account for the effects of limb darkening such as that of claret & bloemen we attempt to mimic the pipeline by looking at the empirical limb darkening values chosen for existing kois we find that the two limb darkening parameters used to fit planet transits within the pipeline are strongly correlated to stellar temperature the best fit line to this correlation is as follows: u1 = - 25 *10-4 *teff- 4601 we warn that these correlations mimic the choice of the pipeline rather than the true stellar features and should not be used for more evolved stars with log <   with the given calculated parameters, it is now possible to calculated the expected mes of the kepler pipeline as presented by burke & catanzarite finally, we account for the systematic detection effciency using the gamma distribution cdf described in section   combining all of the discussed probabilities provides an estimate of the detection likelihood of the highest mes planet within the syste  this probability is dependent on detection order and we shall now discuss in the next section how higher multiplicity planets can be accounted fo  to best capture the probabilities of our simulation in section  1, we interpolate between simulated data points for the transit probabilit  this can be clearly seen in the data provided by figure   since no such simulated value exist at this exact point, we interpolate between the the two neighboring estimations to establish this valu  here we use the new detection effciency for higher m planet 4% of the light curve is lost with the addition of each plane  this now produced 7 distinct detection grids the first four grids can be seen in figure   the detection order of the exoplanet in question will dictate which grid is most appropriate for applicatio  to summarize, we have described how the recovery probabilities are a function of detection order we use this to create 7 different detection effciency maps in order to create a map for m=1 planets, we sample across planet period and radius spac  doing so, we calculate the probability of detection and averaged over all stars within the kepler stellar sampl  we expand upon this idea, creating a map for m=2 planet  here the new recovery cdf is implemented to account for the additional loss of planets at higher detection order  furthermore, we account for the probability of two planets within the system transiting using a mild mutual inclination model jumping from m=1 to m=2 we lose an additional   this is due to properties of the pipeline when fitting multiple transit system  this procedure is repeated for m=3:7 each accounting for the appropriate number of transiting planets according to the data in figure 2 there is an additional loss of nearly 70% at each respective discovery order due to the unlikely event of multiple orbital alignment with our line of sigh  it is clear that these two factors, geometric transit likelihood and pipeline recovery, have a significant effect on the multiplicity extracted from the kepler data se ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "7", "Section": "7 The Likelihood Function", "Text": "using the effciency grids derived in the previous section, we can infer properties of the underlying planetary populatio  here we will discuss the likelihood function required to implement bayes theorem and extract these population parameter  we adopt the approach of previous studiesmodeling the underlying population as characterized by independent power-law distributions in period and radiu  we also make explicit the assumption that there is a single planetary population - assuming that systems which show only one transit are drawn from the same underlying distribution as those which show multiple transit  we will examine the validity of this assumption in section   our focus on multiple systems also radius rguy1  5 density period sort probability  5 radius sort cdf  5 density period sort cdf   the sorting simulation for m=1 and m=  the solid blue line represents the beta distribution fit to the respective data se  the boxes are a histogram of the simulated data after being sorte  it is apparent that sorting has a more dramatic effect on radius than perio  a mild deviations from the model is noted in the radius ske  this discrepancy dissolves as we move into higher detection order  means that we include more of the kepler parameter space than was used in most previous paper  we require continuity at rbr and pbr through the normalization constants for q and   our method expands on the poisson process likelihood used by youdin the main difference is the separation of planets by detection order previous studies such as burke et a  have used a single occurrence value, providing an average occurrence facto  by separating the occurrence factor as a function of detection order, we can allow for differences in detection effciency while simultaneously fitting for the occurrence of planet multiplicit  the parameters found in testing the sorting effects of me  these parameters correspond to a beta distribution skew expected for the cdf of each multiplicity populatio  m=1 m=2 m=3 m=4 m=5 m=6 m=7 ar ad   this value provides information on the occurrence of each m multiplicit  the function om is the sorting order correction for the probability distribution functio  this function is necessary to account for the bias in the pdf introduced by our sorting in terms of detection order here we are assuming an average probability of detection over the stellar populatio  to properly treat this integral, one would have to compute the detection probability for each sta  such a procedure would be computationally expensive and provide a minimal increase in precisio 2 sorting order here we will provide a brief overview of order statistics and why it is an important feature of this mode  as mentioned previously, the kepler pipeline finds planets in order of decreasing me  such ordering will skew the distribution of planets found in each   to account for such a skew, a joint distribution model ) can be utilized am and bm can range from and forces the skew of the distributio  essentially, the pdf of the distribution is skewed by a beta distribution of the cd  the parameters am and bm can be found analytically for equally sampled orders, but becomes far more complex in the decreasing case at hand to determine the best values for this case, we choose to simulate this sorting mechanism on a uniform distribution, where the skew can be clearly isolated and extracte  in doing so, we force the ratio of each m sample to mimic that of the empirical populatio  each system is then sorted by r2/p1/3, imitating kepler's mes sortin  this is then repeated for 107 system  figure 4 shows how the first two detection orders are skewed by this procedur  if sorting were not an issue, these distributions would maintain the uniform flat appearanc  fitting a beta distribution to this skew, we can determine the best am and bm parameters for our sampl  these parameters are provided in table   since the this joint distribution is separable, we define the skew portion of the distribution as o 3 occurrence factor as noted, the value fm is an integrated occurrence facto  this will lead to an increased contribution to lower detection order  this ratio accounts for the dependence between occurrence factor  if the mutual inclination is purely isotropic and planets are truly independent this ratio would be on  we use our transit simulation from section  1 to extract these marginalized probabilitie  table 3 contains the results from this simulatio  the mixture probabilities for each detection orde  for example, this accounts for the possibility that two and three planet systems may only be found with a single plane  these values were found using our transit probability model described in section  22 will have more than one opportunity to find an f1 plane  the physical interpretation of the fm values is the fraction of stars that have at least m planet ", "Subsections": [{"Section_Num": "7_1", "Section": "7.1 Fitting the Data", "Text": "we employ emceean affneinvariant ensemble samplerto explore the parameter space of our stud  to better constrain the 13 fit parameters, a bayesian framework is implemente  linear space uniform priors are used for all parameter  for rbr and pbr the priors range from rmin and pmin to rmax and pmax of our planet sample respectivel  one unique restriction for our prior is that fm must be larger than fm+  to avoid truncation bias and maintain order, all fm priors range from 0 to fm-  it is important to remember that fm represents the fraction of the population containing m planet  therefore, this cascading prior still allows for larger multiplicity systems to be more common than smaller multiplicity system ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "8", "Section": "8 Discussion", "Text": "in this section, we now apply the formalism we have developed to infer the revised occurrence rate parameters for planets orbiting gk dwarf  this sample includes data from the final kepler release dr25 and updated planet radius measurements from the cks and gaia dr  beyond these recent data improvements, we now include a corrected detection effciency for multiple-planet system  given that many multiple-planet systems span much of the kepler parameter space, we include planets within in implementing two detection effciencies, this study expands on the poisson process likelihood function used by other authors, allowing for the treatment of planet multiplicit  this bayesian framework is fit using an mcmc, where 20,000 steps are used to model the posterior of each paramete  the resulting posteriors are presented in figure  65+/- 05, and beta2 = -  the breaks in our best fit model occur at pbr =  08+/- 31 days and rbr =   one novel feature of our fitting method is the ability to extract exoplanet multiplicit  this information is provided through the fm parameter  these values indicate the probability of a system having at least m planet 72+/- 63+/- 60+/- 54+/- 39+/- ", "Subsections": [{"Section_Num": "8_1", "Section": "8.1 Forward Modeling the Results", "Text": "thus far, we have accounted for various parameter and population dependencie  to ensure that this process yields meaningful results, we choose to sample the extracted population and subject it to the detection constraints described in section   here we present the exomult forward modeling softwar  using this program, we can make far fewer assumptions and directly recover the expected populatio  furthermore, the detection probability will not be marginalized over all stars, but rather reviewed for each system independentl  the first step in our forward model is drawing each system of planets according to the population parameters given in figure   each system is randomly oriented with mutual inclinations drawn from a rayleigh distributio  for planets with detectable impact parametersthe planets within each system are sorted in decreasing me  the probability of recovery is assigned to each planet according to the procedure laid out in sections   based on the calculated probability of detection, the planet is either detected or lost by drawing from a random number generato  figure 5 shows the best fit model to the observed population obtained with this forward mode  fulton et a  and berger et a  have provided evidence for a dip in the radius population around   this gap is apparent in the m=1 case of figure   while the deviation from a broken power-law is mild, we explore the effects her  when we remove the single planet systems from the data set, this gap is no longer apparen  one plausible explanation for this gap is a unique population of single planet systems shows that a weak gap can be seen in the multi-planet systems when aggregated).  to explore this theory, we isolate the multi-planet systems and run our fitting procedure agai  this indicates that if a separate population does exist, the population parameters are weakly affected by their inclusion in our datase  the resulting forward model of this fit is presented in figure   furthermore, the increase in uncertainty seen in these parameters is due to the reduced samples used for fitting70 log10 log10 number of planets figure   a plot of the forward modeled population derived by our bayesian analysi  the red x marks symbolize the model values with their corresponding 6  to find this interval the model is sampled 50 times using the posterior parameter distributions, the uncertainty reflects the fluctuations we find from these trial  the black points show the kepler data with poisson uncertaint  however, it should be noted that our forward model does differentiate between these detection order  left: forward model of multiple and single planet system  right: forward model of only multiple-planet system  this model was produced by only fitting to the data of multiple-planet system  the mentioned radius ga  furthermore, it is possible that a true accounting for planet period and radius covariance could produce such a pea  millholland et a  and weiss et a  show that the planets within multiple systems tend to have similar mass and radius component  although these features are not properly accounted for here, figure 5 shows that these mild population characteristics remains small and do not deviate greatly from a simple broken power-law mode  we hope to include such features in the next iteration of this softwar  it is possible that future studies may use this forward modeling technique to directly determine the population parameter  unfortunately, it remains computationally expensive to properly account for all detection feature  traub overcame this cost by ignoring multiplicit ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "8_2", "Section": "8.2 Comparison with Prior Work", "Text": "we use a bayesian method to infer population parameters for the kepler exoplanet population, following much of the procedure presented in youdin however, we build upon this method to extract information about the population multiplicit 65+/- 05, and beta2 = - 02 provide the best replication of the empirical populatio 08+/- 31 days and rbr =   many prior studies have examined the occurrence of planets as determined by keple 44 and beta =  93 and beta = - 37 at longer periods these suggest a steep rise towards smaller radius planets at all periods, and a sharp rise with increasing periods to the break, followed by a gradual decline to longer period  this is consistent with other analysis at the same time with the accumulation of additional data and more detailed treatment of selection effects, subsequent analyses favored a flatter distribution extending to smaller radiiand a distribution falling offinversely with period at longer periods the plateau at small radii is also found around lower mass hosts burke et a  have presented an extensive discussion of planet occurrence using the q1-q16 kepler sampl  a plot of the modified poisson survival function and the system fraction provided by our bayesian analysi  this model is fit using a likelihood maximization technique, with the assumption of gaussian uncertainty the posterior distribution for the model is plotted by sampling 5000 models from the parameter posterior distributions in re  we have included the models provided by hansen & murray and fang & margot for compariso  and beta2 = - 17, with only weak evidence for a break in radius and assuming no break in period this is perhaps the most directly comparable to our analysis, as it uses the completeness estimates from christiansen et a  ; where this study uses the updated christiansen completeness dat  we find very similar values in a comparable regim  in particular, we note that both of these studies find an increasing occurrence of small radius planets down to the detection threshold, a result also supported by another bayesian methods estimate in hsu et a  previous studies have used more limited parameter ranges to avoid issues of parameter covariance and susceptibility to completeness mappin  we approach the problem with a rigorous treatment of completeness mapping and a larger parameter space, recovering a similar power-law distributio  this congruity is an encouraging sign as it shows that the inclusion of a larger parameter space does not largely effect the model being inferre  our inclusion of a broader range of periods and radius allow us to constrain the powerlaw uncertainty for radius and period to   we find breaks in our period and radius distributions occur at pbr =  08+/- 31 days and rbr =   these results are consistent with those found by prior author ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "8_3", "Section": "8.3 Survival Function", "Text": "within this study, we only use planets provided by the kepler pipelin  the highest multiplicity seen is m=7 for a gk type sta  this is certainly not the actual highest multiplicity within this parameter spac  shallue & vanderburg use a deep convolutional neural network to extract an 8th planet from the kepler-90 light curve, proving this assertion to be tru  using a poisson survival function we can extrapolate the probability of existence for these higher multiplicity system  the fm values found by this study represent the fraction of stars with at minimum m planet  this lends itself well to a survival function, where the probability of existing up to a certain value is obtaine  in this case we use a modified poisson distribution to model multiplicit  poisson distributions are ideal for planet multiplicity as these distributions are used for counting statistic  this modification allows for an excess or scarcity of zero planet system  we are only interested in stars that do harbor planets, thus this modification is necessar  further discussion of this modified poisson distribution can be found in section  3 of fang & margot the results of this fit are presented in figure  40+/- 70+/- 01 provide the best match for this distributio 18 planets per sta  this is likely a lower bound as we have excluded the single jupiter sized planets that have cleared their systems through migratio  these values are more diffcult to compare as they are strongly dependent on the range of planet radius and period include in each stud  using our population parameters and making similar cuts we find a comparable value turning the focus towards small planets and long periodsburke et a 73+/-.  when we apply these same bounds to our model we again find a slightly larger value23 using a nearly identical parameter rang  with this function in hand, we can extrapolate to higher multiplicit  for example, our model suggests that 3 3+/-  a representation of the expected empirical multiplicity as a function of selection effect  each column shows the expected population using the best fit model from this study starting from the left, moving right, each effect is adding in addition to all previous effect  the multiple detection effciency is broken into two column  the data column directly used the multiplicity values shown in figure   in contrast, the model column uses the modified poisson distribution inferred from the multiplicity data the radius of mercury is slightly smaller than our range allow 0+/- 2% harbor only two planets within the range of this stud  this lack of multiplicity in our solar system could be important for habitability, but such claims still lack strong evidenc ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "8_4", "Section": "8.4 Kepler Dichotomy", "Text": "analysis of the statistics of the kepler multiple planet systems suggest that the underlying planetary population requires a two component mode  one component is composed of systems with high planet multiplicity and a low inclination dispersion, while the other requires either low intrinsic multiplicity or a large inclination dispersion to reduce the frequency of transits by multiple planet  this has been termed the kepler dichotom  lissauer et a  inferred that the two populations had roughly equal frequencies and subsequent analyses confirmed thi  there have been several models proposed to explain this on dynamical grounds the simplest solution is to consider a single population of planets in which some fraction have experienced excitation of their mutual inclination  however, to meet the requirements of the transit statistics, the excitation is suffciently large that dynamical stability is hard to maintain thus, the kepler results seem to imply the existence of a low multiplicity population of planetary systems, whether due to formation or later dynamical instabilit  however, this finding rests on the relative frequencies of systems with single transiting planets versus multiple transiting planet  if the completeness is a function of the detection order, this may weaken the claim for a kepler dichotom  in figure 6 we show that a single poisson distribution can account for the multiplicity probabilities extracted from our analysi  to test the robustness of this low multiplicity contribution we forward model the inferred population using the poisson multiplicity mode  in table 4 under the label multiple detection effciency we present the multiplicity results of this mode  this indicated that that apparent deviations in our infer fm values can be described by statistical fluctuations in populatio  additionally, our fm are very dependent on the choice of mixture values displayed in table   a proper accounting of these values would require distribution dependenc  averaging over these parameters, as done here, can cause mild deviations in the inferred fm value  in extracting the population fm values, we have only employed a mild rayleigh distribution to account for mutual inclination of each system as directed by fang & margot and have no larger inclination componen  it appears that accounting for systematic loss of planets at higher multiplicity substantially reduces the low multiplicity population inferred as per the kepler dichotom  we shall now discuss how this work  using the forward model presented in section  1, we look at how the inclusion of detection effciency affected the gap seen between systems with one transiting planet and those with two transiting planet  using our population parameters and a mild mutual inclination model show that this anomaly is largely due to kepler detection effcienc  table 4 shows how the frequency of detected systems of different transit multiplicity changes as we include different systematic effect  in the first column, we include only the correction of the probability of transit due to geometric alignmen  for a simple numerical comparison, this results in a ratio of double transit to single transit systems of  37, to be compared to the observed value of   in the third column, we show the model in which we include the completeness corrections from christiansen without the multiplicity treatment discussed her  this results in a partial improvement of the ratio to   it is also notable that the number of expected high transit multiplicity systems also drops significantly with the inclusion of this effec  we find that the expected number of different transit multiplicities are now very well matched to the observed numbers, substantially weakening the need for an additional population to explain the observation  furthermore, the improved stellar radius measurements from gaia suggests mnras 000, 1-17 transit multiplicity in planet occurrence rates 13 that many stars have larger radii than previously believed increasing the stellar radius of system will decreases the probability of detection for an exoplane  this correction will overall increase the inferred occurrence measurement  it is important to remember that our dataset does not include single hot jupiter planets as discussed in section   this observed population of 120 planets does not follow our power-law trend and appears to be uniquely single while these outliers do provide some type of population dichotomy, their presence is not the most prominent cause of the excess of single  our extracted population parameters f1 and f2 indicate that   there is dynamical evidence that single transiting systems are more dynamically excited than multiple systems and this is consistent with the notion that some fraction of compact planetary systems are dynamically perturbed by the existence of giant planets on larger scale  previously, hansen found that explaining the original excess of single transits required a frequency of giant planets on large scales that was roughly double that found by radial velocity survey  the reduction found here substantially alleviates that discrepanc  other recent work also supports the notion that single transiting systems are drawn from the same underlying planetary population as multiple harbor system  weiss et a  find that both populations share essentially the same stellar and planetary properties, while zhu et a  use transit timing variations to infer that there is a strong correlation between multiplicity and dynamical excitatio ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "8_5", "Section": "8.5 Considering Eccentricity", "Text": "including eccentricity into our model increases the number of detected planet  these parameters are fit using an analog to the hansen & murray eccentricity mode  the original modified gamma distribution is unique to hansen & murray we map this model to a beta distributionwidely used among recent authors, for consistenc  this model was inferred by simulating in situ gravitational assembly of planetary embryos and observing the resulting eccentricity population of the fully formed planet  although derived within a specific scenario, this distribution matches well with a model in which planets explore the full range of available phase space subject to the constraint of dynamical stability as such, it represents a plausible description of the level of eccentricity to be expected in such system  recently, van eylen et a  provided evidence for two distinct populations of eccentricity using our forward modeling software 0 eccentricity empirical cdf single multi true figure   a cdf showing the retrieved eccentricities from our forward modeling pipelin  the red line illustrates the eccentricities used to draw the underlying the beta distribution the black line represents the empirical cdf of the detected single planet systems and the blue line represents the eccentricities of the detected multi-planet system  we test the strength of this hypothesi  implementing only one true underlying eccentricity model, we inspect the detected eccentricity populations from both the single and multi-planet population  when tested with the hansen & murray modelwe find no significant difference between the the observed eccentricities of multi-planet and single planet system  this indicates that the differences noted by van eylen et a  may be rea  however, van eylen et a  this is a significantly larger average eccentricity than expected by the hansen & murray mode  when larger eccentricities are tested we do find observable differences between the single and multi-planet system  the kipping model was calculated using radial velocity discoveries and contains a significant fraction of massive planet  this distribution is probably too eccentric for the tightly packed model discussed here, but illustrates the effects of detection bias on the eccentricity populatio  in figure 7 we present the results of our test on the kipping mode  we find that multi-planet systems tend to produce more low eccentricity detections than single planet detections despite being drawn from the same underlying populatio  analyzing the statistical difference with an anderson-darling test produces a p-value of 10-7, suggesting these differences would appear statistically significan  furthermore, we can see that neither of the detected populations closely mimic the true beta distribution, highlighting the importance of detection effciency consideration when performing eccentricity occurrence measurement  this effect is caused by the increased transit duration for higher eccentricity transit  increasing the transit duration improves the planet mes, making the signal easier to detec  since the highest mes planets are the most likely to be detected, this biases the empirical population toward higher eccentricit  the sorting order in combination with the multiplicity detection effciency of the kepler pipeline will further exaggerate this bias in the single planet system 18 will produce statistically significant differences between the empirical eccentricity population of singles and multiple planet system  since van eylen et a 05 model for the multi-planet systems, it is diffcult to determine the effect of detection bias on their eccentricity mode  at this point we cannot rule out that two distinct populations of eccentricity exist between the single and multi-planet systems, but propose that such claims require further evidenc ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "8_6", "Section": "8.6 Extrapolation to Longer Periods", "Text": "as mentioned above, our general populations parameters do not differ greatly from those of previous studie 07, consistent with the previous value of burke et a 75+/-  furthermore, we find tension with foreman-mackey et a  foreman-mackey et a  avoid the assumption of a particular functional form for the extrapolation to longer periods, by using a gaussian process regression to determine the shape of the distributio  however, they use the results of the terra pipeline in it's original form, in which it only reported the highest signal to noise candidate around each sta  although they back out an estimate of the detection effciency from the results of petigura et a we have shown in section 7 that detection order can bias the result  in particular, we expect foreman-mackey et a  to undercount small planets and long period period  result as a lower limi  for the occurrence of habitable planets we follow the procedure provided by burke et a 10 found in burke et a ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "9", "Section": "9 Conclusion", "Text": "we present a new method for determining the frequency of exoplanet multiplicity within the kepler datase  in doing so we provide the following new fitting features and conclusions:   for occurrence calculations these procedures are often too complex and computationally expensive to carry ou  we provide a new method which marginalizes over mutual inclination and the empirical kepler period set to determine the transit probabilities for kepler multi-planet system  using this, we provide the transit probabilities for multiple systems containing up to 7 planet  this simplification is important and useful when trying to fit multiplicity parameters via mcmc or some other fitting method that requires 104 calculation  our method does make some simplification assumptions in the interests of spee  we assume the measurements of planet radius and period are perfec  the uncertainty in period is negligible, however the radius measurements retain significant uncertainty and the present dispersion may yet mask finer features in the distributio  in accounting for mutual inclination, we adopt the model provided by fang & margot this is derived using a different multiplicity model than that found her  all orbits are assumed to be circular in our base mode  because many of the systems are very compact, circular orbits are required for any type of stabilit  tidal circularization will also force many of these planets into circular orbit  we show that any amount of eccentricity will increases our the overall multiplicity values, but decreases the fraction of systems with planet  we have assumed the appropriate model for exoplanet occurrence is a broken power-la  furthermore, we assume period and radius and uncorrelate  it has been shown by owen & wu and weiss et a  that a mild correlation exist between period and radius at short periods where photoevaporation can take effec  nevertheless, the fact that our forward modeling matches the data inspires confidence that the model provides a coherent description of the dat  in systems with more than one detected planet, we find that detection effciency decreases for higher detection order planet  this conclusion was achieved by re-visiting the christiansen injections and looking at systems with pre-existing planet  this type of increased selection effects indicates that a larger fraction of the population is being misse  being able to infer a larger population of multiple exoplanet systems significantly decreases the gap between single and double planet system  the initial motivation for additional detection effciencies for multi-planet systems, was the 61 known kois lost during the christiansen injection  because we find that 61 kois are lost we suspect higher order detection effciencies may be necessary for an accurate accounting of the true underlying population  using bayesian statistics, we expand the poisson process likelihood to account for variations in detection orde  furthermore, we are able to infer population multiplicity from this fitting proces  the results from this fit match that of burke et a  using a poisson process likelihood requires that each planet is drawn independently, which is clearly not the case for planets mnras 000, 1-17 transit multiplicity in planet occurrence rates 15 in multiple system  much of the work in this study is accounting for these dependencie  ignoring the independence requirement of poisson process could be suspect, but is again justified by the success of our forward model, where this assumption is not necessar  the independence of radius between planets within a system has also not been accounted for within this stud  given our inferred multiplicity model we can extrapolate to higher multi-planet system  we find that 3  the existence of a single 7 planet system and a single 8 planet system indicates these systems should be rare but still detectabl  we would expect to find < 1 eight planet systems within the constraints of this stud  we introduce and demonstrate that forward modeling a broken power-law distribution can still provide a reasonable model for the exoplanet population, despite growing evidence for a gap in   we find that our fitting model also produces similar populations of multiplicity to that of the empirical kepler data set, indicating the success of this metho  using the the eccentricity model of hansen & murraywe show that eccentricity can affect the multiplicity occurrence by slightly decreasing the expected number of planets around each sta 18 the kepler pipeline will significantly skew the empirical population of eccentricity for single transiting systems, suggesting that differences seen between the single and multiple planet systems may be artificia ", "Subsections": [{"Section_Num": "9_1", "Section": "9.1 Future Goals", "Text": "as mentioned previously, the uncertainties in the radius measurement are still quite larg  using a bayesian hierarchical model, this uncertainty can be incorporated when fitting for population parameters we hope to include this feature into our next generation of occurrence fittin  the multiplicity parameters derived here can be use in determining an eta earth measuremen  the importance of neighboring planets could be essential for the long term stability of an earth analogthus it is important to understand the likelihood of this earth analog within a multiple syste  ideally, we would want the detection effciency for each detection orde  to do so one would need to perform an alternative injection experiment, where numerous planets are injected into each system and the recovery of each order can be better sample  it would also be useful to understand the effects of resonance on detection effcienc  looking at a select group of stars and injecting many planets at various period ranges could provide an understanding of these features with the loss of kepler and the upcoming release of tess it will be essential to combine data across missions to calculate a more robust occurrence measuremen  doing so will require accounting for differing detection effciencies across each missio  the method described here may provide a unique way of incorporating these different selection effects while producing a uniform population distributio  acknowledgement we would like to thank the anonymous referee for useful feedbac  the simulations described here were performed on the ucla hoffman2 shared computing cluster and using the resources provided by the bhaumik institut  this research has made use of the nasa exoplanet archive, which is operated by the california institute of technology, under contract with the national aeronautics and space administration under the exoplanet exploration progra ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190  harper and gene   kim abstrac  an antichain a in a poset p is a subset of p in which no two elements are comparabl ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1. Introduction", "Text": " a chain is a poset in which every pair of elements is comparabl  a graded poset is a poset equipped with a rank functio  if p is not explicitly weighted, the weight is implicitly the counting measur  date: december 21, 201  2010 mathematics subject classificatio  primary 05d05, 05e9  key words and phrase  harper and gene   so, the ratio does go to infinity, but very slowl  in 1999, this result was designated one of ten outstanding results in order theory by the editor-in-chief of the journer orde  it is also worth mentioning another partial order, called absolute order, on s ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2. Flow morphisms", "Text": "in this section, we establish the groundwork to introduce the category flo  the objects of flow are networks in the sense of ford-fulkersonand its morphisms preserve the ford-fulkerson flows on those network  an overflow on n is defined in the same way except that the inequalities are reverse  also, minflow = maxantichai  sperner showed in his original problem that he only had to consider consecutive ranks at a time and if they satisfy hall's condition, then the poset under consideration is sperne  when trying to prove rota's conjecture, graham and harper came up with a strengthening of hall's matching conditio  the normalized matching condition is dual of the normalized flow property let m and n be network  this leads us to the category flow, whose objects are acyclic vertex-weighted networks and morphisms are precisely these flow morphism ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3. Sn is indeed Sperner", "Text": "in this section, we will prove that sn has normalized flow property which implies that sn is indeed sperne  sn has normalized flow propert  harper and gene   kim proo  we proceed by induction on   the base case is trivia  as for the inductive step, let us assume that sn has normalized flow propert  the red edges connect permutations from the raised copy to permutations of other copies, and the gray, dashed edges connect permutations from the lower copies to other lower copie  collapsing the n copies of sn we claim that this new network satisfies the normalized matching conditio  harper and gene   since the former has nfp by the product theorem, the latter has nfp als  thus, sn is indeed sperne ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "References", "Section": "References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190 00198v1 1 jan 2019 s-dual amplitude and d3-brane couplings komeil babaei velni1 and   inspired by this observation, we will find the treelevel s-matrix elements of one ramond-ramond and three open strings by imposing this symmetry on the tree-level s-matrix elements of one kalb-ramond and three open string  we also find a sl invariant form of the d3-brane effective action containing four gauge fields with derivative corrections that was derived from one-loop level four-point amplitud  using the expansion of the nonlinear sl invariant structures, we find the action with derivative corrections at the level of more gauge field  1babaeivelni@guila a ir 2 babaei@um ac", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "it has been shown that the consistency of an electrodynamics theory under the duality transformation can be expressed as a requirement in which the lagrangian must transform under duality in a particular way, defined by the noether-gaillard-zumino identity this theory could be enjoyed the sl s-duality after taking axion and dilaton fields into accoun  this action is not invariant under the s-duality, however, its equations of motion and energy-momentum tensor are invariant under the s-duality as it was shown for the electric-magnetic dualit  the supersymmetric p-branes that supported by ramond-ramond sources are the solutions of type ii superstring theorie  these branes have alternative description in terms of open strings with dirichlet boundary condition  d-branes are described by an effective dirac-born-infeld action which is closely connected to the born-infeld type effective action of open string theor  we consider the d3-brane of type iib theory which play a special role in this theor  it has been demonstrated that the d3-brane action and the corresponding equation of motion and energy-momentum tensor are invariant under the sl symmetry of type iib theory2 in fact, this action satisfies the ngz identit  as was pointed out in that the duality invariance of hamiltonian and thus of the corresponding energy momentum tensor should imply the invariance of the s-matri  1from the presence of two two-form gauge fields b2 and c2 in the string theory, a string can carry two types of charg  these two-forms form a doublet of sl it follows that the string also transform as a double  2this symmetry is not shared by the full type iib superstring theor  indeed, it is broken by a variety of stringy and quantum effects to the infinite discrete subgroup s  1 it is known that the s-matrix elements satisfy the ward identity corresponding to the s-dualit  at the level of the equations of motion, d3-brane effective action is invariant under nonlinear s-duality transformation so at higher orders, one dose not expect the effective action to be consistent with the nonlinear s-dualit  it was pointed in that the bi action involving on-shell gauge field is invariant under the linear s-duality up to f 4 term  it has been discussed in that by imposing the linear s-duality transformation on the s-dual invariant amplitude of four gage fields, the pole part of the amplitude of six gage fields could be constructe  then, f 6contact-terms could be found by applying the linear s-duality agai  this result confirms the bi action with abelian gage fields as the effective action of a single d-bran  neither acontact nor an-poles are invariant under the linear s-dualit  a, must be invarian  in the case that the s-matrix elements have no massless poles, one can find the corresponding couplings from the consistency of them with the dualitie  unlike the gauge field transformation that is carried by its field strength, all other transformations are done by field potentia  so, the pole part of s-matrix dose not transform to contact part under the closed string sl transformatio  2 contact term as well as a pole term have been calculated in it has been shown that the combination of these two amplitudes could be appeared in terms of a sl invariant structur  inspired by the above observation that the s-matrix elements of one closed string and three open strings should satisfy the ward identity corresponding to the s-duality, it could be proposed the following expression for the massless n-pole amplitude of n rr two-for  the derivatives of gauge fields could be appeared in d3-brane effective actio  the study of the behavior of four point function under the linear s-duality could be extended to four point function with derivative correction  this action satisfies the ngz identity, and is consistent with electro-magnetic self-dualit  we will find the form of above action that is manifestly s-dua  in fact, we will find the above action in terms a nonlinear s-dual structur  from the nonlinear expansion of the 4we would like to thank   tseytlin for discussions on this poin  the outline of the paper is as follows: we begin in section 2 by studying the s-duality transformations of bosonic fields and find some sl invariant structures involving these field  we find a s-dual invariant form of these amplitudes and then predict the axion amplitud  this expression could be extended to include more gauge field  using the above transformations, one can find that the structures f tmf and btmb are sl invariant structure  at the presence of axion and dilaton couplings, it could be useful to express the lagrangian in the following form which the contribution of axion coupling is separate  at first, we are going to find the corresponding amplitude in which the two-form is an antisymmetric rr field   the amplitude dose not satisfy the ward identity corresponding to the s-duality transformations, however is invariant under the ward identity corresponding to abelian gauge symmetry and to the rr gauge symmetr  the amplitude corresponding to antisymmetric b-field has massless pole as well as contact ter  the contact part of b-field amplitude can be found using the feynman rule and considering the relevant higher derivative correction of d-brane actio  applying the above relations in the amplitudeone obtains the amplitudes and in fact the amplitude of one rr, two scalar and one gauge field transform to the amplitude of one nsns b-field, two scalar and one gauge field under the s-dualit  therefore, to get the nsns coupling to brane, one can apply the s-duality on the pole amplitude contains rr stat  at the presence of the axion field, it is expected that one can find the combination of the amplitudesand in terms of sl invariant structure  it is clear that this sl invariant amplitude transformation) also contains the amplitude a 4 s-dual amplitude of three gauge fields and one twoform it has been shown that the amplitude of three gauge fields and one two-form in which the two-form is rr field c or nsns b-field could be appeared in terms of sl invariant structures in the presence of the background dilaton and r-r scalar fields we are going to show that these two amplitudes transform to each other under the s-duality transformation when the axion field dose not tack into accoun  we strart with the amplitude of three gage fields and one rr field c that is founded by explicit calculatio  this amplitude contains two independent structure  we apply s-duality transformation to these two structures separatel  therefore, the gauge field permutations can produce six different terms of i-type and three different terms of j-type that contribute in the dual amplitud  the calculation in this case is more complicated than the case in the previous section due to the presence of four levi-civita tensor  to rewrite this terms in terms of various contractions of gauge fields, one has to use the identity here, one encounters with different choice  replacing and in the above relation and using the conservation of momentum, one finds the following expression, after a straightforward calculatio  9 as we have mentioned inthis is exactly equal to the amplitude of one b-field and three gauge fields containing pole term as well as contact term that calculated in this result has been found from supersymmetry fixing and string disk-level scattering amplitude calculatio  the d3-brane effective action at the order of 4 is invariant under the sl dualit  from the lagrangian viewpoint, this action saturates the ngz identit  we are going to find this action in terms of some sl invariant structure  any of these parings of levicivita tensors led to different results in which the gauge fields appear in different kinds of contraction terms   but one can show that these results are equal when the identities presenting in the appendix have been tack into accoun  by substituting these nonzero coeffcients inwe can find the sl invariant form of the action inserting the above expansion into the nonzero invariant structurs ww inone can find the action containing six gauge fields with four derivatives that is compatible with s-dualit  this action could be appear in the simple form when one consider the total derivative term  actualy, at the level of six gauge fields and four derivatives, there are 584 contractions with structure ffdfdfdfd  to find the total derivative terms, we note that there are 724 total derivative terms with structure   using their coeffcients to eliminate the terms with structure fffddfdfdf, one finds 483 constraint equation  applying these constraints, the total derivative terms with structure ffdfdfdfdf could be foun  because of the presence of the nonlinear sl invariant structures in the final s-dual form of the actionone can derive new couplings at the order of n gauge fields with four derivatives by expanding the invariant structure  we see that the consistency of the action with the s-duality predicts the couplings and the couplings at the level of more gauge fields in the einstein fram  it would be interesting to confirm these couplings by direct calculations  5the s-matrix elements may be constrained by both manifest lorentz and duality invarianc  considering the duality invariance, as we did in this paper, may be interpreted as an intermediate step towards an analysis of the scattering amplitude 13 acknowledgments: the authors would like to kindly thank  garousi and   tseytlin for useful comments and discussions on related topic  apendix in this appendix we are going to find all identities that should be taken in to account when one wants to write the action in terms of sl invariant structures w  in fact, the terms containing four dual gauge fields and four derivatives should be written in terms of four gauge fields and four derivative  let us begin with the simple case, where we have four dual gauge fields with zero derivativ  using the identity one can write any of these terms in terms of four gauge fields contraction in the next case where we have four dual gauge fields and two derivatives, the result is dependent on the levi-civita paring choice  doing the same procedure for the third term in led to the following identity that is independent of previous identitie  let us now consider the third case where we have four gauge fields and four derivative  as the previous cases, we do the same procedure to find the above dual gauge fields in terms of relevant gauge field  these equalities led to ten independent identities which we present three of them in the followin  to investigate the s-duality behavior of amplitude and actions at the presence of a d-brane at the level of four gauge fields and four derivatives, one has to use these identities as we have done to write the action in terms of the s-dual invariant structures", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Nonlinear SL(2,R) structure", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 S-dual amplitude of one gauge field, two scalar fields and one two-form", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 S-dual amplitude of three gauge fields and one two-form", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 S-dual effective action with four derivative corrections", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Highly", "Section": "Highly Valley-Polarized Singlet and Triplet Interlayer Excitons in van der Waals Heterostructure", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": " a general class of such models deals with nonlocal fractional power elliptic operator  in order to solve these problems numerically it is proposed to consider equivalent local nonstationary initial value pseudo-parabolic problem  previously such problems were solved by using the standard implicit backward and symmetrical euler method  in this paper we use the one-parameter family of three-level finite difference schemes for solving the initial value problem for the first order nonstationary pseudo-parabolic proble  the fourth-order approximation scheme is developed by selecting the optimal value of the weight paramete  the results of the theoretical analysis are supplemented by results of extensive computational experiments", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "in many recent applications the new mathematical models are proposed, which are based on fractional derivative equations in time and space coordinates very different applied mathematical models of physics, biology or finance describe a subdiffusion or superdiffusion model  the latter problems are often simulated by using fractional power elliptic operator  *corresponding author email addresses: raimonda ciegis@vgt ltvabishchevich@gmai com preprint submitted to journal of computational and applied mathematics january 3, 2019 arxiv:190  in this paper we will use the method of finite elements, since this method is well-suited to solve problems in non-regular domains and to use non-uniform adaptive grids the most important class of iterative methods for this purpose are krylov subspace method  they are used to solve systems of linear equations obtained after approximation of fractional power elliptic problems a comparison of different approaches to solve fractional-in-space reaction-diffusion equations is done in in particular the integral and adaptively preconditioned lanczos method are analyze  the most straightforward algorithm to solve such systems is to construct explicitly eigenvectors and eigenvalues of the given discrete elliptic operator and to diagonalize the matrix a but we should note that the direct implementation of this approach is very expensive for general elliptic operators in multidimensional domain  it requires the computation of all eigenvectors and eigenvalues of very large matrice  a general approach to solve fractional power elliptic problems is based on some approximation of the nonlocal operato  one can adopt a general approach to solve numerically equations involving fractional power of operators by a popular method is to split the task to solve numerically equations involving fractional power into two step  first the original elliptic operator is approximated and then the fractional power of its discrete variant is take  using dunford-cauchy formula the elliptic operator is represented as a contour integral in the complex plan  then applying appropriate quadratures with integration nodes in the complex plane we get a method that involves only inversion of the original elliptic operato  the approximate operator is treated as a sum of resolventsensuring the exponential convergence of quadrature approximation  in paper a more promising quadratures algorithm is proposed, when the integration nodes are selected in the real axi  the new method is based on the integral representation of the power operator in this case the inverse operator of the fractional power elliptic problem is treated as a sum of inverse operators of elliptic operator  such a rational approximation is obtained when the fractional power of the operator is approximated by using the gauss-jacobi quadrature formulas for the corresponding integral representatio  in this case, we have a pade-type approximation of the power function with a fractional exponen  the optimal rational approximations are investigated in a separate class of methods approximates the solution of fractional power elliptic problem by some auxiliary problem of high dimensio  in it is shown that the solution of the fractional laplacian problem can be obtained 2 as a solution of the elliptic problem on the semi-infinite cylinder domai  this idea is used to construct numerical algorithms for solving stationary and nonstationary problems with fractional power elliptic operators infor solving fractional power elliptic problems we have proposed a numerical algorithm on the basis of a transition to a pseudo-parabolic equation, so called cauchy problem metho  the computational algorithm is simple for practical use, robust, and applicable to solving a wide class of problem  we have used this algorithm also for solving the nonstationary problem with fractional power elliptic operators for the auxiliary cauchy problem, standard two-level schemes are applie  depending on the weight parameters the first and second order accuracy of the approximation is obtaine  for many applied problems a small number of pseudo-time steps is suffcient to get a good approximation of the solution of the discrete fractional equatio  the effciency of this algorithm is improved inwhere a special graded grid in pseudo-time is use  another possibility to increase the accuracy of approximations is to use high order discrete schemes for solving the auxiliary pseudo-parabolic equatio  in this paper we propose and investigate a fourth order three-level schem  the paper is organized as follow  in section 2 a problem for a fractional power of elliptic operator is formulate  in section 3 the cauchy problem method is give  the main results are described in section 4, where unconditionally stable fourth-order three-level scheme is proposed and investigate  section 4 provides results of computational experiments, they illustrate the theoretical results on the approximation accuracy of fractional power problem  a model two dimensional problem is solved by using different numerical scheme  at the end of the work the main results of our study are summarize ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Problem Formulation", "Text": " now we define the boundary value problem for the fractional power of   we approximate the problem by using the finite element method", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Cauchy problem method", "Text": "for solving numerically problem we use the cauchy problem method, proposed in this method is based on the equivalence of to an auxiliary pseudo-time evolutionary proble  for the solution of the problem, it is possible to obtain various a priori estimate  to solve numerically the problem, the simple implicit two-level euler scheme can be used5 the difference schemeis unconditionally stable with respect to the initial dat  proo  applying these estimates recursively we prove the validity of", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Three level schemes", "Text": "in this section we consider high order scheme  they are based on three level finite difference scheme  we note that w1 should be computed by applying some two level numerical algorithm and the accuracy of this approximation should be the same as of the main scheme more details will be given belo  it is well-known that for suffciently smooth solutions the symmetrical scheme approximates problem - with the second order accurac  next we formulate the stability conditions for the schemehere we use the general stability results for operator-difference schemes let s be a self-adjoint positive operator in  25 the three-level schemeis unconditionally stable with respect to the initial dat  proo  next we consider how to define the initial condition for w  a general approach is to use some two-level solver for t belong to it is interesting to see if some explicit schemes can be used to find the initial condition for w  in general for suffciently smooth solutions the o accuracy is expected for w  more interesting second order explicit schemes can be constructed by using the well-known method described in due to the obtained stability restrictions the explicit scheme is not recommended for solving real application  the three-level high order difference schemeis unconditionally stable with respect to the initial dat  proo  the schemebelongs to the family of three-level weighted schemesthe implementation of the high order three-level difference schemerequires to specify the second initial condition w  it should be computed with the same fourth order accurac  we do not have any robust, unconditionally stable and effcient two-level high-order difference schem  in all computations presented in the next section the initial condition w1 is computed by using the symmetrical two-level scheme and a suffciently fine grid is constructed on the time interval let this interval be divided into m sub-interval  we note that the computational complexity of the three-level algorithm is increased approximately twice if such approach is applied to compute the initial condition w ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Numerical Experiments", "Text": " the piecewise linear continuous p1 lagrange elements are used to approximate the elliptic operato  the accuracy of different approximations in time will be estimated by a reference solutio  in fi  for the two-level weighted difference scheme the errors of the solution are presented in fig  the main goal of this paper is to investigate the accuracy of the threelevel difference schemefirst we have used the two-level symmetrical scheme to compute the initial condition for w  for suffciently smooth solutions it defines the initial condition with o accurac  next we have investigated the influence of the initial condition for w  the accuracy of the approximation is further increased when the initial condition is computed using the algorithm here we note that the observed convergence rates of the two-level and threelevel schemes depend on the discrete regularity of the solution of the discrete fractional power problem and they are not reaching the maximal possible convergence rates of these scheme  the dependence on the regularity of the solution can be reduced by using geometrically refined time grid ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "6", "Section": "6 Conclusions", "Text": " we have formulated the problem of finding the high order difference schemes for solving the nonstationary cauchy type problem which is equivalent to the fractional power elliptic proble  the high order approximations are used to approximate the time dependence of the solution, while the elliptic operator is approximated by the standard finite element schem  the suffcient stability conditions are given for the two-level discrete schemes with weight parameter  the second order accuracy is proved for the symmetrical crank-nicolson type schem  the family of three-level symmetrical discrete schemes is constructed and investigate  22 suffciently smooth solution  the initial condition on the first time level is computed by using the symmetrical two-level schem  the initial condition on the first time level of the main grid is computed by using the symmetrical two-level scheme with a specially selected fine time gri  the theoretical results are illustrated by results of numerical experiment  a two-dimensional problem is solved for the elliptic operator with the discontinuous sink term coeffcien  acknowledgements this work of second author was supported by the mega-grant of the russian federation government", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "homogenization of the vibro-acoustic transmission on perforated plates   the plate can be replaced by an interface on which transmission conditions are derived by homogenization of a problem describing vibroacoustic fluid-structure interactions in a transmission layer in which the plate is embedde  the reissner-mindlin theory of plates is adopted for periodic perforations designed by arbitrary cylindrical holes with axes orthogonal to the plate midplan  the homogenized model of the vibroacoustic transmission is obtained using the two-scale asymptotic analysis with respect to the layer thickness which is proportional to the plate thickness and to the perforation perio  the nonlocal, implicit transmission conditions involve a jump in the acoustic potential and its normal one-side derivatives across the interface which represents the plate with a given thicknes  the homogenized model was implemented using the finite element method and validated using direct numerical simulations of the non-homogenized proble  numerical illustrations of the vibroacoustic transmission are presented", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "the noise and vibration reduction belongs to important issues in design of structures used in the automotive industry, or civil engineerin zc czvlukes@km zc  this manuscript version is made available under the cc-by-nc-nd   arxiv:190 00202v1 1 jan 2019 an important and well known exampl  however, there are many similar solid structures which can influence the acoustic wave propagation in flui  the straightforward approach to modelling the acoustic wave propagation through vibrating perforated plates consists in solving directly the vibroacoustic problem with a 3d elastic structure describing the plat  however, its numerical treatment using the finite element method can lead to an intractable problem because of the prohibitive number of dofs corresponding to the geometric complexity of the perforated structur  therefore, it is reasonable to replace the elastic plate by an interface on which coupling transmission conditions are prescribe  in this paper, we consider the acoustic wave propagation in an inviscid fluid interacting with elastic structures designed as periodically perforated plate  the aim is to derive non-local vibro-acoustic transmission conditions using the periodic homogenization metho  although similar problems have been treated in the literature, c in this context, the plate elasticity has not been considered ye  during the last decade, a number of works appeared which are based on a homogenization strateg  in contrast with dealing with thin perforated interfaces only, in we were concerned with homogenization of a fictitious layer in which rigid periodically distributed obstacles were place  in particular, a rigid plate perforated by arbitrary shaped pores could be considere  therein nonlocal transmission conditions were obtained as the two-scale homogenization limit of a standard acoustic problem imposed in the laye  here we follow the approach reported in to develop vibroacoustic transmission conditions which substitute the vibroacoustic interaction on an elastic perforated plate immersed in the acoustic flui a rigorous treatment of such a problem has not been treated using the homogenization method so fa  as the result we obtain vibroacoustic transmission conditions in a form of 2 an implicit dirichlet-to-neumann operato  due to this operator, the elastic perforated plate can be replaced by an interface on which a jump of the global acoustic pressure is linked to the acoustic momenta associated with two faces of the homogenized plat  it allows us to obtain an effcient numerical model which takes into account geometrical details of the periodic perforation without need of discretizing the vibroacoustic problem at the global leve  in other words, the homogenized interface provides a reduced model in which a complex 3d elastic structure is replaced by a 2d perforated plate model whose coeffcients retain information about the perforation geometr  to do so, we rely on the homogenized reissner-mindlin plate tailor-made for the simple perforation represented by general cylindrical holes with axes orthogonal to the mid-plane of the plat  elastic strongly heterogeneous plates were treated in where the framework of the reissner-mindlin theory was used to derive a model of phononic plates, c but without the interaction with an exterior acoustic fiel  the proposed modelling conception based on the problem decomposition and using the homogenization provides an alternative framework for modelling of microporous panels which are known for their capabilities of acoustic attenuation in the so-called patch transfer functions were developed for numerical modelling of compliant micro-perforated panel  the plan of the paper is as follow  in section 2 the vibroacoustic problem of the wave propagation in a waveguide containing the perforated plate is decomposed into the problem in a fictitious transmission layer and the outer problem governing the acoustic field out of the laye  the in-layer vibroacoustic problem is treated using the homogenization method in section 3, where the local problems imposed in the representative periodic cell are introduced and formulae for the homogenized coeffcients are give  the limit two-scale model of the homogenized layer is validated in section 5 using direct numerical simulations of the original proble  some technical auxiliary derivations are presented in the appendi  notatio  center: detail of the layer structure; the layer thickness is proportional to the plate thickness and to the perforation perio  spect to a cartesian reference frame   the strain tenso  we also use the jump   formulation and decomposition of the vibroacoustic transmission problem the aim of the paper is to find a representation of the vibro-acoustic interaction on a perforated plat  for this, homogenized vibroacoustic transmission conditions are derived using the asymptotisc analysis   in remark 1 we explain the dual interpretation of the plate thickness used in the asymptotic analysis of the vibro-acoustic proble  2 and fi  vector u = involves also the transversal mode terms we get 1/ which is coherent with the dilation operation applied when dealing with fluid equation, see section   variational formulation of the vibroacoustic problem in the layer in order to derive the homogenized model of the transmission layer, we shall need the variational formulation of problem with the plate model integrals provide a symmetry of the following formulatio  the problem formulatio  as an essential step of the proof, the a priori estimates are derived in the appendix  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Formulation and decomposition of the vibroacoustic transmission problem", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Homogenization of the transmission layer", "Text": "in this section, we introduce the convergence result which yields the limit acoustic pressure and the plate displacements and rotation  these are involved in the limit two-scale equations of the vibroacoustic problem imposed in the transmission laye  the asymptotic analysis is based on the unfolding method which was inaugurated in the seminal paper and elaborated further for thin structures in", "Subsections": [{"Section_Num": "3_1", "Section": "3.1 The convergence results", "Text": "based on the a priori estimates derived in the apendix a, the following theorem hold  due to theorem 1 providing the estimates - we obtain the convergence of the unfolded function  in l2 the limit vibro-acoustic problem can be derived by a formal approach which relies on the recovery sequences constructed in accordance with the convergence resul ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3_3", "Section": "3.3 Limit plate equation", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3_4", "Section": "3.4 Local problems in Y*", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3_7", "Section": "3.7 Homogenized equations associated with the plate", "Text": " in the r", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Global problem with vibroacoustic transmission conditions", "Text": "we recall the problem decomposition according toand conditions the problem describing the vibro-acoustic response in the layer has been homogenized, yielding equations and our further effort will focus on the coupling the acoustic field in the layer with the surrounding environmen  in particular, below we introduce a coupling equation which is associated with the limit equations in the homogenized layer and provide the transmission conditions for the global proble  expression ofwhereas we pass to the limit on the   by virtue of the r", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Validation of the homogenized model", "Text": "the homogenized model derived in this paper provides an approximation of the vibroacoustic interaction in a vicinity of the perforated plat  in this section, we examine how numerical responses of the proposed homogenized vibroacoustic model corresponds with solutions of the original problem associated with the 3d heterogeneous solid structure representing the plat  to this aim, the reference model is established as the finite element approximation of problem introduced in section   for this model, the heterogeneous structure of the transmission layer is built up as the periodic lattice by copies of the reference periodic cell according to -.  the geometries associated with the homogenized and the reference models are illustrated in fi  the validation of the homogenized model - is performed in two step  secondly we compare the responses of the homogenized vibrating plate with the deflections obtained by direct numerical simulations of the heterogeneous 3d elastic structur  the reason for such a simplification arises as the consequence of the fe mesh complexity increasing with the number of the perforating holes, thus, inducing a discretized problem with large number of the degrees of freedom 4, where the slice dimensions are indicate  the waveguide is symmetric   the center of the perforated plate structure which splits the acoustic domain into two mutually symmetric part  the thickness of the perforated plate is   the number of the perforation periods drawn in the x1-axis directio  the homogenized models and the reference model presented in this paper have been implemented in sfepy - simple finite elements in pythona software developed for an effcient solving of multiscale problems by means of the finite element metho  accordingly, the homogenized layer presents the coupling conditions for the acoustic field in the wave guide which is governed by the periodic conditions are prescribed on the two faces orthogonal to the x2-axis direction for the geometry depicted in fi  responses of the reference and the homogenized models are compared using the global acoustic properties expressed by the transmission lossand by the local distributions of the acoustic pressur 2 kg m  the tl curves obtained for both the models are compared in fi  perforations with cylindrical holes were examined for two radii   results 32 figure 5: geometries and fe meshes of the fluid domains related to the reference and homogenized model  geometry of the periodic cell employed in the multiscale simulation and in construction of the reference geometr  from these graphs it is apparent that the result differ only in the vicinity of wave numbers yielding the tl peaks; in those regions associated with higher wave numbers also the shift of the peak positions can be observe  however, this effect can be caused by different fe discretizations of both the model  the difference of the two tl curves is displayed in fi  6 for the two dimensions of the hole 0125 which corresponds to n = 2  we also examined responses of the two models in terms of the acoustic fields fluctuations in the wave guide near to and far from the perforated plat  for this, the acoustic pressure distributions were traced along lines l1 and l2, see fi  the real parts of the acoustic pressure fields are compared in fi  7, distributions of the imaginary parts are similar as the real part  the pressure field phom is reconstructed using the results of the multiscale simulation and the expressions introduced in appendix   fe approximation of   this error is illustrated in fi  8 for the increasing number of perforations   9the left figure shows the distribution of the pressure pre ", "Subsections": [{"Section_Num": "5_2", "Section": "5.2 Validation test \u2013 compliant perforated plate", "Text": "the second part of the validation test concerns the homogenized model of a perforated plate of the reissner-mindlin typ  the aim is to compare responses of the homogenized plate model with the ones of the associated 3d elastic structure with the geometry depicted in fi  the equivalent boundary conditions and loading function are used in the homogenized model, where the 3d structure is represented by the plate model described as a 2d structure, see fi  the plate deflections are computed for the two models,   using the dns of the 3d structure and using the multiscale simulations of the plat  as seen in fi  right: geometric 2d representation of the reissnermindlin perforated plat  the effect of the plate compliance is illustrated in fi  13, where we compare the values computed for the rigid and compliant perforated interface  the relative difference of the values is defined as tldiff = |tlrigid tlcompl|/tlrigi  the influence of the plate compliance on the transmission loss in the waveguide is sensitive on frequency intervals, nevertheless this phenomenon will deserve a further stud ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "6", "Section": "6 Coupled numerical simulation", "Text": "the purpose of this part is to illustrate, how the homogenized vibroacoustic transmission model derived in this paper can be used for numerical simulation  acoustic waves using the two-scale in this pape  to this aim we consider an analogous problem as the one specified in section   zero neumann condition applie  also the same acoustic fluid and the same elastic solid are considered, as in the validation test  the computed macroscopic responses are shown in fi ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "7", "Section": "7 Conclusion", "Text": "in this paper, we derive transmission conditions which serve for coupling acoustic fluid pressure fields on an interface which represents a compliant perforated elastic plat  for this, we consider a fictitious layer which embeds the elastic plate with periodic perforation, such that the perforation period is proportional to the layer and plate thicknesse  to derive the transmission conditions, the layer is decoupled form the outer acoustic field which is respected by introducing neumann fluxes the layer is then treated by the asymptotic analysis based on the periodic unfolding homogenization metho  further averaging procedure based on a weighted integration in the transversal direction   the layer mid-plane yields additional relationships which enable us to couple the outer acoustic field with the in-layer variable  in this way, the dirichlet-to-neumann operator is constructed which couples traces of the outer acoustic pressure 38 figure 14: acoustic pressure fields and plate deformations induced by the incident wave prescribed at the inlet of the waveguid  with its normal-projected derivatives on both sides of the interfac  the numerical examples reported here illustrate the validation tests which have been performed to explore the modelling errors associated with the homogenization and the 3d-to-2d dimension reduction of the layer which is replaced by the interface coupling condition  we used the circular shape of holes, however, arbitrary shaped cylindrical holes can be considere  the validation tests were based on the comparison of responses computed using the homogenized models with the corresponding responses of reference model, here presented by direct numerical simulations of the nonhomogenized vibroacoustic proble  this observation underlines the main advantage of the homogenized model: 39 it provides very good approximation of the reference solution, but at a considerably lower computational cost than the dns solutio 5 * 104 dofs to get the homogenized coeffcient  among the topics of the future research, the homogenization-based modelling of the compliant plate with arbitrarily shaped periodic perforations presents one of the most interesting issues since such structures provide significantly bigger potential to modify the vibroacoustic transmissio  first steps towards optimal design of perforated plates in the acoustic transmission problems were reported in acknowledgmen  this research was supported by project gacr 17-01618s of the scientific foundation of the czech republic and due to the european regional development fund-project application of modern technologies in medicine and industryand in part by project lo 1506 of the czech ministry of education, youth and sport ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "A", "Section": "A Appendix", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "reig@ifi u  the resulting theory is an invisible qcd axion model without domain wall  no dangerous heavy relics appea  arxiv:190 00203v3 1 sep 2019 contents", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 The model", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 The instanton interference effect (IIE)", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Avoiding heavy stable relics", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 HC coupling running and confinement", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "6", "Section": "6 Discussion", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "7", "Section": "7 Conclusions", "Text": "10 1 introduction the appearance of topological defects during spontaneous breaking of symmetries constitutes a clear and profound connection between particle physics and cosmology as the universe cools down several phase transition take place and, depending on the homotopy groups of the manifold of degenerate vacua, stable topological defects may form in particular, the cosmic domain wall problem is a well-known potential issue of axion models recently, it has been pointed out that majoron models can also suffer from domain walls to solve such a long-standing problem, several mechanisms have been propose  being a cosmological-particle physics issue, it is not surprising that one can tackle it from both, cosmology and particle physics side  a couple of well known solutions are: cosmic inflation and the lazarides-shafimechanism in the first one the dangerous walls are pushed beyond the horizon, being a clear example of a cosmological solutio  in the second case, one associates the spontaneously broken discrete symmetry to a gauge symmetr  this removes the physical degeneracy among the different vacua, which become gauge equivalen  another interesting solution that has been recently suggested implements the witten effect to solve the domain wall proble  more exotic postinflationary solutions involve primordial black holes to perforate the walls, change their topology and destroy them one can imagine that the degeneracy of the associated vacua disappears for theories where the breaking of peccei-quinn symmetry is dynamically triggered by new confining force  this is nothing but the standard, well-known hierarchy problem of axion model  we conjecture that the solution of both problems is intimately relate  the reason is that dynamical breaking of symmetries by fermion condensates usually brings associated instanton effects, which explicity break anomalous symmetrie  dynamical breaking of pq symmetry has a long history in the context of composite axion models and has gained interest recently new strongly coupled and confining interactions have been also reconsidered recently to rise the qcd axion mass in this work we build a minimal model where the breaking of pq symmetry arises dynamically at a high scale thanks to a new chiral confining forc  in addition, we show how the associated instantons implement the instanton interference effectsolving the domain wall proble  a similar construction has been explored by barr and kim in this reference it was suggested that new confining interactions can solve the domain wall proble  this issue seems almost unavoidable in the context of confining interactions, since they usually bring associated conserved quantum number  baryon number in the standard model is the most clear exampl  if the lightest of these unconfined bound states, which we will call hyperbaryons, is stable it might overclose the universe depending on its mas  dangerous heavy relics can be diluted after a period of cosmic inflatio  however, in such a case one might ask why we do not also use inflation to avoid the domain wall proble  we will show below which are the basic ingredients to achieve a phenomenologically successful solution to all these problem  upq is the global, anomalous pq symmetr  all sm particles are g singlet  for simplicity, we assume that the pq charges for sm fermions are given by fermion chirality, this is +1 for left-handed fermions and -1 for right-handed fermions  this assignment is compatible with the original pqww and dfsz axion model in the g sector, we assume one of the fermions has pq charge +1 while the other has no pq charg  the reason for this will become clear late  it can be seen that upq symmetry is anomalous under both, qcd and   this is a reasonable assumption, since it seems rather artificial to protect an anomalous symmetry from anomalies of another gauge grou  the first question that arises is which kind of groups are appropriate for   many possibilities emerg  however, one has to deal with the limitations coming from the g triangle anomal  strikingly enough, they admit complex, chiral representations and are anomaly fre  the use of spinor representations will also be important to solve the heavy relic proble  the hc fermions previously mentioned in e  the scalars required to distinguish them are 1this particular choice is compatible with an underlying so gut it was shown by weinberg that this kind of phase transitions can exist this can be seen from their non-trivial homotopy group,   however, being a chiral representation, the spinor does not allow a bare mass ter  it is well known that the topological susceptibility vanishes when there is a fermion with zero mass this situation resembles the one proposed by barr and kim in their work 3 the instanton interference effect since we are extending the qcd axion model with a new confining interaction there are two potential sources of explicit pq symmetry breakin  therefore, the breaking of upq is done in the direction of znhc and znqcd discrete subgroups, with nhc and nqcd the anomaly coeffcient  this may lead to an interesting situation where the explicit breaking is not in the direction of the same subgrou  the residual discrete symmetry unbroken by the combination of non-perturbative effects will be the common subgroup of both, znhc and znqc  this is what we call instanton interference and can be pictorially visualized in figure   if the anomaly coeffcients nhc and nqcd are co-prime numbers, the instanton interference completely solves the domain wall problem, since znhc and znqcd have no common subgrou  there is, however, a subtlety related to the high-scale ii  in other words, there is no epoch when both hc and qcd potentials are turned o  this implies that the highscale iie works only when nhc = 1 1 scalar potential and nqcd the scalar potential of the model described above is relatively simpl  since the model is dfsz-like in the sm sector, the part of the scalar potential corresponding to the higgs 3note that this is not the case for the low-scale version of the iie where nhc can be different from 1 as long as it is relatively prime to nqc  this is because for the low-scale version, the hc and qcd axion potentials are both turned on at low energie  see for detail  the degeneracy of the vacuum is determined by the gauge invariant order parameter  this potential, in combination with the pq charges of and the anomaly coeffcient computed as dictated by ereveals a znqcd = z3 symmetry among the different vacu  then the pq symmetry is connected to the hc sector and the iie operate  this term is also important to connect the spontaneous breaking of pq symmetry to the hc scale and not to the ew scal  such a redefinition would be broken only by qcd instantons in a similar way to the standard dynamical axion this is however not necessarily the cas  in this case, pq symmetry is necessarily anomalous under both qcd and hc and the iie works to solve the domain wall proble  this makes the axion potential coming from hc instantons flat below t  it can be easily seen that the blue and red dots only coincide in one point 4 avoiding heavy stable relics new confining interactions can have a non-trivial cosmological impac  for su gauge groups, a global u symmetry analogous to baryon number protects the stability of the lightest hyperbaryo  analogously, so groups feature a z2 conserved quantum numbe  this z2 symmetry counts the number of indices for the fundamental representation and can stabilize the lightest bound stat  these group theoretic properties have been used to stabilize dark matter another example of exotic matter stabilized by the conserved z2 symmetry of an so confining interaction are hyperbaryons in the context of comprehensive unification it was shown long ago by griest and kamionkowski that a stable particle that was in thermal equilibrium may overclose the universe if it is very heavy one can naively think that because sohc is confining, there will be stable hyperbaryon  however, this is not the cas  as mentioned before, this condensate breaks upq spontaneousl  additionally, the condensate has also non-trivial implications in the dynamics of sohc itsel  here we briefly describe the situatio  therefore, the condensate must break sohc gauge symmetr  this is done following the most attractive channel rules the ssb chain of the confining interaction stops at this point and the so remains unbroke  we are now about to show why no dangerous heavy relics emerge in our framewor  for so with n odd there are two conjugacy classe  the conjugacy classes of the 5the mac rules state that the fermions involved in the condensate get a non-zero mas  since the product of representations decomposes into representations with the same classonly products of an even number of spinors can give us so singlet  all the possible hyperbaryons,   so singlet bound states, are z2 singlets and deca  note this differs from a pure yangmills theory where the lightest bound states are glueball  hc-mesons are allowed to decay into sm degrees of freedom through hc anomaly diagrams and the interaction in e 4which connects the hc and sm sector  5 hc coupling running and confinement we have assumed that the hc interaction becomes strongly coupled at high energie  for the sake of completeness, let us study an explicit example quantitativel  obviously, the coincidence of all interaction strengths in one point of the plane is meaningless unless there is an underlying unified gauge group in the u  running of gauge couplings in a supersymmetric scenari  hc gauge coupling corresponds to the dashed black lin  see tex  reason is that if there is no unified group containing hc and the sm, keeping the coupling evolution together above mgutthe gauge couplings will separate as can be seen in fi  however, we believe that this example does illustrate the principles of model buildin  then, the lazarides-shafimechanism can be naturally implemented without adding extra fermion  7 conclusions new chiral confining interactions with fermions in the spinor representation can simultaneously make the axion invisible and solve the domain wall proble  the instanton interference effect has been described in detai  if pq symmetry remains unbroken during inflation, the instanton interference mechanism suggests itself as a compelling possibility to avoid the domain wall problem, combining different sources of explicit breaking by instanton  this fact strongly suggest them as compelling candidates for the confining interaction of composite axion model  acknowledgments i am especially grateful to   fonseca for helpful discussions about group theory and   agrawal for discussions and insightful comments during the early stages of this wor  i am also grateful to   yamada and   quilez for enlightening discussions about a preliminary version of the manuscrip  this work is supported by the spanish grants fpa2017-85216psev-2014-0398 and prometeo/2018/165", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "hasib a ir department of electrical and computer engineering  knt a  one type of ntcs that has gained huge attention in recent years applies deep learning on packets in order to classify flow  internet is an imbalanced environment   in order to solve this problem, we recommend the use of augmentation methods to balance the datase  in this paper, we propose a novel data augmentation approach based on the use of long short term memory networks for generating traffic flow patterns and kernel density estimation for replicating the numerical features of each clas  first, we use the lstm network in order to learn and generate the sequence of packets in a flow for classes with less populatio  then, we complete the features of the sequence with generating random values based on the distribution of a certain feature, which will be estimated using kd  the contribution of our augmentation scheme is then evaluated on all of the datasets through measurements of precision, recall, and f1 measure for every class of applicatio  the results demonstrate that our scheme is well suited for network traffic flow datasets and improves the performance of deep learning algorithms when it comes to above-mentioned metric ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "I", "Section": "I Introduction", "Text": "with the ever-increasing amount of traffic that goes through the network, network management has become a difficult tas  one of the most important tasks in network management is identifying the types of traffic that are passing through the networ  classifying the applications is a fairly simple task with high evaluation metric  additionally, ntcs have been able to take care of this matter efficientl  two major purposes of ntcs are detecting anomalies in the network and classification of applications for quality of service purposesthere have been several types of ntcs that use different methods for handling the task at hand, however, each one has its own drawback  moreover, if the port is changed, this method is no longer reliable they generally have three major drawback  the first one is that they need to be updated with new patterns in the payloads of emerging applications in addition, they are not able to identify all of the flow  furthermore, if we do not have access to the payload of packets for privacy reasons, their accuracy is very much affecte  this type of algorithms usually work with the features in the header of the packets, but some of them may also take into account the information in the payloadsalthough they are still limited, they have shown great potential in terms of evaluation metrics and will be a great substitution in the future for the aforementioned method  most of the traces that are gathered from real internet traffic are imbalanced   this matter is a lot bolder when it comes to large-scale traffic and will cause some serious problems in the way of algorithms' f1 measur  augmentation is an approach in machine learning that addresses the issue of small amount of data for trainin  this approach usually tries to increase the training data in a way that can be still classified in the same categor  another way of achieving augmentation is through generating artificial data for a clas  in order to address the challenges of machine learning arxiv:190 00204v1 1 jan 2019 algorithms in imbalanced network datasets, we introduce a novel augmentation method to improve the accuracy of deep learning algorithms on real-world traffic traces by using kde and lst  the remainder of this paper is organized as follows: in section ii we review the related works in the area of ntc  in section iii we describe our augmentation schem  the dataset and deep learning model that was used in order to classify the traffic traces are mentioned in sections iv and v, respectivel  finally, the evaluation of our method is demonstrated in section v ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "II", "Section": "II related works", "Text": " considering this, there are several well-known pieces of research that are done up to this poin  there are several works that have applied deep learning architectures or neural networks in order to solve the classification proble  in lopez-martin et a  have presented a deep convolutional recurrent neural network architecture in order to classify network flows and have found the best setting in that environment in terms of hyper-parameters and feature se  nevertheless, they have not taken any measures to handle the imbalance problem of their datase  additionally, the scale of their dataset is approximately fifth of the one that we are usin  in rahul et a  have also proposed using convolutional neural networks in order to classify network traffic but only consider three classes of applications in their work on a limited amount of dat  in a comparison between cnn and stacked autoencoders in order to classify not only types of traffic but also applications in the network in a standard vpn/none-vpn dataset in packet level has been give  the scheme that was used, unlike ours, relies on the features from both header and payload of packets which may not be available in some privacy-preserving dataset  finally, in auld et a  have deploy a bayesian neural network in the form of a multi-layer perception and accordingly classify their datase  in this work, the lowest performance metrics are from the classes with the lowest number of dat  some works in this area have attempted to battle the imbalanced property through different measure  in rotsos et a  have introduced a method through probabilistic graphical models for semi-supervised learning in a naive bayes mode  this is based on the assumption that some classes have a higher probability than other  also has presented a new feature extraction method using a divide and conquer approach for an imbalanced dataset in the networ  as an instance of lstm used for generating sequential data, has introduced a method to generate data using lstm and evaluated the method to show that it can capture the temporal features in the datase  lstm has also been used as an augmentation tool in works such as and for generating handwriting and human movement data, respectively, and has proven to be efficient in both case  ii  augmentation scheme for generating time series network data in this section, we describe our contribution of augmentation scheme for generating new data in network traffic trace g, tcp and ud  every application in the internet creates a flow of packets between communicating peers in order to represent flows in our work, we have to choose a set of features for each one that can capture the nature of a flo  according tothe appropriate set of features that will give acceptable results for classifying flows are mentioned in table   these features are gathered for the first 20 packets of each flow, which are more than enough for capturing the temporal and spatial features of a flo  as shown in table i, we can put the features in two categories: sequential and numerica  each group has its own way of augmentation which are described in the followin    generating sequential features in this section, we demonstrate our approach to generating sequential feature  as mentioned earlier, traffic flow comprises the sequence of packets that are transmitted between a source and a destinatio  some applications are uni-directional  g, uploading a dat  however, in some type of applications, packets go in both directions such as when a client is communicating with the server and gets a response for its reques  whether a packet is sent from source or destination depends on the sequence of packets that have already been sent up to this point in the flo  therefore, we can conclude that the sequence of directions of packets in a specific application is of time-series nature and can be generated through means of sequence generation like intcp window size is another feature of the flow that is dependent on the previous values in the flo  generally, this value is an indicator of the conditions of the connection and processing speed of data in the flow thus, its amount at each step of the flow is affected by previous steps' value  one of the most common ways to generate a sequence is using recurrent neural networkswhich try to learn the patterns in time-series data   in our work, we use one type of rnns called lstm networks each lstm block tries to learn the probability distribution in a step of a sequence whilst taking into consideration the information from previous step  in order to train the network, we gathered the patterns of packet directions in a flow for up to 20 packets in a class of flow applicatio  we encode every direction by 1 or 0 with the former being from source to destination and the latter is the other way aroun  at the end of each sequence, we put a unique character as an indicator of the ending of the flo  then every sequence is shifted by one character to the right and is used as labels in order to train each step of generation in lst  in the generation phase, first, we choose a direction based on the distribution of that direction in the dataset for the first time step and give that as input to the lst  afterwards, we use the output of each step as probability distribution of each character and generate a new directio  then, we feed that output direction to lstm in order to generate next step probabilitie  the maximum number of steps are 19 in order to generate the pattern of flow up to 20 packets let xt and ht denote the direction of the packet in the dataset and the generated direction by the lstm at time step t, respectivel  therefore, the generation process is demonstrated in fi  generating numerical features in this section, we describe our method of generating numerical features of a flo  as shown in table i, we consider four numerical features for each packet of the flo  in order to generate new samples from these features, first, we need to learn their probability distributio  since these features are not sequential, we can use conventional probability density distribution estimation method  one of these methods is kde that is in the category of kernel method  kde, also known as the parzen-rosenblatt window, is one of the most famous methods used to estimate the probability density function of a datase  kde, as a non-parametric density fi  generating 19 time steps of packet sequence with lstm estimator, does not have any assumptions about the density function as opposed to the parametric family of algorithm  this method will learn the shape of the density from the data automaticall  this flexibility that arises from its nonparametric nature, makes kde a very popular method for data drawn from a complicated distributio  in order to prevent bias-variance problem in fixed h cases, we used the bandwidth selection method represented in", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "III", "Section": "III augmentation scheme for generating time series network data", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "IV", "Section": "IV dataset", "Text": "in this section, we describe our dataset and its labeling metho  for this paper, we used real traces of traffic from the campus of amirkabir university of technology that includes more than 70 gigabytes of packets from udp and tcp link layer protocol  next, we label flows using ndpi, which is an open source dpi tool released by ntop for classifying the flows based on applications the percentage of different classes of applications in our datase  of labeling tool is that according tondpi is the most accurate open-source dpi tool among available dpi tool  nineteen classes of traffic from more than 50 gigabytes of packets were chosen which include 904490 flow  85 percent of these flows were chosen for training and the rest are used for test datase  the classes of applications are the ones with the most number of instances in the dataset and can be seen in table i  as shown in table ii, there are different classes of applications in our dataset and the names of our labels are chosen based on the labels given by ndp  the percentage of each class is shown in fi  as demonstrated by the bar chart, the imbalance feature of the dataset is clea  furthermore, more than 83 percent of the whole dataset consists of only 4 classe ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "V", "Section": "V classification scheme", "Text": "in this section we explain the classification scheme that was used to test our augmentatio    augmentation in the augmentation phase, we generate new data from classes that have less population in the datase  first, we train and use lstm to generate the pattern of directions and tcp windows sizes in the flo  after that, we estimate the pdfs of every single numerical feature using kd  then, according to these pdfs, we generate points in every feature domai  these points are our generated features for the packet  if the number of packets in the generated sequence is less than 20, the rest of the array is appended with   these arrays will comprise the generated datase  the pseudo-code for the augmentation process is given in algorithm   training next, we train a convolutional recurrent neural network on the augmented datase  in order to do this, we choose the architecture that was suggested in each of these layers is followed by a batch normalization laye  after that, the output of the last bn is put in time-series format and is fed into an lstm layer of 100 hidden unit  these are followed by a soft-max layer with 19 outputs, each corresponding to 19 classes of traffi  the activation function of every layer in this architecture, except for the soft-max layer, is relu functio ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "VI", "Section": "VI evaluation", "Text": "in this section, we present the evaluation results of the model on three different dataset  the method of sampling in is a simple yet effective approach to handle the problem of imbalanced classification and is widely used in many works such as in furthermore, these are the classes that have low number of samples in the datase  the evaluation metrics that are chosen to measure the performance of our approach are those that are mostly used for imbalanced datasets and give an appropriate analysis of the methods that are employe  the f1 measure shows the overall performance of algorithm on both precision and recal  in fi  3 the precision metric for all three datasets is give  although in some classes with less instances that have been augmented like playstore and instagram, there has been a slight decrease in precision, others have mostly had an fi  precision measure comparison of per class in the dataset fi  recall measure comparison of per class in the dataset improvement in this matte  furthermore, the number of classes that are improved by our augmentation is more than those that performed better in sampled datase  fi  4 depicts the recall of each class in three separate dataset  in every augmented class, there is a clear upgrade in recall measur  this is due the fact that the number of fn predictions are less for these classes compared to the normal datase  this might have some negative effect on the fi  f1 measure comparison of per class in the dataset fi  due to higher generality in our augmentation, it is obvious that the amount of increase in recall in our approach is higher than sampling in augmented classes in every instanc  moreover, sampling has caused a decrease in recall in 12 classes compared to actual datase  fi  5 illustrates the f1 measure in all the classes of the datase  this figure verifies the fact that overall performance of our method is better than sampling in each and every one of the classe  fi  6 shows the overall measures on the whole dataset  as fi  confusion matrix resulted from the actual dataset fi  confusion matrix resulted from the augmented dataset shown in this figure, although sampling improved the recall, it has also a slight decrease in precision due to the lack of generalizatio  on the other hand in our method, in all three metrics, there is a noticeable improvement which is more than any that is caused by sampling metho  fi  7 and fi  8 illustrate the confusion matrices of actual and augmented datasets, respectivel  as shown in fi  fi  8 shows that our method is able to improve this matter and lessen the number of false prediction  additionally, the number of true positives in http and ssl is increase  although dns predictions have less true positives, the number of false negatives is diminishe  moreover, the overall accuracy in our method is increased by  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "VII", "Section": "VII conclusion", "Text": "in this paper, we proposed an augmentation method for imbalanced network traffic classification on real traffic traces based on lstm and kd  in order to compare the performance of our scheme, we considered two sampled and augmented dataset  the results that are obtained from crnn show that our approach gets better results in overall measures of precision, recall, and f ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "References", "Section": "References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "1 nasal patches and curves for expression-robust 3d face recognition mehryar emambakhsh and adrian evans abstract the potential of the nasal region for expression robust 3d face recognition is thoroughly investigated by a novel five-step algorith  first, the nose tip location is coarsely detected and the face is segmented, aligned and the nasal region croppe  then, a very accurate and consistent nasal landmarking algorithm detects seven keypoints on the nasal regio  the last step applies a genetic algorithm-based feature selector to detect the most stable patches and curves over different facial expression  the algorithm provides the highest reported nasal region-based recognition ranks on the frgc, bosphorus and bu-3dfe dataset  the proposed method does not rely on sophisticated alignment or denoising steps, is very robust when only one sample per subject is used in the gallery, and does not require a training step for the landmarking algorithm", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "I", "Section": "I Introduction", "Text": "while much previous research on expression invariant 3d face recognition has focused on modelling expressions and detecting expression insensitive facial parts, there have been relatively few studies evaluating the potential of the nasal region for addressing this issu  despite this, the nose has a number of salient features that make it suitable for expression robust recognitio  although it has been reported that the 2d image of the nose has too few discriminant features to be used as a reliable region for human identificationits 3d surface has much undiscovered potentia  this paper further investigates the 3d nasal region for human identity authentication and verification purposes and presents a novel algorithm that provides very high discriminant strength, comparable with recent 3d face recognition algorithms, which use the whole facial domai  the proposed approach is based on a very consistent and accurate landmarking algorithm, which overcomes the issue of robust segmentation of the nasal regio  the algorithm first finds an approximate location of the nose tip and then finely tunes its location, while accurately determining the position of the nasal root and detecting the symmetry plane of the fac  these landmarks are utilised on feature maps created by applying multi-resolution gabor wavelets to the surface normals of the depth ma  two types of feature descriptors are used: spherical patches and nasal curve 4% for frgc v 5% for frgc's neutral v  neutral and neutral v 2% when one gallery sample per subject is used for the frgc dataset v  the feature extraction algorithm is described in section iv and section v explains the feature descriptors use  the feature selection algorithm is detailed in section vi and experimental results, including a thorough comparison with previous work, is provided in section vi  finally, conclusions are given in section vii    scientific contribution and comparison with previous work the major contribution of this paper is a novel surface normal-based recognition algorithm that provides a thorough evaluation of the recognition potential of the 3d nasal regio  the results achieved are not only better than previous 3d nose recognition algorithms but also higher than many recognition algorithms that employ the whole fac  to localise the expression robust regions on the nose a heuristic ga feature selection is applied to two different geometrical feature descriptor  because of the smoothing effects of the gabor wavelets, there is no need for sophisticated denoising algorithm  indeed, only simple median filtering is required for the surface normals, even with noisy datasets such as the frgc spring 2003 folde  an additional advantage of the proposed approach is that a fast principal component analysis -based self-dependent method can be employed for facial pose correctio  this eliminates the need for sophisticated pose correction algorithms or reference faces for fine tuning the alignmen  the proposed approach significantly extends our previous work in which the nasal landmarking and recognition was performed on the depth ma  this paper increases the number of landmarks and their detection accuracy and presents new feature extraction and selection algorithm  the work is inspired by recent algorithms on utilising facial normal vectors in 3d and regional normal vectors by using multi-resolution gabor wavelets the ability of the algorithm to handle more noisy samples is enhanced, providing higher r1rr than the approach of li et a which excluded the noisy frgc spring 2003 sample  this work also extends the application of facial curves, introduced as feature descriptors by berretti et a ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "II", "Section": "II Recent literature review ", "Text": "robustness against the deformations caused by facial expressions has been a popular research topic in 3d face recognitio  the face is a non-rigid object and therefore 3d matching techniques for rigid objects, such as the iterative closest point algorithmcan become trapped in local minima and fail to provide accurate matching score  an empirical approach to deal with the variations caused by expressions is to capture a range of facial expressions for each subject and store them in the gallery then, the facial biometric features of each test subject can be compared with all the stored expressions and a decision made on the identity of the subjec  this method has numerous disadvantages: capturing a range of facial expressions for each subject is not always straightforward and requires a high storage capacity per subjec  in addition, facial expressions will not necessarily remain constant and may differ between the test and gallery captures one approach to overcome this problem is to use computer graphics algorithms to artificially create different expressions for each facial captur  inexpressions are learned using pca eigenvectors and then used to re-generate the expressions on the probe sample  although this approach does not require multiple samples per subject in the gallery, it is still vulnerable to the number of training samples used to model the facial expression  also, a universal definition of facial expression for all subjects still remains to be found and the need to classify the expression types prior to face recognition increases the computational complexit  spreeuwers proposes a multiple regional approach based on a pca-linear discriminant analysis feature extraction method use a regional registration algorithm in conjunction with lda classifiers, giving an expression robust 3d face recognition approach they also demonstrate that the nasal region has a high discriminatory powe  a focus on integrating multiple regions is provided by queirolo et a  using facial curves is another popular approach to 3d face recognition that can be categorised as a subset of regional algorithm  drira et a  use the intersections of planes with the facial surface to define a set of radial curves which pass through the nose tip, and then perform a quality assessment in order to handle missing data and occlusions another curve-based algorithm is proposed by berretti et a  first, keypoints are detected on the facial surface and then the least variant curves on the face are selected using a statistical model and matched with those in the galler  in another curve-based approach, drira et a  find geodesic curves on the nasal region for a subset of the frgc dataset to overcome the sensitivity of holistic face recognition algorithms to expression variations, mian et a  propose a landmark-based method, in conjunction with a localised feature descriptor that incorporates the 2d texture and 3d point cloud  in an alternative approach, wang et a  apply shape difference boosting to the bosphorus dataset to learn the expressions and identify those facial regions which remain constant over different expressions instead of using depth or the point coordinates for 3d registration, mohammadzade et a  use the surface normals of the points in conjunction with a fisher's discriminant paradigm this approach selects the normals which maximise the concentration of within-class scatter while simultaneously maximising the between-class distributio  recently, li et a  proposed local normals histograms, captured from multiple rectangular regions on the face, to set up an expression-robust feature space and use a novel sparse classifier to perform the matching despite the robustness of these algorithms against facial expressions, they often rely on accurate and consistent facial segmentation, which is not a straightforward task in 3  to address this issue, some researches have focused on the nasal region, which shows high consistency over different expression  for example, in one of the first investigations on 3d nose recognition, chang et a  initially segment the face into different non-overlapping regions, using the curvature information then, three overlapping nasal regions are detected and stored in the galler  the same regions are segmented in the probe images and matched using the icp algorith  wang et a  propose the use of local shape difference boosting for 3d face recognition and also apply the boosting algorithm to different nasal regions when the value of r was increased, the recognition ranks reached a maximum and then plateaue  a  used the dijkstra algorithm to segment the nose and evaluated the performance using a subset of the bosphorus dataset", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "III", "Section": "III Preprocessing and nasal region landmarking", "Text": "the algorithm explained in is used to crop the fac  next, median filtering with a  5 mm2 mask size is applied twice on the cropped fac  the image is then resampled to a uniform grid with  5 mm/pixel horizontal and vertical resolutions using delaunay triangulation and aligned using the iterative pca algorithm the aligned face is then intersected with three cylinders to crop the nasal region, according to5 mm2 mask to further smooth its surface and decrease the spike noise effect  the block diagram in fi  1-a shows how the landmarks in fi  1-b are detecte    this process results in several curves on the nasal region, shown in fi  3-  the proposed landmarking algorithm relies on a minima detector, which finds a set of minima on rotated versions of the curves and then maps them to the original curv  the rotation is required because some of the original curves are strictly decreasing functions that do not have an actual minimu  fi  each projection of any point on pr to the horizontal axis should correspond to only one point on the vertical axi  the minima detector operator of is applied to each curve in fi  fi  the horizontal strip sk y used in 3-  then the image is divided into the left and right halve  assuming the rotated nasal region is translated so that the nose tip l4j is at the origin, for the y-axis indices within the strip sk y shown in fi  this is an example of a min-max optimisation, which finds the best worst case for the optimum a plane passing through l1opt and l4opt is then intersected with the nose surface, with normal vectorsee fi  4-  the locations of the maximum and minimum of the resulting curve are the positions of l1opt y and l4opt y this procedure is illustrated in fi  the location of the lowest minimum of the resulting curve provides the subnasale l5 after applying  fi  5: and rois for detection of the nasal alar groove and eye corners landmark  these are chosen to be able to crop the nasal alar region, while avoiding redundant parts similarly, the roi used to detect the eye corners is depicted in fi  planes parallel with the xz-plane are then intersected with each row of the roi  then, for each row, rminj and lminj are compared and the pairs with the most similar euclidean distances to the nose tip l4 selecte  figures 5-b and -c illustrate these processe  the points found as candidates for the nasal alar groove and eye corners might contain some outlier  this is because of the imaging noise and deformations on the face due to the facial expression  to remove the outliers, an iterative approach is use  first, the 3d euclidean distances between the points on each consecutive row are compute  this process continues until the number of inliers remains unchange  the outlier removal algorithm results in the green points labelled as the inliers in fi  5-b and -  the left and right pairs, which have the closest value of y to that of the nose tip are selected as l3 and l  also, the points amongst the inliers in fi  5-d, with the smallest depth values, are detected and the pair with the most similar distance to l4 are selected as the eye corners the eye corners and nasal alar groove landmarks are the red points in fi  5-b and -d, respectivel ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "IV", "Section": "IV Feature extraction", "Text": "the proposed feature space is based on surface normal  the algorithm proposed by manjunath et a  is used to minimise the wavelets overlap and redundancy in the filtered images the discrete fourier transform of the resampled gabor wavelet gs,o for the sth scale and oth orientation level is computed and its zero frequency component is set to zer sm where ns = is a block matrix containing the normal vectors for the sth scale leve ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "V", "Section": "V Localised feature descriptors using spherical patches and curves", "Text": "the feature descriptors are used to define a part of the nasal region, containing a set of normal vectors from the gabor wavelets filter  this procedure is illustrated in fi  the feature descriptors are used to reduce the dimensionality of the feature space, decrease the redundancy and enable the use of probabilistic feature selection to lower the sensitivity to facial expressions while maintaining the most discriminative part  the basic landmarks previously identified, see fi  1-b, are used to create the new keypoints shown in fi  7-  these new landmarks are easily obtained by dividing the horizontal and vertical lines that connect the landmark  a sphere centralised on each point is then intersected with the nasal surface and its inner parts are croppe  then, the histogram of the normals of gabor-wavelet filtered depth images are computed, based on the procedure explained in section i  the intersection process is depicted in fi  7-  a set of spheres of identical radii are intersected with the nose surfac  these spherical feature descriptors provide the capability to evaluate the potential of overlapping spherical regions on the nasal surface, when used as feature vector  7: grid of landmarks used for the spherical patches in the nasal curves are found using the combination of new landmarks, illustrated in alternatively, using different pairs of landmarks, a set of orthogonal planes to the nasal region can be foun  intersecting the planes with the nose surface results in a set of curves on the nasal regio  when a1 and a2 are selected from the set of landmarks shown in fi  7-c, they can be used to create the set of curves shown in fi  7-d, which provide the feature descriptor ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "VI", "Section": "VI Feature selection using GA", "Text": "the feature selection step selects those subsets of feature vectors extracted from the curves and spherical patches that are more robust against facial expression  here the aim is to find a binary vector to be used as a switch to select the most robust and remove the vulnerable feature descriptors to facial expression  the elements of bi,sk are set to zero or one, depending on the value of the ith element of b  the value of each element of b can be altered using the nucleus binary vector b  a curve or patch is selected or omitted based on the value of bn element  by grouping the neutral samples for the gallery and the non-neutral samples for the test phase, and varying bn, the most expression robust curves and patches can be selecte  as shown in and when bs1 = bs2 = the kernel fisher's analysis algorithm with polynomial kernel is applied to the feature space to project the features to a lower dimensional space using a supervised approac 53 table i: landmarking consistency error in m  matrix containing the matching error  in other words, r1 is the rank one recognition rate which is maximised as bn is change  the excellent capability of ga in high dimensional binary parameter spacemake it well suited for this non-convex optimisation proble  the modified nsga-ii incorporates elitism over the individuals that increase the diversity of the population in addition to those with better fitness outpu  the parameter assignments for the ga are explained in section vii- ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "VII", "Section": "VII Experimental results", "Text": "  3d datasets and experiments three datasets are used to evaluate the proposed recognition algorithm  the first one is the frgc dataset, which is widely recognised as the largest 3d face dataset, with 557 subject  the samples in the spring 2003 folders are known as the v 0while the collection in the other two folders constitute v  frgc v 0 and v  to evaluate the algorithm on this dataset, three sets of experiments are define  for the first set, frgc v 79% creusot et a 33% table ii: landmarking precision accuracy from the ground truth over the bosphorus dataset sample  the second experiment is known is frgc's roc iii on exp iii this is a verification scenario, which uses the between season 3d data sample  for this experiment, usually the equal error rate or  1% false accept rate is reporte  the third evaluation using frgc is termed expression v  expressio  frgc consists of samples with neutral and non-neutral facial expressions, and using different sets of facial expressions for the probe samples enables neutral gallery v  neutral probe and neutral gallery v  non-neutral probe evaluation  the purpose of this experiment is to quantitatively evaluate how the performance of a face recognition system with a neutral gallery changes when the probe samples are replaced by non-neutral sample  the other two datasets used are the bosphorus 3d face dataset and the bu-3dfe datasetwhich contain captures of six prototypic expressions in addition to neutra  it contains four different levels of intensity of each facial expression and only one neutral sample per subject, making it one of the most challenging benchmarks for face recognitio  for the bosphorus database, one neutral sample per subject is used in the gallery and the remaining 2797 samples as probesfor both the bosphorus and bu-3dfe a specific expression robustness evaluation uses the neutral expression as the gallery and the captures for each expression in turn as probeslandmarking consistency and accuracy the face recognition rates reported in the following subsections provide indirect evidence of the landmarking algorithm's consistenc  however, to provide an independent assessment, the within-class similarity of the landmarking is investigate  in this evaluation, all subjects in the datasets are translated so that the nose tip is located at the origi 8 1 distance detection accuracy left alar groove right alar groove nose tip left eye corner right eye corner fi  8: precision curves for the proposed landmarking algorithm computed using bosphoru  subject's different facial expressions is compute  this process is performed for all subjects in the dataset and the averages and standard deviations are calculate  for an ideal landmarking algorithm, the average for each landmark would be zer  however, due to the noise in the data and image acquisition errors, in practice the averages are non-zer  although frgc contains 557 subjects, some only have one sample and these are discarded, as it is impossible to compute the mean distance for such subject  the errors are higher for the frgc dataset as its samples are noisier, especially those in the spring 2003 folde  the most consistent pair of landmarks on both datasets are the two nasal alarwhile the errors are slightly higher for the eye corners the location of subnasale is more consistently detected for the samples of the bosphorus datase  this is mainly due to the 3d reconstruction noise for the higher frequency regions, as is in subnasale, for the frgc sample  the other important factor for a landmarking algorithm is its accuracy and, in order to evaluate this, the locations of the landmarks are compared with the ground truth provided by the bosphorus datase  as the location of nasal root has not been assigned by the dataset providers, it is excluded from the accuracy evaluation  the curves shown in fi  8 are found over the action units samples and samples with neutral and non-neutral expressions in the bosphorus datasetconstituting 2803 sample  a comparison with the landmarking results reported by creusot et a  is shown in table i  although the proposed landmarking algorithm is not as robust as the approach of in the cases of partial and self-occlusions, it has the advantage of not requiring a training ste  robustness against facial expressions this section explains the parameters used for the feature descriptors, gabor wavelets, the kfa and the ga optimise  the supplementary material presents more extensive experiments that evaluate the effects of varying these parameters on the overall face recognition performanc  the gabor wavelets at each orientation and scale can be defined by the lower and higher frequency levels and the maximum number of orientation and scale levels for the gabor wavelet implementation the feature extraction and gabor filtering code is employed  the supplementary material includes extensive results of the effects of the landmarking distribution for the spherical patches and gabor wavelets parameters on the face recognition rank  dr is the input feature space dimensionality, which is reduced to d after kfa is utilised65, respectivel  the kfa implementation is based on a modified version of the publicly available pretty helpful development functions for face recognition toolbox   the population size is 15 times the number of variables, while the pareto and cross-over and migration fractions are   1http://ol visio ec ucs edu/texture/software/ 2luk f uni-l 99 1 ranks recognition performance spherical patches curves spherical patches curves fi  non-neutral prob  the uniform creation function is used to initialise the population and the phenotype distance crowding measurement is utilised to compute the individuals' distance measur  the code is implemented using matlab's global optimization toolbox  for the feature selection stage, the neutral samples are employed for training and all the non-neutral samples for the test phas  the polynomial kernel is used for subspace projectio  then, the mahalanobis cosine distance of is applied for the matching step at each iteration of the g  in order to quantitatively illustrate how a subset of selected features can boost the recognition ranks, as an example, fi  9 shows the face recognition ranks before and after applying the feature selection over the bosphorus datase  for all experiments presented in the following sections, different datasets are used for feature selection and testing, such that the test dataset is always completely unseen by the feature selecto  either the bosphorus or the bu-3dfe dataset is used for feature selection, as these both have high variations in 3http://mathwork com/help/gads/gamultiob  the bosphorus dataset is used for feature selection for all experiments in which frgc or bu-3dfe are the test datasets, while bu-3dfe is used for feature selection with the bosphorus datase  in this experiment, all the folders in frgc from different seasons are merge  then, for each subject, the number of samples in the gallery is changed and the average r1rr are reported after the samples are interchanged in the gallery and prob  the results show the high discrimination of the feature space for the spherical patches over the frgc datase  for example, when only one sample per subject is used in the gallerya r1rr of 9  to the best of our knowledge, this is the highest 3d nasal region recognition rank ever obtained from this dataset for a single training sample and comparable with many state-of-the-art 3d face recognition algorithms, which use the whole facial domai  although the curves have lower r1rr for one training sample per subject, there is a big increase in their recognition performance when the samples per subject are increase 0 and roc iii: the recognition performance for the frgc v 0 dataset and the roc iii experiment are widely used face recognition benchmark  the dataset contains samples with different facial expressions for both the gallery and prob  the rank recognition performance shown in fi  10-a increases with the rank to > 99% for the spherical patche  a similar trend exists for the cumulative matching characteristic curves increasing with a high gradient for all ranks >   the results of applying different patches and curves to this dataset are given in table i  the table also compares the performance of the proposed approach with recently proposed 3d face recognition techniques that employ the nasal region and also the whole fac  for the frgc v 2 false accept rate : logarithmic scale false reject rate spherical patches curves eer curve fi  10: cmc curves for frgc v  it is interesting to note that there is even a slight increase in the recognition rates when non-neutral samples are used as probeswhich shows how robustly the algorithm has learned the facial expression  in comparison with the algorithm of li et a  roc iii curves are the frgc's cross-seasonal verification scenario and these are plotted in fi  10-b using a logarithmic scal  the eer and   although these results are higher than some previous 3d nose and face recognition methodsthey are outperformed by a number of algorithms that use the whole face, which show a higher robustness for the verification scenari  one conclusion from this may be that, for verification scenarios, the whole facial region might provide a higher confidence level when matched with a claimed identit 0 eer roc iii  1% far roc iii neutral v  neutral neutral v  non-neutral spherical patches curves 3d nose 9 5% smeets et a  3d face 8 2% osaimi et a  3d face 9 8% spreeuwers et a  3d face 3d nose 9 7% drira et a  3d face 9  3d face 3d nose 9 40% wang et a  3d face 9 7% wang et a  3d nose 95% 78% drira et a  3d face/nose 88% 7 5% chang et a 1% emambakhsh et a  3d nose 8 61% li et a  3d face 9 2% queirolo et a  3d face 9 8% berretti et a  3d face 9 8% berretti et a  3d face 9 0% mohammadzade et a  3d face 9 2% mian et a  3d face 9 7% mian et a  2d+3d face 2d+3d nose 9  both contain samples with known expression type  for the bu-3dfe results the bosphorus dataset is used for feature selection and vice vers  for the first set of experiments, bu-3dfe is used for feature selection and the bosphorus dataset for tes  the gallery consists of 105 neutral samples, one for each subject, and all the remaining non-neutral and neutral samples are used as probe  a comparison with the previous results is provided in table   while most comparison approaches have used the whole 3d face, the results of the nasal spherical patches is highly competitive when applied over the same datase 1% li et a  3d face 9 6% li et a  3d face 9  3d face 9  gallery samples/ n  of probe sample  algorithm facial expression happy surprise fear sadness anger disgust neutral spherical patches curves 9 88% li et a  the second experiment is based on using various expressions for the probe sample  to investigate the algorithm's performance in recognising probe samples with unseen facial expressions, for each subject one expression type is selected for the gallery samples and another for the prob  the results are shown in table vi, where each column relates to a facial expression and the number of samples used in the probe set is give  the average recognition rank for the 76 subjects with more than one neutral sample per subject is reported for the neutral v  neutral experiment, as shown in the last colum  the spherical patches again outperform the nasal curve  the disgust expression deforms the noses more significantly than the other expressions and produces the lowest recognition rank  the algorithm is compared with that of li et a in which similar evaluations are performe 8% hajati et a  for a similar experiment with the bu-3dfe dataset, the 100 neutral samples are used as the gallery and the remaining samples with different intensity levels of each expression used as probe  table vii shows the r1rr for each expression typ  as the expressions in the bu-3dfe dataset are significantly more intense than those in the bosphorus dataset, the recognition rates are lower than those in table v ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "VIII", "Section": "VIII Conclusions", "Text": "to address the problem of expression invariant face recognition, a novel algorithm is introduced, that utilises the 3d shape of nos  the algorithm is based on a highly consistent and accurate landmarking algorithm, a robust feature space, discriminative feature descriptors and feature selector  the proposed method is applied over three well-known face datasets, frgc, bu-3dfe and bosphoru  the matching results show that the algorithm is very successful for both the identification and verification scenarios, producing a r1rr of 9 9% on frgc v  the proposed method does not rely on sophisticated preprocessing algorithms for its denoising and alignmen  for the bosphorus dataset a r1rr of 9 35% is obtained when one neutral sample per subject is used for gallery and the remaining samples with various expression types as probe  the results of the proposed method reveal the high potential of the nasal region for 3d face recognitio  the recognition ranks are not only significantly higher than previous nasal region-based algorithms, but also have a better performance than many 3d holistic and multi-modal approache  there are several aspects of the algorithm which can be utilised in other application  the application of the proposed landmarking algorithm can be investigated for performing facial alignment, low dimensional face recognition and pattern rejectio  finally, the application of the feature extraction step on the whole facial region, to make it robust against occlusions is an interesting area of future researc ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "References", "Section": "References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Biographies", "Section": "Biographies", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190  as the most important consequence, we can prove the splitting theorems of jacobi pairs which was proposed by dazord, lichnerowicz and marle in as an application we provide a alternative proof of the splitting theorem of homogeneous poisson structure  contents", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Preliminaries and Notation", "Text": "", "Subsections": [{"Section_Num": "2_1", "Section": "2.1 Notation and a brief reminder on Jacobi Geometry", "Text": ".  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2_2", "Section": "2.2 The Omni-Lie Algebroid of a line bundle and its automorphisms", "Text": ".  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2_3", "Section": "2.3 Dirac-Jacobi bundles", "Text": ".  ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Submanifolds and Euler-like Vector Fields", "Text": "", "Subsections": [{"Section_Num": "3_1", "Section": "3.1 Normal Bundles and tubular neighbourhoods", "Text": "", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Normal Forms of Dirac-Jacobi bundles", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Normal forms and Splitting Theorems of Jacobi bundles", "Text": "", "Subsections": [{"Section_Num": "5_1", "Section": "5.1 Cosymplectic Transversals", "Text": ".  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5_2", "Section": "5.2 Cocontact transversals", "Text": ".  ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "6", "Section": "6 Application: Splitting theorem for homogeneous Poisson Structures", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "7", "Section": "7 Generalized Contact bundles", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "A", "Section": "A The Moser trick for Jacobi manifolds", "Text": "25 *jschnitzer@unisa", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "References", "Section": "References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "the surface diffusion and the willmore flow for uniformly regular hypersurfaces jeremy lecrone, yuanzhen shao, and gieri simonett abstrac  we consider the surface diffusion and willmore flows acting on a general class of hypersurfaces parameterized over a uniformly regular reference manifold possessing a tubular neighborhood with uniform radiu  the surface diffusion and willmore flows each give rise to a fourth-order quasilinear parabolic equation with nonlinear terms satisfying a specific singular structur ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1. Introduction", "Text": "the surface diffusion and willmore flows are geometric evolution equations that describe the motion of hypersurfaces in euclidean space the normal velocity of evolving surfaces is determined by purely geometric quantitie  for both flows, the mean curvature is involved in the evolution equations, while the willmore flow additionally depends upon gauss curvatur  these flows have been studied by several authors for compact hypersurface  in this paper, we consider uniformly regular hypersurface  it should be emphasized that these surfaces may be non-compac  the concept of uniformly regular riemannian manifolds was introduced by amann and it contains the class of compact riemannian manifolds as a special cas  the study of geometric flows on non-compact manifolds is an active research topic, both from the point of view of pde theory and in relation to its applications in geometry and topolog  our work generalizes the study of these two flows to a larger class of manifold  2010 mathematics subject classificatio  key words and phrase  this work was supported by a grant from the simons foundation our analysis relies on the theory of continuous maximal regularity and the results and techniques developed in the results in theorem  1 are ne 3 was obtained in for the surface diffusion flo  for the willmore flow, theorem   the authors in showed the existence of a lower bound on the lifespan of a smooth solution, which depends only on how much the curvature of the initial surface is concentrated in spac  inthe authors proved convergence to round spheres under suitable smallness assumptions on the total energy of the surfac  in particular, we obtain global existence and convergence for non-convex initial surface  the organization of the paper is as follows: in sections  2, we introduce the concept of uniformly regular manifolds and define the function spaces used in this pape  in sections  4, we review continuous maximal regularity theory and its applications to quasilinear parabolic equations with singular nonlinearit  these results form the theoretic basis for the study of the surface diffusion and willmore flow  we utilize these concepts to parameterize the evolving hypersurfaces driven by surface diffusion and willmore flows as normal graphs over a reference hypersurfac  in section 5, we likewise establish well-posedness properties for solutions to the willmore flow over -hypersurfaces in r  we conclude the paper with an appendix where we state and prove some additional properties of normal graphs over -hypersurface  = y means that they are equal in the sense of equivalent norm  we denote by sdf and wf for uniformly regular surfaces 3 gm the euclidean metric in r ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2. Preliminaries", "Text": " uniformly regular manifold  the concept of uniformly regular manifolds was introduced by   amann in and we will now state some structural properties of uniformly regular manifolds which will be used in the analysis of the the surface diffusion flow and the willmore flow in subsequent section  simonett remark   it is geodesically complete, of positive injectivity radius and all covariant derivatives of the curvature tensor are bounde  in particular, every compact manifold without boundary is uniformly regular and the manifolds considered in are all uniformly regula  throughout the rest of this paper, we will adopt the following conventio  here, and in the following, it is understood that a partially defined and compactly supported tensor field is automatically extended over the whole base manifold by identifying it to be zero outside its original domai  in this subsection we follow amannsee also the spaces bcs and bcs are defined in a similar manne  we also refer to", "Subsections": [{"Section_Num": "2_3", "Section": "2.3. Continuous maximal regularity", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2_4", "Section": "2.4. Quasilinear equations with singular nonlinearity", "Text": " following the convention in andwe call the index j subcritical if is a strict inequality and critical in case equality holds in suppose satisfies -. ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3. URT\u2013hypersurfaces", "Text": " any smooth compact embedded hypersurface without boundary has a tubular neighborhood, see for instance for the reader's convenience, we include a proof of this equivalenc  proo  we only need to prove the injectivity of   every smooth compact hypersurface without boundary embedded in rm+1 is a -hypersurfac  all of the manifolds considered in are -hypersurface  proo  we refer to the proof of claim 1 in proposition  1 in the appendix for a more general situatio  an analogous argument shows that the ball bm+1 lies below the grap 1, this proves that gr has a tubular neighborhood of radius   then, is a -hypersurfac  but it is obvious that does not have a tubular neighborhoo  there also exist connected uniformly regular hypersurfaces that are not then is a uniformly regular hypersurface that is not one can take the product of c with rm to produce higher dimensional example  additionally, one can rotate the curve c around the x-axis to obtain a connected rotationally symmetric uniformly regular hypersurface which is not", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4. The surface diffusion flow", "Text": " we use the convention that a sphere has negative mean curvatur  we note that this convention is in agreement withbut differs from by remark   note that although was derived for compact hypersurfaces inthis expression still holds true for our problem as it is purely loca  in the scalar case, it is not diffcult to see that the notion of uniformly strongly elliptic is equivalent to the notion of uniformly normally elliptic introduced insee also this shows that a is uniformly strongly ellipti 1 and now imply the following resul  next, we will verify that the operator f satisfies by proposition   in every patchby the discussion inwe have the following estimat  it follows from proposition   this corresponds to =which is subcritica  we can estimate the remaining terms of by using propositions   the remaining terms,   combining the above discussions, we apply theorem  4 to produce the following well-posedness result for proo  we have already proved part abov  part follows directly from the argument in for partwe first note that lipschitz continuity of the semiflow follows from ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5. The Willmore flow", "Text": " by proposition   the following well-posedness result for then follows from theorem   sdf and wf for uniformly regular surfaces 17 proo  part follows from and part follows exactly as in the proof of theorem 4", "Subsections": [{"Section_Num": "5_1", "Section": "5.1. Stability of spheres", "Text": " proo  the result then follows from we note here that theorem  2 also holds true for the surface diffusion flow, as was shown in", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "Appendix", "Section": "Appendix A. ", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "References", "Section": "References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "mapping areas using computer vision algorithms and drones bashar alhafni saulo fernando guedes lays cavalcante ribeiro juhyun park jeongkyu lee university of bridgepor  bridgeport, ct, 0660 bridgepor edu, jelee@bridgepor edu abstract the goal of this paper is to implement a system, titled as drone map creator using computer vision technique  dmc can process visual information from an hd camera in a drone and automatically create a map by stitching together visual information captured by a dron  the proposed approach employs the speeded up robust features method to detect the key points for each image frame; then the corresponding points between the frames are identified by maximizing the determinant of a hessian matri  finally, two images are stitched together by using the identified point  our results show that despite some limitations from the external environment, we could have successfully stitched images together along video sequences", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "drones are getting popular in a wide range of research in recent year  many companies have developed their own drone technology such as google, facebook, and amazo  in addition, drones are used in drone journalism so they can obtain videos of areas that are hard to reac  a quadcopter is one kind of drone that has four rotor  two spin clockwise, and the other two spin counter-clockwis  they work in tandem to balance the dron  additionally, a drone can fly autonomously based on a pre-entered program without pilotin  since most of drones carry video cameras with considerably high resolution, the use of them for image processing and computer vision techniques has become a very interesting topi  applications using drones and cameras have been developed to solve many problems and make people's lives easie  one of challenges of such drone applications is that a drone cannot work properly when a gps signal or a pre-captured satellite map is not availabl  this research will address the limitation by proposing a drone map creator that utilizes computer vision techniques for video streams captured by a dron  in this paper, we will show how we get the images by remotely controlling the ar-drone to feed the image stitching algorithm, which will generate a big image by blending these images togethe ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Background", "Text": "the aim of our research project is to come up with an image processing algorithm that will let us cover the university of bridgeport parking lot, and this can be done by taking successive images and then combining these images into one large imag  to have images with overlapped areas so that they can be combined, we needed a stable device with a good quality camera, which will capture the images in a certain orde  that's why we chose the ar-drone   therefore, the front camera of the drone will provide us with good quality stabilized overlapped images which can be combined into one image using the algorithm that we develope  along with the ar-drone  0, we also used existed free software libraries that we found helpful to develop our algorith  the libraries are opencv and boofc  opencv is an open source computer vision and machine learning software librar  opencv also leans mostly towards real-time vision applications and it has many available interfaces according to all of these properties, we used the java interface of opencv, which is a primary component of our algorith  in addition to opencv and all the image processing features it has, boofcv has also demonstrated very high-level image processing capabilitie  boofcv is a java library for realtime computer vision and robotics application  it includes low-level image processing routines, feature tracking, and geometric computer visio  the library has also an example for an image stitching algorithm, which is the main goal of our research projec  image stitching refers to combining two or more overlapping images together into a single large imag  when stitching images together, the goal is to find a 2d geometric transform which minimizes the error in overlapping region  there are many ways to do thi  boofcv uses an example where point image features are found, associated, and then a 2d transform is found robustly using the associated feature  2 another way to achieve image stitching is by the scale invariant feature transform algorith  according to andrea vedaldi, a sift feature is a selected image region with an associated descripto  additionally, there's another image stitching technique on which our algorithm is based o  this technique is called speeded up robust features the surf and sift techniques are very similar but they have a few minor differences in some detail  the main concept of our image stitching algorithm is based on the opencv library as well as the surf algorithm, which gave us the results that we almost were aiming fo ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Objective and Approach", "Text": "our goal in this project is to use the drone's camera to take successive pictures from the university parking lot and blend those images together to build a new image, which will cover the entire parking lo  to do that, we implemented two approache  first, we used the boofcv library that has its own image stitching algorithm and feature  second, we developed our own algorithm based on the opencv library and the surf algorith  the images were taken during certain periods of time so that the drone captures overlapped image  this way, we can identify and adjust these areas to create one large imag  to perform our code in the field test, we divided our environment like the image belo  we had the parking lot divided in a 4x4 matrix so that we could organize and better control the drone's movemen  figure 1: drone's movement grid the first step in our approach is to take an image at our starting poin  then, the drone will move forward taking pictures to cover the first column of our matri  after completing the first column, the drone will move to realign with the next column, but now taking pictures in the opposite directio  we repeated these steps until the matrix was fully covere  each column is composed by four smaller images that were stitched together, then we stitched the columns together after receiving appropriate modifications, such as rotating the image  second, stitching images from side to sid ", "Subsections": [{"Section_Num": "3_1", "Section": "3.1 First algorithm: BoofCV", "Text": "the boofcv library has its own image stitching algorith  this algorithm, finds some image points features and then a 2d transform is found robustly using the associated feature  then associate features together, only with the common key points between the image  after that, apply a robust fitting to find a transform that will make the necessary changes in the images, such as rotation  the final step is to render the combined images to generate the final imag  after that, when we try to stitch this result with a third image, we will not get the desired resul  this happens because the black background will be considered as a part of the image, so the algorithm will try to find a correspondence between the background and some features of the third image and will not find the correct correlation between the image  even if we use some techniques to cut offthe background, the result will still present distortion  this will happen because of the translations that boofcv doe  these translations change the rectangular shape of the image, so there will always be some black backgroun  thus, the result of stitching more than two images using boofcv will be a very distorted imag  to avoid this problem, we decided to implement our own algorithm, using openc ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3_2", "Section": "3.2 Stitching algorithm using OpenCV", "Text": "our stitching algorithm was developed based on the opencv library and mostly prepared using the image processing technique sur  to perform the main feature of our algorithm, the key point detection using surf is our first and main ste  according to herbert bay, the surf algorithm is a local feature detector and descriptor that can be used for tasks such as object recognition or registration or classification or 3d reconstructio  it is partly inspired by the scale-invariant feature transform descripto  the surf algorithm is based on the same principles and steps as the sift, but details in each step are differen  surf uses a blob detector based on the hessian matrix to find points of interes  the determinant of the hessian matrix is used as a measure of local change around the point and points are chosen where this determinant is maxima  we always keep the last image for performance and level detail  the next step is simply to blend those two images together; the first in the bottom and the second in the to  the result of this rendering will be the first input to the algorithm followed by the pictures taken when the drone moves forward", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Results", "Text": " this distortion 6 makes it diffcult to bind more images because increasing the amount of images will increase the amount of distortions, which will decrease the quality the result  however, our algorithm will not present distortions, but it will have clear lines of transitions between the bound image  since some details of the images can get lost during this process, this will also affect the quality of the result  therefore, classifying which algorithm works better for this problem is a diffcult decisio ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Conclusion", "Text": "after running some examples, we decided that, the best approach for creating a bigger image from other small images is the algorithm developed by u  even though boofcv can fit two images with better accuracy, the drone mapping will have a large number of images to stitch, and boofcv's results would be impracticable because of the amount of distortion it present  thus, the algorithm developed by us is a good candidate for the proposed tas  at the end of this project, we could not reach the goal of mapping the parking lo  working with image stitching algorithms requires very high quality images, which unfortunately the ar-drone  0 does not provide to u  one good possibility for completing this task is using the new version of it, or even using some different drone, with preferentially a 1080p camer  another problem while dealing with the use of drones for this project is the low stability of the dron  because of external environment, the pictures taken by the drone were often without a common area, so the algorithm did not perform as expecte  thus, the best drone to be used in such project is one with not only a high quality camera, but also with a good stabilit  with the effcient resources, a parking lot mapping can be done, and it can be used to create a very reliable vigilance syste ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "References", "Section": "References", "Text": "", "Subsections": [], "Groundtruth": ""}]