[{"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190  these basic assumptions lead to a big rip universe; the physical quantities diverging during a finite rip time t  we then consider the mass accretion of a black hole in such a univers  we also discuss other options and include, for the sake of comparison, some essential properties of mass accretion in the early univers  pacs numbers: 0 -b, 9 sf, 98", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "I", "Section": "I Introduction", "Text": " among various candidates of dark energy, the phantom energy is the most esoteric one as it violates the weak and null energy condition  the recent astrophysical data suggests that w = - 04+  if phantom energy does exist, than its energy density will increase with time and scale factor will go to infinity in a finite time causing the event known as big rip which is a sort of future singularit  brevik et al proposed that the cosmic fluid will undergo a turbulent phase under extreme cosmic expansion there exists at present quite a big number of studies on this kind of future singularity, also with the inclusion of viscosity and even of turbulenc  some references to the big rip literature are given in another, and milder, variant of the future singularity is called the little ri  some references to the little rip can be found in the evolution of black holes in a dark energy dominated universe is an active direction of research in particular near the future rip singularity, it is expected that black holes will also go under ripping apart   evaporating much like hawking radiation culminating in a naked singularity this entails the violation of cosmic censorship conjecture itsel  we wish to investigate the rate of change of mass of the black hole by taking the effective approac  the turbulent phase, of course, is connected with viscosit  usually, in a cosmological context one limits oneself to the bulk viscosity only, because of the assumed spatial isotropy of the cosmic flui  this implies naturally the transition to a turbulent epoch, which we will assume to be isotropi  we will formulate simple assumptions for the turbulent phase belo  when considering the accretion rate dm/dt for a black hole in the phantom universe, it is in our opinion natural to put this rate proportional to the sum of the effective energy density and effective pressur  detailed calculations on this case are presented in section i  we also consider briefly other options, for the strength of the turbulent fluid component, and for the form of dm/d  for the sake of comparison, we make in section iii some comments also on black holes in the early 2 univers  also in that case it has been found that negative mass accretion can occur, although then due to the interaction with a scalar fiel ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "II", "Section": "II  Modeling", "Text": "before embarking on the case of turbulent fluids, let us make some comments on the use of laminar viscous fluid models in cosmolog  the introduction of a bulk viscosity was made a long time ago, a classic reference being the 1971 paper of weinberg if the fluid is in the phantom region, w is less than -  moderate turbulenc  assume, as in re  that de sitter-like solution is physically unreasonable, however, because it would imply the turbulent part of the energy to be negativ  we therefore focus on the solution h2 in the following, calling it   the form of such an equation is not evident beforehand, in view of the insufficient knowledge about the phantom univers  ref  we consider the mass accretion to be quasi-stati  we will now outline what are the consequences of these option    this assumption is of course simple, but it seems to conform with the same role played by energy density and pressure in general relativit  = 0 in standard notatio  these are characteristic properties for the big rip singularit  notice the contrast with the little rip singularity in which an infinite time is needed to reach infinity this behavior is influenced solely by the phantom fluid and is not related to any hawking radiatio  then, with h0 = 6 74 km s-1 mpc-1 =   strong turbulenc  instead of assuming the turbulent component in the fluid to be moderate, as represented by eq  that means physically that the turbulence is taken to be the overwhelming factor in the late phantom flui  the occurrence of the phantom-divide equation of state thus follows from the assumption about turbulence, and is not a condition imposed initially by han  instead of e  with ts still the rip time as given by e  the behavior of this expression is complicate  where does this accretion of energy come from? the only plausible explanation is that it is extracted from the phantom fluid itsel  however, the kind of behavior shown by e  is after all strange, and we conclude that the basic assumption is less physical than that of the previous subsectio ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "III", "Section": "III Comparison with the early universe", "Text": "for the sake of comparison, let us briefly review some characteristics of the theory of black hole evaporation in the early univers  this form is thus different from the basic form that we assumed above for the late univers  6 we may integrate e  it is moreover worthwhile to notice that the negative accretion of mass is found also in the early universe theor  thus rodrigues and saa considered the evolution of the mass of schwarzschild black holes in the presence of a nonminimally coupled scalar field, and found that any black hole with initial mass m0 will disappear due to the accretion of the scalar fiel  this was found to hold true even in the absence of hawking radiatio  in the case considered in re the disappearance of the black hole would require an infinite amount of tim ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "IV", "Section": "IV Summary", "Text": "the concept of viscosity has attracted considerable attention in modern cosmolog  it corresponds to a hubble parameter h that is given by e  above, as function of the redshif  the main objective of the present work was however to abandon the assumption about a homogeneous cosmic fluid and allow for an isotropic turbulence componen  we focused on the late universe, assuming that the equation of state parameter w is constant and less than -  this means a phantom flui  this is the same model as we introduced earlier, in re  it corresponds to a moderate degree of turbulenc  based upon these assumptions, we found by use of the friedmann equations that the scale factor a diverges at a finite rip time ts; c  e  this is characteristic for the big rip singularit  that means, we assumed the effective energy density and effective pressure to act on the same footin  the result, shown in section i  otherwise, for the effective energy density and pressure, the big rip characteristics were recovere  e  in section i  the result was quite a different evolution of the universe, being of de sitter type with a hubble parameter simply equal to   section i  this case was found to yield positive mass accretion, but the solution for m showed a strange behavior making us conclude that this option is of less physical interes  in the final section iii, we made some simple comparisons with the evolution of a black hole in the early univers  at the inflationary stage, m is typically found to increase with time until a stationary state is approache  however, also in the early universe, negative mass accretion has been found to occur planck collaboration, planck 2018 result  v  cosmological parameters, arxiv:180  nakamura et a  phy    odintsov, turbulence and little rip cosmology, phy  re  kamionkowski and   weinberg, phantom energy and cosmic doomsday, phy  re  let  nojiri and   odintsov, quantum de sitter cosmology and phantom matter, phy  let  nojiri and   re  hao and   li, phantom-like gcg and the constraints of its parameters via cosmological dynamics, phy  let  sola and   let    brevik and   gorbunova, dark energy and viscous cosmology, ge  relati  gravi    nojiri and   odintsov, on isotropic turbulence in the dark fluid universe, eu  phy    brevik,   rabochaya and   zerbini, turbulence accelerating cosmology from an inhomogeneous dark fluid, astrophy  space sc  wang and   meng, phantom dark energy as an effect of bulk viscosity, phy  re    frampton and   ludwick, seeking evolution of dark energy, eu  phy  ludwick and   scherrer, the little rip, phy  re    nojiri and   odintsov, viscous little rip cosmology, phy  re  nojiri and   re    osetrin and   timoshkin, little rip cosmological models with time-dependent equation of state, mo  phy  let  elizalde,   obukhov and   jamil, effect of vacuum energy on evolution of primordial black holes in einstein gravity, phy  let  astrophy  ast  jamil, accretion processes for general spherically symmetric compact objects, eu  phy  akbar, generalized second law of thermodynamics for a phantom energy accreting btz black hole, ge  re  gra  jamil,   qadir, primordial black holes in phantom cosmology, ge  re  gra  jamil,   qadir, black holes in bulk viscous cosmoloy, in  theo  phy    jamil, accretion on reissner-nordstrom--de sitter black hole with global monopole, clas  quantum gra  weinberg, entropy generation and the survival of proptogalaxies in an expanding universe, astrophy    saridakis, viscous cosmology for early- and late- time universe, in  mo  phy  normann,   normann,   brevik, characteristic properties of two different viscous cosmology models for the future universe, mo  phy  let  dokuchaev and y  eroshenko, black hole mass decreasing due to phantom energy accretion, phy  re  let  azreg-ainou,   ahmed and   jamil, spherical accretion by normal and phantom einstein-maxwell-dilaton black holes, clas  quantum gra  tsujikawa, properties of singularities in dark energy universe, phy  re      halnes, cosmic evolution and primordial black hole evaporation, phy  re  rodrigues,   saa, accretion of non-minimally coupled generalized chaplygin gas into black holes, phy  re ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "References", "Section": " References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190 edu ulukus@um  this generates an r-regular graph structure for the storage system where the vertices of the graph are the messages and the edges are the database  we then specialize the problem to storage systems described by two special types of graph structures: cyclic graphs and fully-connected graph  to that end, we propose novel achievable schemes for both graph structures that are capacity-achievin  the central insight in both schemes is to introduce dependency in the queries submitted to databases that do not contain the desired message, such that the requests can be compresse  in both cases, the results show severe degradation in pir capacity due to non-replicatio ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "private information retrievalintroduced inis a canonical problem to study the privacy of users as they download content from public database  1 non-colluding databases, in such a way that no database can know the identity of the user's desired fil  the pir problem has become a vibrant research topic within information theory starting with trailblazing papers insun and jafar introduce the pir capacity, which is the supremum of the ratio of the number of bits of desired information that can be retrieved privately to the total downloaded informatio  they characterize the pir capacity of the classical pir problem to be cpir = -  the fundamental limits of many interesting variants of the problem have been investigated in a common assumption in most of these works is that the entire message set is replicated across all database  this is crucial for constructing capacity-achieving schemes, as in many existing schemes the undesired symbols downloaded from one database are exploited as side information in the remaining databases, and replication is the key that enables downloading any bit from any database and using it as side information at any other databas  however, the replication assumption may not be practical in next-generation storage systems and network  from a storage point of view, message replication is impractical as it incurs high storage cost, especially for storage systems with a large number of messages or files with a large siz  from a network structure point of view, in next-generation networks where peer-to-peer connections will be prevalent, nodes may not necessarily possess the same set of message  these practical scenarios, which challenge the replication assumption, motivate investigating pir in non-replicated storage system  we aim at evaluating the loss in the pir rate due to non-replication and investigating the interplay between the storage structure and the resulting pir rat  a few works have considered relaxing the replication assumption:", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Problem Formulation", "Text": "consider the problem of pir from n non-replicated and non-colluding database  each message wk belong to fl q is a vector of length l picked in an    we note that for a feasible storage systemwe have kr = m  to fully characterize the storage system in this case, we represent the storage system as r-regular graph1; see fi  1 and table 1 for a exampl  this graph is an r-regular graph, since each message is repeated r times across the storage syste  in the following, we define specific parameters of the graph, which are needed while constructing the converse proo  1we note that the graph used in our formulation may be considered as the dual graph of the one used in d1 d2 d3 d4 d5 d6 d7 d8 d9 w1 w1 w1 w2 w2 w3 w3 w4 w5 w2 w4 w6 w3 w5 w4 w6 w5 w6 table 1: contents of databases for the example system specified by graph in fi  an example graph reduction for the storage system given in fi  1 and table 1 is shown in fi  neighbors of w1 are specified and the connected databases are enumerate  all neighboring vertices except w2 are removed, and so o  if every two vertices are connected by a unique edg  in pir, the user wants to retrieve a message wk without leaking any information about the identity of the message to any individual databas  first, we have the privacy requiremen  6 the second requirement is the reliability requiremen  the user needs to be able to reconstruct wk perfectly2 from the collected answers,   an achievable retrieval scheme is a scheme that satisfiesfor some message length  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Main Results", "Text": "in this section, we present the main results of this pape  the proof of theorem 1 is given in section    e, the upper bound cannot be parameterized by onl  this opens the door for joint optimization of the storage system together with the retrieval schem  2the results of this work do not change if we relaxed the reliability constraint to allow arbitrarily small probability of error,   in the following two results, we characterize the pir capacity of cyclic graphs and fullyconnected graph  meanwhile, the number of side information equations generated is limited due to non-replicatio  this implies that the retrieval rates in non-replicated pir systems in may be improve  nevertheless, the results in are more general which are valid for all graph-based storage system  the results in also cover collusion resistance, which is outside the scope of our work her ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Converse Proof", "Text": "in this section, we prove theorem   this assumption is without loss of generality, since any asymmetric retrieval scheme can be transformed into a symmetric one by means of time-sharing without changing the retrieval rat  we need the following lemm  follows from the fact that answer strings are deterministic functions of the messages and queries, and follows from the database symmetry in rearranging concludes the proo  in order to obtain the spread of the graph, we begin by the node representing w1, then we enumerate all the edges connecting to w ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Achievability Proof", "Text": "in this section, we begin first with a motivating example of = to show the basic ingredients of the achievable schem  example is both cyclic and fully-connectedtherefore, this motivating example can be considered as a unifying instance of the optimal scheme for both cyclic and fullyconnected graph  then, we present general capacity-achieving schemes for cyclic graphs and fully-connected graph  this is a cyclic and also a fully-connected graph as shown in fi  without loss of generality, assume that the desired message is w  denote the permuted version of w1 by the vectorthe permuted version of w2 byand the permuted version of w3 by a straightforward solution for this problem is to apply sun and jafar scheme in the query table for this scheme is shown in table   note that although the sum b3 + c3 is irrelevant to the decodability of w1, the user 12 needs to download it to satisfy the privacy constrain  although this scheme outperforms the scheme in in terms of the retrieval ratethere is room for improving i  moreover, the user downloads new independent bit b3 + c  therefore, a3, a4 are decodable by canceling b2 and c  to remedy this problem, the user should repeat the compression of the downloads over all databases,   in previous pir works, the user downloads new and independent undesired symbols at each round, which can be used in later rounds as side informatio  however, in this work, the user downloads undesired symbols which are dependent on the undesired symbols downloaded from other database  we download these dependent symbols even from the databases that do not contain the desired messag  we call these dependent downloads to differentiate them from side information downloads, which are intended to be used to decode the desired message directl  furthermore, by compression, we mean downloading shorter answer strings than the greedy algorithm in by exploiting the knowledge of the dependent symbol  a8 + c  the complete query structure is given in table   next, we discuss privacy, decodability and the rate of this achievable schem  hence, all queries are equally likely, and the scheme is privat  regarding decodability: we note that each repetition is decodable separatel  regarding the achievable rate: the user downloads 12 bits from w1 out of a total of 24 download  this implies a loss in the pir capacity due to storing full messages here as opposed to storing uncoded parts of the messages in subject to the same memory-size constraint", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "6", "Section": "6 Conclusion", "Text": "in this paper, we investigated the pir problem from non-replicated and non-colluding database  this system is uniquely described by an r-regular grap  we proved a general upper bound, which depends on the spread of the grap  we derived the capacity of two classes of graphs, namely: cyclic graphs and fully-connected graph  for these two classes of graphs, we proposed novel achievable schemes, whose retrieval rate matches the developed upper boun  our results showed that non-replication significantly hurts the retrieval rat ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": " kochanek1,2, todd   received yyy; in original form zzz abstract using asas-sn data, we find that the bright variable star macho 8 1718 is the most extreme heartbeat star yet discovere 00846 d, and is located towards the lh58 ob complex in the lm  we also present a frequency analysis to identify the pulsation frequencies corresponding to the tidally excited oscillations", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "heartbeat stars are short periodeccentric binaries where oscillations are excited by the tidal forcing at each periastron passag  heartbeat stars were first discovered in data from the kepler space telescope the light curves of heartbeat stars are defined by oscillations outside of periastron combined with a brief, high amplitude ellipsoidal variation at periastron that gives rise to a unique heartbeat signature resembling the normal sinus rhythm of an electrocardiogra  the light curves of these systems are dominated by the effects of tidal distortion, reflection and doppler beaming close to periastron heartbeat stars continue to oscillate throughout their orbit due to tidally excited stellar oscillation  the variability amplitude of most heartbeat stars is very smalledu multiples of the orbital frequency teos were first discovered in the eccentric binary system hd 174884 and later confirmed in koi 54 and several other systems the largest amplitude teos are driven by resonances between harmonics of the orbital frequency and the normal mode frequencies of the sta  both the amplitudes and phases of teos can be predicted from linear theory the vast majority of the heartbeat stars that have been discovered are relatively low-mass a and f type star  however, the heartbeat phenomenon extends to more massive ob stars as wel  the dearth of massive heartbeat stars is likely an observational bias because massive stars are rare and kepler observed only a small fraction of the sky, mostly lying offthe galactic plan  ground-based surveys cover most or all of the sky but find it challenging to detect the low variability amplitudes of typical heartbeat stars jayasinghe et a  we have written a series of papers studying variable stars using asas-sn dat  in paper iiiwe conducted a variability search towards the southern ecliptic pole in order to overlap with the the transiting exoplanet survey satellite continuous viewing zone tess is currently conducting science operations by monitoring most of the sky with a baseline of at least 27 day  sources closer to the tess cvz will be observed for a substantially longer period, approaching one year at the ecliptic pole  here we discuss the identification of the most extreme amplitude heartbeat star yet discovered, macho 8 1718using both asas-sn and tess photometr  the macho survey reported that the source was a variable, but classified it as an eclipsing binary1718 was first identified as a likely heartbeat star during our asas-sn variability searc  we discuss archival data and the asas-sn and tess observations in section   in section 3, we fit an analytical model for the tidal distortions to estimate several orbital parameters of the binary syste  in section 4, we discuss our sed fits to this source and the physical implications of these fit  2 archival, asas-sn and tess data for macho 8 1718 the source macho 8 1718 was identified as a variable by the macho surveywho classified it as a generic eclipsing binar 11 mag with estimated values for the photometric temperature log =   this source is part of the lh58 ob association in the large magellanic cloudnorthwest of 30 doradu  an archival spectrum classified it as a b 5 ib/iievolved blue sta  the gaia dr2 counterpart is source_id=465848906733287155 05 mas yr-  on the other hand, the renormalized unit weight error of the source is  96 and lindegren et a  argue that gaia dr2 astrometric solutions are accurate if they have a ruwe <   these 56 stars had medians of - 670 mas yr-1 and  697 mas yr-1 for their parallax and proper motion  hence the parallax and proper motions of macho 8 1718 are typical of the local population of luminous star  if we use the median proper motions to define a local standard of rest, the relative motion of macho 8  the median motion of the nearby stars relative to this standard of rest is  215 mas yr-1 or roughly 50 km s-  asas-sn v-band observations were made by the cassius quadruple telescope between 2013 and 201  each camera has a field of view of   the light curves for this source was extracted as described in kochanek et a  using aperture photometry with a 2 pixel radius apertur  the aavso photometric all-sky survey dr9 catalog was used for absolute photometric calibratio  we derived possible periods for this source following the procedure described in jayasinghe et a  the astrobase implementation of the generalized lomb-scarglethe multi-harmonic analysis of varianceand the box least squares periodograms were used to search for periodicity in these light curve  due to the large pixel size of tess and the crowded region surrounding macho 8 171, we used image subtraction on the full frame images from the first tess data release to produce high fidelity light curve  the 27 day baseline for tess observations in each sector is insufficient to obtain a full orbit for this source, but the final tess light curve with data from all the sectors in the south should sample the variability of this source very wel  the ephemeris for the minimum of the ellipsoidal variation in asas-sn is ephemi = bjd 245814  jayasinghe et a 3 amplitude known hb stars macho 8  the period-amplitude diagram for the sample of heartbeat star  heartbeat stars listed in the vsx database are indicated by blue diamond 1718 is shown with a red sta  where the epoch e is the number of orbits since the time of minimu  the phased asas-sn and tess sector 1+2 light curves are shown in figure   the uncertainties in relative flux are virtually indistinguishable from the points in the tess light curve  the teos are clearly visible in the tess light curvesbut are less distinguishable in the asas-sn light curve this is illustrated in figure 2, where we compare the period and amplitude of macho 8 1718 to those of the heartbeat stars in the vsx catalog the flux variations outside of periastron are also extreme fuller notes that very large amplitude teos are unlikely to arise from a chance resonance and are more likely to stem from a resonantly locked mod  3 modelling the eccentric ellipsoidal variations kumar et a  developed an analytical model for the flux variations produced by the tidal distortions produced by eccentric binaries at periastro  thompson et a  successfully applied this model to fit the light curves of the heartbeat stars observed by keple  best fit parameters for macho 8 002 transcendental equation we performed a trial fit through the levenberg-marquardt chi-square minimization routine in scikit-learn the parameters from the trial fit were then used to initialize a monte carlo markov chain sampler with 100 walkers, and was then run for 5000 iteration  we used the mcmc implementation through emcee the errors in the parameters were derived from the mcmc chain  these fits do not capture the depth of the minimum or the height of the maximum completely, suggesting that this model is an incomplete description of the light curv  while this model does not account for effects such as irradiation and doppler boosting, it is a good approximation of the tidal distortions during the orbit and can be used to estimate the orbital parameters of this system without requiring further knowledge about the properties of the stars in the syste  the best-fit models for the asas-sn and combined sector 1+2 tess data for macho 8 1718 are shown in the top panel of figure 3 with the solid red line  the best fit parameters are summarized in table   we attempted fitting a standard eclipsing binary model including irradiation and reflection effects to the asas-sn v-band light curve using phoebe  1 but were unable to replicate the observed variability amplitud  this is not very surprising because the high amplitude pulses are almost certainly due to transient dynamical tides that are not included in normal eclipsing binary modeling code  unfortunately, there are no public codes for modeling heartbeat stars which include these effect  we have put offattempts at more detailed models until we have the radial velocity data needed to better constrain the orbit and masse  we will also explore a simultaneous fit to the light curve incorporating both the binary star features and the tidally induced pulsations with phoebe  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Archival, ASAS-SN and TESS data for MACHO 80.7443.1718", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Modelling the eccentric ellipsoidal variations", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 SED fitting", "Text": "we fit the spectral energy distribution of macho 8 1 dust and used castelli & kurucz model atmospheres for the sta 2 relative flux asas-sn   the phased asas-sn and tess light curves and residuals for the source macho 8 1718 after fitting with the kumar et a  model for eccentric tidal distortion  uncertainties in flux are not shown for clarit  the best-fit models are shown in red", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Tidally Excited Oscillations", "Text": "with an approximate model for the tidal distortions, we can subtract the effect of the impulsive forcing and search for tidally excited oscillations teos occur at integer multiples of the orbital frequency, thus we carefully consider the orbital harmonics in the fft spectru  we calculated the fast fourier transform of the residuals using the period04 software package and kept only harmonics with signal-to-noise ratios > 2 for further stud  the frequencies were optimized to reduce the light curve residual  we also repeated this analysis for the tess residual  the combined tess light curve shows significant variations outside of periastron with good snr when compared to the asas-sn light curv  in order to reduce the impact of the tidal distortions on this calculation for both the asas-sn and tess data, we only select the epochs with phases in the range due to the time-sampling properties and the limited baseline of the tess data, the fwhm of the peaks in the fft power spectrum differ fft spectrum of the residuals after subtracting the best-fit tidal distortion model from the asas-sn v-band data and tess data figure 5 illustrates the fft power spectrum for the residuals after fitting the best-fit tidal distortion model to the asas-sn and tess dat  the significant orbital harmonics are highlighted in re  these are likely caused by aliasing and are absent in the tess spectru  we calculated the uncertainty in the frequencies, amplitudes and phases, using the monte carlo simulation in period0  the final multi-sector tess light curve will provide a significantly better characterization of the teo ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "6", "Section": "6 Conclusions", "Text": "we discovered that the variable star macho 8 1718 is actually the highest amplitude heartbeat star discovered to date rather than an eclipsing binar  using both the asas-sn and the tess light curves, we find that macho 8  a more complete model of the system incorporating radial velocity information should improve our constraints of the orbital parameter  pulsation frequencies for macho 8 1718, phased to periastro  the errors in frequency, amplitude and phase are calculated through a monte carlo analysi  the identification of this source in asas-sn and its further characterization using data from the tess satellite highlights the excellent synergy between these two project  asas-sn is a long baseline survey and provides all-sky light curves that are well suited to study long term variability, whereas tess light curves are more precise and sampled at a more rapid cadence even though they have a shorter baseline than asas-s  the combination of data from these two surveys will advance the study of variability across the whole sk  for a more complete characterization of this fascinating system, a radial-velocity follow-up campaign is necessar  these massive heartbeat stars should advance our understanding of the intricacies of stellar evolution and mergers in binary star system  furthermore, the tidally induced pulsations in these massive heartbeat systems also probe stellar structure and test theories of dynamical tide  acknowledgements we thank the anonymous referee for their useful comment  we thank the las cumbres observatory and its stafffor its continuing support of the asas-sn projec  we thank jim fuller for useful comment  asas-sn is supported by the gordon and betty moore foundation through grant gbmf5490 to the ohio state university and nsf grant ast-151592  development of asas-sn has been supported by nsf grant ast-0908816, the m  this work is supported in part by scialog scholar grant 24216 from the research corporatio  csk is supported by nsf grants ast-1515876, ast-1515927 and ast18144  tat acknowledges support from a simons foundation fellowship and from an ibm einstein fellowship from the institute for advanced study, princeto  this paper includes data collected by the tess mission, which are publicly available from the mikulski archive for space telescopes funding for the tess mission is provided by nasa's science mission directorat  this work has made use of data from the european space agency mission gaiaprocessed by the gaia data processing and analysis consortium this research was made possible through the use of the aavso photometric all-sky surveyfunded by the robert martin ayers sciences fun ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190 00006v2 17 apr 2019 hydrodynamics of fermi arcs: bulk flow and surface collective modes     miransky,3     shovkovy,4,5 and   by using the kinetic theory, the fermi arc hydrodynamics is derived and the corresponding effects on the bulk flow and surface collective modes are studie  for the bulk flow, the key effect of the proposed fermi arc hydrodynamics is the modification of the corresponding boundary condition  as to the spectrum of the surface collective modes, in agreement with earlier studies, it is found that the fermi arcs allow for an additional gapless spectrum branch and a strong anisotropy of the surface plasmon dispersion relations in momentum spac  the gapped modes are characterized by closed elliptic contours of constant frequency in momentum spac ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "I", "Section": "I Introduction", "Text": "weyl semimetals are materials with a relativisticlike energy spectrum in the vicinity of isolated weyl nodes in the brillouin zon  the nodes have nonzero topological charges with the monopolelike berry curvature and always occur in pairs of opposite chirality the nontrivial topology and the relativisticlike nature of quasiparticles also affect the transport properties of weyl semimetals,  g, leading to a negative longitudinal magnetoresistivity that was first predicted in re  the nontrivial bulk topology of weyl semimetals is also reflected in unusual surface states known as the fermi arcs unlike surface states in ordinary materials, the fermi arcs form open segments in momentum space that connect weyl nodes of opposite chirality the surface states in weyl semimetals were first observed via the angle-resolved photoemision spectroscopy and reconfirmed later by the observation of the quasiparticle interference patterns it is important to note that the energy dispersion of the fermi arc states is effectively one dimensional and linear this may suggest that their transport properties are similar to that of the one-dimensional chiral fermions and should be nondissipativ  however, as we showed in re  the dissolution of fermi arcs in the presence of strong disorder was also confirmed numerically in ref  electronic collective excitations provide additional powerful probes of the nontrivial properties of weyl semimetal  the effect of the fermi arcs on the surface plasmons was studied in ref  the authors of re  employed a simple phenomenological model valid in the long-wavelength limi  the hybridization of the fermi arc states and conventional surface plasmons is controlled via the anomalous hall conductivity and a phenomenologically included drude weigh  further, the surface plasmon excitation spectrum in weyl semimetal within the random-phase approximation was determined in re  the treatment of the surface plasmons in re  is more sophisticated and is based on the direct quantum-mechanical calculation  despite the difference in their approaches, studies in ref  predict the open hyperbolic constant-frequency contours for the surface plasmon  the nontrivial patterns of the surface plasmons can be measured by the scattering-type near-field optical spectroscopy as well as the momentum-resolved electron energy-loss spectroscopy experimentally, the electron energy loss in weyl semimetals was recently studied in re  since weyl semimetals are typically characterized by low impurity scattering ratesone might suggest that a hydrodynamic regime of electron transport could be eventually realized in many such material  originally, the possibility of such a regime for charge carriers in suffciently clean solids was discussed in the pioneering papers by gurzhi electron hydrodynamics requires that the electron-electron scattering rate dominates over the electron-impurity and electron-phonon one  the latter includes several types of chern-simons contributions in the electric current and charge densities that affect not only the electron transport in weyl semimetalsbut also their various collective excitations a low sensitivity of the surface fermi arc states to disorder makes them promising candidates to sustain a surface electron fluid in weyl semimetal  if this is indeed the case, the fermi arcs could realize not only the fermi level plumbingbut act as true aqueducts for the surface electron flui  because of the inevitable surface-bulk transitions and interactions, of course, such a surface electron fluid should be necessarily coupled to the bulk on  in this study, we derive the hydrodynamic equations for the surface fermi arc states from the kinetic theory and phenomenologically describe the corresponding surface-bulk couplin  our principal finding is that the fermi arc electron liquid modifies the boundary conditions for the bulk on  depending on the coupling parameters, the bulk flow in a slab of finite thickness could be noticeably altered and even enhanced near the surfac  in addition, we study the surface collective modes in the hydrodynamic approximatio  in agreement with the earlier studiesthe presence of the fermi arcs is manifested in a strongly anisotropic dispersion relation of the surface plasmon  additionally, we find that while the constant-frequency contours of the surface modes are given only by the elongated ellipses, the open hyperbolic contours correspond to the bulk modes hybridized with the surface excitations similarly to the usual semi-infinite plasma finally, there is also a gapless surface mode, which is related exclusively to the fermi arc  while such a mode resembles a conventional surface acoustic plasmonwhich also has a linear dispersion relation, the fermi arc mode is different and its frequency is determined by the surface dispersion relatio  these qualitative effects can be potentially used to experimentally verify the realization of the electron hydrodynamics in weyl semimetal  our paper is organized as follow  in se  ii, we introduce the phenomenological hydrodynamic model of the fermi arcs and discuss the coupling of the surface and bulk electron fluid  the explicit realization of the coupling is given and the effects of the surface states on the hydrodynamic flow are studied in se  ii  section iv is devoted to the investigation of the surface collective modes in the hydrodynamic approximatio  our results are discussed and summarized in se ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "II", "Section": "II Model", "Text": "in order to derive the hydrodynamic equations for the fermi arc quasiparticles, we start from the kinetic theor  as usualthe euler equation is obtained by calculating the appropriate moments of the kinetic equatio  in the hydrodynamic regime, the distribution function describes local equilibrium,   further, we assume that the weyl semimetal has a broken tr symmetry and contains two weyl nodes separated by 2b along the z direction in momentum spac  in other words, there is no hydrodynamic flow due to the fermi arcs in the z directio  we note that this is qualitatively different from the setup in re where a diffusive surface transport along the z direction was allowe  the derivation of the hydrodynamic equation for the fermi arc electron fluid is given in appendix   the enthalpy and the fermion-number density of the fermi arc states in equilibrium are derived in appendix a   now, let us briefly discuss the bulk hydrodynamic  compared to the conventional navier-stokes equatione  contains a few additional contribution  it comes from the electron scattering on phonons and impurities and, as is clear from its explicit dependence on the fluid velocity, breaks the galilean invarianc  as for the term on the right-hand side of e it describes the transfer of momentum between the surface and bulk fluid  4 this inclusion of the source term in e  implies that the boundary conditions for the electron fluid should be modifie  additionally, the normal components of the electron fluid velocity should vanish on both surfaces,   in order to illustrate the nontrivial effects of the fermi arcs, we will also consider the benchmark case, where the chiral shift is absent or directed normal to the surfaces of the sla  in such a simplified setup, there are no fermi arc surface states and the bcs for the bulk fluid velocity take the standard no-slip form,   or as we argue below, this benchmark case is useful to identify the effects of the fermi arcs on the hydrodynamic flow without making any a priori assumptions about the state of the surfac  it is worth noting that the navier-stokes equation should be amended by the energy conservation relation as well as the electric current continuity relatio  while the latter has a profound effect on both the charge transport and collective excitations, the former is important only for the thermoelectric effects and can usually be neglected in electron transport in general, however, the effect of the energy conservation on the electron hydrodynamics may become important when the fluid velocity is not small compared to the speed of sound vsd here q s describes the electric charge transfer between the bulk and surface states of the semimeta  the explicit expressions for the fermi arc charge and current densities are derived in appendix a   the last term in e  by integrating e  before concluding this section, it is instructive to sum up the general features of the surface and bulk flows, as well as to reiterate the critical role that the bcs play in their interpla  the bulk equation should be also supplemented by the appropriate bcs for the normal component of the fluid velocity, see e as well as the bcs for the tangential components, see eq  or, in the absence of the fermi arcs, eq  and in addition, as we will see below, the study of the longitudinal flow requires specifying either the fermi arc fluid velocity at some contacts or an explicit form of the transfer term  in our study, we use the latter option that allows for a self-consistent determination of the surface fluid velocity as well as the bulk flo  as for the collective excitations, the use of the continuity relations and will be needed in order to determine the evolution of the electric charg ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "III", "Section": "III Hydrodynamic flow", "Text": " we assume that the slab is suffciently thick so that the interaction between the fermi arcs on the opposite surfaces y+ = 0 and y-= ly is negligible and the arcs could be considered as independen  at the same time, the thickness should be small enough in order for the surface flow to have a noticeable effect on the net hydrodynamic flo  without loss of generality, we also assume that a uniform background electric field is applied in the x directio  note that this is the same setup that we used in re  since the bulk electron fluid couples to the surface states, the transfer term on the right-hand side in e  plays an important role in the hydrodynamic flo  these terms are derived by using the relaxation time approximation in appendix a 3 for the surface to bulk transfer term, the relaxation time approximation might be indeed physical because the dissipation of the fermi arc states is primarily due to the surface to bulk scatterings in the hydrodynamic picture, this corresponds to the outflow into the bul  as for the bulk to surface transfer term, it is estimated in a similar wa  in general, the transfer terms in e  can be viewed as the leading terms in the gradient expansion about the global equilibrium stat  taking into account that the right-hand sides of the bulk navier-stokes and continuity equations are nonzero only at the surfaces of the slab, it is reasonable to take them into account only via the bc  in particular, we will use e  for the velocity on the surface and e  for the normal component of the curren  here, as in re  then, the general solution to e  in particular, e  in the opposite limit,   in this case, there is a strong coupling between the surface and bulk fluids leading to the vanishing fermi arc fluid velocit  in addition, the boundary conditions for the bulk fluid are modified significantly and are affected by the electric field and the chiral shif  in a general case, the expression for the fluid velocity in the bulk of a weyl semimetal can be obtained in an analytical form by using the general solution in e  and the bc in e  the corresponding expression is, however, rather cumbersome and not very informativ  in order to better clarify the role of the surface states, we start with the benchmark case without the fermi arcs on the surfaces of the sla  such a situation is realized naturally when the chiral shift is absent or when its direction is perpendicular to the surface  andrespectivel  the corresponding profile of the longitudinal bulk flow velocity as a function of the y coordinate is shown in the left panel of fi  additionally, in the right panel of fi  now, let us discuss the case of hydrodynamic flow with fermi arcs on the surface  this is in a drastic contrast to the case of the conventional no-slip or free-surface bcs presented in fi  the increase of both fluid velocity and integrated fluid flow velocity is caused by the fermi arc fluid that tends to push the bulk one near the surface  as expected, the net enhancement of the flow is noticeable only for suffciently small widths of the sla  it should be noted that, as expected on the basis of e  red solid and blue dashed lines correspond to the standard no-slip and free-surface bcs, respectivel  to obtain the numerical results, we used the bc in e ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "IV", "Section": "IV Surface collective modes", "Text": "in this section, we study the effect of the fermi arcs on the collective modes in a semi-infinite weyl semimetal in the hydrodynamic regim  in particular, we focus on the surface plasmon  we assume that the weyl semimetal is located in the upper half-space and the vacuum in the lower half therefore, in the notation of se  henceforth, for simplicity, we omit the corresponding subscript in the fermi arc variable  previously, the fermi arc plasmons have been already studied in ref  while re  employs a simple phenomenological model, the authors of provide a more rigorous quantum-mechanical non-local approac  in this study, by following the approach similar to that used in ref we employ the hydrodynamic approximation without the retardation effects in maxwell's equation  following the arguments in ref  similarly to the fluid flow, the bcs are also important for the surface collective mode  for the oscillating fluid velocity, we impose the same bcs as for the flow in eq  and  as is clear from the above equations, the transfer term i in e  should be set to zer  in particular, an explicit form of the transfer term should be defined in order to solve the hydrodynamic equations for the longitudinal flow in se  ii  according to re the bulk states themselves should not induce a localized surface charge densit  and  another boundary condition can be derived from the y component of e  equations - are suffcient to reexpress all integration constants in eq , and in terms of a single constant that is then fixed by a normalization conditio  also, after satisfying all the boundary conditions, one can determine the dispersion relations for the surface mode  as for the dispersion relations for the hybridized surface-bulk modes, they are obtained from e  it is also instructive to start from the benchmark case without the fermi arcs on the surface of a weyl semimeta  then, after satisfying all bcs, e  qualitatively agrees with the 10 results obtained in ref  as we see, the spectrum of the surface plasmons is isotropic and has a nonzero ga  moreover, the value of the gap agrees with that obtained in re  now let us analyze the effect of the fermi arcs on the surface collective mode  because of a nonzero surface charge density in e  in addition, as in usual metalsthe hydrodynamic approximation is applicable even for the phase velocities of order vf by making use of the approximate dispersion relation in e  in general, however, the landau damping could provide an additional dissipation mechanism and should be included in a more rigorous treatment beyond the hydrodynamic approximatio  their frequencies are similar to those in e  of re  it can be also verified that, in agreement with the analysis in re the dispersion relations in e  then, by taking into account that the characteristic root defined in e  in contrast to the surface plasmons, which exist even in the absence of the fermi arcs, the mode with the dispersion relation in e  originates exclusively from the surface state  it is somewhat reminiscent of the usual surface acoustic plasmon with a linear dispersion relatio  however, the new mode stemming from the fermi arcs has a rather unconventional directional dependenc  the frequencies of the surface plasmons and the fermi arc mode are presented in fi  3, where the solid and dashed lines correspond to the numerical solutions of e  and the approximate dispersion relations in eq  andrespectivel  black dots indicate the frequencies at which the surface modes hybridize with the bulk one  in agreement with the previous analysis in re we found three roots of the characteristic equation in addition, as one can see from fi  3, the leading order approximate expressions and describe the collective modes rather well only when the wave vector is suffciently smal  in order to clarify the dependence of the surface mode frequencies on the wave vectors, we present the contour plots in momentum space for the positive frequency plasmon and the fermi arc surface mode in fi  as one can see from the left panel in fi  the contours of the fermi arc mode are bell-shaped with the maximum at kz =   by noting that the group velocities of the surface collective modes are given by the derivatives of their frequencies with respect to momenta, they can be represented by the vectors normal to the contour line  then, as is clear from fi  it should be emphasized that the constant-frequency contours for the plasmon modes obtained in this study are closed ellipse  this may appear to be qualitatively different from the open hyperbolic contours in ref  3: the solutions for the surface collective modes in the presence of the fermi arc  solid and dashed lines correspond to the exact solutions of the characteristic equation and the approximate ones in eq  andrespectivel  the red lines correspond to the gapped surface plasmons and the blue lines describe the fermi arc surface mod  black dots indicate the frequencies at which the surface modes hybridize with the bulk one  fi  modes with large frequencie  in addition, the open contours at large enough negative kx in fi  2 of re  could, presumably, stem from the hybridization of the gapped and gapless mode  in general, we identify two qualitative features that could be used to analyze the effects of the fermi arcs on the surface collective modes in weyl semimetal  first of all, unlike the conventional surface plasmons with the frequency given in e the fermi arc surface plasmons are described by a strongly anisotropic dispersion relatio  therefore, if experimentally observed, the latter mode can be used to extract the information about the separation between the weyl nodes as well as the dispersion relation of the fermi arc  experimentally, the anisotropy induced by the surface states can be probed using the near-field optical spectroscopyas well as the momentum-resolved electron energy loss spectroscopy because of a possible interference between the surface modes from different pairs of weyl nodesthe most suitable materials are the weyl semimetals with a single pair of node  therefore, the magnetic heusler compounds with a broken tr symmetry might be promising candidates for the study of the surface collective modes", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "V", "Section": "V Summary", "Text": "in this paper, we proposed that the weyl semimetals with a broken tr symmetry may possess a hydrodynamic regime with a nontrivial interplay between the bulk electron fluid and the fluid formed by the surface fermi arc quasiparticle  the hydrodynamic equations for the latter are derived from the kinetic theory under the assumptions that the electron-electron scattering rate dominates over the electron-impurity and electron-phonon one  further, we considered only the case where the hydrodynamic regime is achieved for both surface and bulk quasiparticles of the semimeta  in principle, however, the regime where the electron fluid is formed only on the surface but not in the bulk could be also realize  such a scenario is likely to lead to unique features and deserves a separate stud  in the proposed two-fluid framework, we studied the role of the fermi arc fluid on the bulk flow and on the spectrum of surface collective mode  for simplicity, we assumed that the surface fluid is inviscid and couples to the bulk via the phenomenological inflow and outflow term  the latter describe the fermi arc dissipation into the bulk and the transitions from the bulk to the surface state  we found that the fermi arcs modify the boundary conditions for the bulk electron flui  depending on the rate of the surface-bulk transitions as well as the value of the chiral shift, the bulk fluid velocity could change significantly near the boundarie  when the electrons are transferred to the surface at a greater rate than to the bulk, the bulk fluid could be dragged by the surface on  such a regime, however, is characterized by large surface flow velocities at which the hydrodynamic description may become ill define  on the other hand, an unconventional increase of the bulk fluid flow near the boundaries is seen when the surface to bulk transitions dominat  such a manifestation of the fermi arc flow could be, in principle, observed via the decrease of the resistivity in samples of small widt  in this study, we also demonstrated that the fermi arcs profoundly affect the surface collective modes in the hydrodynamic regim  in particular, we found that the dispersion relations of the surface plasmons become anisotropic in momentum spac  this is in contrast to the conventional surface plasmons with the isotropic dispersio  the origin of the anisotropy is the dispersion relation of the surface fermi arc quasiparticle  in general, we identified two types of surface mode  while one of them is a gapped surface plasmon hybridized with the fermi arc oscillations, the other is a gapless mode triggered exclusively by the surface state  similarly to the usual surface acoustic plasmonsthe gapless fermi arc mode has a linear dispersion relation, but it is sensitive to the sign of the wave-vector component along the direction of the fermi arc velocit  while our results agree qualitatively with those in ref we argue that the only true surface plasmon modes are those with the closed elliptic contours of constant frequenc  in passing, let us discuss a few limitations of this stud  the hydrodynamic model proposed in this paper is phenomenological and the underlying reasons for the fluid formation have not been rigorously addresse  in addition, the reliable estimate of the hydrodynamic window,   in our analysis, we used a simplified model for the fermi arcs without any curvatur  while we believe that the results will remain qualitatively the same for slightly curved arcs, the precise role of a nonzero curvature should be addressed in the futur  in our study of the surface collective modes, we also neglected the viscosity and dissipation effects, which could be rigorously taken into account via nonlocal corrections as in re  such an investigation is beyond the scope of this study, howeve  acknowledgments the work of   was partially supported by the program of fundamental research of the physics and astronomy division of the national academy of sciences of ukrain  the work of    was supported by the natural sciences and engineering research council of canad  the work of    was supported by the   national science foundation under grant phy-171395  is grateful for the hospitality of nordita during the program quantum anomalies and chiral magnetic phenomena, where a part of the study was done, as well as appreciates the discussion with pro  appendix a: derivation of the fermi arc hydrodynamics in this appendix, we present the technical details of derivation of the hydrodynamic equation for the fermi arc surface state  the semimetal is finite along the y direction and infinite in the other tw  kinetic theory we follow the standard approach of deriving the hydrodynamic equation from the kinetic theor  the euler equation for the fermi arc fluid in order to derive the euler equation, we multiply e  by the x component of the momentum px and integrate over p  the integration of the first term in e  and use the formulas in appendix   the overall coeffcient f is defined by the integration over the length of the fermi arc,   the integral with the term containing the electric field in e  can be calculated in a similar wa  transfer term here, we present the derivation of the transfer term on the right-hand side of the euler equation in general, it may contain two different parts: one describing the surface to bulk transitions and the other describing the inflow from the bul  the term describing the bulk to surface transitions, on the other hand, can be calculated by using the method in the supplemental material of re  then, the final expression for the transfer term i takes the form as in e  in the main tex  electric charge and current densities of fermi arcs for completeness, we present the explicit expressions for the electric charge and current densities for fermi arc quasiparticle  the corresponding expressions can be obtained by using the kinetic theory,   yan and   felser, ann  re  conden  matter phy -  xu,  -  huang, ann  re  conden  mattter phy  mele, and   vishwanath, re  mo  phy  berry, pro  so  nielsen and   ninomiya, nuc  phy  nielsen and   ninomiya, nuc  phy  nielsen and   ninomiya, phy  let  lu and   shen, fron  phy -  lin,  - - -  liao, ad  phy    miransky,     sukhachov, low tem  phy  wan,   turner,   savrasov, phy  re  haldane, arxiv:140 -  xu,  - -  hasan, science 349, 613 ding, phy  re -  chen, na  phy -  alidoust,   - - -  wang,  -  neupert,   hasan, na  phy -  xu,  - - - - -  jeng,   hasan, sc  ad  shi, na  commu -  sanchez,  - - - -  yao,   hasan, acs nano 10, 1378 felser,   beidenkopf, sc  ad  inoue,     bernevig,   yazdani, science 351, 1184     bernevig, and   phy  okugawa and   murakami, phy  re    miransky,     sukhachov, phy  re  roy, phy  re    das sarma, phy  re    polini, phy  re  hofmann and   das sarma, phy  re  kawabata, arxiv:181    miransky,     sukhachov, phy  re  let    miransky,     sukhachov, phy  re  tokman, and   belyanin, phy  re  let  song and   rudner, phy  re  polini, phy  re  phy  novotny and   stranick, ann  re  phy  che  zhang, ultramicroscopy 59, 109 mo  phy  das sarma, and   politano, phy  re - -  xu,  - - - - - - -  jia, na  commu -  xu, phy  re  chen, crys  growth de  ex  theo  phy  gurzhi, so  phy  gotsmann, na  commu    miransky,     sukhachov, phy  re    miransky,     sukhachov, phy  re    miransky,     sukhachov, phy  re  gorbar,       phy  conden  barton, re  pro  phy  echenique, phy  re  lifshitz and   pitaevskii, physical kinetics huang, statistical mechanics     sachdev, pro  nat  aca  sc  alekseev, phy  re  let  landau and   lifshitz, fluid mechanics kovtun and   ritz, phy  re    hartnoll, nature phy  high energy phy    gout eraux, and     high energy phy    phy - -  ran, phy  re      burkov and   balents, phy  re  let      balents, phy  re    grushin, phy  re      zyuzin and     burkov, phy  re  goswami and   tewari, phy  re      burkov, phy  re  let  troyer,     yazyev, phy  re  let  shekhar, na  commu  day,   felser, and   damascelli, phy  re      querry, app  op  phy  so  jp  ritchie and   wilems, phy  re  echenique, re  pro  phy  ritchie, phy  re - -  alidoust,   hasan, sc  re  chulkov,     bernevig, phy  re  let ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Acknowledgments", "Section": " Acknowledgments", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "A", "Section": "A Derivation of the Fermi arc hydrodynamics", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "B", "Section": "B Polylogarithm functions", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "References", "Section": " References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": " they are supposed to form as a result of rapid accretion of primordial gas, although it can be obstructed by the time variation caused by circum-stellar disc fragmentation due to gravitational instabilit  to assess the occurrence of fragmentation, we study the structure of marginally gravitationally unstable accretion discs, by using a steady one-dimensional thin disc model with detailed treatment of chemical and thermal processe  motivated by two sms formation scenarios,   on the other hand, in the case of molecular flows, there is a critical disc radius outside of which the disc becomes unstabl  those conditions appears to be marginally satisfied according to numerical simulations, suggesting that disc fragmentation can be common during sms formation", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": " although the origin of those bhs is still a mystery, short available time for their growth seems to favor massive seed bhs they can grow smbhs in the available time if either with continuous accretion at the eddington limit or with short episode of super-eddington growt matsukoba@ast tohok a jp however, the bh's growth can easily be hindered by its own radiative feedback here, smss are supposed to form from a primordial gas in some peculiar site  matsukoba et a  due to high temperature in addition to the above scenario for sms formation, hirano et a  recently proposed another channel where the suppression of h2 cooling is not always require  once the accretion rate falls below this value for sometime, however, the radiation feedback may become significant and eventually terminate the stellar growt  adopting a simple sporadic accretion history with repeating burst and quiescent phases, sakurai et a  such time variations would be caused by clump formation in the circum-stellar disc, which is formed due to the finite angular momentum of the clou  in fact, numerical simulations of sms formation found a sign of disc fragmentation by gravitational instability and resulting accretion variation although their spatial resolution and chemical/thermal modeling are not good enough to draw definitive conclusion  their conclusion needs to be justified by more detailed modelin  in this paper, we construct models for one-dimensional steady accretion discs by solving non-equilibrium chemical and thermal evolution and investigate their gravitational stabilit  the paper is organized as follow  we describe our disc model and stability criterion in section   we then present the obtained disc structures and the result for the stability analysis in section   discussion and conclusion are given in section   appendices are for the details of radiative and chemical processe ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Model", "Text": "1 basic framework we construct the one-dimensional axisymmetric and steady model for an accretion disc feeding the central star under a given central stellar mass and accretion rat  we assume the disc is marginally gravitationally unstable and the angular momentum is transported by the spiral arm  thermal and chemical evolution in the flow is calculated as in sections   another possible mechanism of the angular momentum transfer is via the turbulent viscosit  this means that the turbulent viscosity alone cannot maintain the disc in a gravitationally stable state and the disc eventually settles in a marginally unstable state where the angular momentum is transferred due to the spiral arm ", "Subsections": [{"Section_Num": "2_2", "Section": "2.2 Thermal Evolution", "Text": " we consider viscous heating, continuum cooling, h2 molecular and li atomic line cooling and chemical cooling as the heating and cooling processe  in our model, the kinematic viscosity is determined so that the surface density satisfies equation for a given accretion rat  we use fitting functions for betaesc from fukushima et a  as chemical cooling/heating processes, we consider those associated with h ionization/recombination and h2 dissociation/formatio 48 ev are the binding energie 333 * 10-2 the number fraction of he relative to hydrogen nucle  we do not include radiative heating by the central star in our calculatio  we estimated it under the assumption of optically-thin irradiation at the disc midplane and found it is generally lower than the viscous heating rate even at the eddington luminosity because the coupling between the stellar radiation and disc is very ineffcient due to the geometrical dilution of radiation and transparency of the disc although this somewhat stabilizes this part of the disc, the disc remains unstable as in the case without stellar radiatio  we thus expect that the effect of stellar radiation heating does not change our conclusion on disc fragmentatio ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2_3", "Section": "2.3 Chemical Reactions", "Text": " all the reactions are paired with their reverse reaction  matsukoba et a  table c  we assume he to be all neutral with fractional abundance yhe =  333 * 10-2 as our temperature range is limited below 104   we also assume that li remains neutral with yli =  15 * 10-10 since they are still dominantly atomic in the calculated range and the small amount of li ions does not affect thermal evolution at al  h2 photodissociation and h-photodetachment by external radiation are not include  because the number density is larger than 108 cm-3 in our calculation range and the shielding of far-uv radiation is effective due to the large column density, the collisional dissociation works more effciently than the dissociation due to external far-uv radiatio  the effect of h-photodetachment is also negligible because h2 formation via the three body reaction is more effective than the h-channel at such high densities as considered in this wor ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2_4", "Section": "2.4 Fragmentation Condition", "Text": " in other words, there is an upper limit on the angular momentum transported by the spiral arms and if more matter is accumulated than the disc can extract its angular momentum, the disc will fragmen ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2_5", "Section": "2.5 Model setup", "Text": " the disc structures at different central stellar masses can be regarded as temporal evolution around a growing sm  the calculated range of accretion rate encompasses these value  we set the outer boundary of the disc at 1000 au and assigning the temperature and chemical fractions ther  we evaluate the surface density by equation using the temperature and the central stellar mass, and calculate the radial velocity using equation equation gives the kinematic viscosity that is required to maintain such a disc structur  we then calculate the heating, cooling and chemical reaction rate, and update the temperature and the chemical fractions along the inflo g, to baryonic streaming motion  at number density nh = 1010 cm- ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Result", "Text": "we present the disc structures and examine their gravitational stability for atomic flows in section  1 and for molecular flows in section  ", "Subsections": [{"Section_Num": "3_1", "Section": "3.1 Atomic Accretion Flows", "Text": " to understand the temperature evolution in fi  1a, we show in fi  this is also true for all the cases shown in fi  this is because the actual rate is far smaller than the lte rate as the electron number density is generally several orders of magnitude below the critical density, even with the photon trapping effec  in the panelsolid and dashed lines correspond to heating and cooling rates, respectivel  cooling rates by other processes,   matsukoba et a  same as fi  same as fi  although the electron density increases, the continuum optical depth exceeds unity before reaching the critical density and the cooling rate decreases exponentially thereafte  we use the optically thin rate as the optical depth is still small at this epoch 1  the slight outward temperature increase in fi  1 is due to the smaller actual cooling rate than that by equation as a result of smaller electron fraction than the equilibrium value used abov  it should be noted that the minimum temperature in the disc given as a solution of equation depends only on the accretion rate and not on the central stellar mass, as seen in fi  1  next we study the dependence on the accretion rat  at lower accretion rates,   this is a result of h2 formation and its cie coolin  at radii outer than 9 au, the gases are mostly atomic and the temperatures are set by the balance between the viscous heating and the cooling by the h-free-bound emissio  note that this molecular transition in the disc has not been recognized before and only becomes able to be followed by our detailed thermal and chemical modelin  similar dependence of disc structures on the accretion rate is also observed for other central stellar masse  3  as seen in fi  as seen in section   fi  as seen in fi  since the typical accretion rate for smss formation falls on around this critical value, whether fragmentation actually occurs or not would depend on detailed dynamics of collapse and accretion", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3_2", "Section": "3.2 Molecular Accretion Flows", "Text": "1 disc structures the disc structure for a molecular flow is shown in fi  matsukoba et a  same as fi  same as fi  rate same as fi  same as fi  cally thick, as already seen for the atomic flows note that the decrease of h2 fraction with density here looks more abrupt than in the case of collapsing prestellar cores this difference can be attributable to smaller chemical cooling rate in our case: the latent heat associated with h2 dissociation is extracted in a longer timescale for density enhancement,   the temperature in the disc increases more easily, resulting in the sudden h2 dissociatio  the dependence of the disc structure on the accretion rate is similar also in the cases with other values of stellar mas  matsukoba et a  h2 region fra  in fra  the critical radius for gravitational instability for molecular accretion flows as a function of the accretion rat  the chemical fractions and the heating and cooling rates in this case are shown in fi  unlike in higher accretion rate cases, h2 now survives as the dominant species until the flow reaches the innermost par  the h fraction increases from 70 a  after peaking at 30 au, it then decreases inward as the temperature also decreases the temperature decrease at < 30 au is caused by the cooling by h2 cie the temperature more inside is set by the thermal balance between this cooling and the viscous heating until the disc becomes optically thick to the h2 collision-induced absorptio 2 disc fragmentation next, we examine the disc fragmentation condition for the molecular accretion flow  we thus expect that fragmentation, if any, occurs only in the outer h2 regio  8  10 as a function of the accretion rate for each central stellar mas  if the size of a disc is smaller than this, the disc remains stabl  otherwise, its outer part would become gravitationally unstable and fragmen  in all the cases shown in fi  also, the critical radius increases with the central stellar mass, reflecting that the temperature profiles shift toward outer radius as seen in fi  6  with increasing stellar mass by accretion, the disc at a given radius becomes more stable and only the more outer part can be gravitationally unstabl ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Summary and Discussion", "Text": "seed black hole formation by the direct collapse and its subsequent growth is one viable scenario for the supermassive black hole formation in the early univers  we have considered two possible compositions of the inflow,   we have constructed marginally gravitationally stable disc structur  in the case of atomic flows, we have found the following two types of disc structures depending on the accretion rat  at lower rate, in contrast, the compositional transition to the h2 phase occurs at some radius and h2 collision-induced emission dominates the cooling further inwar  if the inflow is mainly composed of a molecular gas, the h2-line cooling keeps the temperature below 2000 k in outer part of the disc although the temperature gradually increases inwar  with a lower rate, the gas remains in the molecular form until it becomes optically thick to the h2 collision-induced absorptio  for a given accretion rate and stellar mass, such disc is always unstable at suffciently large radius based on the analysis above, let us briefly discuss the possibility of disc fragmentation during sms formation in different formation sites, whose main component can be either atomic or molecula  in hirano et a  if a disc fragments into multiple clumps, the accretion onto the central protostar becomes episodic with repeating bursts accompanying falls of the clumps and intervening quiescent phase  with a quiescent phase longer than the kelvin-helmholtz timescale, the protostar contracts to a main-sequence star and starts emitting ionization photons, which can terminate further growth of the central star for quantitative modeling of the accretion-rate variability, hydrodynamical simulations incorporating with the chemical and thermal networks are awaite  the outcome of the disc fragmentation studied here might be supermassive binary stars, which can be detectable by future gravitational wave observations fragmentation condition is, however, still in dispute furthermore, it is not clear whether the condition obtained for protoplanetary discs is applicable to the those around sms  here the disc structure is obtained under the thin disc approximatio  in reality, however, the disc size would be smaller than our adopted value of 1000 au in such an early stage since the disc size increases with time accompanying the accretion of large angular momentum material originally located at the outer part of the parental dense cor  considering uncertainties above, further studies are needed to conclude whether smss actually form or no  we plan to perform hydrodynamics simulations that follow the formation of smss from the collapse of cores, where the disc fragmentation and subsequent evolution of fragments supposedly play an important rol  acknowledgments the authors would like to thank kohei inayoshi, daisuke nakauchi, hidekazu tanaka and hidenobu yajima for discussion  we also thank shingo hirano and takashi hosokawa for providing their simulation results and the anonymous reviewer for useful comments on improving thermal and chemical modellin  rm acknowledges financial support from the graduate program on physics for the universe of tohoku universit  this work is supported in part by mext/jsps kakenhi grant number 17h06360 and 17h01102, 17h02869 and by naoj alma scientific research grant numbers 2016-02a", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "A", "Section": "A Radiative processes", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "B", "Section": "B Lithium cooling rate", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "C", "Section": "C Chemical Reactions", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "ms com ; massim meneghetti@gmai sinic ed rasi oats@gmai princeto a jp; te  older clusters tend to be more concentrated than younger cluster  their structure, represented by the characteristic radius rs and mass ms of the navarro-frenk-white density profile, is related to their formation tim  this tight correlation indicates that the icm temperature is also determined by the formation time of individual cluster  numerical simulations showed that clusters move along the fundamental plane as they evolv  the plane and the cluster evolution within the plane could be explained by a similarity solution of structure formation of the univers  the angle of the plane shows that clusters have not achieved virial equilibrium in the sense that mass/size growth and pressure at the boundaries cannot be ignore  the distribution of clusters on the plane was related to the intrinsic scatter in the halo concentration-mass relation, which originated from the variety of cluster age  the well-known mass-temperature relation of clusters can be explained by the fundamental plane and the mass dependence of the halo concentration without the assumption of virial equilibriu  the fundamental plane could also be used for calibration of cluster masses", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "clusters of galaxies are the most massive objects in the univers  since the fraction of baryons in clusters is not much different from the cosmic mean value, dark matter accounts for most of the mass of clusters thus, the structure of the clusters is mainly determined by the halos of dark matter, or the dark halo  cold dark matter cosmology predicts that more massive halos form late  thus, clusters form after galaxies d 3390/galaxies6010001 ww mdp com/journal/galaxies arxiv:190  a current trend may be associating the formation time with the internal structure of dark halo  navarro et a  pointed out that the characteristic parameters of the nfw profile reflect the density of the background universe when the halo was forme  this issue has been addressed in many studies, especially by n-body simulations these studies have indicated that the halo structure is determined by their mass-growth histor  the inner region 1 of current halos develops in the early fast-rate growth phase when the halos grow rapidly through matter accumulatio  their outer region is formed in the subsequent slow-rate growth phase in which halos grow slowly through moderate matter accumulatio  during this phase, the inner region is almost preserve  thus, halos form inside-ou  the formation time of a halo can be defined as the transition time from the fast-rate growth phase to the slow-rate growth phas  there are a few specific definitions of the formation time that well represent the transition tim  one is the time at which the mass of the main progenitor was equal to the characteristic mass ms of the halo at its observed redshift zobs moreover, numerical simulations have shown that clusters are dynamically evolving systems and such evidence is often found in their outskirt  in fact, the ambient material is continuously falling toward clusters, which creates surfaces around cluster  for example, the outskirt profiles of dark matter halos can become extremely steep over a narrow range of radius this features in the density profiles are caused by splashback of collisionless dark matter on its first apocentric passage after accretion accretion of collisional gas toward clusters also creates 1 more precisely, the boundary radius between the inner and outer regions is a few times rs these discontinuities mean that clusters are neither isolated nor in an equilibrium stat  in this review, we first introduce the fundamental plane we discovered, and its implications for structure formation of the univers  in particular, we show that clusters in general have not achieved virial equilibrium in contrast with conventional view  then, using the fundamental plane, we discuss that a scaling relation can be explained without assuming virial equilibriu  we also show that the fundamental plane can be used for mass calibratio 73, and the hubble constant of h0 = 70 km s-1 mpc- ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Fundamental Plane", "Text": "the hot intracluster medium is distributed in the potential well of dark halo  since the x-ray emission from the icm is proportional to the square of the density, it mainly comes from the central region of the cluster where the density is hig  thus, the observed x-ray temperature tx represents that of the central region and should reflect the gravitational potential ther  based on this motivation, fujita et a  analyzed 20 massive clusters in the cluster lensing and supernova survey with hubble observational sample for these clusters, rs and ms had been obtained from the joint analysis of strong lensing observations with 16-band hubble space telescope observations and weak-lensing observations mainly with suprime-cam on the subaru telescope the x-ray temperature had been obtained with chandra temperatures are estimated for a cylindrical volume defined by the projected radii r = 50-500 kpc to avoid the influence of cool core  figure 1a shows the data distribution in the spac  as can be seen, the data are distributed on a plan 76+ 56+ 32+  figure 1b shows the cross-section of the plane; the dispersion of the data around the plane is very small and is only  045+  in figure 1c, error bars for individual clusters are show  in the vertical directionwe show the temperature errors of individual cluster  in the horizontal direction, the errors of rs and ms are strongly correlated, and we display them as a single ba  we note that, when we calculate the plane parameters, we properly account for the correlation for each cluster using the joint posterior probability distribution of the nfw parameters thus, the actual error is not represented by a single bar in a precise sens  figure 2 shows the direction of the plane normal p3 in the space the fundamental plane 2has been reproduced by numerical simulation  figure 3 shows the results of music n-body/hydrodynamical simulations these simulations do not include radiative cooling or non-gravitational feedback by active galactic nuclei and supernovae the mass resolutions for the dark-matter particles that for the gas particles are mdm =   the gravitational softening is set to be 6 h-1 kpc for the both gas and dark-matter particles in high-resolution region  in this analysis, we included the core 2 other fundamental planes of clusters with different combinations of three parameters have also been studied the absolute position of the plane is very close to that of the clash observational data figure 2 shows that the plane angle for the music sample is consistent with the clash data at the 90% confidence leve  p1 log log log p2 p1 p3 log log log figure   the orange plane is the best fit of the dat  the orange plane is translucent, and the grayish points are located below the plan  the lengths of the pins show the distance to the plan  the arrow p1 shows the direction to which the data distribution is most extended, and the arrow p2 is perpendicular to p1 on the plan  the cross-section of the plane in the large black dots are the clash data, and the small red dots are the music results shown in figure   the latter is projected on the p1-p3 plane determined for the forme  the direction p3 is the plane norma  note that the scales of the vertical and horizontal axes are differen  the same as but error bars for individual clusters are include  the viewing angle is changed so that the relation between the error bars and the plane is easily seen p1 log log log p2 p1 p3 figure   red dots are the results of the adiabatic music simulations the axes are normalized by the average parameters of the sample the orange plane is the best fit of the dat  the arrow p1 shows the direction to which the data distribution is most extended, and the arrow p2 is perpendicular to p1 on the plan  the cross-section of the plane in the direction p3 is the plane normal the results of simulations including radiative cooling and feedbac  the axes are normalized by the average parameters of the combined sample the orange plane is the best fit of the dat  the arrow p1 shows the direction to which the data distribution is most extended, and the arrow p2 is perpendicular to p1 on the plan  the cross-section of the plane in the direction p3 is the plane normal figure 4 presents the results of other numerical simulations including phenomena such as heating by agns and sne in addition to radiative cooling the mass resolution for the dark-matter particles and the initial gas particles are mdm =   in high-resolution regions the gravitational softening is set to be  75 h-1 kpc both groups of dots are located on almost the same fundamental plane, and the plane angles for the two samples are almost the same this means that clusters evolve along the unique plane in the direction of p1 in figure 4  the plane angles for fb0 and fb1 are not much different from those for the clash data and the music adiabatic simulations in figure 2, nf0 is the result of a simulation that is the same as fb0 but not including radiative cooling and feedbac  since their angles are almost the same, this indicates that radiative cooling and feedback do not much affect the fundamental plan ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Origin of the Fundamental Plane and Cluster Growth", "Text": "fujita et a  explained the origin of the fundamental plane using an analytic similarity solution developed by bertschinger this solution treats spherical collapse of an overdense region in the einstein-de sitter universe and subsequent matter accretion onto the collapsed objec  the solution has a constant called the entropy constan  we note that the similarity solution describes the matter profile in the region where the matter is later accreted however, the non-dimensional profiles are not much changed, even if objects are mostly composed of dark matter here, rita and tita are the turnaround radius and the turnaround time of the overdense region, respectivel  the evolution of the overdense region is described by the conventional spherical collapse mode  the radius rita3 and the mass mita of the overdense region can be connected to the characteristic radius rs and mass ms of the nfw profil  this is because the evolution of both overdense region in the similarity solution and the inner region of the nfw profile is related to the background universe, and they evolve in a similar wa  in fact, the evolution of the former is described by the conventional spherical collapse of an overdense regionand thus the typical density is proportional to that of the background universe at the collaps  in summary, the similarity solution has scales such as rita and mit  since rita and mita are proportional to rs and ms, respectively, the solution has scales of rs and m  these scales represent the border between the initially-collapsed overdense region and the later-accreted region, although the actual transition is gradua  this also means that the initial collapse and the later accretion correspond to the fast-rate growth phase and the slow-rate growth phase, respectivel  equation forms a plane in the spac  the similarity solution indicates that clusters are not in virial equilibrium, because clusters are growing through matter accretion from their outer environments one is the term representing the increase of mass and size of clusters and another is the boundary term originating from the flux of inertia through the boundary and the pressure at the boundar  the boundary corresponds to the splashback radius for dark matter and the shock front for gas note that the similarity solution shows that clusters are almost in hydrostatic equilibrium in the sense that gas motion is negligible within clusters even if they are not in virial equilibrium the relation between matter accretion and the cluster structure has also been numerically studied figure 5 shows the projection of the fundamental plane shown in figure 3 on the log rs-log ms plan  this direction is also close to that of cluster evolution in figure 4 projected on the log rs-log ms plane in fact, numerical simulations have shown that an individual cluster intermittently moves in the direction of the solid arrow in figure 5 every time it undergoes mergers while the cluster temporarily deviates the general motion in the middle of a major merger, the deviation is confined in the fundamental plane and thus mergers do not much affect the thinness of the plane this reflects the fact that the density perturbations of the universe are described by a gaussian random field in other words, clusters form a two-parameter famil  thus, a correlation between two physical quantities is generally represented by a band rather than a line unless some special combination of quantities is chose  while the temperature of each cluster tx is affected by its formation time, it also depends on the track the cluster choose  projection of the fundamental plane on the log rs-log ms plane shown in figure   the red points show the music clusters and rs0 and ms0 are the same as those in figure   the solid arrow shows the direction of cluster evolution and msr1/2 s increases in this directio ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Mass\u2013Temperature Relation and the Concentration Parameter", "Text": "the fundamental plane can be used to relate the cluster structure to the temperatur  as an application, we discuss the mass-temperature relation in this sectio  this relation is obtained by both observations and numerical simulations here, we consider cluster temperature outside cool core  equation is associated with assumption however, the assumptions are clearly inconsistent with the inside-out scenario of cluster formation and the fundamental plan  for example, the inside-out scenario indicates that clusters are not well relaxed and keep the memory of their formation in their structur  the angle of the fundamental plane shows that clusters are not virialized, as discussed in section   the nfw profile ) is not an isothermal profile these are inconsistent with assumption moreover, the tight correlation of the fundamental plane shows that tx is determined by rs and ms, which contradicts assumption showed that the relation in equation can be derived using the fundamental plane and the halo concentration-mass relatio 7 kev based on the results of the music simulations one example is c200 =   figure 6a shows the results for n = -2 using a general formula of c200 developed by correa et a  instead of equation note that figure 6 indicates that the red lines are slightly below the black lines for example, if higher-redshift clusters tend to have smaller masses and lower temperatures than lower-redshift clustersthe slope is slightly steepened we note that voit already addressed this issu  he considered accretion history of clusters and the effects of cluster surfaces as we d  while we focused on the inner structure of clusters, he studied the evolution of global properties of cluster  although our approach is different, our results support the conclusio  blue and green curves present the evolutions of two of the clusters shown in figure   as expected, the clusters move along the bands enclosed by the dotted and dashed line  the clusters frequently move in the horizontal direction, which corresponds to temporal temperature increase during cluster mergers", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Cluster Mass Calibration", "Text": "the thinness and solidity of the fundamental plane inspires applications in cosmolog  here, we show that the plane can be used to calibrate cluster mass precise estimation of cluster mass is importan  for example, when cosmological parameters are derived from cluster number counts, scaling relations among observables are used and they are affected by the calibration of cluster mass figure 8a shows the cross sections of the fundamental plan  the red open circles are the clusters of the clash samplefor which rs and ms are derived through gravitational lensin  the black dots are those of an x-ray samplefor which rs and ms are derived through x-ray observations assuming that the icm is in hydrostatic equilibriu  we discuss the fundamental plane formed by the clash sample and the one formed by the x-ray sample separatel 031+ 039 dex in the space of thus, the position of the fundamental planes are consistent with each othe  however, the xfp seems to be located slightly above the cfp in figure 8  the shift dfp may be caused by a possible systematic difference of observed rs or ms between cfp and xfp because they are obtained through different methods the plane shift in the direction of rs or ms can be estimated from df  cross sections of the fundamental plan  red open circles are the clash clusters and black dots are the x-ray clusters solid lines are the fiducial relations and the dash-dotted lines show uncertaintie  the difference of black and red lines come from the different assumptions of the plane shift85+  since the error is rather large, the current dataset may not be accurate enough for the calibration purpos  however, the error could be reduced by using larger and more accurate datasets in the futur 85 is consistent with the results of numerical simulations showing that hydrostatic mass tends to be smaller than the true mass", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "6", "Section": "6 Sparsity", "Text": "finally, we would like to make comments on the halo sparsity, which has been proposed recently as a valid alternative to the full description of the dark matter profil  the advantage in using the halo sparsity is that it has an ensemble average value at a given redshift with a scatter much smaller than that associated to the distribution in mass concentration and does not require any modeling of the mass density profile, which might significantly deviate from a nfw one in particular in systems still in the process of complete relaxation, but only the integrated mass measurements within two overdensitie  the use of the halo sparsity has also been proposed as new cosmological probe for galaxy clusters because it carries cosmological information encoded in the halo mass profile and, at given redshift, the average sparsity can be predicted from prior knowledge of the halo mass functio  both the fundamental plane and the halo sparsity reflect the halo concentration of cluster  while the fundamental plane gives us the direct information of cluster formation time, it is generally difficult to measure rs and ms observationally, compared with the sparsit  in future study, we will discuss the relation between the fundamental plane and the halo sparsit ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "7", "Section": "7 Conclusions", "Text": "it has been known that the concentration of dark halos reflects their formation histor  in particular, the halo structure represented by the characteristic radius rs, and mass ms is related to the formation time of the hal  the tight correlation shows that tx is also affected by the formation time of individual cluster  numerical simulations supported the results and showed that clusters evolve along the plan  the plane and its angle in the space of can be explained by a similarity solution, which indicates that clusters are still growing and have not reached a state of virial equilibriu  in other words, when cluster formation and the internal structure is considered, matter accretion after the initial collapse cannot be ignore  the motion of clusters on the plane was determined by the spectrum of the initial density perturbations of the univers  the spread of clusters on the fundamental plane is related to the scatter of the halo concentration-mass relatio  we also discussed applications of the fundamental plan  for example, we showed that the mass-temperature relation of clusters can be explained by the fundamental plane and the halo concentration-mass relation without assuming virial equilibriu  we also showed that the solidity and thinness of the fundamental plane can be used to calibrate cluster mas  since the fundamental plane associates the structure of dark halos with the gas temperature, other applications may be possibl  for example, the gas temperature tx of a dark halo could be estimated from rs and ms obtained through n-body simulations without calculating gas dynamic  provided the x-ray data and contributed in the interpretation of the finding  analyzed the data of gravitational lensing and contributed in the interpretation of the finding  analyzed the numerical simulations, kindly provided by the computational groups in trieste and madrid, and fit the data with the nfw profil  contributed in the interpretation of the optical and x-ray results and provided extensive feedback on the stud  funding: this work was supported by mext kakenhi n  18k03647 acknowledges financial contribution from the contracts naro15 asi-inaf i/037/12/0, asi 2015-046- 2017-14-  acknowledges support from the ministry of science and technology of taiwan and from academia sinica acknowledge support from the exanest and euroexa projects, funded by the european union's horizon 2020 research and innovation programme under grant agreements n  671553 and n  754337, respectivel  acknowledgments: we thank referees for their useful comments, which were very helpful to clarify explanation  conflicts of interest: the authors declare no conflict of interes  seven-year wilkinson microwave anisotropy probe observations: cosmological interpretatio  astrophy  supp  doi:1 1088/0067-0049/192/2/1  planck 2018 result  v  cosmological parameter  arxiv 2018, arxiv:180  the structure of cold dark matter halo  astrophy  doi:1 1086/17717  a universal density profile from hierarchical clusterin  astrophy  1997, 490, 493-50  doi:1 1086/30488  profiles of dark haloes: evolution, scatter and environmen  mo  no  astro  so  2001, 321, 559-57  doi:1 1046/ 1365-871  the power spectrum dependence of dark matter halo concentration  astrophy  2001, 554, 114-12  doi:1 1086/32134  concentrations of dark halos from their assembly historie  astrophy  2002, 568, 52-7  doi:1 1086/33876  the growth and structure of dark matter haloe  mo  no  astro  so  2003, 339, 12-2  doi:1 1046/ 1365-871  statistics of physical properties of dark matter cluster  astrophy  2006, 646, 815-83  doi:1 1086/50501  neto,   mo  no  astro  so  2007, 381, 1450-146  doi:1 1111/ 1365-296  mo  no  astro  so  2008, 387, 536-54  doi:1 1111/ 1365-296  accurate universal models for the mass accretion histories and concentrations of dark matter halo  astrophy  2009, 707, 354-36  doi:1 1088/0004-637x/707/1/35 a; cuesta,   mo  no  astro  so  2012, 423, 3018-303  doi:1 1111/ 1365-296  ludlow,   the mass profile and accretion history of cold dark matter haloe  mo  no  astro  so  2013, 432, 1103-111  doi:1 1093/mnras/stt52  the accretion history of dark matter haloes-ii  a physical model for the concentration-mass relatio  mo  no  astro  so  2015, 452, 1217-123  doi:1 1093/mnras/stv136  the accretion history of dark matter haloes-i  the connections with the mass power spectrum and the density profil  mo  no  astro  so  2015, 450, 1521-153  doi:1 1093/mnras/stv69  the splashback radius as a physical halo boundary and the growth of halo mas  astrophy  doi:1 1088/0004-637x/810/1/3  dependency of halo concentration on mass, redshift and fossilness in magneticum hydrodynamic simulation  arxiv 2018, arxiv:181  dependence of the outer density profiles of halos on their mass accretion rat  astrophy  doi:1 1088/0004-637x/789/1/  splashback in accreting dark matter halo  cosmo  astropar  phy  doi:1 1088/1475-7516/2014/11/01  properties of cosmic shock waves in large-scale structure formatio  astrophy  2000, 542, 608-62  doi:1 1086/31702  cosmological shock waves and their role in the large-scale structure of the univers  astrophy  2003, 593, 599-61  doi:1 1086/37672  discovery of a new fundamental plane dictating galaxy cluster evolution from gravitational lensin  astrophy  doi:1 3847/1538-4357/aab8f  the cluster lensing and supernova survey with hubble: an overvie  astrophy  supp  doi:1 1088/0067-0049/199/2/2  the music of clash: predictions on the concentration-mass relatio  astrophy  doi:1 1088/0004-637x/797/1/3  astrophy  doi:1 3847/0004-637x/821/2/11  hubble space telescope combined strong and weak lensing analysis of the clash sample: mass and magnification models and systematic uncertaintie  astrophy  doi:1 1088/0004-637x/801/1/4  clash: weak-lensing shear-and-magnification analysis of 20 galaxy cluster  astrophy  doi:1 1088/0004-637x/795/2/16  clash-x: a comparison of lensing and x-ray techniques for measuring the mass profiles of galaxy cluster  astrophy  doi:1 1088/0004-637x/794/2/13  the fundamental plane of galaxy cluster  mo  no  astro  so  1993, 263, l2  doi:1 1093/mnras/26 l2  the eso nearby abell cluster surve  i  the fundamental plane of clusters of galaxie  astro  astrophy  1998, 331, 493-50  the x-ray fundamental plane and lx-t relation of clusters of galaxie  astrophy  1999, 519, l51-l5  doi:1 1086/31209  are clusters standard candles? galaxy cluster scaling relations with the sunyaev-zeldovich effec  astrophy  2002, 581, 5-1  doi:1  the scaling relations of galaxy clusters and their dark matter halo  astrophy  2004, 600, 640-64  doi:1 1086/37985  lx-t relation and related properties of galaxy cluster  astrophy  2006, 640, 673-69  doi:1 1086/50029  cosmology and cluster halo scaling relation  mo  no  astro  so  2009, 400, 1317-133  doi:1 1111/ 1365-296  cool core clusters from cosmological simulation  astrophy  2015, 813, l1  doi:1 1088/2041-8205/813/1/l1  the origin of icm enrichment in the outskirts of present-day galaxy clusters from cosmological hydrodynamical simulation  mo  no  astro  so  self-similar secondary infall and accretion in an einstein-de sitter univers  astrophy  supp  1985, 58, 39-6  doi:1 1086/19102  locations of accretion shocks around galaxy clusters and the icm properties: insights from self-similar spherical collapse with arbitrary mass accretion rate  mo  no  astro  so  2016, 461, 1804-181  doi:1 1093/mnras/stw141  evolution and clustering of rich cluster  mo  no  astro  so  1986, 222, 323-34  doi:1 1093/mnras/22  baryonic features in the matter transfer functio  astrophy  1998, 496, 605-61  doi:1 1086/30542  a universal model for halo concentration  astrophy  doi:1 1088/0004-637x/799/1/10  a new interpretation of the mass-temperature relation and mass calibration of galaxy clusters based on the fundamental plan  astrophy  doi:1 3847/1538-4357/aacf0  in the beginning: the first sources of light and the reionization of the univers  phy  re  2001, 349, 125-23  doi:1 1016/s0370-157300019-  duffy,   dark matter halo concentrations in the wilkinson microwave anisotropy probe year 5 cosmolog  mo  no  astro  so  2008, 390, l64-l6  doi:1 1111/ 1745-393  statistical properties of x-ray clusters: analytic and numerical comparison  astrophy  1998, 495, 80-9  doi:1 1086/30526  gravitating mass profiles of nearby galaxy clusters and relations with x-ray gas temperature, luminosity and mas  astro  astrophy  2002, 391, 841-85  doi:1 1051/0004-6361:2002090  the xxl survey i  mass-temperature relation of the bright cluster sampl  astro  astrophy  2016, 592, a  doi:1 1051/0004-6361/20152688  cosmological hydrodynamical simulations of galaxy clusters: x-ray scaling relations and their evolutio  mo  no  astro  so  2018, 474, 4089-411  doi:1 1093/mnras/stx292  cosmological simulations of galaxy cluster  ad  sc  let  doi:1 1166/as  large-scale structure formation: from the first non-linear objects to massive galaxy cluster  space sc  re  2015, 188, 93-13  doi:1 1007/s11214-014-0045-  dark matter halo profiles of massive clusters: theory versus observation  astrophy  doi:1 1088/0004-637x/766/1/3  dutton,   cold dark matter haloes in the planck era: evolution of structural parameters for einasto and nfw profile  mo  no  astro  so  2014, 441, 3359-337  doi:1 1093/mnras/stu74  an accurate physical model for halo concentration  arxiv 2018, arxiv:180  cluster temperature evolution: the mass-temperature relatio  astrophy  2000, 543, 113-12  doi:1 1086/31708  on the evolution of the temperature-virial mass relation for clusters of galaxie  astrophy  1998, 500, l111-l11  doi:1 1086/31141   planck 2013 result  x  cosmology from sunyaev-zeldovich cluster count  astro  astrophy  2014, 571, a2  doi:1 1051/0004-6361/20132152  mass profiles and c-mdm relation in x-ray luminous galaxy cluster  astro  astrophy  2010, 524, a6  doi:1 1051/0004-6361/20101527  testing x-ray measurements of galaxy clusters with cosmological simulation  astrophy  2007, 655, 98-10  doi:1 1086/50986  total mass biases in x-ray galaxy cluster  astro  astrophy  2008, 491, 71-8  doi:1 1051/0004-6361:20080973  on the influence of non-thermal pressure on the mass determination of galaxy cluster  astro  astrophy  2010, 510, a7  doi:1 1051/0004-6361/20091185  lensing and x-ray mass estimates of clusters phy  doi:1 1088/1367-2630/14/5/05501 -  imprints of dark energy on cosmic structure formation-ii  sparsity of dark matter halo profile  mo  no  astro  so  2014, 437, 2328-233  doi:1 1093/mnras/stt205  probing cosmology with dark matter halo sparsity using x-ray cluster mass measurement  astrophy  doi:1 3847/1538-4357/aaccd  licensee mdpi, basel, switzerlan  this article is an open access article distributed under the terms and conditions of the creative commons attribution license", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "References", "Section": "References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": " -  holoien4, todd     the transiting exoplanet survey satellite has started to produce high-quality light curves with a baseline of at least 27 days, eventually for most of the sk  the combination of asas-sn and tess light curves probes both long and short term variability in great detail, especially towards the tess continuous viewing zones at the ecliptic pole  the light curves and characteristics of the variables are all available through the asas-sn variable stars database3 million sources studied in this work", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "the study of stellar variability has been invigorated by the advent of modern large scale sky surveys in the modern er  variable stars are excellent astrophysical probes and have been used in numerous astronomical context  pulsating variables such as cepheids and rr lyrae stars are commonly used as distance indicators owing to the period luminosity relationships seen amongst these variables jayasinghe et a  variable stars are also useful for the study of stellar populations and galactic structure the 3 new units all started with g-band filters and the 2 original units have now switched to g-band as wel  the asas-sn telescopes are hosted by the las cumbres observatory in hawaii, chile, texas and south afric  asas-sn team members have also studied the relative specific type ia supenovae rates and the largest amplitude m-dwarf flares seen in asas-sn we have also explored the synergy between asas-sn and apogee with the discovery of the first likely non-interacting binary composed of a black hole with a field red giant and a detailed variability analysis of the apogee sources to identify 1914 periodic variables the transiting exoplanet survey satellite will produce a large number of high-quality light curves with a baseline of at least 27 days for most of the sk  oelkers et a  recently identified variable sources in a sample of 4 million tic sources, but did not classify these variables into explicit type  these sources were classified in paper ii using asas-sn dat  these tess light curves will probe short period variability in great detai 3 million sources within 18 deg of the southern ecliptic pol  these sources are within the southern tess cvz and will have well-sampled tess light curve  in this work, we systematically search this sample for variable source  this is, in part, a test run for carrying out such a search over the entire sk  all the light curves of these sources are made available to the public through our online databas ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Observations and Data reduction", "Text": "we started with the aavso photometric all-sky survey dr9 catalog as our input source catalo  asas-sn v-band observations were made by the brutus and cassius quadruple telescopes between 2013 and 201  each camera has a field of view of   the light curves for these sources were extracted as described in jayasinghe et a  using image subtraction and aperture photometry on the subtracted images with a 2 pixel radius apertur  the apass catalog was used for calibratio  the zero point offsets between the different cameras were corrected as described in jayasinghe et a  figure 1 illustrates the relationship between the measured mean asas-sn v-band magnitudes and the apass dr9 v-band magnitude  this is due to blending in fields with significant stellar densities the relatively large asas-sn pixel scale of   thus, asas-sn v-band measurements are systematically brighter for most sources in the lmc due to blended ligh  the light curve extraction provides a statistical error estimate, but the scatter in the light curves of apparently non-variable sources is generally larger than expected given the nominal statistical uncertaintie  in jayasinghe et a  in this work, we update our approach to estimating the systematic uncertainties in the asas-sn photometr 3m sources in our sampl  comparison of the mean asas-sn v-band magnitudes to the apass dr9 v-band magnitude  the red dashed line illustrates a perfect calibratio 3m source  the polynomial smoothly joins to   we also identified a systematic issue that affects the light curves of certain sources due to malfunctioning shutter 48-52155  top: examples of images that are affected by a malfunctioning shutter for the non-variable source j07295 48-52155 48-52155 8 with the affected epochs highlighted by the red boxe  the asas-sn cameras periodically start to fai  degraded shutters do not close completely, and stray light from neighboring bright sources can impart trails on the images during readout this systematic is illustrated in the light curve for the non-variable source j07295 48-52155  some fraction of our asas-sn light curves will be affected by this systemati ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Variability Analysis", "Text": "here we describe the procedure we used to identify and characterize variables in the source lis 2, we describe the procedure we took to identify candidate variable source 3, we discuss the application of the v2 random forest classifier model from jayasinghe et a 4, we discuss the corrections done to mitigate the effects of blending on the candidate variable ", "Subsections": [{"Section_Num": "3_1", "Section": "3.1 Cross-matches to external catalogs", "Text": "we identify cross-matches to the apass sources with gaia dr2 using the pre-computed cross matches from marrese et a  the sources were also crossmatched to the probabilistic distance estimates from bailer-jones et a  we also crossmatch the sources with 2mass and allwise using a matching radius of 1  jayasinghe et a  to cross-match the apass sources with the 2mass and allwise catalogs the large magellanic cloud lies within the tess southern cv 97 kpc in our variability classifie 2 variability cutoffs there are numerous methods to identify variable sources from a sample of light curve  the asassn observations used in this work are made with a single filter, which makes the use of multi-band variability statistics impossibl  we used the astropy implementation of the generalized lomb-scargle periodogram to search for periodicity over the range  3m source  we utilize the false alarm probability and the power of the best gls period as means of identifying significantly periodic source 25 were selected for further analysis red variables typically have long periods that result in noticeable structure in their temporal light curve  we also compute the ratio of magnitudes brighter or fainter than average for all the source 5 for further analysi  this variability cut is expected to improve the identification of detached eclipsing binarie  we also identify sources with a light curve rms larger than the 95th percentile for the other stars in magnitude bins of  25 mag in order to select sources with significant flux variation  the variability cuts and the number of variable candidates isolated through each cut are summarized in table   the combination of these cuts help identify different variable sources and increase our completeness when compared to relying on just one or two parameter  the astrobase implementation of the generalized lomb-scarglethe multi-harmonic analysis of varianceand the box least squares periodograms were used to search for periodicity in these light curve  following the period search, we use the variability classifier implemented in jayasinghe et a  to classify these variable candidate  we choose to visually review the classifications in order to improve our catalo  the majority of the periodic variables within this period range should show significant improvements in the string length statistic when phased with a perio  all irregular and aperiodic sources are also selected for visual revie  during the process of visual review, we identified incorrect classifications and periods and corrected the  sources with significant systematic and spurious variability were removed part of this is that we were deliberately generous in our initial selection so that we can use the results to improve this aspect of the pipeline as we progress with our effort to identify variable sources over the full sk ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3_4", "Section": "3.4 Blending Corrections", "Text": "the large pixel scale of the asas-sn images and the fwhm results in blending towards crowded region  the apass catalog was constructed with images that have a significantly smaller pixel scaleand as a result of this, multiple apass sources can fall into a single asas-sn pixe  we do not correct for the contaminating light in the photometry of the blended sources, but we identify and correct blended variable groups in our catalo  since we extracted light curves for the positions of apass sources, we can have two or more such sources inside a single asas-sn resolution elemen  if we select the sources with another apass neighbor within 3  we compute the flux variability amplitudes for these sources using a random forest regression model the majority of the variable groups consisted of two sources, with a few groups consisting of up to three source 3m sources and the set of known variables from jayasinghe et a 3m source 5 mag used to identify red variable source ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Results", "Text": " table 2 lists the number of sources of each variability type in the catalo 686 for variability classificatio  we have sorted the variables into groups to highlight the different classes of variable source  owing to the magnitude limit of asassn, we are only able to detect suffciently bright sources in the lm  this is evident in the wesenheit color-magnitude diagram for the lmc source  this essentially highlights the large dynamic range in period probed by the asassn light curve  we note that the identification of mira variables in the lmc is hindered by blendin  in reality, some fraction of the lmc semi-regular variables in this catalog are actually mira variable  jayasinghe et a 3m source 5 are shaded in blue and the sources with log rms > 95th percentile in each magnitude bin are shaded in re  is plotted in re  summary of the variability selection cuts variable cut used to identify sources gls power >   the plr sequences for the cepheids and semi-regular variables in the lmc are well defined most of the eclipsing binaries identified in the lmc are either detached or semi-detached systems, and do not follow the well defined plr for contact binaries that is observed in the plr diagram for the sources outside the lm  we also show the sky distribution of the variables identified in this work in figure 1  we see that the distribution of eclipsing binaries and rotational variables is random, but the distribution of cepheids is strongly clustered towards the lmc as is expecte  we also note the clustering of semi-regular/irregular variables towards the lmc and the galactic dis  we matched our list of variables to the vsx catalog available in october 2018, with a matching radius of 1  the variables discovered by the all-sky automated survey and the catalina real-time transient survey are included in the vsx databas  the majority of these known variables consist of eclipsing binariesirregular/semi-regular variables and cepheids most of the new discoveries are eclipsing binaries and irregular/semi-regular variables we also discovered 128 new gcas variables, 81 of which are located in the lm  gcas variables are rapidly rotating, earlytype irregular variable stars with mass outflow from their equatorial region  example light curves for the newly identified gcas variables are shown in figure 1  we also discovered another long period detached eclipsing binary in this wor  asassnv j08070 46-59102  a non-negligible fraction of these new discoveries were classified as generic variables these are mostly low amplitude or faint source  example light curves are shown for a subset of the newly discovered periodic variables in figure 9", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Conclusions", "Text": " jayasinghe et a  the points are colored by the perio  satellite and thus, a large majority of these sources will have excellent tess light curve  variable sources identified in the lmc largely consist of luminous variables, including cepheids, gcas variables and red giant  we have developed a user friendly interface to retrieve precomputed asas-sn v-band light curves for apass source 3m sources studied in this work are available online at the asas-sn photometry database to highlight the possible blended sources, a flag is assigned to each source if the distance to the nearest apass neighbor is <1  the new variable sources have also been added to the asas-sn variable stars database this work provides long baseline v-band light curves for a large fraction of the sources in the tess southern cvz and is a useful supplement to the short baseline tess light curves that possess better photometric precisio  acknowledgements we thank the referee for their useful comment  we thank the las cumbres observatory and its stafffor its continuing support of the asas-sn projec  we also thank the ohio state university college of arts and sciences technology services for helping us set up and maintain the asas-sn variable stars and photometry database  asas-sn is supported by the gordon and betty moore foundation through grant gbmf5490 to the ohio state university and nsf grant ast-151592  development of asas-sn has been supported by nsf grant ast-0908816, the m  this work is supported in part by scialog scholar grant 24216 from the research corporatio  tat acknowledges support from a simons foundation fellowship and from an ibm einstein fellowship from the institute for advanced study, princeto  sd acknowledges project 11573003 supported by nsf  support for mp and op has been provided by the primus/sci/17 award from charles universit  this work was partly supported by nsfc 1172130  this work has made use of data from the european space agency mission gaiaprocessed by the gaia data processing and analysis consortiu  the points are colored as in figure   micron all sky survey, as well as data products from the widefield infrared survey explore  this research was also made possible through the use of the aavso photometric all-sky surveyfunded by the robert martin ayers sciences fun  this research made ualso se of astropy, a community-developed core python package for astronomy", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Defining", "Section": "Defining a bulk-edge correspondence for non-Hermitian Hamiltonians via singular-value decomposition", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "en fr, enric olivucci@des de, michelangel preti@lp en  we give full description of bulk behavior of large feynman graphs: it shows a generalized dynamical fishnet structure, with a dynamical exchange of bosonic and yukawa coupling  we compute certain fourpoint correlators in the full chiral cft4, generalizing recent results for a particular onecoupling version of this theory - the bi-scalar fishnet cf  we sum up exactly the corresponding feynman diagrams, including both bosonic and fermionic loops, by bethesalpeter metho  this provides explicit ope data for various twist-2 operators with spin, showing a rich analytic structure, both in coordinate and coupling space  arxiv:190 00011v2 18 jun 2019 contents", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Feynman graphs and correlators of CFT \u2013 the strongly -deformed N=4 SYM theory ", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Bethe-Salpeter equation for four-point correlators and conformal data", "Text": "5 the four-point correlation function 49", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Exact four-point correlations function for O1(x)=O2(x)=1(x)", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Exact four-point correlations function for O1(x)=1(x) and O2(x)=2(x)", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "6", "Section": "6 Correlation functions at weak coupling from Feynman diagrams", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "7", "Section": "7 Conclusion and discussion", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "A", "Section": "A Notation and conventions", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "B", "Section": "B The -deformed N=4 SYM theory", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "C", "Section": "C The uniqueness relations", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "D", "Section": "D Eigenvalue of the fermionic graph-building operator t", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "E", "Section": "E Cancellation of the spurious poles", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "F", "Section": "F Operator mixing and logarithmic multiplet", "Text": "1 tr sector 64   in spite of the considerable simplifications in the properties of cfts   a considerable progress in this direction has been achieved due to the conformal bootstrap methods based on the basic properties of cfts following from the conformal symmetry, such as crossing symmetry in various channels for the four-point correlation function  but this approach stays to a great extent experimental, based on heavy numerical computations rather than on explicit analytic formulation of the final result  however, the effcient all-loop solution of these problems is still hindered by outstanding technical complexit  the gauge fields and the gaugino decouple in this limit and one is left with three complex scalars and three complex fermions with certain chiral structure of interactions ,).  it was further studied inin particular, by the asymptotic bethe ansatz method  we will employ this name in what follow  the planar feynman graphs for typical physical quantities in such a bi-scalar theory appear to have, at least in the balk, the fishnet structure where the massless scalar propagators form a regular quadratic lattice this theory will be called in what follows the bi-scalar, or fishnet cf  the fishnet graphs of simple shape, such as a torus, appear to represent an integrable statistical mechanical system remarkably, there exists also an integrable generalization of the fishnet cft to any dimension d among the studied quantities are anomalous dimensions of the operators tr dominated by wheeltype fishnet graph  the biscalar fishnet cft gives a unique opportunity of the study the single-trace multi-point correlators and of the related exact planar scalar amplitudes, revealing their explicit and well-defined yangian symmetry one is even able to compute exactly, using the above mentioned exact four-point correlators, the simplest non-planar scattering amplitude the bulk integrability of all three cases of regular fishnet planar graphs was predicted in it is worth noticing that, unlike the bi-scalar fishnet cft, the reasons for the integrability of this model remain mysteriou  the paper explores an interesting subject of study of non-unitary spin chains, having a rich and complicated structure of the spectrum, including the jordan cells as specific multiplets of state  the jordan multiplets leading to the logarithmic behavior in non-unitary cft'sare noticed and studied perturbatively in the fishnet cft in addition, they share many basic common features with unitary cfts and help to understand their general feature  first of all, we will give the complete description of the bulk structure of feynman graphs a pictorious way to describe these graphs is to introduce the regular triangular lattice and the to do all possible baxter moves of all three types of lines, as shown on fi  these configurations should be summed up, so that the collection of such graphs could be called the dynamical fishne  then we will compute exactly the 4-point correlation functions of certain short, protected scalar operators, similar those obtained in fishnet cft for that we identify all the graphs contributing these quantities and sum them up using the bethesalpeter approach helped by the conformal invarianc  we study in detail the related perturbative expansions of these correlators, as well as their strong coupling limit  we suppressed in the last equation the spinorial indices assuming the scalar product of both fermions in each ter  the graphs of the first line represent the quartic scalar interactions and the ones in the second line are the yukawa interaction  tick solid lines and dashed lines represent scalar and fermionic propagators respectivel  arrows symbolize the fixed orientation of the vertices and, according to our notation, it points always to the fields with bars or dagger  the second chirality of yukawa interactions   this represents a step forward,   we will also describe the general bulk structure of the underlying planar graph  indeed, one interesting feature of those models is the drastic simplification of their weak coupling expansions in terms of feynman diagrams in the planar limi 1, connected by scalar and fermionic propagators  the arrows indicate the fixed orientation of the interactions,   in a propagator it is directed from a field to its hermitian conjugat  an essential feature of is the absence of the hermitian conjugate of every interaction vertex, or of the vertices obeying the reality conditio  the chirality of this theory makes it non-unitary 1apart from the double-trace vertices whose role will be discussed below - 5 and plays a crucial role for the underlying conformality and integrability in the 't hooft limi  in fact, in the absence of the hermitian conjugate vertices, all the graphs which could renormalize the couplings and the mass are non-plana  as a consequence, we will see in se 2 that the planar weak coupling expansion of physical quantities   this lattice structure is richer, and more dynamical than in the bi-scalar theory where the unique regular square fishnet structure dominates at any order in perturbation theor  they are needed to renormalise the 2-point correlators of the local operators tr, tr and tr respectivel  let us turn to the theory with the double-trace terms to make the theory conformal at the quantum level, one needs to tune the doubletrace couplings to a fixed poin  this conformal theory appears also to be integrable and its spectrum of anomalous dimensions can be treated by such a powerful tool as quantum spectral curvezamolodchikov4, by methods of conformal su quantum spin chai  the obvious physical defect of such cfts is the loss of unitarit  moreover in the ads/cft context, this fact can be interpreted as the presence of true tachyons in the bulk on the string theory side notice that both the fixed points and the anomalous dimensions are complex conjugate, as expecte  in se 3 and se  non-unitary cfts are usually logarithmic  with an interesting, logarithmic behavior of certain correlator 2 the bulk structure of large planar graphs let us try to describe the general structure of an arbitrarily big feynman graph in the bulk, far from the boundarie  the generic picture is illustrated on fi  in fi  in this framework, a set of parallel lines represents any combination of fermionic and scalar propagators of a given flavou  this system of three dotted lines forms a lattice which combines the features of both regularity and irregularit  any such lattice can be obtained from the regular triangular lattice by arbitrary baxter moves of all lines: displacements in the direction orthogonal to the line,   conserving its directio  the links of the resulting lattice are propagators while nodes are quartic effective interaction  the intersections correspond to six different effective vertices that can be written in terms of the usual ones following the map given in ta  which propagators enter the corresponding crossing the quartic interaction can involve four scalars, four fermions or two of eac  moreover, we chose the directions of the arrows to be consistent with the feynman rules in fi  depending on the orientation of the mixed interactions we will refer to them as crossing or scattering interactions as in ta  given three sets of parallel lines crossing each other with quartic interactions, the resulting irregular lattice is formed by a finite set of convex polygon  the smallest possible n-gon is a triangle and the largest one is a hexago  the local interaction of lines with only two directions forms a square lattice as in since we are considering three colors, we can have three different squares depending on their direction  indeed let's start with the crossing of three lines with three different direction  locally, they form a triangle that can have two different orientation  since we can add a line of any color and there are two possible triangle orientations, we can draw 6 different square  iterating this cutting procedure by adding one and two lines we obtain pentagons and the hexago 2 in terms of the feynman rules of fi  number of different ways the same polygon can appear in the grap 1 for the bulk topology represented in fi  the diagrams at the two sides of the figure represents the parts of the graph in the light-blue circles in terms of real vertices of fi 1 according with the rules given in ta  we stress that given a set of effective vertices, the translation in real vertices is uniqu  the structure of the fishnet bulk is very rich, indeed once the topology of the lattice is defined as in fi  now we can estimate the number of feynman diagrams for a given topology of the dottedfishnet bul  this number has the sum of all the nd's for all the polygons as an upper bound and we can estimate its order of magnitud  then the number of possible feynman diagrams for the topology of the fishnet bulk given in fi  one of those configurations is represented in fi 3 can be interpreted as one planar graph contributing to an n-point functions of the kinddrawn in terms of effective vertice  as it results from ta 1, each effective vertex can be replaced in a univocal way in terms of structure made of real vertice  of course must have zero overall r-charge, to have a non-zero answe  the ends of the cut edges represented external fixed coordinates and the integrals were taken over all vertices inside the dis  similarly, for each of the quantities there exist a collection of graphs of the disc shape cut out of the lattice of the type drawn on fi  we present an example in fi 4, where the disc is drawn on the concrete realization of the lattice as given in fi  a big difference   this corresponds to different realizations of a disc segment of the dotted-lattice in fi 2 with boundary conditions fixed by the external leg  the number of possible graphs can be estimated by considerations of the previous subsectio  this single-trace correlator can be used to define the scattering amplitudes via lehmann-symanzik-zimmerman procedure, by going to the dual momentum space and taking on-shell external momenta, in the spirit of the papers it is worth noticing that not all the planar single-trace correlators are obtained out of this procedur  indeed certain external states can be cut out only drawing a circle on the actual feynman graph where all propagators are explicitly draw  it would be interesting to show the yangian invariance of these single-trace correlators, in the same spirit as it was done in for bi-scalar cas  in this paper we will limit ourselves by the proof of integrability of the model which will be given in the next subsectio  it mixes together a square lattice structure of quartic scalar interactions and the brick-wall domain made by yukawa interaction  it represents one of the conserved charges generated by the transfer matrix of the integrable quantum su spin chain of l sites in the scalar = representation gray blobs are external coordinates, black dots are integration points and we denoted lines which coincide due to periodic   with blu  in the middle: the result of integration over yukawa vertice  it satisfies the yang-baxter equation rijrikrjk = rjkrikrij black dots are integration points, while gray blobs are external coordinate  figures on the left and on the right are respectively the   of both sides can be transformed in the hexagonal object in the middl  first the triangle is opened into a star integral using or doing so, each of the three black dots will become the end of only three line  then integration can be performed byand leads to the hexago  graphically represented in fi  indeed follows immediately from our derivation straightforwardly shows that from the point of view of integrability the regular square lattice and the brick-wall lattice built by yukawa vertices can be combined into the same integrable structure and form a mixed lattic  this concludes the demonstration of integrability of the two-coupling model then we will extract from it the ope data, anomalous dimensions and structure constants, for length-2 unprotected operators exchanged in the s-channel of the correlation functions can be remarkably written as a geometric sum of primitive divergencies in the perturbative expansio  for this reason, we will study those diagrams with the help of the bethe-salpeter equatio  in the following we will review the bethe-salpeter method pointing out how to extract the spectrum and the ope data from the four-point functions in se  however, at the fixed point, their combination is finite due to conformal symmetry these integral operators commute among themselves and they are diagonalized by the same basis of conformal triangles - the 3point correlators of the protected operators with un protected operator with spin, described belo  3such insertions should always split a graphs, and its color structure, into two disconnected piece hi in se 4 and se 5, we will verify that diagonalizes these hamiltonians and perform a direct computation of the eigenvalue  this symmetry is indeed satisfied for every studied case,and before studying the integral inwe want to focus on the role of the doubletrace hamiltonian and its eigenvalues in the perturbative and bethe-salpeter approache  - 20 to find the correlation functionwe have to sum up diagrams of the kind shown at the beginning of this sectio  since in general the integrals over the positions of the single-trace vertices develop ultraviolet divergencies at short distances, one needs the double-trace interactions to produce other uv divergent contributions which cancels against the  therefore, the weak coupling expansion of the four-point correlation function remains uv finite at any order as expected for protected o1 and o  in the context of the bethe-salpeter equation the story is slightly differen  since we want to write go1o2 in the standard ope form, we will consider the limit in which two of the external points are approaching,   in this case, the bosonic and fermionic operators do not develop uv divergencies with this argument, we are able to neglect the double-trace contributions when we compute the four-point function go1o2 with the bethe-salpeter metho  the physical poles are given by the zeros of denominator under the integral,   in ap which happens to be satisfie  finally go1o2 is given by the sum of only the residues at the physical pole the scaling dimensions of the exchanged operators and the ope coeffcient  since the four-point correlator constructed from the second operator of is trivialin the following two sections we will analyze the remaining tw  6this condition in the ope is equivalent to the restriction re > 0 in the contour integral in the black dots stand for double-trace vertices and tick and dashed lines correspond to bosonic and fermionic propagators respectivel  the propagators are not crossing and are curved to stress the fact that they have a cylindrical topolog  this correlation function was extensively studied in in the simplest case of the family of theories we are inspecting,   the bi-scalar theory in the following we will study the correlation function with the bethe-salpeter metho  in fi  in se  white dots represent external points and black dots integration over the full space r  in se 6 we will present in detail how the relation between single- and double-trace terms is crucial for the setting of the fixed point the first two orders of the perturbative expansion are given by the diagrams represented in fi  the kernels transform covariantly under conformal transformations7, then the corresponding hamiltonian integral operators commute with the generators of the conformal grou 2 eigenvalues of the hamiltonian graph-building operators writing the four-point correlation function in the standard ope form, as presented in detail in se 3, involves the computation of the spectrum of the graph-building operators substituting in the latter the kernels and using the definitionwe will end up with a set of integrals that can be computed with the help of the star-triangle relations presented in ap  the fact that all the integrals that we have to compute can be computed by means of the star-triangle relations is a consequence of the underlying conformal symmetr  7the easiest way to prove it is to apply the inversion operator to for the fermionic hamiltonian it is convenient to use its representation after the two integrations will be performed later in5 derived in the computation can be otherwise done in a more tedious and explicit way as presented in detail in this is a new object, absent in the similar correlator of bi-scalar model treated in 8it is convenient to perform this and other similar computations, together with the pictures, with the str package let's focus on the relation in fact, the same integral of appears in the study of the spectrum of the graph-building operator associated to the 2-magnon correlation functio  the spectrum of the exchanged operators is defined by the solutions of the equation for the physical poles for each value of the twist t there are two solutions; they describe the exchange of an infinite tower of local primary operators in the ope of remarkably, the expressions in square brackets inare in fact rational number  in both cases, we present only the first few terms since the following ones are cumbersom 5, the sum of the two corresponding ope contributions has a well defined expansion  twist-4 operators start mixing with each othe  at this stage the log-cft effects due to chiral interaction vertices in show u  we will restrict from here on most of our analysis to solution of twist two and four, whose contribution to the ope expansion appears to be enough for complete description of the first non-trivial order of the weak coupling expansion, confirmed by direct computations in terms of feynman diagram  9 we are grateful to   korchemsky for the enlighting discussion about this poin 10 alternates bosonic and fermionic wheels attached to the diagrams with two quartic or four yukawa single-trace vertice  the zero-spin case presents some peculiar behaviour  the same behavior was noticed in and the reason is similar to the one explained above the divergence in the expansion of the scaling dimension of the twist-two operator is not a surpris  hence, the contribution of the double-traces is needed in this case to produce a non-vanishing term that cancels this divergence at weak couplin  again, we stress that at finite couplings the solutions of are well-defined even at zero spi  of lowest twis  this becomes clear if one expands the eigenvalues appearing in thus we want to obtain the spectrum of exchanged operators for each theory of this family simply taking the limit on the couplings in the spectral equation of the most general doublyscaled theor  first of all, we recall the well-known result for the spectrum for the simplified lagrangian also known as 4d bi-scalar fishnet cf  by the bethe-salpeter method it is possible to compute the correlator at all-loops, since its perturbative expansion is generated only by a bosonic graph-building operator hb ofthen we can extract the non-perturbative scaling dimension of the exchanged operators in the ope s-channe  the analytic properties of those solution and their weak- and strong- coupling expansions have been studied in detail in even directly on the weak- and strong-coupling expansion  in the ta 2 we present the summary of our result  notice that in this case the number of solution of twist-four operators reduces to a single one, while the higher-twist operators get protecte  in this case, due of the restoration of one supersymmetry, the operator of twist-two is protected as pointed out in and confirmed by our computation plugging these eigenvalues into and performing the derivative, we obtain a rather cumbersome result that we will present in the next paragrap  again, the expressions in square brackets are in fact rational number  indeed, as discussed in se  naively, the expansion looks the same as the one of the structure constant of the bi-scalar modelbut actually it is no  indeed, one can notice from the definition that the ope coeffcient in our model depends explicitly on the couplin 5 the four-point correlation function once the conformal data in sec 4 is computed, one can determine the four-point function by means of from the study of the spectrum of exchanged operators in se 3 it turns out that infinitely many operators are exchanged in the ope channe  despite of the presence of singularities in the weak-coupling expansions of scaling dimensions and ope coeffcients, their combination in is well-define  indeed, plugging the conformal data intowe obtain an expansion in powers of the couplings that is compatible with the interpretation of the correlation function as a sum of feynman diagrams in perturbation theory this correlation function was trivial in the bi-scalar model but in the general doublescaled theory it has a rich diagrammatic structur  in the following we will compute the conformal data of with the bethe-salpeter metho  the black squares and dots stand for single- and double-trace vertices respectivel  tick lines are bosonic propagators and dashed lines fermionic one  the propagators are not crossing and are curved to stress the fact that they have a cylindrical topolog  in fi 12, we present an example of a generic feynman diagram contributing to in se  the contribution given by vertices and is crucial to calculate the fixed point in se 6 we will present this computation in detai  the first few orders of the perturbative expansion are given by the diagrams represented in fi  the fermionic kernel is more involve  considering the diagrams in fi 13 and 13 and their integral representationit is clear that they are not generated by the same repeated hamiltonian operato  white dots represent external points and black dots - integration over the full space r  these hamiltonians, graphically represented in fi  the kernels and transform covariantly under conformal transformations, then the corresponding hamiltonian integral operators commute with the generators of the conformal grou  following the conventions we are using for the raising and lowering of spin indices, as explained in ap  then the other hr kernels have to alternate upper and lower indice 2 eigenvalues of the hamiltonian graph-building operators in order to compute the four-point correlation function with the operator method presented in se 3, one has to obtain the spectrum of the graph-building operators and as was done in se 2 for another four-point correlato  substituting in the latter the kernels and and using the definitionwe will end up with a set of integrals that can be computed with the help of the star-triangle relations the fact that all the integrals that we have to compute can be solved by means of the star-triangle relations is a strong evidence of the underlying conformal symmetr  fermionic eigenvalue: the fermionic eigenvalue is defined in indeed, due to their fermionic structure, individually they will turn the scalar conformal eigenfunction into a fermionic stat  then e  in this limit we are able to compute the resulting integral going to momentum spac  we leave the details of the computation in ap  going through the calculation we obtain the spectrum of the exchanged operators is defined by the solutions of the equation for the physical poles four of them correspond to the scaling dimensions of physical operators satisfying this constrain  the remaining three solutions are their shadow operator  changing the couplings we can pass from one sheet to another, observing the transitions between various dimension  the branch points correspond to the collisions of physical dimension  in contrast to this case, the spectral equation for the previous four-point function is not algebraic and its riemann surface contains infinitely many sheet  in both cases we presented only the first few terms of the expansions since the following ones are quite cumbersom 5 the sum of the corresponding ope contributions has a well defined expansion taking into account the quantum numbers of the external state tr we can list the operators of twist-2 and -4 which mix, obtaining the expression for the exchanged operators in the ope from the diagonalization of the mixing matri  as pointed out in ap  the zero-spin case presents some peculiar behavior  moreover, similarly to the case of the spectrum of the length-two operator in the correlation function computed in se  the explanation is the same of the previous cas  hence, the contribution of the double-traces is needed in this case to produce a non-vanishing term that cancels this divergence at weak couplin  again we stress that at finite couplings the solutions are well-defined even at zero spi  strong coupling expansion: equation has 6 solutions at strong coupling  t the weak coupling case can be identified by the analysis of the exchanged operators in ap  and the remaining solutions are associated to the shadow operator  in ta 3 we summarize our result  in this case, the reduction doesn't give a unique spectru  this is clear when considering the spectral equation in which only the first term on the left-hand side will contribute and clearly it describes two protected solutions for the correlator we are considering, any choice of the vanishing couplings leads to protected solutions, as expecte  then their spectrum can be easily read offapplying the equal couplings limit, for example at weak coupling, to the expansions and plugging the eigenvalues into and performing the derivative, we obtain explicit but rather cumbersome result that we will not present her  in the following paragraphs we will consider its weak- and strong- coupling expansion  indeed, as discussed in se 2 for twist-2 solution  strong coupling expansion in se  since in this limit the coeffcient c2 in is dominant   the one ofthe first terms of the expansion are exactly the same as in x155 the four-point correlation function once we computed the conformal data in se 3 and se 4, we can determine the fourpoint function by means of from the study of the spectrum of exchanged operators in se  at weak coupling they are computed in andand at strong coupling - in the structure constants are defined by and they are computed at weak and strong coupling inand respectivel  despite the presence of singularities in the weak-coupling expansions of scaling dimensions and ope coeffcients, their sum in is well-defined and non-singula  indeed plugging the conformal data intowe obtain an expansion in powers of the couplings that is compatible with the interpretation of the correlation function as a sum of feynman diagrams in perturbation theory expanding at higher order in the couplings we obtain a cumbersome resul  however, we notice that the maximum transcendental weight of the involved functions grows linearly with the orde  6 correlation functions at weak coupling from feynman diagrams in se 4 and se 5, we have analyzed two different four-point functions computing their conformal data by means of the bethe-salpeter metho  with this procedure we were able to diagonalize the graph-building operators and write exact equations for the spectrum of exchanged operators even though we ignored on the way the contribution of the doubletrace interactions the double-trace counterterms are necessary in the action to have a consistent description of the double-scaled theory in the perturbative regime and in particular for the restoration of conformal symmetr  in this section, we will study the weak coupling expansions of the four-point functions related to the operators and clarify the role of the double-trace terms for this expansio  as we already mentioned, bosonic and fermionic wrappings in the related feynman graphs develop uv divergencies at short distance  adding the double-trace vertices in the perturbative expansion we will be able to determine the conformal fixed points canceling divergencies generated by the single trace term  however, they will not affect the finite coupling solutions computed in the section abov  the double-trace counterterms are given by the lagrangian for any four-point function only one double-trace term is contributing generating a new local four-scalar vertex this fact is crucial to ensure that conformal symmetry is restore  if we focus on the feynman diagram expansion of the four point-functions we have to deal with divergent integral  one important observation is that the diagrams containing fermionic contributions produce the same divergence as the bosonic one  however, expanding at weak-coupling in terms of feynman diagrams, one can demonstrate order-by-order how conformal symmetry is restore  in the following sections, we present some examples of this mechanism for the four-point correlation functions associated to the operators these vertices insert into the graphs identical primitive divergencie  vertices, which works well for finite coupling  since any diagram in fi 10 is uv divergent, we will re-introduce in this section the double-trace counterterms in the perturbative expansion in order to make the weakcoupling expansion uv finite and to restore conformal symmetr  in terms of feynman diagrams, we have to compute the graphs given in fig  the first correction is given by the diagram in fi  this term corresponds to the feynman diagram in fi 10 and its integral representation in four dimensions is given in the last line of since we are only interested in the computation of the uv divergent part of the diagram, we can avoid computing the whole integral in dimensional regularization and proceed in a more naive wa  indeed, representing the integral as in the last line of and considering that the fermionic hamiltonian can be written as a combination of the bosonic one and some finite reminder function as inwe know that all the uv divergence is arising from h and notice that it matches exactly the prediction given in this perfectly matches the same quantity computed via ope, with conformal data fixed by the bethe-salpeter method since we want to analyze 10this choice is coherent with the sign convention used in se  the weak coupling perturbative expansion of the correlator using feynman diagrams, the graphs generated by this operator are needed in order to cancel the uv divergencies arising from the diagrams in fi  there is here a substantial difference   for this reason, we redefine the double-trace coupling as taking into account both contribution  in terms of feynman diagrams, we have to compute the first three graphs given in fi 13 and the ones in fi  the first correction is given by the diagram in fi 16 that is a half11 of the one computed in the previous section in16 and fi 13 and they are divergen  this term corresponds to the feynman diagram in fi 13 and its integral representation in four dimensions is given in the next-to-the-last line of in this case, our goal is also to identify the uv divergent part of the diagra  thus we will not compute the whole integral in in dimensional regularization but we will rather proceed following the method of the previous sectio  performing two integrations by means of the star-triangle relation and then simplifying the spin structure with the help of andone can identify a divergent 11the two diagrams have a different symmetry factor given by the wick contraction  - 54 integral of the same kind as intogether with a finite remainder integra that perfectly matches the same quantity computed via ope with conformal data fixed by the bethe-salpeter method these graphs can be dubbed as dynamical fishnet, since, unlike the usual regular fishnet of the bi-scalar model they have a certain dynamics preserving at the same time a kind of irregular fishnet structure shown on fig  2,  interestingly, this bulk structure is neatly realized as feynman graphs describing arbitrary 12this choice is coherent with the sign convention used in se  4,  it would be very interesting to find the realisation of the yangian symmetry of these correlators, and of the related planar amplitudesgeneralizing the results of for the bi-scalar cf  it would be the neatest demonstration of the integrability of the full mode  in se  a new phenomenon presented in the correlators of the full theory is the non-perturbative behavior of certain individual ope data - anomalous dimensions and structure constants of exchange operators, in the weak coupling limi  but the perturbative behavior of the four point correlator is restored in the sum over all ope term  we also demonstrate the relevance of the double-trace terms for the correct feynman graph interpretation of our results obtained via bethe-salpeter conformally symmetric procedur  as concerns the study of the structure constants, the first all-loop results for multi-magnon operators in bi-scalar fishnet cft have been obtained in the very recent paper acknowledgements we are thankful to   levkovich-maslyuk for discussion  our work was supported by the european research council the work of   is supported by the german science foundation under the collaborative research center 676 particles, strings and the early universe and the research training group 167  where the summation is assumed   doubly and triply repeating indice  c the uniqueness relations we present here some useful formulas for integrals of the star-type, namely three propagators of various types linked in one integration poin  under the condition of conformal invariance, the integrals considered below can be reduced to the computation of simple convolution  such formulas are actually particular reductions of a more general one derived in as shown in se  let's focus on the integral in the first line of e cancellation of the spurious poles in order to confirm the validity of equation we should show that the physical poles given by the zeroes of the spectral equation are the only contributions to the fourpoint correlators under stud  it is easy to check that plugging intoone is left with the conditionwhich means that together with are a suffcient condition for moreover, equation has been checked inwhere it was enough to state the cancelation of spurious poles in tr secto  let us verify the second condition at even integer   equation actually coincides with the vanishing condition for spurious contribution in the one-magnon tr sector of bi-scalar theory, and is verified in we can finally check for the sector t  f operator mixing and logarithmic multiplet in both sectors tr and tr of our theory the exchanged physical operators in the ope s-channel of the 4-point correlators under analysis present mixin  and they do not mix among each othe  the physical interpretation of jordan blocks leads to the formulation of logarithmic cft in the example the block corresponds to a rank-2 logarithmic multiple caetano for its biscalar reduction and some examples of its occurance in the context of fishnet cft have been presented in the equation shows physical solutions for every even twis indeed for any s there is no other twist-2 conformal primary with charge this fact is apparently in contrast with the presence of only two twist-4 exchanged operators in the ope expansion of se  one can actually check that there is no conjugate transition to at any orde  logarithmic operator  dealing with single trace operators made of two elementary fields the effect of chiral interaction is cancelled by the trace cyclicit  the resulting transitions are symmetric and the anomalous dimension matrix is diagonalizabl ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "unveiling hidden orders: magnetostriction as a probe of multipolar-ordered states adarsh   examples of such hidden orders include spin-nematic order in quantum magnets, and quadrupolar or higher multipolar orders in various correlated quantum material  we show that the resulting magnetostriction coefficient is directly proportional to the octupolar order parameter, providing the first clear access to this subtle order paramete  along other field directions, we show that the field dependence of the magnetostriction provides a window into quadrupolar order  our work provides a springboard for future experimental and theoretical investigations of multipolar orders and their quantum phase transitions in a wide variety of system  the quantum mechanically defined multipole moments provide a useful measure of the resulting complex angular distribution of the magnetization and charge densities most conventional broken symmetry phases in solids involve the magnetic dipole moment of the electro  studying the mysterious ordering patterns of higher order multipoles is also often rendered challenging since they typically coexist with conventional dipolar moment  the quest to probe such orders has led to novel experimental techniques,  g, elastoresistivity to elucidate the quadrupolar order associated with orbital nematicity in the iron pnictide  motivated by these experiments, in this work we theoretically discuss how magnetostriction provides a novel means to directly probe multipolar order parameter  the central observation of this paper is that an applied magnetic field allows for a linear coupling between lattice strain fields and a uniform octupole moment which depends strongly on the applied field directio  in the absence of a dipolar moment, this enables measurements of the magnetostriction to directly reveal the hidden octupolar order paramete  we investigate such field-scaling behaviour of the magnetostriction for various magnetic field directions by employing a symmetry-based landau theory, which allows us to highlight the universal aspects of the physics and to show that this idea is broadly applicable to a wide class of material  our work is motivated by a recent series of beautiful experiments on the pr-based cage compounds belonging to the pr220 family which form an ideal setting to study multipolar moments and associated hidden orders uncovering and understanding the pattern of multipolar ordering across this family of materials has remained an important open proble  the nature of the quadrupolar ordering in these cage compounds has been indirectly examined with a few techniques such as ultrasound experimentsas well as nmr measurements ).  more recently, magnetostriction and thermal expansion strain experiments have also lent themselves as possible probes to study the transitions and the underlying quadrupolar phas  in this study, motivated by showing how magnetostriction behaves in the presence of quadrupolar and octupolar orders, we focus on a landau theory which permits both antiferroquadrupolar ordering and ferro-octupolar ordering we study the scaling behaviour of the relative length change of the system with respect to an applied magnetic field strength along different field direction  our studies predict a linear-in-h scaling behaviour for length changes for a magnetic field applied along the direction for t < t  the coeffcient of the linear-in-h term,   the magnetostriction coeffcient, is directly proportional to the ordered ferrooctupolar moment, thus providing a clear and distinct means to directly probe this order paramete  this direct relation between the linear-in-h magnetostriction coeffcient and the ferrooctupolar order parameter for a magnetic field along the direction is one of the central results of our pape  furthermore, we predict a characteristic hysteresis in the octupolar moment and the associated parallel length change as a function of magnetic field, arising from the symmetry-allowed cubic-in-h coupling of the magnetic field to the octupolar momen 65  our theoretical results for magnetostriction in the presence of octupolar order thus lend strong support to the idea that these seminal experiments herald the first and unambiguous discovery of octupolar orde  table 2 provides a complete summary of the scaling behaviour of a variety of length change directions under different magnetic field directions, including the effect of octupolar as well as quadrupolar order parameter  our predictions are expected to aid the investigation and identification of multipolar moments, as well as provide key signatures that indicate the presence of specific multipolar orderin  in particular, since we show that magnetostriction provides a direct probe of the octupolar order parameter, future experimental studies of this observable may shed light on the thermal and quantum critical behaviour associated with octupolar ordering in these compounds and a wide range of other material ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Landau Theory of Multipolar order", "Text": "we present in this section, for the sake of self-containedness and to specify our notation, the landau theory of multipolar order first introduced in re  surrounding each pr3+ ion is a frank-kasper cage the octupolar momen  the ordering of these multipolar degrees of freedom acts as a mean field on the pseudospins, and breaks the degeneracy of the non-kramers double  the local td symmetry instilled by the fk cage provides a constraint on the possible terms permitted in the landau theor  the generating elements of td are s4z and c31 the behaviour of the multipolar moments under these symmetry constraints is detailed in table s1 in si   in this work, we focus on a system where the primary order parameters are afq and f  such mixing does not occur for the octupolar order parameter; motivated by explaining experiments on prv2al20we choose to work with only fo order and ignore the afo order paramete ", "Subsections": [{"Section_Num": "A", "Section": "A Interacting multipolar orders", "Text": " phase diagram at zero magnetic field the temperature regimes studied in se  here the critical temperatures are tq and to a measure of how close the two transition temperatures are to each other is provided by the ratio /.  the remaining non-trivial terms in eqn  we present in fi  the dotted vertical lines denote specific temperature regions studied in se  coupling of magnetic field to multipolar moment  in order to study magnetostriction, it is important to understand how the magnetic field couples to the multipole moment  due patri et a  in the above e  based on the form of the coupling in e  the first line in e  13 is the symmetry allowed coupling to the afq the third line involves couplings permitted due to pure symmetry reasons that renormalize the mass terms of the afq and f  physically they arise from conduction electron mediated magnetic couplings ; similar coupling to the octupolar moment is also permittedwhich is formally introduced in se  3 via the magnetic field assisted coupling of the octupolar moment to the lattice strai  in the subsequent sections, we discuss magnetic fields applied along theand direction  we also formulate the relative length change expression in terms of the elastic strain component    elastic energy of a cubic crysta  note that we use the common abbreviation of the elastic modulus tensor's indices   of oh group; here the subscript g indicates even under time-reversal and spatial inversion we henceforth use e  15 for the cubic crystal's elastic energ  general expression for relative length chang  we apply e  16 to particular length change directions in se  symmetry allowed coupling of multipolar moments and cubic crystal normal modes we now turn our attention to the problem of coupling the lattice normal modes of the cubic crystal to the multipolar moment  we recall that the cubic crystal structure supports macroscopic normal modes that transform as irrep  of oh, while the landau free energy of the multipolar moments is constructed subject to symmetries of the local td environmen  the symmetry constraints on f ensure that in principle only select normal modes of the crystal that transform as the irrep  of td are permitted to couple to the multipolar moment  in the present case, all the cubic normal modes presented in e  15 also transform as irrep  under tdand so all of the aforementioned strain modes can participate in the couplin  in the next two subsections, we consider the direct coupling of quadrupolar moments to the cubic normal modes, and then tackle the magnetic field assisted octupolar coupling to the lattice normal mode    coupling of quadrupolar moment to lattice strai  coupling between the quadrupolar moments and the lattice normal modes appears as a natural choice, as the quadrupolar moments and the lattice strains are both even under time-reversa  of td this similarity in how they transform under td allows a linear coupling between the aforesaid lattice normal modes and quadrupolar moment  thus, the landau free energy of the multipolar moments shown in eq  note that we include the coupling of the lattice strain to the quadrupole moment on each sublattic  substituting the expression for the minimized lattice strains from eq  18 back into e  coupling of octupolar moment to lattice strai  a direct linear coupling between the octupolar moment txyz and the lattice normal modes is not permitted, as the octupolar moment is odd under time-reversa  thus, the landau free energy of the multipolar moments shown in eq  6, and go is the coeffcient of coupling between the octupolar moment and lattice strai  we also include another symmetry allowed direct coupling between the magnetic field and the same lattice normal modes physically, this kind of term could arise from the independent coupling of the magnetic field and lattice strain to the conduction electrons ; we discuss this matter briefly in se  substituting the expression for the minimized lattice strains from eq  21 into e  7, fmag is defined in e  19 and 22, respectivel  we present in si c, the values of the landau parameters chosen for the study conducted in this and the subsequent section  patri et a  | 5 the scaling relations can be inferred by substituting the expressions for the strain in eq  18 and 21 into e  16 to yield the following expressions in eq  here we have used the definition of the normal modes introduced in e  24; this definition will be helpful in table 2 belo  the constant and quadratic scaling-coeffcients depend on the value of the landau parameters as well as the temperature being probed; the quantitative value is thus not of great importance for the scaling behaviou  the key point to retain is that the value of these coeffcients is small as compared to the conduction electron generated termsreflecting the weak, parasitic nature of the fq momen  we present in table 2 the scaling behaviours of length change parallel and perpendicular to the three primary magnetic field direction  the conclusions that can be drawn from e  24 and table 2 are strikin  one can consider the scenario of the zero field-limit being achieved by applying a magnetic field along and tuning it to zero; this causes one of the three degenerate solutions to be chosen   secondly, the hitherto mysterious octupolar moment can now be determined by measuring the slope of the linear-in-h behaviour of the length change both parallel and perpendicular to magnetic fields applied along the direction; the linear behaviour is also apparent for perpendicular length changes to magnetic field applied along directio  this provides a clear signature for the onset of the octupolar ordering as well as a means to study the general behaviour of the octupolar moment with respect to other external variables such as temperature,   thirdly, for magnetic fields applied along the direction the length change parallel to the magnetic field has twice the slope of the linear-in-h term and twice the quadratic background as the length changes perpendicular to the fiel  this provides a distinct verification as to the validity of the theor  in next subsections we elaborate on important details regarding the multipolar moments and examine their scalingdependency on the magnetic field, to provide justification of the results presented in table   in minimizing the free energy in e    the complex-angle dependent parts of e  24 are included in the definition of the quadrupolar-lattice strain coupling, gq; the exact form of the complex angle term can be inferred from consulting e  the octupolar-lattice strain coupling is denoted by g  the numerical solution of the order parameters in the three temperature regimes is presented in si d fi  s  extremizing e  we affrm these scaling behaviours by performing a thorough numerical study of the full landau free energy in the temperature regimes of interes  as can be seen in si d fi  s1, the fq moment is indeed an even function-in-  for magnetic fields along this direction, unlike the direction, there is a lack of harmony in the complex angles that minimize the landau free energ  thus a simple analytical solution for the scaling behaviour of the relative length change is not as easy to derive s2 and corroborated by numerical fits of the order parameter  for magnetic fields applied along this direction, all the multipolar moments couple quadratically to the magnetic fiel  we first discuss the magnetic field dependency of patri et a  | 7 the quadrupolar moment  s  the quadratic-in-h scaling of the octupolar moment can be observed by performing a simple analytical approximation, where for the sake of simplicity, we assume only pure fo ordering thus substituting this approximate solution into e  24 yields a linear in h behaviour in the length change in the presence of all order parameters, we obtain the same scaling relation of the octupolar moments   the even-in-h scaling behaviour of the multipolar moments is apparent qualitatively in si d fi  s  it is important to note that this scaling behaviour was performed under the assumption that we could neglect the o coupling between the octupolar moment and the magnetic field   neglected hxhyhzm in the landau free energ  for magnetic fields along and this term is zero, and plays no role anyway, but for magnetic fields along the direction it is non-zero more importantly, the cubic-in-h coupling breaks the z2 symmetry of the octupolar momen  a similar phenomena is observed in usual ferromagnetism, below the ordering temperatur ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Cubic Crystal Normal modes, and Relative Length Change Expression", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Symmetry Allowed Coupling of Multipolar Moments and Cubic Crystal Normal Modes", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Relative Length Change Under Magnetic Field Along Different Directions", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Hysteretic Behaviour of Octupolar Ordering", "Text": "we are motivated in this section by recent unpublished experiments where hysteretic behaviour is observed in the length change along the direction below the supposed-octupolar temperatur  in order to incorporate -  such effects, we adapt the phenomenological approach due to jiles and atherton which has been used to study hysteresis loops in ferromagnetic and ferroelastic material  this approach identifies the order parameter as its ideal bulk value, where the landau theory includes a direct coupling ufmh3 of the ferro-octupolar moment m and the external magnetic fiel  where c is a constan  a heuristic derivation of these equations is given in the si   the algorithm to determine the total macroscopic octupolar moment is straightforwar  first we minimize the landau free energy to obtain the ideal octupolar moment   next, we solve e  33 for mi  finally, we obtain mexp by using e  we now examine hysteresis behaviour occurring at a temperature of t < t  figure 2 depicts the hysteresis behaviour for the total bulk octupolar moment mexp, which is reminiscent of the hysteresis in ferromagnet  the initial condition used to solve e  inserting this solution for the octupolar moment into e  24, we obtain the length change along the direction, as shown in fi ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "6", "Section": "6 Conclusions", "Text": "in this work, motivated by recent and ongoing experiments on pr220, we have used landau theory of multipolar orders coupled to lattice strain fields to study magnetostriction in systems with quadrupolar and octupolar order 65k in addition, we can qualitatively understand the quadratic-in-field background magnetostriction observed in these experiment  from our studies we conclude that linear-in-h scaling of the length change is observed for length changes to magnetic fields applied along the direction, and for length changes perpendicular tobelow t  moreover, the coeffcient of the linear-in-h term is directly proportional to the octupolar moment, thus giving a distinct signature for the onset of octupolar ordering as well as a means to detect/measure the octupolar momen  for magnetic fields applied along the and directions, the length changes also acquire quadratic-in-h scaling behaviou  this scaling arises from the quadrupolar moments and/or direct coupling of the conduction electrons to the external magnetic field and the lattice normal mode  the summary of the scaling behaviours is presented succinctly in table   finally, we demonstrate a characteristic hysteresis behaviour in the octupolar order parameter and the associated length change for magnetic field applied along in terms of future work, an interesting avenue to explore is that of the coupling of the conduction electrons to the multipolar moments, as well as to the lattice strain and magnetic fiel  in particular, the origin of the conduction electron term in e  20, introduced in our phenomenological model from symmetry arguments, is a fascinating direction to explore understanding the nature and role of the conduction electrons will also help shed light on the quantum critical behaviour and superconductivity in such multipolar kondo lattice systems acknowledgment  we thank piers coleman and premi chandra for helpful discussions on landau theory of magnetostriction, in particular relating to the coupling between the octupole and magnetic fiel  we also thank wonjune choi and li ern chern for helpful comments regarding the manuscrip  this work was supported by nserc of canada, and canadian institute for advanced researc  is supported by the kaist startup and national research foundation grant fazekas p lecture notes on electron correlation and magnetis  journal of the physical society of japan 78:07200  kusunose h description of multipole in f-electron system  journal of the physical society of japan 77:06471  sakai a, nakatsuji s kondo effects and multipolar order in the cubic prtr2al20 journal of the physical society of japan 80:06370  journal of the physical society of japan 85:08200  coleman p introduction to many-body physic  flouquet j on the heavy fermion road, progress in low temperature physic  vo  15, p  journal of the physical society of japan 81:08370  matsubayashi k, et a  pressure-induced heavy fermion superconductivity in the nonmagnetic quadrupolar system prti2al2  phy  re  let  109:18700  phy  re  let  113:26700  kotegawa h, et a  evidence for unconventional strong-coupling superconductivity in pros4sb12: an sb nuclear quadrupole resonance stud  phy  re  let  90:02700  kuwahara k, et a  direct observation of quadrupolar excitons in the heavy-fermion superconductor pros4sb1  phy  re  let  95:10700  santini pet a  multipolar interactions in f-electron systems: the paradigm of actinide dioxide  re  mo  phy  81:807-86  journal of the physical society of japan 66:1741-175  shiina r multipolar moments in pr-based filled-skutterudite compounds with singlettriplet crystal-field level  journal of the physical society of japan 73:2257-226  kiss a p  thesis kiss a, fazekas p group theory and octupolar order in uru2si  phy  re  phy  re  onimaru t, et a  antiferroquadrupolar ordering in a pr-based superconductor prir2zn2  phy  re  let  106:17700  iwasa k, et a  well-defined crystal field splitting schemes and non-kramers doublet ground states of f electrons in prt2zn20 journal of the physical society of japan 82:04370  sakai a, nakatsuji s kondo effects and multipolar order in the cubic prtr2al20 journal of the physical society of japan 80:06370  araki k, et a  magnetization and specific heat of the cage compound prv2al2  onimaru t, et a  simultaneous superconducting and antiferroquadrupolar transitions in prrh2zn2  phy  re  new journal of physics 7:5  phy  re  nature 417:831 ep -.  journal of physics: condensed matter 15:s196  journal of physics: condensed matter 17:528  nature physics 3:78 ep -.  santander-syro afet a  nature physics 5:637 ep -.  nature physics 5:796 ep -.  epl 89:5700  phy  re  okazaki r, et a  rotational symmetry breaking in the hidden-order phase of uru2si  science 331:439-44  rau jg, kee hy hidden and antiferromagnetic order as a rank-5 superspin in uru2si  phy  re  stewart gr heavy-fermion system  re  mo  phy  56:755-78  cox dl quadrupolar kondo effect in uranium heavy-electron materials? phy  re  let  59:1240-124  cox dl, zawadowski a exotic kondo effects in metals: magnetic ions in a crystalline electric field and tunnelling centre  advances in physics 47:599-94  phy  re  patri et a  riggs sc, et a  evidence for a nematic component to the hidden-order parameter in uru2si2 from differential elastoresistance measurement  nature communications 6:6425 ep -.  phy  re  sakai a multipolar order, non-fermi liquid and heavy fermion superconductivity in the quadrupole kondo lattice prtr2al20 tokunaga y, et a  magnetic excitations and c-f hybridization effect in prti2al20 and prv2al2  phy  re  journal of the physical society of japan 84:06370  freyer fet a  two-stage multipolar ordering in prt2al20 kondo material  phy  re  sato tj, et a  ferroquadrupolar ordering in prti2al2  phy  re  shimura y, et a  field-induced quadrupolar quantum criticality in prv2al2  phy  re  shimura y, et a  giant anisotropic magnetoresistance due to purely orbital rearrangement in the quadrupolar heavy fermion superconductor prv2al2  arxiv e-prints   arxiv:180  nakanishi y, et a  elastic anomalies associated with two successive transitions of prv2al20 probed by ultrasound measurement  physica b: condensed matter 536:125 12  ishii i, et a  antiferro-quadrupolar ordering at the lowest temperature and anisotropic magnetic field-temperature phase diagram in the cage compound prir2zn2  journal of the physical society of japan 80:09360  ishii i, et a  antiferroquadrupolar ordering and magnetic-field-induced phase transition in the cage compound prrh2zn2  phy  re  koseki m, et a  ultrasonic investigation on a cage structure compound prti2al2  journal of the physical society of japan 80:sa04  taniguchi t, et a  nmr observation of ferro-quadrupole order in prti2al2  journal of the physical society of japan 85:11370  highly anisotropic strain dependencies in prir_2zn_2  arxiv e-print  re  let  85:2188-219  kopmann w, et a  magnetic order in npo2 and uo2 studied by muon spin rotatio  journal of alloys and compounds 271-273:463-46  walstedt re the nmr probe of high-tc materials and correlated electron system  257-26  phy  re  journal of the physical society of japan 83:03470  journal of magnetism and magnetic materials 61:48 - 6  massad je, smith rc a domain wall model for hysteresis in ferroelastic material  cox dl, zawadowski a exotic kondo effects in metals: magnetic ions in a crystalline electric field and tunnelling centre  advances in physics 47:599-94  emery vj, kivelson s mapping of the two-channel kondo problem to a resonant-level mode  phy  re  cox dl, ruckenstein ae spin-flavor separation and non-fermi-liquid behavior in the multichannel kondo problem: a large-n approac  phy  re  let  71:1613-161  ramirez apet a  nonlinear susceptibility: a direct test of the quadrupolar kondo effect in ube1  phy  re  let  73:3018-302  journal of the physical society of japan 84:11471  onimaru t, et a  quadrupole-driven non-fermi-liquid and magnetic-field-induced heavy fermion states in a non-kramers doublet syste  phy  re  kusunose h competing kondo effects in non-kramers doublet system  journal of the physical society of japan 85:06470  10 | patri et a  supporting information", "Subsections": [{"Section_Num": "A", "Section": "A Symmetry transformations of multipolar order parameters", "Text": "under the symmetry constraints detailed in the main text se  1, the multipolar moments transform as denoted in table s  transformation of multipolar order parameters under generating elements of tdbond centre inversion and time reversal", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "B", "Section": "B General expression of length change in different directions", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "C", "Section": "C Values of Landau Parameters", "Text": "the values of the landau parameters are arbitrary to a certain extent, and depending on the choice of the parameters the subsequent scaling coeffcients are altere  the values for the elastic modulus tensor components are chosen to be large, because as seen in e  23 they are only responsible for shifting the critical temperatures of afq, fo multipolar order parameters with respect to magnetic field strengt  figures s1, s2, s3 presents the solutions of the order parameters from a thorough numerical study of the complete landau free energ  as can be seen, the fq moment is indeed an even function-in-  we stress on the atypical nature of the direction, in that it introduces a degeneracy into the syste  nevertheless, this degeneracy does not impact the observed scaling behaviou  hence, the value of hkink grows the lower in t we go we also note that for large enough h, the afq is tuned to zero with the ferro-like moments survivin ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "E", "Section": "E Domain Wall model of octupolar moments", "Text": "we provide a more detailed derivation of the hysteresis model of octupolar moment  the derivation follows that of jiles and atherton with a few modifications that we elaborate o  the basic premise is that of two equally large domains of octupolar order separated by a domain wal  this domain wall is assumed to lie directly on top of a so-called pinning sit  a pinning site can be any object that obstructs the motion of domain walls under the influence of a magnetic field; the true nature of the pinning site is not of great importance to the derivation presented her  the domains possess an octupolar moment per unit volume, m  we now consider the application of a magnetic field on the system that encourages the expansion of the +md domain   it is energetically favourable to have both the domains align as +m  in the absence of the pinning site, this domain wall slides over easily thus enabling the expansion of the domai  however, the pinning site obstructs this simple motio  in the spirit of jiles and atherton, we consider the energy required to overcome the pinning site to be equal to the energy required to align the octupolar moment of -md with +md   ecost = c0 x00ufmdfe -x01 = c0 x002ufmdfe x01 this is the energy required to overcome a single pinning sit  we now generalize the scenario where we have a collection of such patri et a 4 order parameter fi  s  qualitatively, the multipolar moments possess even-in-h symmetr 3 order parameter fi  s  qualitatively, the multipolar moments possess even-in-h symmetr 3 order parameter fi  s  qualitatively, the multipolar moments possess even-in-h symmetr  pinning sites over a distance dx, and the magnetic field is applied such that the domain wall is swept over that distance d  although the derivation is quite involved, the final result physical makes sense in that the total work done in moving the domain wall past the pinning sites is proportional to the change in the bulk octupolar moment taking the derivative of the above e  33 in the main tex  patri et al", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "ut-komaba/18-6 prepared for submission to jhep arxiv:190 u-toky a jp, hmarrochio@perimeterinstitut ca, rmyers@perimeterinstitut ca, lquintaqueimada@perimeterinstitut ca, byoshida@perimeterinstitut ca abstract: we revisit the complexity=action proposal for charged black hole  we investigate the complexity for a dyonic black hole, and we find the surprising feature that the late-time growth is sensitive to the ratio between electric and magnetic charge  in particular, the late-time growth rate vanishes when the black hole carries only a magnetic charg g, it is absent for purely magnetic black hole  we then show how the inclusion of a surface term to the action can put the electric and magnetic charges on an equal footing, or more generally change the value of the late-time growth rat  next, we investigate how the causal structure influences the late-time growth with and without the surface term for charged black holes in a family of einstein-maxwell-dilaton theorie  finally, we connect the previous discussion to the complexity=action proposal for the two-dimensional jackiw-teitelboim theor  since the two-dimensional theory is obtained by a dimensional reduction from einsteinmaxwell theory in higher dimensions in a near-extremal and near-horizon limit, the choices of parent action and parent background solution determine the behaviour of holographic complexity in two dimension  arxiv:190 00014v1 31 dec 2018 contents", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Reissner-Nordstrom Black Hole", "Text": "", "Subsections": [{"Section_Num": "2_1", "Section": "2.1 Complexity Growth", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2_2", "Section": "2.2 Maxwell Boundary Term", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2_3", "Section": "2.3 Shock Wave Geometries", "Text": "", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Charged Dilatonic Black Hole", "Text": "", "Subsections": [{"Section_Num": "3_1", "Section": "3.1 Complexity Growth", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3_2", "Section": "3.2 Boundary Terms", "Text": "", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Black Holes in Two Dimensions", "Text": "", "Subsections": [{"Section_Num": "4_1", "Section": "4.1 Jackiw-Teitelboim Model", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4_2", "Section": "4.2 JT-like Model", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4_3", "Section": "4.3 Boundary Terms?", "Text": "", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Discussion", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "A", "Section": "A More on Shock Waves", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "B", "Section": "B Bulk Contribution from Maxwell Boundary Term", "Text": " the concept of holographic complexity naturally emerges from considerations on the bulk causality in the ads/cft correspondencee, even tiny perturbations to the system have a measurable effect on the complexity to produce a dimensionless quantity, the volume is divided by newton's constant gn and the ads length   the study of holographic complexity is actively developing in two related direction  the first is to explore the properties of the new gravitational observables which play a role in the cv and ca conjectures and the implications of these conjectures for complexity in the boundary theory,   in particular, we note that a number of new proposals have been made for the holographic dual of complexity in the boundary theor  for example, one new proposal is known as the complexity= conjecture, which suggests that the bulk dual of complexity is the spacetime volume of the wdw patch further, more recently, a relation was conjectured between momentum of an infalling object in the bulk radial direction and complexity of the corresponding time-evolved operator on the boundary our motivation for the present paper was to understand holographic complexity in the two-dimensional jackiw-teitelboim 1a more sophisticated approach to choosing the latter scale was described in - 2 model of dilaton-gravity as such, the jt model should be an ideal platform to study the complexity in various dynamical settings and investigate further the implications of holographic complexit  this result, of course, in tension with our common expectations for complexit  further since the jt model is supposed to capture the physics of the syk model, which exhibits maximal chaos, we would certainly expect the complexity should increase as fast as it possibly ca  rather than considering the ca prescription of holographic complexity, one can also examine the cv proposal in this setting and in this case, we found the extremal volume continues to grow at a constant rate for arbitrarily late time  this apparent failure of the ca proposal motivated us to re-examine holographic complexity for charged black holes in higher dimensions since the jt model can be derived from an appropriate dimensional reduction,   in particular, jt dilaton-gravity describes the near-horizon physics of certain near-extremal black holes in higher dimension  previous studies of holographic complexity of charged black holes,   our first calculation in the following is to examine holographic complexity for a dyonic black hole with both electric and magnetic charge  in this case, even if the geometry is held fixed, we find that the complexity growth rate is very sensitive to the ratio between the two types of charg  however, there is a boundary term involving the maxwell field, which one might add to the gravitational actio  this term arises naturally in the context of black hole thermodynamics when defining different thermodynamics ensemble,   we find that with the ca proposal, the holographic complexity is also sensitive to the introduction of this maxwell surface ter  in particular, the late-time growth rate is nonvanishing for magnetic black holes with this surface term, while tuning the coeffcient of the surface term can yield a vanishing growth rate in the electrically charged cas  given these results, we are then lead to re-examine the dimensional reduction in the presence of the maxwell surface term and the behaviour of the corresponding holographic complexity for the jt model and for a related jt-like model, derived from the reduction of electrically charged black hole  to better understand the vanishing of the complexity growth for the magnetic black holes, we might also ask whether this result is special to the einstein-maxwell theor  alternatively, the question can be phrased as which features of the corresponding reissner-nordstrom-ads black holes are important in controlling the behaviour of the holographic complexity for the ca proposa  as a step in this direction, we also investigate holographic complexity for charged black holes in a family of fourdimensional einstein-maxwell-dilaton theorie  holographic complexity of einsteinmaxwell-dilaton theories has been previously studied for several models in these theories, the maxwell field is coupled to a scalar field and as a result, the charged black holes also carry scalar hai  with the dilaton excited in these solutions, the nature of the spacetime singularities and the casual structure of the corresponding black holes can be modifie  hence we can investigate to what extent these changes to the spacetime geometry modify the behaviour of the holographic complexit  our conclusion will be that the causal structure of the spacetime geometry is the essential feature leading to the unusual late-time growth rate with the ca proposa  we first show how for a fixed geometry, the complexity rate of change is sensitive to the ratio between electric and magnetic charge  we also show how the inclusion of the maxwell surface term to the action can also have a dramatic effect on the late-time growth rate for the ca proposa  in addition, we also briefly investigate the switchback effect by injecting small shockwaves into the dyonic black hol  in section 3, we investigate holographic complexity for charged black holes in a family of einstein-maxwell-dilaton theorie  in particular, we show that the late-time growth rate vanishes for the jt model, but that this situation can be ameliorated if the maxwell surface term is included in reduction from four to two dimension  we summarize our findings and consider their implications in section 5, as well as discussing some possible future direction  we leave certain technical details to the appendice  in appendix a, we describe in more detail the calculations of the holographic complexity in the dyonic shock wave geometrie  in appendix b, we comment on subtleties concerning the evaluation of the maxwell surface term when magnetic charges are presen  as this project was nearing its completion, we became aware ofwhich has significant overlap with the present pape  we also add that an independent approach to understanding holographic complexity for the jt model recently appeared in these results are easily extended to general dimensions, if one also couples the gravity theory to a -form potential field our main objective is to understand the effect of a new boundary term associated with the maxwell fiel  as mentioned in the introduction, we will find that although this boundary term does not modify the field equations, it has a strong influence on the action of the wheeler-dewitt patc  hence we must ask which choices yield a wdw action which produces the behaviours expected of holographic complexit  further, it was shown with a careful study of shock wave geometries in that this surface term must be included on the null boundaries of the wdw patch if the ca proposal is to reproduce the expected properties of complexit  the final contribution in e  while introducing this boundary term does not change the equations of motion, it does change the nature of the variational principle of the maxwell fiel  that is, it changes the boundary conditions that must be imposed for consistency of the variational principl  we will also find that it modifies the wdw action, but we reserve a complete discussion of this term for section   that is, we examine the holographic complexity working with the action i0 = itot with this action, we apply the ca proposal to study the holographic complexity for a spherically symmetric dyonic reissner-nordstrom-ads black hole as indicated above, the black hole carries both electric and magnetic charge  where qe and qm denote the electric and magnetic charge 1 complexity growth next, we evaluate the growth rate of the holographic complexity for the dyonic black hole this analysis reveals the puzzling feature that despite the fact that magnetic and electric charges are interchangeable at the level of the equations of motion, the complexity growth in the ca proposal seems to be sensitive to the nature of the charg  in the following, we provide salient points in the calculation and we refer the interested reader to for further detail  a typical wdw patch is illustrated in the penrose diagram in figure   the time evolution of the wdw patch can be encoded in the time dependence of points where the null boundaries intersect in the bulk,   the shaded blue region corresponds to a typical wdw patch anchored to symmetric boundary time slices with tl = tr = t/  penrose-like diagram of an reissner-nordstrom-ads black hole with a shock wave inserted on the right boundary at the boundary time tr = -tw - see e  in order to not tilt the asymptotic boundary after the shock wave, we adopt the dray-t'hooft prescription that the null geodesics crossing the collapsing shock wave shift  bulk contribution we start by evaluating the time derivative of the two bulk terms in e  however, the boundary contributions coming from this time-like surface segment and the corresponding joints yield a fixed constant,  e, they do not contribute to the time derivative of the actio  further, with affnely-parametrized null normalsthe null surface term in e  vanishe  using e the time derivative of e  note that at late times, r1,2 m approach the horizons and so the first term above vanishe  hence only the second term contributes to the late-time growth rat  - 9 the term in the first line comes from the uv regulator surface and again only contributes a fixed constan  hence the time dependence comes only from the terms evaluated at the meeting points in the second lin  the time derivative of e  again at late times, this contribution vanishes and so it only changes the transient behaviour in the growth rate at early time  it is useful to combine eq  note that in contrast to e the electric and magnetic charges contribute with the same sign abov  total growth rate the growth rate of the holographic complexity is then given by the sum of eq  we fix the parameters that determine the geometry, but vary the ratio between electric and magnetic charge  as predicted by e when the charge is mostly magnetic, the growth rate of complexity approaches zero at late time 1 curves are indistinguishable on this scal  figure 2 illustrates the full time-dependence of the growth rate, as we change the ratio of the electric and magnetic charges while keeping the spacetime geometry fixe  as expected from e the rate approaches zero at late times when the black hole is mostly magneti  adding the boundary term to the euclidean action produces the legendre transform to the helmholtz free energy, associated with the canonical ensemble where the temperature and total charge q are held fixe  as we noted above, adding this surface term changes the boundary conditions in the variational principle of the maxwell fiel  consider varying the maxwell action in e  - 12 the bulk action as a boundary ter  in any event, combining e  hence in evaluating the wdw action for the general action itot,  e, including the contribution of the maxwell boundary term in e the only change that has to be made to the previous calculation is to change the overall coeffcient of the maxwell contribution in e  as a result, e  the above discussion shows us that the growth rate is symmetric under electric-magnetic duality,  e, we modify the coeffcient of the maxwell boundary term as indicated abov  of course, looking back at e we see that the combination of the bulk and boundary terms for the maxwell field vanishes on-shel  however, the complexity is still sensitive to the electromagnetic field through its back-reaction on the geometr  for example, e  - 13 of course, the reader may wonder why we should expect that that magnetic and electric black holes should compute at the same rat  first, let us recall the expectation that the late-time growth of the complexity should be given by e  this conclusion can also be motivated by the shock wave geometries, which we study in the next sectio  in this context, both electric or magnetic black holes exhibit the same back-reaction and hence it is natural to think that the holographic complexity should respond in the same manner independent of the nature of the charg  we will follow closely the analysis and notation of to examine this feature, we consider a vaidya geometry where a thin shell of null fluid collapses into a charged black hol  if the shell only injects a small amount of energy into the system, then the black hole's event horizon shifts by a small amount,   our goal here is to investigate to what extent the ca proposal reproduces this behaviour for the charged black holes discussed in the previous section  for simplicity, we assume that the thin shell is neutral,  e, it carries energy but no charge  denoting the usual heaviside function).  however, we must evaluate the tortoise coordinate for each region and then following e  let us note that with these choices, e  we begin by considering the ca proposal for the action without the maxwell boundary term  as in section   the dashed vertical line is the scrambling time for the shock wave with these geometric parameter  appendix   hence the switchback effect vanishes for a black hole with pure magnetic charg  this result might be expected since there is a close connection between the late-time rate of growth of the complexity in the static black hole and dca/dtw, as discussed in 5comparing e  with the result in e  hence as for the growth rate of the eternal black hole case in section  1, which is slightly shifted to the righ  the dependence on this ambiguity in the definition of the wdw action is illustrated in figure   we note, however, that this ambiguity does not effect the final rate dca/dtw but only the transition between the two regimes in e  further, e  however, this regime actually becomes smaller as the black hole becomes mostly magnetic,   in addition, we see that the exponentially growing mode is only the dominant contribution at earlier time  in particular, if the black hole is purely magneticthe exponentially growing mode is absen  of course, the above results are modified if we include the maxwell boundary term in particular, e  the influence of the transient behaviour for the complexity in the shock wave geometr  for example, with this choice, black holes with magnetic charges exhibit the desired switchback effect while those with a purely electric charge would no  further, similar to the discussion in section   therefore, with this choice, both electric and magnetic black holes exhibit the same switchback effec  for the rate at large tw are essentially the sam  further, this rate is essentially twice the late-time growth rate in e  6of course, there is a similar relationship between the rates in eq  the exponentially growing mode only dominates at very early times because it must compete with other transient effect  in the limit that the black hole has only magnetic chargesthis exponentially growing mode is absen  hence for large tw, dca/dtw is related to the late-time growth rates of the complexity on either side of the shock wav  of course, this is precisely the behaviour found in this subsectio  then the right-hand side of e  has a contribution corresponding to the growth rate on the right boundary at very late times and another coming from very early times on the left boundar e, the minus sign in e  this investigation is motivated by the question of understanding to what extent our results in the previous section are special to the precise couplings of the einsteinmaxwell theor  the presence of these new couplings lead to scalar hair on the charged black holes, and may change the nature of the spacetime singularities and the casual structure of the corresponding black hole  hence we would like to understand if these changes to the spacetime geometry modify the behaviour of the holographic complexity in an essential wa  the corresponding charged dilatonic black holes were introduced for asymptotically flat geometries in and they were extended to asymptotically ads geometries in the ads solutions were further explored in,   holographic complexity of dilatonic black holes has been previously studied for several models our investigation of the holographic complexity for these dilatonic black holes will show that the vanishing of the late-time growth rate found in the previous section is not a generic result for charged black hole  rather, for the theories studied here, the analog of the maxwell boundary term modifies the complexity growth rate but the coeffcient can not be chosen to reduce the rate to zero generall  however, we will see that the latter can still be accomplished in the theories where the charged black holes have the same causal structure as the reissner-nordstrom black hole  hence our conclusion is that the causal structure of the spacetime geometry is the essential feature leading to the vanishing late-time growth rate in the previous sectio  andrespectivel  in subsection   here, we are again focusing on the case of four bulk dimensions for simplicit  hence in this limit, the theory reduces to the einstein-maxwell theory from the previous section coupled to an additional massless scalar fiel  we note that this solution interpolates between the reissner-nordstrom black hole and schwarzschild the causal structure for these solutions is illustrated in figure   in this case, the threshold is reached when the event horizon meets the singularity,   it would be interesting to study which black hole solutions are thermodynamically and dynamically stable1 complexity growth we will now study the time-dependence of the holographic complexity of the charged dilatonic black holes presented above using the ca proposa  we will follow the discussion inwhich is straightforward to adapt to these solution  further, we will only be considering the action here and defer the discussion of additional boundary terms to the next section  for black holes with just one horizon,   we must evaluate the gibbons-hawking-york term, given in e  hence the corresponding joint contributions vanis  as expected, when eq  total growth rate now we combine all of the contributions in eq  for the latter solution  in fact, the result in e  the behaviour is very similar to that of the latter neutral black holes, as shown in the detailed analysis of in analogy to the schwarzschild-ads black hole, the complexity does not change until a certain critical time, where the wdw patch leaves the past singularit  behaviour and then by a time of the order of the inverse temperature, it has overshot the late-time limit which it subsequently approaches from abov 2 boundary terms next we examine how the growth rate of the holographic complexity for the charged dilatonic black holes is effected by the addition of two boundary terms, involving the maxwell and dilaton field  following the same reasoning as in section  2, this boundary term changes the boundary condition imposed on the maxwell field in the variational principl  however, we should add that implicitly we would also be assuming a dirichlet boundary condition for the dilaton,   hence adding in the above expression, the late time limits in e  are unchanged,   this precisely matches the behaviour found in section   however, we observe that in the uncharged limite  as for the maxwell boundary term in the previous section), this term modifies the character of the boundary condition which must be imposed on the dilaton in the variational principl  more general choices of this parameter lead to mixed boundary condition  in this case, the boundary term lives only on the null boundaries of the wdw patch but in this case, the derivative appearing in e  is actually tangent to the boundar  hence, adding the dilaton boundary term does not change the complexity growth rate at late times for these black hole  nevertheless, the transient behaviour of the holographic complexity at early times will be modified by this term, but we will not explore this her  evaluating e  therefore adding the dilaton boundary term spoils the good behaviour of the regularization procedure at the singularit  therefore, we do not consider these boundary terms further her  hence, our general results for the late-time growth rate of the holographic complexity including the maxwell boundary term are summarized e  for the electrically charged black hole  of course, these results match the growth rates without the maxwell boundary term in e  however, when the maxwell boundary term was included, we also showed in e  no such choice was possible when the causal structure had the form of the schwarzschild-ads black holes,   the former behaviour was analogous to that found in the einstein-maxwell theory in section 2 and therefore it appears that the causal structure of the black hole was one of the essential features producing the unusual behaviour found ther  however, we note that our analysis here focused only on electrically charged black holes and we did not consider dyonic or magnetically charged black hole  unfortunately the latter solutions are not yet known for the einsteinmaxwell-dilaton theory we return to this point in section   4 black holes in two dimensions in this section, we will focus on studying dilaton gravity models in two bulk spacetime dimension  our main motivation is evaluating the growth of holographic complexity for the jackiw-teitelboim modelwhich has a simple action linear in the dynamical dilaton fiel  this theory has received great deal of attention recently as the gravitational dual of the sachdev-ye-kitaev model in the low energy limit, where the system acquires an emergent reparametrization invariance one perspective of jt gravity is that it describes physics in the near-horizon region of near-extremal charged black holes in higher dimensions,   in addition in this section, we will analyze an analogous two-dimensional theory that can describe the near-horizon physics of four-dimensional black holes carrying a purely electric charge,   the two-dimensional maxwell field is an essential ingredient for this jt-like theory and so it has a form reminiscent of the brown-teitelboim modelwhere the effective cosmological constant is dynamically controlled by the energy density of an antisymmetric d-form field strength in d dimension  further, our analysis of holographic complexity in the previous sections has shown the important role of the maxwell boundary term hence while we begin by examining the dimensional reduction without this term,  e, by reducing i0 in e we also consider the dimensional reduction of this boundary term and its contribution to the holographic complexity for both the jt and jt-like model  as might be expected, we will find the holographic complexity for both models behaves in the same way as for the corresponding four-dimensional black holes discussed section   we will discuss these theories and the holographic complexity in more detail in an upcoming work1 jackiw-teitelboim model we begin with the dimensional reduction of the action but without the addition of the maxwell boundary term,   substituting e  - 31 the boundary term in the second line of e  results from integrating by parts in the dimensional reductio  we emphasize that it arises from the bulk terms in the four-dimensional action and is unrelated to the surface terms or the null countertermwhose dimensional reduction we will explicitly examine belo  the action illustrates the fact that restricted to spherically symmetric solutions, our theory can be recast as a two-dimensional gravity model with a dilaton fiel  however, no approximations have been made at this point, and so the full four-dimensional solution can be recovered from e  next, we are interested in describing the near-horizon region of the near-extremal black hole  recall that in extremal limit, the charged black holes develop an infinitely long throat of a fixed radius rh now, in considering small deviations from the extremal throat, we expand the dilaton around the extremal value in e  physical boundary is depicted with a blue curv  in the dilaton solution, we have introduced the cut-offradius r  the dynamics of the boundary position reproduces the ir physics of the syk model, as has extensively been studied in recent years the mass mjt and temperature tjt are taken as energies conjugate to the coordinate time t of course, one can treat the jt model as an independent theory, or one can match the jt solutions with a description of the near-extremal throats of the reissner-nordstrom-ads black holes - 33 complexity growth next, we consider the growth of holographic complexity for the jt model using the ca proposa  first, we evaluate e  however, recall that the reduced action included a surface term which was not incorporated in the jt action10 substituting e  the remaining boundary terms introduced in eq  and have a simple dimensional reductio  this leaves only the null joint terms and the null surface counterter  dimensionally reducing these two terms and substituting e  combining these surface terms with e  yields the total of the boundary contribution for the 10one might also wish to consider jt gravity in its own right, without any reference to higher dimension  in this case, we would not include the total derivative contribution as part of the wdw actio  adding this expression to e  in e  yields the holographic complexity for the ca proposa  as the higher dimensional calculations in section   in analogy to e  hence the prefactor of f in e  it is interesting to recast e  into an expression involving the boundary or physical parameters of the jt model,   first, we define dimensionless coordinates for the meeting point  finally we can rewrite e  different colours correspond to different temperature  we show examples of the time evolution of the holographic complexity for different values of the temperature in figure   as we already saw in e the late-time limit of the complexity growth is zer  further, we note that this limit is generically approached from below, given the expression in e in contrast to the expectation from higher dimensional black hole  the vanishing of the complexity growth rate at late times may seem puzzling for the jt gravity when we consider that this model is supposed to capture the low energy dynamics of the syk model, which is maximally chaotic and hence would be - 36 expected to exhibit nontrivial complexity growth for very long time  however, from the perspective of the dimensional reduction, this feature is not so surprisin  the result in e  was our main motivation to revisit the holographic complexity of charged black hole 2 jt-like model we now turn our attention to another possible two-dimensional theory that is derived from a purely electrically charged black hole in four dimension  in the latter black holes, the maxwell field strength has a single component frt and hence the dimensionally reduced theory incorporates a maxwell potential and the corresponding field strength is a form of maximal rank in two dimension  in this sense, the twodimensional theory has a form reminiscent of the brown-teitelboim modelwith a dynamical cosmological constant controlled by a field strength of maximal ran  a similar two-dimensional action was also studied in in and more recently in in it was argued to describe the physics of the extended syk models with complex fermions with conserved charge studied in we note that it is possible to show that leads to the same equations of motion as after putting the gauge field on-shell however, we will show that holographic complexity derived with this action using the ca proposal yields a different result from the jt model studied abov  in particular, the throat of an electrically charged extremal black hole is identical to that of a magnetic extremal black hol  therefore the corresponding two-dimensional geometry and the dilaton are identical in the present case as in the previous subsection,  e, as described by eq  now, as in the previous subsection, we wish to construct a theory which captures small deviations from the extremal throa  hence, we expand the dilaton as in e  however, in the present case, we also wish to capture small corrections to the extremal field strength in e  when we expand the bulk action in e  where i jt bulk is precisely the action given in e  we will call this theory the jt-like mode  for the jt mode  further, the mass, entropy and temperature all take the same form as given in e  now in e  we can write the solution of e  the second term represents the leading correction to the field strength created by the running of the dilato  lastly, we turn to the dilaton equation of motion in e  upon substituting this extremal field as well as the perturbatione  - 39 we can absorb the last term with a simple constant shift of the dilaton,   as described above the geometry is precisely the same as in the jt model and so the penrose diagram in figure 8 still describes the solution for the jt-like model complexity growth given the close connection of e e, the null joint terms and the null surface counterterm are given by e  hence we can easily extend the analysis of the holographic complexity for the jt model from the previous subsection by simply investigating the contribution of the second term in e  to the ca proposal for the jt-like mode  again, the nonvanishing result here may not be so surprising since the corresponding black holes in four dimensions also exhibited a constant growth rate at late time  in fact, a careful translation of the parameters shows that e  matches the higher dimensional result in e  to leading order in the near extremal limi 13 the expression in e  can be combined with e  to give a full description of the growth rate for the holographic complexity in the jt-like mode  some examples of the full time profile of the growth rate are illustrated in figure 1 1 for the corresponding black holes in four dimension  in section   jt-like model we start by analyzing the role of the maxwell surface term in the jt-like mode  in this case, the maxwell field is still a dynamical field in the two-dimensional theory and so e  reduces in a straightforward way to a boundary term for the wdw patch in two dimension  12given the prefactor in e  in fact for large black holes,   then the leading terms for the late-time growth in eq  and agree,  01e0 and lct = l  expanding to linear order in the dilaton with e  alternatively, when the maxwell field is on-shell, we can also use e  to express the maxwell surface term in terms of a bulk integra  next, we evaluate e  - 42 without the maxwell surface term, the late-time growth rate for the jt-like model was given by e  comparing this expression to the late-time growth for four-dimensional black holes in section  2, we find agreement between eq  jt model as we described in section   the interesting contributions to the surface term actually live on the surface dividing the patches where the gauge potential is well defined,   alternatively, we can again use the bulk expression in e  for the on-shell maxwell field, which yields precisely the same result as shown in e  then substituting e  recall that this two-dimensional model describes gravity in the nearextremal throat with qm = qt,ex  note that since the magnetic maxwell field and the relevant surfaces are integrated out in the dimensional reduction, there is no way to think of eq  or as a surface term in the two-dimensional theor  rather, here we are modifying the standard ca prescription in the jt model by adding a new bulk contribution to the holographic complexit  in particular, we observe that the first contribution in e  is simply proportional to the spacetime volume of the wdw patch and so this contribution is reminiscent of the cv 0 proposal where the holographic complexity is equated with the spacetime volume to the wdw patc  without the maxwell surface term, the late-time growth rate for the holographic complexity vanished, as shown in e  hence once we reconsider the holographic complexity with the addition of the surface termthe late-time growth rate is governed entirely by e  we found that the results were very sensitive both to the type of charge and to the inclusion of the maxwell boundary term the general result for dyonic black holes carrying both kinds of charge is given in e  in section  3, we also noted that the switchback effect exhibited a similar sensitivity to the type of charg  however, this picture changes dramatically when the maxwell boundary is include e, the late-time growth rate vanishes with electric charge and is nonvanishing with magnetic charg e, with the same form as the bulk maxwell action a possible alternative then would be to define complexity=.  that is, we could drop both the bulk and boundary terms involving the maxwell field for e  to define the gravitational action and then define the complexity by evaluating this action on the wdw patc  it might be interesting to investigate this proposal in other setting  in section 3, we investigated the ca proposal for charged black holes in a family of einstein-maxwell-dilaton theories generally, the late-time growth rate of the holographic complexity was nonvanishing for the electrically charged black hole  however, when the maxwell boundary term was included, we showed in e  no such choice was possible when the causal structure appeared as in the schwarzschild-ads black holes,   the former behaviour was analogous to that found in the einsteinmaxwell theory and therefore it appears that the causal structure of the black hole was one of the essential features producing the unusual behaviour found in section   in the einstein-maxwell theoryit is straightforward to produce magnetic solutions given the electrically charged black holes using electric-magnetic dualit  except for the appearance of v generally then, with the transformationthe electrically charged solutions of the original theory would become magnetically charged solutions of the new theory however, as we noted above e  leaves the theory invarian  note that all three of these special cases,   hence at least of these three cases, it is straightforward to verify that the late-time growth of the magnetic black holes is nonvanishin  it would, of course, be interesting to construct magnetic or dyonic black holes for the einstein-maxwell-dilaton theories more generally and to fully investigate holographic complexity in these theorie  in section 4, we turned to holographic complexity for black hole in two dimension  in particular, we showed that the late-time growth rate vanishes for the jt model in e  the latter mirrored the behaviour for the magnetic reissner-nordstrom-ads black holes in four dimensions, which play a role in constructing the jt action via dimensional reductio  this situation can be ameliorated by instead considering a dimensional reduction describing the near-horizon physics of near-extremal electric black holes in the resulting jt-like theorythe late-time growth rate is nonvanishing, as shown in e  we also considered the dimensional reduction of the maxwell boundary term in both case  again, the effect of this surface term mimicked that found in four dimension  in particular, the latetime growth rate of the jt model is now nonvanishing, as shown in e  the slightly unusual feature is that as a result of including the maxwell boundary term in four dimensions, the complexity=action prescription becomes a complexity= prescription for the jt model,  e, we have a mixture of the ca and cv 0 proposals in two dimension  we will discuss the two-dimensional results further belo  it should not be surprising that holographic complexity, or more specifically the ca proposal, can be sensitive to surface terms since an analogous behaviour was already observed with the null counterterm in the gravitational action of course, another guiding principle that suggests that this boundary term should be included is to ensure that the action is invariant under reparametrizations of the null boundaries, as emphasized in in fact, it is this principle that fixes the overall coeffcient with which the null counterterm is added to the actio  in the present case, we have not definitely fixed the coeffcient of the maxwell boundary ter  while maxwell boundary term does not effect the equations of motion, it does play a role in the variational principle by changing the boundary conditions imposed on the gauge field, as described around e e, it becomes an important part of the euclidean action depending on the thermodynamic ensemble of interest hence one might ask if different ensembles will compute differently,  e, if the electric and magnetic charges would make distinct contributions in different ensemble 14 preliminary investigations with simple qubit models seem to indicate that this is indeed the casebut of course, it would be interesting to study this question furthe  even though the maxwell term is associated with modifying the boundary conditions for the field equations, we are not suggesting that these boundary conditions should be applied on the boundary of the wdw patc  on the other hand, we might wonder if quantum corrections to this saddle point evaluation of the holographic complexity would involve fluctuations of fields on the wdw patch which respect the boundary conditions determined by our choice of surface term  in passing, we note that while the holographic complexity defined by the ca proposal is sensitive to the presence of the maxwell boundary term, it is only the infrared properties which exhibit this sensitivit  the asymptotic contributions to action are es14implicitly, we are suggesting that the action used to evaluate the holographic complexity would be the same as that used to evaluate the euclidean action for the thermodynamic ensembl  for example, for an electrically charged black hole given in eq e, there are no uv divergent contributions coming from the maxwell boundary ter  however, to close our discussion here, we observe that the limit of zero charge is subtl  the first factor corresponds to the late time growth rate of a neutral black holeand hence we only recover this expected rate when the second factor is equal to on  back to two dimensions in section 3, the causal structure was identified as an essential feature in determining the unusual behaviour of the holographic complexity for charged black hole  for example, the vanishing of the late-time growth rate for the ca proposal found for the jt model in e  - 48 matches with the vanishing rate found for the four-dimensional black holes carrying only magnetic charge in e  on the other hand, the close parallels between the two- and four-dimensional results may seem unexpected when one recalls that the dimensional reduction producing the jt and jt-like models focuses on the near-horizon region of near-extremal black holes in four dimension  hence the wdw patches correspond to very different regions of the spacetime in the two different context  that is, the four-dimensional wdw patch is anchored to a cut-offsurface near the asymptotic ads4 boundary, while the two-dimensional wdw patch is anchored to a constant radius surface deep in the throat of the four-dimensional black hol  given these differences, one must examine the results more closely to understand the similarities in the complexity growth rates in two and four dimension  first, in eq  that is, the growth rate is determined infrared part of the four-dimensional geometry,  e, by the near-ads2 throat of the near-extremal black hole  further, in these geometries with two horizons, the late-time rate is completely determined by quantities evaluated at the corresponding bifurcation surface  andand this explains the close match between the results in two and four dimension 16 in fact, one finds that the same late-time growth rate will be derived for wdw patches anchored to any fixed r surfaces in the four-dimensional geometry this again points to the importance of the causal structure in determining the behaviour of the holographic complexity for the ca proposal  however, we note that when the cut-offor anchor radius is varied, the details of the early-time transients are modifie  some recent progress in connecting these developments with holographic complexity was reported in ,17 however, it remains 16we should add that the dimensional reduction does not require any modification of the time coordinate and hence another important ingredient in this match is that the rates in two and four dimensions are measured with respect to the same time coordinate 17we note that ref  suggest a very different understanding of holographic complexity for jt gravity than developed here in particular, this approach relies on defining a new - 49 an interesting future direction to fully develop these connection  one of our most interesting results in section 4 was that when the maxwell surface term is included in the usual complexity=action prescription in four dimensions, the dimensional reduction produces a complexity= prescription for the jt mode 18 note that with the latter prescription, the purely topological sector exhibits complexity growt  of course, this is in keeping with the recent results of indicating that the entire entropy of the extremal black hole contributes to the late-time complexity growth this also reminds one of the suggestion that the jt model should be interpreted as a lowenergy sector embedded in a quantum gravity theory with a larger hilbert space of course, the latter is a natural perspective here where the two-dimensional model and the prescription for holographic complexity were both derived with a dimensional reduction from four dimension  other future directions we have already commented on various future directions above, but let us close with a few more observation  the preceding discussion of the jt model reminds us of the other prescriptions of holographic complexity, in particular, the cv proposal and the cv  while our analysis in this paper focused almost entirely on the ca proposalit is straightforward to examine the behaviour of these other proposals to the four-dimensional charged black holes or to the two-dimensional near-ads2 geometries for the cv proposal, the techniques developed in are easily generalized to these new metrics and for the cv 0 proposal, one need simply adapt the appropriate results for the bulk actio  in either case, the late-time growth is found to be in keeping with the general expectations of e  of course, these approaches to describing holographic complexity are only sensitive to the spacetime geometry and they would not be sensitive to the type of thermodynamic ensemble in questio  however, it would be an interesting question to examine either of these proposals had a well-motivated extension where the ensemble played a role in determining the behaviour of the holographic complexit  for instance, one might explore the role of boundary conditions in the context of the recent understanding of the holographic dual of the bulk symplectic form cut-offsurface behind the horizon using the lloyd bound however, we must remind the reader that the conjectured relation between the ca proposal and the lloyd bound is known to fail 18we observe that in this construction, the spacetime volume contribution comes with a very specific prefacto  in general, the normalization of the holographic complexity is an ambiguity for the cv  - 50 the jt model provides a simple setup to study traversable wormholes and hence another interesting extension of the present work is to consider holographic complexity growth for traversable wormhole  in order to retrieve a quantum state which has fallen deep into the bulk, one would need to perform some operation which would prevent or reverse the natural tendency of the system to complexif  perhaps it would be interesting to investigate the relation between the amount of quantum information that can be transmitted through the wormhole and the corresponding holographic complexit  examining these ideas from the perspective of the hayden-preskill recovery protocol may also prove fruitfu  we would also like to thank adam brown and lenny susskind for sharing with us a draft of their paper research at perimeter institute is supported by the government of canada through industry canada and by the province of ontario through the ministry of research & innovatio  rcm and by are supported in part by discovery grants from the natural sciences and engineering research council of canad  rcm also received funding from the simons foundation through the it from qubit collaboratio  lq would like to thank the perimeter scholars international and perimeter visiting graduate fellows programs for support during this researc  the work of kg was supported in part by the jsps research fellowship for young scientist  kg thanks perimeter institute for their hospitality during various stages of this projec  rcm would also like to thank the kitp at uc santa barbara for their hospitality during the final stages of this projec  at the kitp, this research was supported in part by the national science foundation under grant n  nsf phy-174895  the shock wave geometry is represented in the penrose-like diagram in the right of figure   this calculation below follows the analysis inso we refer the reader there for more details from e  we will show next how to obtain the complexity of formation as a function of how early the shock wave is inserted we will assume a dyonic black hole in four bulk dimensions and with a spherical horizon, as in the main tex  further, we only consider neutral shock waves,  e, the black hole charges before and after the shock wave remain equa  the shock wave is then inserted at vs = -t  we are interested in investigating the switchback effect, so as in section   joint and counterterm contributions we now evaluate the joint and counterterm contributions to the shock wave spacetim  because of the affne parametrization condition, the joints at rb and rs do not contribute combining equations eq  there is one big difference with respect to the switchback effect for schwarzschild black holes as discussed inbecause now both the past and future boundaries of the wheeler-dewitt patch end at joints, instead of at a spacelike singularit  we denote rm the solution for meeting point equation in e  if we include the surface term in e  b bulk contribution from maxwell boundary term in section   is nonvanishing - see details belo  we elucidate the resolution of this inconsistency in the followin  consider the gauge potential for the magnetic charge by setting qe = 0 in e  as a result, this problem is inherited by the integrand appearing in the boundary term,   hence we can not properly apply stokes' theorem in the way that was implicitly done in deriving e  we can evade this problem by describing the gauge potential on two patches, one covering the north pole and the other covering the south pole let us explicitly illustrate this point for the reissner-nordstrom-ads black holes discussed in section 2 but in the case, where the charge is purely magneti  now setting qe = 0 in e  next we turn to the boundary term as given in e  hence if we integrate over this boundary, e  and yield exactly the same resul  hence the lesson that we take away here is that when we introduce the boundary term to evaluate the wdw action for a magnetically charged system, we should understand that this term is not only integrated over the boundary of the wdw patch but rather it is integrated over the boundaries of all of the patches introduced to produce a properly defined gauge potential everywhere", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190 unib u-toky a ryukok a up de we study the heat equation on a half-space or on an exterior domain with a linear dynamical boundary conditio  our main aim is to establish the rate of convergence to solutions of the laplace equation with the same dynamical boundary condition as the diffusion coeffcient tends to infinity", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": " the rate of convergence was not studied in some of similar issues for elliptic equations with dynamical boundary conditions were considered infor instanc  let t belong to ).  the following was shown in : theorem   4 the role of assumption is explained in section   our remaining results are concerned with the question what the optimal rate of convergence in and i  the upper bounds from theorems 4 and 5 are shar  the paper is organized as follow ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Preparation: Introducing further notation. The space XT.", "Text": " proo  proo  proo  4 estimates: r3 b1 the assertions of the lemmata in this section parallel those in the previous sectio  before we begin dealing with their statements and proofs, let us first bring some of the quantities that have been defined in the introduction in the explicit form in which the radially symmetric 3-dimensional setting allows us to express the  proo  proo  proo  if we finally combineandwe can readily conclude proof", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Estimates: Recalling the case =RN+", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Estimates: R3\\B1(0)", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Existence. Proof of Theorem 3", "Text": "proof of theorem   we hence restrict the class of admissible initial data and will attempt to apply the fixed point theorem not on xtbut on a bounded subset thereo  we postpone the proof ofseeing that it is a corollary of theorem   the proof of the existence result in theorem 2 in proceeds analogousl ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "6", "Section": "6 Upper bounds. Proofs of Theorems 4 and 5.", "Text": " proo  proof of theorem   proof of theorem   7 a remark on condition long-time behaviour of the heat equation on the exterior of the bal ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "7", "Section": "7 A remark on condition (10). Long-time behaviour of the heat equation on the exterior of the ball.", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "8", "Section": "8 Lower estimates in the halfspace: Proof of Theorem 6.", "Text": "proof of theorem   9 lower estimates in the exterior of the bal  proof of theorem   proof of theorem   acknowledgement  the first author was supported in part by the slovak research and development agency under the contract n  apvv-14-0378 and by the vega grant 1/0347/1  the second author was supported in part by the grant-in-aid for scientific research from japan society for the promotion of scienc  the third author was supported by the grant-in-aid for young scientists from japan society for the promotion of scienc ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "9", "Section": "9 Lower estimates in the exterior of the ball. Proof of Theorem 7.", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Lattice-QCD", "Section": "Lattice-QCD Determination of the Hyperon Axial Couplings in the Continuum Limit", "Text": " these couplings are important parameters in the low-energy effective field theory description of the octet baryons and fundamental to the nonleptonic decays of hyperons and to hyperon-hyperon and hyperon-nucleon scattering with application to neutron star  we use clover lattice fermion action for the valence quarks with sea quarks coming from configurations of nf = 2+1+1 highly improved staggered quarks generated by milc collaboratio 438statsy ", "Subsections": [{"Section_Num": "I", "Section": "I Introduction", "Text": "the octet-baryon axial couplings are important quantities for studying hadron structure in qc  specifically, the hyperon couplings are important in the effective field theory of octet baryonsbecause they enter the expansions of all quantities in chiral perturbation theor  in addition, the coupling constants also appear in the calculations of hyperon nonleptonic decays and of hyperon-hyperon and hyperon-nucleon scattering matrix elements they are also useful for calculating equations of state and other properties of nuclear matter in neutron stars studying these couplings also allows us to explore the extent of the symmetry breaking of su flavo  su symmetry has been widely studied in the hyperon hadronic matrix elements and this symmetry is used in many applications where strange data is limite  for example, the global analysis of the polarized parton distribution function has commonly used this assumption for extracting individual quark flavor pdfs ; knowing to what extent of this symmetry holds will help us quantify the systematic uncertainty introduced by the use of this assumption in the polarized pdf however, experimentally it is much harder to determine the hyperon couplings than those in the nucleon case, since the hyperons weak decay in nature quickl  lattice-qcd calculations can provide more stringent direct and reliable calculations of these coupling  lattice qcd is an ideal theoretical tool to study the parton structure of hadrons, starting from quark and gluon degrees of freedo ms edu calculation  take the nucleon tensor charge for exampl  on the lattice side, there are a number of calculations of gt ; some of them are done with more than one ensemble at physical pion mass with high-statistics calculations and some with multiple lattice spacings and volumes to control lattice artifact  such programs would have been impossible 5 years ag  as a result, the lattice-qcd tensor charge calculation has the most precise determination of this quantity, which can then be used to constrain the transversity distribution and make predictions for upcoming experiments the hyperon couplings are also not precisely known from experiments, and we hope a better determination of these couplings will lead to advancements in multiple subfield  ensemble information and parameters used in this calculatio  momentu  there have been several previous lqcd calculations of the hyperon coupling constant 277statsy  a follow-up study by a japanese group used 2-flavor latticesproducing results consistent with the heavier pion masses of re 248 after extrapolating to the physical pion mas ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "II", "Section": "II Lattice-QCD Calculation Setup", "Text": "in this work, we use clover lattice fermion action for the valence quarks on top of 2+1+1 flavors of hypercubic -smeared highly improved staggered quarks in configurations generated by milc collaboratio  the quark mass and clover parameters are the same as those used by pndme collaboration a summary of the ensemble information and parameters used in our calculations can be found in table   a0 and a1 are overlap amplitudes for the ground and excited states, tsep is the source-sink separation, and m0 and m1 are the masses for ground and excited states of the corresponding octet baryon  4 can be combined into a three-term fit using all the tsep data we hav  based on the measurements using the same lattice spacing and pion mass from the smaller-volume a12m220 ensembles, there is no sign of the excited-state contamination at the chosen sourcesink separatio ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "III", "Section": "III Results and Discussion", "Text": " secondly, the signal-to-noise of the ratios are significantly improved due to the correlations in the data, since they are taken using the same qcd configuration  thirdly, we expect some of the lattice artifacts to be canceled or reduced in the ratio combination  as shown in fi  the blue band in fi  this yields 18 possible continuum-extrapolation form 22 g tsep=10 tsep=12  76 2g tsep=8 tsep=9 tsep=10 tsep=11 tsep=12 fi  the extracted ground-state hyperon matrix elements from two-state fit using all the tsep are marked by the black line and gray ban  sum of squared fit residual 2124statsy 2703statsy 277statsysbut we achieve much improvement in statistical uncertainty and control of systematic 463 using semileptonic decay data under the same assumption of su symmetry585 commonly used in polarized global fits as a constraint a06m310 a09m130 a09m220 a09m310 a12m220s a12m220 a12m220l a12m310 0  087statsy  there is a noticeable increase with x at heavier quark mass but it is mild when only looking at the lightest 2 pion masse ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "IV", "Section": "IV Summary and Outlook", "Text": " for the first time, not only have these quantities been studied directly at the physical pion mass but also with careful study of the sources of systematic uncertainty, including the lattice-spacing and the finitevolume effect  we calculated multiple source-sink separations for the three-point correlators and used a twostate strategy to fit all separation data simultaneously to remove excited-state contaminatio 2703statsy  we also examined the su symmetry breaking using these couplings and found around 9% effect in this updated stud  acknowledgments we thank the milc collaboration for sharing the lattices used to perform this study; the lqcd calculations were performed using the chroma software suite this work is supported by michigan state university through computational resources provided by the institute for cyber-enabled researc  hl is supported by the us national science foundation under grant phy 1653405 career: constraining parton distribution functions for new-physics searche  savage and   walden, phy  re  winston, an  re  nuc  par  sc  bedaque,   savage, nuc  phy  lattimer and   prakash, phy  rep  schaffnerbielich, phy  re  jenkins, and   manohar, phy  re -  lin et al, pro  par  nuc  phy  100, 107arxiv:171 - -  bhattacharya, phy  re  d98, 034503arxiv:180 - - - - -  yoon, phy  re  d94, 054508arxiv:160  gupta,  -  yoonphy  re  d92, 094511arxiv:150  negele,   krieg, phy  re  d86, 114509arxiv:120 -  yamazaki, phy  re  d82, 014501arxiv:100    abdel-rehim et al, phy  re  rdl,   sldner, and   sternbeck, phy  re  d91, 054501arxiv:141  zanottiphy  re  let  100, 171602arxiv:080 -  melnitchouk,   shows, phy  re  let  120, 152502arxiv:171 -  lin and   orginos, phy  re  d79, 034507arxiv:071  takahashi, phy  let  b686, 36arxiv:091  kallidonis, phy  re  d94, 034502arxiv:160    hasenfratz and   knechtli, phy  re  wongphy  re    bazavov et a  re  d87, 054505arxiv:121  tanabashi et a  re  d98, 030001 edwards and   joolattice field theor  phy  pro  supp ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "One-dimensional", "Section": "One-dimensional few-electron effective Wigner crystal in quantum and classical regimes", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190  lieber, yuri   manin, and matilde marcolli abstrac  we construct geometric lifts of the bost-connes algebra to grothendieck rings and to the associated assembler categories and spectra, as well as to certain categories of nori motive  these categorifications are related to the integral bostconnes algebra via suitable euler characteristic type maps and zeta functions, and in the motivic case via fiber functor  we also discuss aspects of f1-geometry, in the framework of torifications, that fit into this general settin ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1. Introduction and summary", "Text": " connes and   consani ;   huber, s  in this sense, our present paper can be considered as a continuation and further extension ofand we will be relying on much of the work in that paper for details and example  the main theme of is an exploration of how this structure manifests itself beyond the usual constructions of f1-structures on certain classes of varieties over   in particular, the results of focus on lifts of the integral bostconnes algebra to certain grothendieck rings and to associated homotopy-theoretic spectra obtained via assembler categories, and also on another form of f1-structures arising through quasi-unipotent morse-smale dynamical system  the main difference between the present paper and consists in a change of the categorical environment: the unifying vision we already considered in was provided by   zakharevich's notions of assemblers and scissors congruences: c  as inwe focus primarily on various geometrizations of the bost-connes algebr  some of these constructions take place in grothendieck rings, like the previous cases considered inand are aimed at lifting the bost-connes endomorphisms to the level of homotopy theoretic spectra through the use of zakharevich's formalism of assembler categorie  we focus on the case of relative grothendieck rings, endowed with appropriate equivariant euler characteristic map  lieber, yuri   manin, and matilde marcolli admits torifications, we introduce zeta functions based on the counting of points over f1 and over extensions f1  we present a more general construction of bost-connes type systems associated to exponentiable motivic measures and the associated zeta functions with values in witt rings, obtained using a lift of the bost-connes algebra to witt rings via frobenius and verschiebung map  we then consider lifts of the bost-connes algebra to nori motives, where we use a version of nori motives, which may be of independent interest in view of possible versions of equivariant period  in this categorical setting we show that the fiber functor from nori motives maps to a categorification of the bostconnes algebra previously constructed by tabuada and the third author, compatibly with the functors realizing the bost-connes structur ", "Subsections": [{"Section_Num": "1_1", "Section": "1.1. Structure of the paper and main results", "Text": "below we will briefly describe the content of the subsequent sections, and the main results of the paper, with pointers to the specific statements where these are prove  bost-connes systems and relative equivariant grothendieck ring  the main result in this part of the paper is theorem  11, where the existence of this lift is prove  the rest of the section covers the preliminary results needed for this main resul 1, in the form in which it was introduced in4 the associated equivariant euler characteristic ma  by an analysis of the structure of periodic points in lemma   bost-connes systems on assembler categories and spectr  again this extends to the equivariant relative case results that were obtained in for the non-relative settin  the main result in this part of the paper is theorem  1 we recall the formalism of assembler categories ofunderlying scissor congruence relations and grothendieck ring 4 we then lift this formalism by endowing the main relevant objects with an action of a finite cyclic group, with appropriate compatibility condition  it is this further structure that provides a framework for the respective lifts of the bost-connes algebras, as in the cases discussed in and in the ones we will be discussing in the following section  we give here a very general definition of bost-connes systems in categories, based on endofunctors of subcategories of the automorphism categor 15 on the lift of the bostconnes structure to functors of these assembler  bost-connes systems on grothendieck rings and assemblers of torified varietie  the main results of this part of the paper are proposition  5 where we construct assembler categories of torified varieties and we show the existence of a lift of the bost-connes algebra to these categorie 1 we introduce a relative version of these grothendieck rings of torified varietie 4 we prove the first main result of this section by constructing the lift of the bost-connes structur  lieber, yuri   manin, and matilde marcolli   torified varieties, f1-points, and zeta function  this section continues the theme of torified varieties from the previous section but with main focus on some associated zeta function 1, and dynamical zeta functions associated to endomorphisms of torified varieties that are compatible with the torificatio  the use of dynamical zeta functions is motivated by a proposal made in for a notion of f1-structures based on dynamical systems that induce quasi-unipotent maps in homolog  the two main results of this section are proposition  2 the counting of f1-points of a torified variety and its relation to the grothendieck clas 3 some explicit examples of computations of grothendieck classes in simple cases that have physical significance in the context of bps counting in string theor 4 we introduce the f1-zeta function and we prove proposition  5 we explain how the f1-zeta function can be obtained from the hasse-weil zeta functio 6 we consider torified varieties with dynamical systems compatible with the torification and the associated lefschetz and artin-mazur dynamical zeta function 1 and we prove in proposition   spectrification of witt vectors and lifts of zeta function  the motivic measure provides in this way a map that lifts the bost-connes structur  our main results in this section are proposition  3 of a spectrification of the ring w  this is obtained using its description in terms of the k0 of the endomorphism category er and of r, and the formalism of segal gamma-space  the spectrification we use here is not the same as the spectrification of the ring of witt vectors introduced in14 is prove  another main result in this section is proposition   bost-connes systems in categories of nori motive  when we replace the formalism of assembler categories and homotopy theoretic spectra underlying the grothendieck rings with geometric diagrams and associated tannakian categories of nori motives, with the same notion of categorical bost-connes systems introduced in definitions   the main result of this part of the paper is theorem  2 we review the construction of nori motives from diagrams and their representation 3 we construct a category of equivariant nori motive 4 we describe the endofunctors of this category that implement the bost-connes structure and we prove the main result in theorem  6 we generalize this result to the relative case, using arapura's motivic sheaves version of nori motive 6 we consider nori diagrams associated to assemblers and we formulate the question of their universal cohomological representation  this is a contemporary embodiment of the primordial grothendieck's dream that motives constitute a universal cohomology theory of algebraic varietie  lieber, yuri   manin, and matilde marcolli", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2. Bost-Connes systems in Grothendieck rings", "Text": "in it was shown that the integral bost-connes algebra of admits lifts to certain grothendieck rings, via corresponding equivariant euler characteristic map  the rest of the section then develops the intermediate steps leading to the proof of the main results of theorem  ", "Subsections": [{"Section_Num": "2_1", "Section": "2.1. Bost-Connes algebra", "Text": " formality means here that the sum is not related to the additive structure of   in an integral model of the bost-connes algebra was constructed in order to develop a model of f1-geometry in which the bost-connes system encodes the extensions f1m, in the sense ofof the field with one element f  lieber, yuri   manin, and matilde marcolli", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2_2", "Section": "2.2. Relative Grothendieck ring", "Text": "we describe here a variant of construction ofwhere we work with relative grothendieck rings and with an euler characteristic with values in a grothendieck ring of locally constant sheave  we show that this relative setting provides ways of lifting to the level of grothendieck classes certain subalgebras of the integral bost-connes algebras associated to the choice of a finite set of non-archimedean place  we will write s as shorthand notation for the class in k  when s = spec one recovers the ordinary grothendieck ring k ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2_3", "Section": "2.3. Equivariant relative Grothendieck ring", "Text": " the relative equivariant grothendieck ring kg 0 is obtained as follow  we will write z/nzeffectively finite when we need to explicitly keep track of the level  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2_4", "Section": "2.4. Equivariant Euler characteristic", "Text": " 10 joshua   lieber, yuri   manin, and matilde marcolli proo  by precomposing with the euler characteristic one then obtains the map fixed points and delocalized homolog  equivariant characteristic classes from constructible sheaves to delocalized homology are constructed in as an observation, we can see explicitly the relation of delocalized homology to the integral bost-connes algebra, by considering the following cases", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2_5", "Section": "2.5. The geometric Verschiebung action", "Text": "we recall here how to construct the geometric verschiebung action used in to lift the bost-connes maps to the level of grothendieck ring ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2_6", "Section": "2.6. Lifting the Bost\u2013Connes endomorphisms", "Text": " proo  lieber, yuri   the main difference with the relative case considered here lies in the fact that the lifts to the equivariant relative grothendieck rings given by the maps and need to transform in a compatible way the actions on both x and  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2_7", "Section": "2.7. Prime decomposition of the Bost\u2013Connes algebra", "Text": " we refer to az,f as the f-part of the integral bost-connes algebr  we refer to a z as the f-coprime part of the integral bost-connes algebr ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2_8", "Section": "2.8. Lifting the FN-coprime Bost\u2013Connes algebra", "Text": " proo  lieber, yuri   thus, one also wants to recover the structure of the complementary part of the bost-connes algebra with the group ring z and the semigroup nf ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2_9", "Section": "2.9. Lifting the full Bost\u2013Connes algebra", "Text": " proo  for simplicity we consider the case where the fixed point set and periodic points sets of the action are all finite set  16 joshua   lieber, yuri   then the maps and intertwine the bost-connes structure in the sense of definition   proo ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3. From Grothendieck Rings to Spectra", "Text": "in this section we show that the bost-connes structure can be lifted further from the level of the relative grothendieck ring to the level of spectra, using the assembler category construction of the results of this section are a natural continuation of the results in this section deals with the corresponding generalization of the remaining step 1 of zakharevich's formalism of assemblers which axiomatizes the scissors congruence relations15, on the lifting of the bost-connes structure to this assembler categor  18 joshua   lieber, yuri   manin, and matilde marcolli", "Subsections": [{"Section_Num": "3_1", "Section": "3.1. Assemblers", "Text": "below we will recall the basics of a general formalism for scissors congruence relations applicable in algebraic geometric contexts defined by   zakharevich in and the abstract form of scissors congruences consists of categorical data called assemblers, which in turn determine a homotopy-theoretic spectrum, whose homotopy groups embody scissors congruence relation  this is used to obtain a characterisation of the kernel of multiplication by the lefschetz motive, which provides a general explanation for the observations ofon the fact that the lefschetz motive is a zero divisor in the grothendieck ring of varietie  20-2  let c be a category with a grothendieck topolog  zakharevich's notion of an assembler category is then defined as follow  morphisms of non-initial objects are induced by those of c  the data are called the abstract scissors congruence 2, then provides the homotopy theoretic spectra associated to assembler categories as in this construction of assembler categories and spectra provides the formalism we use here and in the previous paper to lift bost-connes type algebras to the level of grothendieck rings and spectr  the construction can be generalized to symmetric monoidal categories, lieber, yuri   the construction is functorial in c, with respect to functors preserving sums and the zero objec  the segal construction determines a functor from the category of small symmetric monoidal categories to the category of -1-connected spectr  it is shown in that this functor determines an equivalence of categories between the stable homotopy category of -1-connected spectra and a localization of the category of small symmetric monoidal categories, obtained by inverting morphisms sent to weak homotopy equivalences by the functo ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3_3", "Section": "3.3. Automorphism category and enhanced assemblers", "Text": "we describe in this and the next subsection a general formalism of enhanced assemblers underlying all the explicit cases of bost-connes structures in grothendieck rings discussed in and in some of the later sections of this pape  we first recall the definition of the automorphism categor  thus, we can make the following general definitio  in the following we will be especially interested in the case where the chosen subcategory is determined by a group action, see remark   let c be a categor  however, one expects other interesting examples that are not necessarily given by group actions, hence it is worth considering this more general formulatio  assume that c is endowed with a structure of assemble  in particular the bost-connes type structures we are investigating can be formulated broadly in this setting of enhanced assemblers as follow ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3_4", "Section": "3.4. Bost\u2013Connes systems on categories", "Text": " 22 joshua   lieber, yuri   the object in definition   the more general formulation given in definition   such more general classes of categorical bost-connes systems are not discussed in the present paper, but they are a motivation for future work, for which we just set the general framework in this sectio  a generalization of definition   the reason why we need the following modification of definition  5, in the specific case where the automorphisms are determined by a group action", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3_5", "Section": "3.5. Assemblers for the relative Grothendieck ring", "Text": " the category cs of definition   lieber, yuri   the argument is the same as inand in morphisms are embeddings compatible with the structure maps as in hence in particular monomorphism  the argument is as in the previous case and in lemma  1 ofusing the inclusion-exclusion relations as in proposition  ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4. Torifications, F1-points, zeta functions, and spectra", "Text": "in this section we relate the point of view developed inwith lifts of the bostconnes system to grothendieck rings and spectra, to the approach to f1-geometry based on torification  this was first introduced in weaker forms of torification were also considered inwhich allow for the development of a form of f1-geometry suitable for the treatment of certain classical moduli space  lieber, yuri   manin, and matilde marcolli determine a diagonal embedding in each torus and an action by multiplicatio  this is a very restrictive class of varieties, because the existence of a torification on a variety implies that the grothendieck class is a sum of classes of tori with nonnegative coeffcient  the resulting construction will be more restrictive than the one considered in torification  similarly, there are different possibilities when one considers morphisms of torified varieties, see in view of describing associated grothendieck rings, we review the different notions of morphism  the grothendieck classes are then defined with respect to the corresponding type of isomorphis  the torifications and are strongly equivalent if the identity map on x is a torified morphism as abov  this complemented condition is very stron  28 joshua   lieber, yuri   manin, and matilde marcolli the reader can consult the explicit examples given in to see how these notions can be differen  these two torifications are related by a weak isomorphism, hence the elements and define the same class in k0w, but they are not related by an ordinary isomorphism so they define different classes in k0  relative cas  the construction for ordinary and weak morphism is similar, with the appropriate changes in the definitio ", "Subsections": [{"Section_Num": "4_2", "Section": "4.2. Group actions", "Text": "in order to operate on grothendieck classes with bost-connes type endomorphisms, we introduce appropriate group action  torified varieties carry natural q/z actions, since the roots of unity embed diagonally in each torus of the torification and act on it by multiplicatio 5 we need to be able to describe the cyclic permutation action of z/nz on the finite set zn as an action factoring through z/n  assembler and spectrum of torified varietie  lieber, yuri   the grothendieck topology is generated by covering families with g-equivariant embedding  proo  the argument is again as insee lemma   we check that the category admits pullback  this means that the tori of the torification ty are restrictions to y of tori of the torification tx of   the weak case is constructed similarly to the ordinary case on the pieces of the decompositio  the equivariant cases are constructed analogously, as discussed in the case of equivariant grothendieck rings of varieties in similarly, for the g-equivariant cases of proposition   proo  since for morphisms strong implies ordinary and ordinary implies weak, one obtains inclusions of assemblers as state  lifting of the bost-connes system for torification  we write for this objec  proo  the proof is completely analogous to the case discussed in theorem  15 and to the similar cases discussed in bost-connes type quantum statistical mechanical systems associated to individual toric varieties were constructed in here instead of bost-connes endomorphisms of individual varieties we are interested in a bost-connes system over the entire grothendieck ring and its associated spectru  32 joshua   lieber, yuri   manin, and matilde marcolli", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5. Torified varieties and zeta functions", "Text": "we discuss in this section the connection between the dynamical point of view on f1-geometry proposed in and the point of view based on torification 2 the notion of f1-points of a torified variety and its relation to the torification of the grothendieck class, with some explicit example 4 and we show its main properties in proposition  5 we explain the relation between the f1-zeta function and the hasse-weil zeta functio 6 we consider the point of view on f1-structures proposed in based on dynamical systems inducing quasi-uniponent endomorphisms on homology, in the particular case of torified varieties with dynamical systems compatible with the torificatio  we then prove in proposition  8 that the resulting dynamical zeta function have similar properties to the f1-zeta function in the sense that both define exponentiable motivic measures from the grothendieck rings of torified varieties to the witt rin ", "Subsections": [{"Section_Num": "5_1", "Section": "5.1. Counting F1-points", "Text": " any form of torified structure in particular implies that the variety is polynomially countable, hence that the counting function nx is a polynomial in q with z-coeffcient  similarly, one can define extensions f1m of f1, in the sense of these corresponds to actions of the groups mm of m-th roots of unit  in terms of a torified structure, the points over f1m count m-th roots of unity in each torus of the decompositio 10 of and theorem 1 of ).  summarizing, we have the followin  bialynicki-birula decompositions and torified geometrie  we recall the result here, in a particular case which gives rise to examples of torified varietie  let x be a smooth projective k-variety x endowed with a gm action such that the fixed point locus xgm admits a torification of the grothendieck clas  then x also admits a torification of the grothendieck clas  proo  the class in the grothendieck ring k0 decomposes then as = x i ld  it is then immediate that, if the components zi admit a geometric torification then the variety x also doe  lieber, yuri   an example of torified varietie  a physically significant example of torified varieties of the type described in lemma  2 arises in the context of bps state counting of it is natural to ask whether the counting of f1m-points, which corresponds to the counting of roots of unity in the tori of the torification, can also carry physically significant informatio  bps counting and the virtual motiv  the formulation of the refined bps counting given in can be summarized as follow  the formal square root of the leftschetz motiv  lieber, yuri  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5_4", "Section": "5.4. Counting F1-points and zeta function", "Text": " this is the counting of the number of m-th roots of unity in each torus ti = of the torificatio  in the case of torified varieties, there is an analogous zeta function over f  we think of this f1-zeta function as defined on torified grothendieck classes, zf  proo  as in the case of the hasse-weil zeta function over fqthe f1-zeta function gives an exponentiable motivic measur  proo  the ghost map is an injective ring homomorphis  lieber, yuri   manin, and matilde marcolli", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5_5", "Section": "5.5. Relation to the Hasse\u2013Weil zeta function", "Text": " let zfq be the hasse-weil zeta function of a torus t  proo  the f1-zeta function of is obtained from the hasse-weil zeta function of in the following wa  proo  to obtain the f1-zeta function we then use lemma 5", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5_6", "Section": "5.6. Dynamical zeta functions", "Text": " we recall the definition and main properties of the relevant dynamical zeta functions, which we will consider in proposition   properties of dynamical zeta function  a particular class of maps with the property that they induce quasi-unipotent morphisms in homology is given by the morse-smale diffeomorphisms of smooth manifolds, see morse-smale diffeomorphisms are structurally stable among all diffeomorphisms,40 joshua   lieber, yuri   the case of morse-smale diffeomorphisms can be treated as in to obtain rationality and a description in terms of the homological zeta function  in the setting of real tori rd/zd, one can considers the case of a toral endomorphism specified by a matrix m belong to m  torifications and dynamical zeta function  the zeta functions define exponentiable motivic measures on the grothendieck ring k0a of torified varieties with values in   proo  the product is given by the cartesian product * thus, we can use as in proposition   the case of the torified varieties and the zeta functions is analogous, combining the additive and multiplicative behavior of the fixed point counting and the lefschetz numbers on each torus and of the decomposition into tori as in proposition   lieber, yuri   manin, and matilde marcolli that is completely determined by the map on the first homolog  similar phenomena in the more general case of endomorphisms of abelian varieties in positive characteristic have been investigated in", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "6", "Section": "6. Spectra and zeta functions", "Text": "6 zeta functions arising from certain counting functions that define ring homomorphisms from suitable grothendieck rings to the witt ring   we consider here a more general setting of exponentiable motivic measure  it is viewed as an element in the witt ring   we investigate here how to lift the zeta functions of exponentiable motivic measures to the level of spectr ", "Subsections": [{"Section_Num": "6_1", "Section": "6.1. The Endomorphism Category", "Text": "let r be a commutative rin  we denote by er the endomorphism category of r, which is defined as follows the category of finite projective modules over r is identified with the subcategory corresponding to the objects with trivial endomorphis  an exact sequence in er is a sequence of objects and morphisms in er which is exact as a sequence of finite projective modules over r this determines a collection of admissible short exact sequence lieber, yuri   manin, and matilde marcolli in the case of k0 an explicit description is given by the following,let k0 denote the k0 of the endomorphism category e  it is a ring with the product structure induced by the tensor produc ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "6_2", "Section": "6.2. Spectrum of the Endomorphism Category and Witt vectors", "Text": "let pr denote the category of finite projective modules over a commutative ring r with uni  proo  arguing as in lemma   lieber, yuri   one can obtain in this way a reformulation in terms of spectra of the result of we give a variant of lemma  3 that will be useful in the followin  again we consider p+/r as a subcategory of e+/r with trivial endomorphism  proo  the map is clearly compatible with sum  moreover, it maps k0 to k  the following result follows as in lemma   nonetheless, the circle action on thh that is used to obtain the spectrum tr is closely related to the bost-connes structure investigated in the present pape  a more direct relation between bost-connes structures and topological hochschild and cyclic homology will also relate naturally to the point of view on f1-geometry developed in we will leave this topic for future wor  the problem of lifting to the level of spectra the hasse-weil zeta function associated to the counting motivic measure for varieties over finite fields was discussed in to this purpose, we make some assumptions of rationality and the existence of a factorization for our zeta functions of exponentiable motivic measure  48 joshua   lieber, yuri   manin, and matilde marcolli lemma   proo  to an object x we assign an object of er obtained in the following wa  the pair is an object of the endomorphism category e+/  proo  the map of grothendieck rings given by the composition also lifts to a map of spectr  proo 4 at the level of spectr  the k-theory spectrum of an abelian category a is weakly equivalent to the k-theory spectrum of the category of bounded chain complexes over   in fact, this holds more generally for a an exact category closed under kernel  thus, the composition can also be lifted at the level of spectr  lieber, yuri   manin, and matilde marcolli it should be noted that the construction of a derived motivic zeta function outlined above is not the first to appear in the literatur  essentially, the approach in was to start with a weil cohomology theory and then to construct a derived motivic measure realizing on the level of k-theory the assignment to a variety x of its corresponding cohomology group  this method has yielded deep insight into the world of algebraic geometr  this method still needs to be studied further to yield additional insights into what it captures about the geometry of varietie ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "6_4", "Section": "6.4. Bost\u2013Connes type systems via motivic measures", "Text": " it is worth noting that the endofunctors of definition  12 are akin to those used in the definitions of topological cyclic and topological restriction homology, the frobenius and verschiebung fn and vn of definition   proo  the action of the frobenius fn = which is an endofunctor of e  this extends to a compatible endofunctor of e+/r by fn = lieber, yuri  7, that is of homotopical bost-connes typ  proo  by lemma   this shows the compatibilities of the natural transformations in the diagrams above", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "6_5", "Section": "6.5. Spectra and spectra", "Text": " in this section the term spectrum will appear both in its homotopy theoretic sense and in its operator sens  bost-connes tannakian categorification and lifting of the spectral euler characteristi 6 ofwhile for the right-hand-side of we use the categorification of bost-connes system constructed in we begin by recalling the categorification of the bost-connes algebra of as shown in theorem   54 joshua   lieber, yuri   proo 7 the categorification of the bost-connes algebra obtained via nori motives with the one of recalled her ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "7", "Section": "7. Bost\u2013Connes systems in categories of Nori motives", "Text": "we introduce in this section a motivic framework, with bost-connes type systems that on tannakian categories of motive  the main result in this part of the paper will be theorem  9 then extends this bost-connes structure to the relative case of motivic sheave ", "Subsections": [{"Section_Num": "7_1", "Section": "7.1. Nori diagrams", "Text": "more precisely,we have the following definition  we will consider only diagrams with one identity for each verte  diagrams can be considered as objects of a category, with obvious morphism  notice that a considerably more general treatment of graphs with markings, including diagrams et  in the operadic environment, can be found in we do not use it here, although it might be highly relevan ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "7_2", "Section": "7.2. From geometric diagrams to Nori motives", "Text": "we recall the main idea in the construction of nori motives from geometric diagram  for more details, seep  140-14  56 joshua   lieber, yuri   d4) the fact that c has a functor to r-mod follows directly from the definition and the finite cas  the result is called the diagram category   universal diagram categor  the following results explain why abstract diagram categories play a central role in the formalism of nori motives: they formalise the grothendieck intuition of motives as objects of the universal cohomology theor  the functor l is unique up to unique isomorphism of exact additive functor  for proofs, c  140-141 and   nori geometric diagram  b1) let and be two pairs of closed embedding  b2) let be a stair of closed embedding  homological representatons of nori geometric diagram  for a survey of such pairs that were studied in the context of grothendieck's motives, seep  31-13  below we will consider the basic example of cohomological representations of nori diagrams that leads to nori motive  effective nori motive  we followp  207-20  we can then define the nori diagram d as abov  the category of effective mixed nori motives is the diagram category c where hi is the respective singular cohomology of the analytic space xan 58 joshua   lieber, yuri   manin, and matilde marcolli", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "7_3", "Section": "7.3. Category of equivariant Nori motives", "Text": "we now introduce the specific category of nori motives that we will be using for the construction of the associated bost-connes syste  b2) let be a stair of closed embeddings compatible with enhancements", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "7_4", "Section": "7.4. Bost\u2013Connes system on Nori motives", "Text": "bwe now construct a bost-connes system on a category of nori motives obtained from the diagram d described above, which lifts to the level of motives the categorification of the bost-connes algebra constructed in we are going to use here the second descriptio  proo  we proceed in the following way to obtain the bost-connes structure in this settin  lieber, yuri   proo  by lemma   for such elements the product is given by * = here we presented a different motivic categorification of the bost-connes algebra by lifting the bost-connes structure to the level of the category of nori motive  in a motivic bost-connes structure was also constructed using the category of motives over finite fields and the larger class of weil numbers replacing the roots of unity of the bost-connes syste ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "7_5", "Section": "7.5. Motivic sheaves and the relative case", "Text": "the argument presented in theorem   lieber, yuri   the categorical setting of nori motives that is appropriate for this relative case is the nori category of motivic sheaves introduced in we recall here briefly the construction of the category of motivic sheaves of and we show that the bost-connes structure on the category of nori motives described in theorem  7 extends to this relative settin  here in the setting of the tate motives are accounted for in the diagram construction by the presence of the twist w and the last class of edge  it is shown in that the nori formalism of geometric diagrams applies to this setting and gives rise to a tannakian category of motivic sheaves mn  we modify the construction of in the following wa  proo  the argument is as in proposition  7 the fact that maps of diagrams induce functors of the resulting categories of nori motives", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "7_6", "Section": "7.6. Nori geometric diagrams for assemblers, and a challenge", "Text": "we conclude this section on bost-connes systems and nori motives by formulating a question about nori diagrams and assembler categorie  64 joshua   lieber, yuri   in particular, each such c is endowed with a grothendieck topolog  briefly, i translates to the level of nori geometric diagrams the weight filtration of various cohomology theoriesand the existence of such translation and its structure are encoded in several versions of nori's basic lemma independently and earlier discovered by   beilinson and   vilonen the most transparent and least technical version of the basic lemma shows that in algebraic geometry the existence of weight filtration is based upon special properties of affne scheme  as we will see in the last section, lifts of bost-connes algebras to the level of cohomology based upon the techniques of enhancement also require a definition of affne assembler  since we do not know its combinatorial version, the enhancements that we can study now, force us to return to algebraic geometr  acknowledgmen  we thank the referee for many very detailed and useful comments and suggestions on how to improve the structure and presentation of the pape  the first and third authors were supported in part by the perimeter institute for theoretical physic  the third author is also partially supported by nsf grant dms-1707882, and by nserc discovery grant rgpin-2018-04937 and accelerator supplement grant rgpas-2018-522593", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "References", "Section": "References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Persistent", "Section": "Persistent gravitational wave observables: general framework", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190  we also derive the same formula from the quantization of d3-branes in s5/z arai@t phy titec a fujiwara@t phy titec a titec a jp", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "four-dimensional superconformal field theories have been studied for many year  in recent years, some progress has been made toward understanding of the theorie  see also for more comprehensive analysis of marginal deformation  s-folds are generalization of the orientifol  we call them s-fold theorie  we denote the theory by   because s-fold theories are defined by using the brane construction, it is natural to analyze them with the ads/cft correspondence 1 a relation between wrapped branes around non-trivial cycles in s5/zk and gauge invariant operators was also discussed in in this paper we focus on bps partition functions of s-fold theorie  the taylor expansion of these grand partition functions generates the partition functions for an arbitrary   the gauge invariant operators made of the scalar fields form the coordinate ring of the moduli spac  the moduli spaces of the s-fold theories are all orbifolds of the form ms = c3n/w  there is a general formula known as the molien series which gives the partition function for an arbitrary orbifol  see for an application to the theories on d3-branes probing orbifold  the molien series is also applicable to s-fold theories, and in this sense our result is not nove  however, the formula we derive in this paper has very simple structure which cannot be seen in the molien series, and the correspondence to the holographic picture is cleare  in particular, the formula gives separately the contribution of each sector specified by the d3-brane winding number in the internal space s5/z  we show that the same formula is reproduced from the analysis of d3-brane  this paper is organized as follow  the resulting partition function is the same as that of n bosonic particles in a threedimensional harmonic potentia  we confirm that the partition function is consistent to the supersymmetry enhancement proposed in we also comment on some relations among partition functions via discrete gauging  in section 5 we reproduce the same formula by quantizing d3-branes in s5/z  finally, in section 6 we discuss our results and open question  in appendix a we give the definition of the molien series and its application to the u theor  analysis in this section is based mainly on see also for a generalization to a large class of theories associated with calabi-yau  j1, j2, and j3 are cartan generators of su  we use an so basis for these generator  there are several types of bps operator  the corresponding partition function is often called the coulomb branch hilbert serie  the corresponding partition function is often called the higgs branch hilbert serie  in this paper we are interested in bps operators in theories with non-vanishing coupling constan  in the following, we describe bps operators in two way  the first is the casimir representation in which we represent operators as polynomials of trace operators the other is the oscillator representation in which we give gauge invariant operators as polynomials of diagonal components of scalar field  the former is convenient to describe 1 2-bps operators and we can easily calculate the 1 2-bps partition function with this representation for an arbitrary gauge grou  the latter is suitable to describe more general bps operators and enable us to calculate the 1 8-bps partition function of the u sy  an operator in the 1 2-bps sector consists only of one adjoint scalar field   the number of independent casimir operators is the same as the rank of the gauge grou  let oi be the independent casimir operators and di be their scaling dimension  the dimensions di for some groups are shown in table   the casimir operators are oi = tr, we diagonalize the scalar field z by the gauge transformation and let zi be the diagonal component  we can use the set of following symmetric polynomials as a basi  gauge invariant operators can be regarded as functions in the moduli space, and the symmetric polynomials are generators of the coordinate ring of the coulomb branch moduli space mc = cn/w  this is the same as the hilbert space of n one-dimensional harmonic oscillator  two vectors with different orders of the components are identified, and this is interpreted as the bose statistics of the particle  the equivalence of two descriptions, the casimir representation and the oscillator representation, becomes obvious if we use partitions to specify operator  elements of both bases are labeled by a non-ascending series of non-negative integers, each of which can be represented as a young diagra  the two young diagrams are transposition of each other, and the two descriptions give the same partition functio  we can easily show by the q-binomial formula that is obtained from by the taylor expansion with respect to   labels energy eigenstates of the harmonic oscillato  it is important that k starts from zero corresponding to the ground stat  the finiteness of n affects the degeneracy of states only when the energy is comparable to or greater than   the oscillator representation is more suitabl  let be the diagonal component  in appendix a we show that the bps partition functions obtained from are equal to those from the molien serie  interestingly, this partition function can be reproduced on the gravity side as the contribution of sphere giants or ads giants descriptions of two types of giant gravitons are complementary, and each of them gives the result identical to we call the function i in the single-particle partition function by two reasons:   i is the partition function of a single three-dimensional harmonic oscillato  from the viewpoint of the gravity dual in the large n limit i can be regarded as the partition function of a single kk particle in s5 -1 in the last expression eliminates the contribution of the harmonic oscillator ground stat  therefore, only the excited states of the harmonic oscillator correspond to the kk graviton  we can define the single-particle partition function iu for finite n by zu = pexp x00iu x01 3 bps partition functions for so and sp theories  1 1 2-bps partition function let us extend the derivations of bps partition functions in the last section to the so and sp gauge theorie  we first consider the 1 2-bps secto  let us first consider the so theor  this is obtained simply by replacing z in zu by z  let us consider the so case firs  we treat a triplet as the coordinates of a three-dimensional harmonic oscillato  again, we can regard this as a system of n bosonic particles in the three-dimensional harmonic potentia  the invariance under the sign change requires the wave function of each particle to be even under the z2 actio  on the ads side, has a clear interpretation at least in the large n limi  in the so case, we can again use n triplets to describe gauge invariant operator  although we can change signs of two triplets simultaneously, it is not possible to change the sign of a single triple  the twisted boundary condition removes even energy eigenvalues of the harmonic oscillato  in particular, oscillators in the twisted sector cannot be in the zero energy ground stat  therefore, it is natural to identify the twisted sector to the contribution of a wrapped d3-bran  we can calculate the partition function of the sp theory in a similar wa  each sp factor is associated with the coordinates of a three-dimensional harmonic oscillato  unlike the so case we can flip the sign of the coordinates of each particle independently by the rotation in the sp factor, and therefore wsp = ws  the condition imposed on the wave function is the same as in the so case, and the partition function is given by this is of course the expected result from the montonen-olive dualit  from the viewpoint of the gravity dual, p is related to the discrete torsion of the three-form fluxes in string theory so and sp theories are realized by using o3-plane  namely, the following relations hol ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 BPS partition functions of U(N) SYM", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 BPS partition functions for SO and Sp theories", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 S-fold theories", "Text": " in the following subsections we study bps partition functions of such theorie  before starting the analysis let us carefully choose the zk action on the scalar fields and comment on the relations to the coulomb branch and higgs branch hilbert serie  this must be consistent to the definition of the bps partition function, in which we need to use one supercharge to write down the bps conditio  namely, a choice of supercharge fixes a complex structure in r  what is important is that the supercharge chosen in the construction of the s-fold should be different from the one used in the bps conditio  this again gives a corresponding complex structure, which is different from the ones appearing abov ", "Subsections": [{"Section_Num": "4_1", "Section": "4.1 Grand partition functions for Z3,4,6 S-folds", "Text": " we use the oscillator representatio  namely, we express gauge invariant operators as polynomials which are invariant under the weyl group w  we simply generalize it to the total moduli space, and define ws as the group generated by the following operation  the integer p in can be assumed to be a divisor of   we want to interpret these operations from the viewpoint of the oscillator descriptio  these states are related to the pfaffan-like operator  on the gravity side we regard m-th sector as the contribution of wrapped d3-brane with the winding number m belong to h3 = z  the grand partition function is obtained by summing up the contribution of sector  the contribution of the m-th sector is pexp x00tizk m x01and the sectors summed up are determined by the invariance under this is the main result in this pape  the sectors summed up are determined by the condition that the gauge bundle of the corresponding wrapped d3-brane is consistently define  otherwise, the nontrivial ns-ns and r-r three-form fluxes induce electric and magnetic charge on the worldvolume of the wrapped d3-bran  holds not only for the bps partition function but also for the superconformal inde  this was used in to calculate the superconformal indices of s-folds in the large n limi  let us confirm the consistency of our formula to this phenomeno  this is expected to be the same as zg  we can also compare this partition function to the partition function of the z2 gauging of the su sy  as is pointed out in the 1 2-bps partition function of the g2 sym is obtained from the su sym by the discrete gauging of z2 charge conjugation symmetr  it is also possible to use the molien series to obtain the same partition functio  we can directly confirm the agreement of and zs = zg  and means that zs and zs are related by the z2 discrete gaugin  this is explicitly shown in the next subsectio ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4_3", "Section": "4.3 Discrete gauging", "Text": "the discrete gauging is the prescription to obtain a new theory from a parent theory by gauging a discrete symmetry of the parent theor  we remark that the gauging is different from the s-folding, and s-fold partition functions are not necessarily obtained from zu by a gaugin  instead, the gaugings give additional relations among s-fold partition functions as we will show shortl  some of the relations below belong to the class of discrete gaugings associated with the principal extensions of the gauge groups, which are investigated in as is pointed out in the s-fold theory s has a zp global symmetr  let q be a divisor of   let us denote the new theory by s/z  17 note that this relation is a reflection of the coincidence of the moduli space, and does not mean the equivalence of the two theorie  in general, the two theories are different theories that have different central charge  /z2 is the o sym while s is the so sy  this is in contrast to the s-fold projection in the large n limit in which the projection is carried out before the plethystic exponentia  this is the relation we wanted to sho ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 D3-brane analysis", "Text": "in this section we reproduce the partition function by quantizing d3branes in s5/z  the analysis is quite similar to the analysis of sphere giants in actually we can use the essential part of the calculation in as it is for our purpos  the analysis in starts from the bps brane configuration obtained by mikhailov due to the coupling of the d3-brane to the background rr flux the wave function is not just a function but a section of the line bundle o over this configuration spac  therefore, the quantization reduces to the simple problem to find holomorphic sections of 19 this line bundl  there are two issues which make the problem complicate  one is that different functions f may give the same brane configuration, and we should remove the redundanc  the detailed analysis in shows that even if we take account of these issues the result is the same as what we obtained by naive analysis neglecting these issue  let us assume that this is the case for the s-fol  we identify m with the winding number of a d3brane around the non-trivial cycle in s5/z  this is easily shown as follow  the resulting configutation obviously has the winding number   because such a deformation does not change the homology class of the brane configuraion, an arbitrary brane configuration given by a function satisfying has winding number   now, let us follow the quantization procedure of under the restriction", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "6", "Section": "6 Discussion", "Text": "in this paper we derived the bps partition functions for arbitrary s-fold theorie  we confirmed that the formula is consistent to the lie algebra isomorphism  we also gave some relations among partition functions via discrete gauging  the formula gives the partition function as the sum of contributions of sector  from the holographic point of view, different sectors correspond to different winding numbers of d3-branes around the non-trivial cycle in the internal space s5/z  we derived the same formula by quantizing d3-branes in s5/zk following the similar analysis of sphere giants the derivation on the scft side is based on the harmonic oscillator description of bps operator  in the large n limit the sectors with wrapped branes decouple, and only the untwisted sector contributes to the partition functio  each excited state of a harmonic oscillator can be regarded as a kk mode in s5/z  for finite n, this correspondence is not so obviou  in particular, the twisted sector gives a pfaffan operator as a bound state of n harmonic oscillator  naively, this may be interpreted on the gravity side that a d3-brane wrapped on the non-trivial cycle in s5/zk is a bound state of kk modes satisfying the twisted boundary conditio  however, it is not possible to impose the twisted boundary condition on kk modes due to the absence of gauge fields minimally coupling to graviton  at present, unfortunately, we have no clear explanation how this is realize  it may be interesting to study the relation between the harmonic oscillator description and the quantization procedure of d3-branes we used in section   other than the bps partition function, there is another important quantity reflecting the operator spectrum: the superconformal inde  it has many connections to physical quantitie  it would be very interesting to inves21 tigate the relation of our analysis to the superconformal inde  acknowledgments we are grateful to   mori for wonderful discussion  the work of   was partially supported by advanced research center for quantum physics and nanoscience, tokyo institute of technolog  the work of    ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "A", "Section": "A The Molien series", "Text": " in this appendix we show that the grand partition function for the unitary gauge groups is also obtained by using the molien serie  the coordinate ring of the orbifold v/g is spanned by polynomials of vi invariant under let cn be the number of linearly independent ginvariant homogeneous polynomials of degree   this formula can be applied to an arbitrary orbifol  let us first consider 1 2-bps partition functio  22 because the summand takes the same value for permutations in the same class we can replace the summation over permutations g belong to sn by the summation over conjugacy classes with appropriate multiplicities inserte  let kj be the number of j-cycles in   note that for a specific n only finite number of kj are non-vanishing due to the constraint corresponding to the cycle decomposition, the matrix in -zd) appearing in takes the block-diagonal for  each block corresponds to each cycl  it is straightforward to extend the analysis above to the 1 8-bps partition functio  as the result, we obtain the molien series for the full moduli space c3n/sn, which is given by with the factor 1/kj replaced by 1/3k  to obtain the partition function with generic fugacities we need to consider a refinement of the molien series gives the grand partition functio ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "noname manuscript n  mendl and herbert spohn the date of receipt and acceptance should be inserted later abstract using the framework of nonlinear fluctuating hydrodynamicswe examine equilibrium spatio-temporal correlations in classical ferromagnetic spin chains with nearest neighbor interaction  in particular, we consider the classical xxz-heisenberg spin chain evolving deterministically and chaotically via hamiltonian dynamics, for which energy and z-magnetization are the only locally conserved field  for the easy-plane case, this system has a low-temperature regime in which the difference between neighboring spin's angular orientations in the xy plane is an almost conserved fiel  according to the predictions of nfh, the dynamic correlations in this regime exhibit a heat peak and propagating sound peaks, all with anomalous broadenin  e-mail: aviji das@ict re in kedar damle tata institute of fundamental research, mumbai 400005, indi  david   arxiv:190  tio  we find that, in a suitable intermediate temperature regime, the system shows two sound peaks with kardar-parisi-zhang scaling and a heat peak where the expected anomalous broadening is less clea  in high temperature regimes of both easy plane and easy axis case of lll, our numerics show clear diffusive spin and energy peaks and absence of any sound modes, as one would expec  we also simulate an integrable version of the xxz-model, for which the ballistic component instead moves with a broad range of speeds rather than being concentrated in narrower peaks around the sound spee  keywords hydrodynamics dynamical correlations heisenberg spin chain contents", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": ".  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 High-temperature, non-integrable and integrable Lattice Landau-Lifshitz equations", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Easy plane at low-temperatures", "Text": ".  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Numerical results at low temperature", "Text": ".  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Discrete time LLL dynamics", "Text": ".  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "6", "Section": "6 Conclusions", "Text": ".  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "7", "Section": "7 Acknowledgements", "Text": ".  23 a coupling coeffcients for nonlinear fluctuating hydrodynamics ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "A", "Section": "A Coupling coefficients for nonlinear fluctuating hydrodynamics", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "B", "Section": "B Low temperature approximation", "Text": ".  25 1 introduction for generic classical and quantum spin chains the only conservation law is the energy, perhaps in addition one spin component momentum conservation is destroyed by the underlying lattice and in thermal equilibrium the average currents vanis  one therefore expects that a local perturbation of the thermal state will spread diffusively, a behavior which is actually observed in a large variety of systems, as prototypical examples we refer to there are obvious exceptions such as integrable spin chains, for which a small perturbation induces a ballistic respons  these peaks broaden sub-ballistically but faster than diffusio  the theoretical argument is based on nonlinear fluctuating hydrodynamicswhich refers to long wavelength behavior and should thus be equally valid for both classical and quantum chain  in this contribution we thus restrict our study to classical spin chains of xxz typ  the results presented here for the xxz model bear close resemblance to those seen for the rotor model, where early results in indicated a diffusive to super-diffusive transport, and has recently been understood in the framework of nfh it should also be noted that there are no phase transitionsrather the anomalous scaling is observed over very long transient time-scale  then, at high temperatures, energy and the z-component of the spin diffus  however at low temperatures the spin motion is confined to a plane orthogonal to the z-axis and phase differences between neighboring spins are smal  in this regime, the phase differences are an almost conserved field, so there is a broad range of time scales where this conservation law dictates the hydrodynamic  under such conditions, nonlinear fluctuating hydrodynamics can be applie  the theory predicts the dynamical correlations contain a central non-propagating heat peak and left- and rightmoving sound peak  the three peaks broaden as nontrivial powers of time according to characteristic explicitly known scaling functions to illustrate the difference between sound peaks and ballistic broadening, we also simulate the integrable spin chain of fadeev and takhtajan however, to our knowledge the present paper is the first exploration of nfh in classical spin chain  clearly |sj| = 1 for all time  the s-s interaction could have additional contribution  many of our results are valid in greater generality, but we explore the simplest case we note that the hamiltonian is even and the currents are odd in r  thus the conventional expectation is to have a diffusive spreading of the equilibrium time-correlations for spin and energ  since they have opposite signature under time reversal, the cross diffusion coeffcient should vanis  in fi  1 plots of cab for xxz easy plane regime at high temperatur  the diffusion constants are  72 for spin and energy respectively, obtained from the corresponding gaussian fits and the decay of css, ce  as expected, energy and spin are uncorrelate  6 avijit das et a  we see that spin and energy autocorrelations indeed show diffusive behavior, while there are no cross correlation  the novelty of our contribution is to establish that at lower temperatures the dynamical properties change dramatically through the appearance of ballistic sound propagatio  but before embarking on that discussion we use the opportunity to illustrate that equilibrium time-correlations for an integrable spin-chain are dominated by a broad ballistic spreading, in contrast to the sub-ballistic broadening of the two sound peaks in the case of a nonintegrable chai  the form of dynamical correlations in classical integrable models have been discussed in several earlier work including for the toda chain and in for a particular integrable spin chain, which we will now discus  2 plots of cab for the integrable lll model in easy plane regime at high temperature at different time  the energy and spin are uncorrelate  energy correlation reaches the boundary of the system faster than the spin correlatio  from the maxima of the peaks, the estimated speed is  4448 for the spin mode, whereas it is  5888 for the energy mod  scaling plots are shown in fi  nonlinear fluctuating hydrodynamics for the classical xxz spin chain 7 fi  3 scaling plots of css and cee for the integrable lll model in easy plane regime at high temperatur  the figures show the ballistic scaling of css and cee at different time  is no longer quadrati  the hamiltonian seems to be the only known integrable classical spin chai  the infinitely extended faddeev-takhtajan spin chain has a countable number of locally conserved fields, which are constructed by successive differentiations of the r-matrix, see in the scaling plot fi we see that the energy and spin correlations show good ballistic scaling already at short time  physically this corresponds to easyaxis regim  we refer to the discussions inalso reporting on parameters with diffusive spreadin  8 avijit das et a  fi  we observe perfect diffusive behaviour in fi  we show the easy axis dat  the equations of motion are given by the dynamics of the rj's has the following generic structur  thus the field of phase differences rj is locally conserved only until the first phase slip even  however, as we show in fi at low temperatures most phase slip events come in closely-spaced pairs with no change in the total winding number, so the coarse-grained dynamics actually respects this conservation law until one has unpaired phase slip event  as illustrated in fi  andfor high temperatures there are lots of phase slip  however for easy-plane and low enough temperatures the zcomponent is approximately constant and the phase difference is trapped by the cosine-potentia  then phase slips are very much suppressed, see fi  and therefore, there is an emergence of an approximately conserved quantit  phase differences are thus approximately conserved in this regim  let us first attempt a rough estimate for the presence of a third conservation la  let us compute the energy required for a phase slip event caused by motion of a single spi  in fi  in fi  we see that eslip takes the values 4 and 2 from the two energy estimates mentioned abov  to understand such behavior we have to investigate in more detail the actual process of phase slip  let us consider the spin chain with zero external magnetic field and in the easy-plane regime fi  it shows that the phase slip events become rare with decreasing temperature and decreasing external magnetic fiel  red boxes indicate sites where phase slips occur,   nonlinear fluctuating hydrodynamics for the classical xxz spin chain 11 fi  6 the figure illustrates that the average number of phase slip events decays exponentially with bet  inset shows the same plot in log scal  in log scale the slope is - 73 which is consistent with our theoretical estimation within error bar  in the ground state all spins are aligned and lying on the xy plan  we are interested in finding the minimum-energy spin configuration in which there is a phase sli  these phase slip events are produced at this low density by the chaotic equilibrium dynamics of the lll chai  to work with an almost conserved field is somewhat vague and it is more convenient to modify the dynamics such that rj is strictly locally conserve  of course, this is a valid approximation only in a regime with a very low density of phase slip  actually, for what we want to show the precise shape of the potential barriers plays no role, as long as phase slips are forbidde  12 avijit das et a  under the constraints the lll equilibrium time-correlations of sj, ej should be well approximated by the same time-correlations as computed from the dynamics governed by hl  but hlt is just one particular anharmonic chain and, as explained inthe time-correlations can be predicted from nonlinear fluctuating hydrodynamic  this part of the equation is a phenomenological ansatz for the effective noise and dissipation produced by the deterministic chao  in particular g determines the dynamical universality clas  using this property the precise form of g and its relation to second derivatives of the free energy have been established in we turn to the proof of 14 avijit das et a  this does not change the free energ  thus at the end the coupling matrix g is given in terms of the lll free energ  we refer to appendix a for more detail  apart from the spin and energy correlations defined in e  fi 005: plot of cab at different time  the equilibrated configuration was then evolved with our hamiltonian dynamics using either a fixed step size and sometimes an adaptive step size 4th order runge-kutta integrato  averages were compute over around 106 initial condition  in all our simulations we fixed beta = 1  at much smaller temperatures, phase slips would be even rarer but the dynamics of small fluctuations about the ordered state is expected to be closer to integrable, and the time required to see the asymptotic scaling becomes inaccessibl  case-i : in this regime we expect to confirm the kpz sound modes and levy heat modes as predicted by nf  the z-component of the total spin, energy, and individual spin length are conserved up to 10-15 and 10-5 and 10-10 respectivel  phase slip processes are rare but not completely absen  in fi  fi  the 2nd row shows the sound modes with kpz scaling and heat mode with the predicted levy scaling, while the 3rd row shows diffusive scaling of the same dat  16 avijit das et a 0118 uf8f6 uf8f8 uf8eb uf8ed  01179 uf8f6 uf8f8 g0 uf8eb uf8ed - 411 uf8f6 uf8f8 uf8eb uf8ed - 4079 uf8f6 uf8f8 g+ uf8eb uf8ed  0352 uf8f6 uf8f8 uf8eb uf8ed  03536 uf8f6 uf8f8 speed of sound c  85217 in fi we plot the sound and heat modes obtained after normal mode transformations and with different scaling  values of the g-matrices are given in table- 005: plot of cab at different time  from the results of our simulations, shown in fi we find that the sound mode is better described by a kpz-type scaling while the heat mode appears to be closer to diffusive than the expected levy-5/  case-ii : in this regime, the r- interaction potential is symmetric under reflectio  this leads to a distinct universality clas  on the basis of nfh the sound modes are expected to be diffusive and the heat mode to be levy-3/  the 2nd row shows the diffusive scaling of both the sound modes and the heat mode with levy scaling, while the 3rd row shows the same data with kpz scaling of sound modes and diffusive scaling of the heat mod  18 avijit das et a  phase slip events are absent in this regim  the z-component of the spin, energy, and individual spin length are conserved up to 10-12 and 10-5 and 10-10 respectivel  in fi  in fig we plot the sound and heat modes obtained after normal mode transformations and with different scaling  the g-matrices are given in table- 358e-5 uf8f6 uf8f8 uf8eb uf8ed 0 - 4494 uf8f6 uf8f8 uf8eb uf8ed - 4488 uf8f6 uf8f8 g+ uf8eb uf8ed   however, as can be seen from our simulations,   fi these scalings are not seen conclusivel  as will be apparent from the description nonlinear fluctuating hydrodynamics for the classical xxz spin chain 19 below, the smallness of the violation of the fixed-length constraint for each spin is controlled by the size of the time-step used in the numerical integratio  additionally, this error does not grow with the total time over which the system is evolve  since all our general arguments and theoretical analysis rely heavily on the existence of a conserved energy and angular momentum density, and the fixed-length constraint on each individual classical spin does not play a similar central role in the theoretical analysis, our procedure allows us to use relatively large time-steps while preserving the universality class of the dynamics to machine precisio  our procedure may be viewed as a modification of the so-called odd-even dynamics that has been employed previously in the literature below we refer to these individual steps as odd and even precession  consider eq  with j odd and all bj held fixed with all bj fixed, the right hand side is a bj dependent rotation of all the odd spin  let us denote the corresponding linear operator by ro here ro and re are antisymmetric matrices that serve as the generators of the corresponding rotation  here, we have explicitly displayed the dependence of ro on be in the configuration on which the operator act  let us schematically denote the configuration of odd spins after the action of the first term by s1/2 o the configuration s1/2 o in order to end up finally with the initial configuration s0 o of the odd spin  however, o1 is not orthogona  next we note that the product o2o1 provides a second order accurate approximation to the actual spin dynamic 5e-05 2e-05 cee t=2048 t=2560 t=3072 t=3584 t=4096 fi  11 results from discrete dynamics with dt =  025 t 1/2c++ t=2048 t=2560 t=3072 t=3584 t=4096 fi  12 results from discrete dynamics with dt =  0004 t 1/2c00 t=2048 t=2560 t=3072 t=3584 t=4096 fi  13 results from discrete dynamics with dt =   therefore, we integrate the lattice landau lifshitz equations using o2o1 to evolve the system over one time ste  as noted earlier, this can be viewed as a modification of the standard odd-even dynamics used earliea  we display our results from discrete time dynamics with dt =   case-iii : in fi we show plots for evolution of the correlations css and cee, while in fig  we plot the sound mode and heat modes after appropriate translation and with different scaling  as with the continuous time dynamics, in fi we find again that the sound mode is better described by a kpz-type scaling while the heat mode appears to be closer to diffusive than the expected levy-5/  case-iv : in fi we show plots for evolution of the correlations css and cee, while in fig  we plot the sound mode and heat modes after appropriate translation and with different scaling  in this case the expectation from theory is that the sound modes are diffusive while the heat mode is levy-3/  however, as can be seen in fig these scalings are not seen conclusively in the simulation  note that, in contrast to the continuous time model, we do not have a simple construction of the stationary equilibrium stat  hence even for the sound speed we do not have a reliable predictio  6 conclusions in this work we have mainly studied the classical xxz or lll model and an integrable counterpart of it in easy-plane regime  for the lll model, being non-integrable, these two are the only known conserved 22 avijit das et a  14 results from discrete dynamics with dt =  06 t 1/2c++ t=2048 t=2560 t=3072 t=3584 t=4096 fi  15 results from discrete dynamics with dt =  0003 t 1/2c00 t=2048 t=2560 t=3072 t=3584 t=4096 fi  16 results from discrete dynamics with dt =   quantities but at low temperature a third almost conserved quantity emerges, producing sound mode  we have put this into the framework of nfh to get some predictions about the dynamical scalin  we have shown that the nonintegrable xxz chain in the easy-plane regime displays diffusive behaviour at high temperatures, and sound modes with kpz broadening at low temperature  the integrable spin chain however demonstrates ballistic behaviour in nonlinear fluctuating hydrodynamics for the classical xxz spin chain 23 the easy plane regim  we also find that at high temperature the easy-axis nonintegrable case gives us perfectly diffusive behavio  our various results are summarized in table   for the heat mode, while nfh predicts anomalous levy broadening, we have not been able to clearly observe this yet in the simulation  it is possible that one requires larger system sizes and longer times to observe this and further studies are necessar  table 3 summary of the transport properties observed in this wor  7 acknowledgements ad would like to thank the support from the grant ednhs anr-14-ce250011 of the french national research agency and from indo-french centre for the promotion of advanced research under project 5604-  mk gratefully acknowledges the ramanujan fellowship sb/s2/rjn114/2016 from the science and engineering research boarddepartment of science and technology, government of indi  mk would like to acknowledge support from the project 6004-1 of the indo-french centre for the promotion of advanced research dah was supported in part by doe grant de24 avijit das et a  sc001624  the numerical simulations were done on mowgli, mario and tetris clusters of icts-tifr and gaggle and pride clusters of dtp-tif  a coupling coeffcients for nonlinear fluctuating hydrodynamics given the magic identityone can follow to obtain the couplings of the quadratic nonlinearities of nf  as second order taylor coeffcients, the g-matrices are symmetri  the free energy has to be numerically evaluate  an effcient method is to use transfer operator techniques03536 uf8f6 uf8f8, g0 = uf8eb uf8ed - 4079 uf8f6 uf8f 4488 uf8f6 uf8f  in table i and ii these results are compared with data coming from molecular dynamic  nonlinear fluctuating hydrodynamics for the classical xxz spin chain 25 b low temperature approximation it is instructive to work out the prefactors of g-matricesin the harmonic approximatio  in principle, also the next order correction could be computed, compare with 26 avijit das et a 5 uf8f6 uf8f ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190  we also present some intriguing questions in the case of q = 13 ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "the vast and prolific area of permutation patterns considers permutations as linear order  a recent survey on permutation patterns can be found in and a book on the subject is therefore, it is not surprising that pattern avoidance questions become much more difficult if the symmetric group concept is present in the  key words and phrase  cycles, permutation pattern, permutation squares ams 1991 subject classification  let savn denote the number of strongly q-avoiding permutations of length   this is only true for monotone increasing pattern  proo  then p is the union of k -1 decreasing subsequence  then we claim that those same k entries form an increasing sequence in p  as layered permutations are involutions, this means each p has order fou  however, finding a good p makes the construction challengin  from left to right, each interval is made up of the largest remaining 16 entrie  that is, from left to right, each interval is made up of the largest remaining 25 entrie  there are also nice graphical symmetries within the intervals in these small example  as such, there is reason to believe it could be possible to generalize these constructions to show the bound given in theorem  1 is always the bes  indeed, the identity permutation strongly avoids all patterns that are not monotone increasin ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 The pattern 12@let@token k", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 The pattern 312", "Text": " that means that p will be strongly 312-avoiding if and only if its restrictions to those two intervals are strongly 312-avoidin  this motivates our analysis of strongly 312-avoiding permutations that end in the entry  ", "Subsections": [{"Section_Num": "3_1", "Section": "3.1 Permutations ending in 1", "Text": "our goal in this section is to prove the following theorem that characterizes strongly 312avoiding permutations that end in   the permutation p is strongly 312-avoidin  we will prove this theorem through a sequence of lemma  first, we will show that strongly 312avoiding permutations must end in a long decreasing subsequenc  proo  now consider p  hence at least one of these large entries appears before the 1 which creates the forbidden 312 pattern in p  we now extend lemma   as with lemma   we now consider p  note first p2 =   notice p2 = pm+  this means p2p2p2 form a 312-pattern in p  hence, p2p2p2 form a 312 pattern in p  thus every strongly 312-avoiding permutation p must end with a consecutive decreasing sequence of at least half of its smallest entrie  proo  now, as mentioned in remark   by lemma   we now extend this to complete our proof that p must have the unimodal structure described in theorem  1 be a strongly 312-avoiding permutatio  proo  let k be the length of the longest consecutive decreasing sequence at the end of   if p does not satisfy the structure, then at least two of these large entries must form a descen  of all these descents, choose the descent pipi+1 such that pi+1 is the smalles  hence a 213 of large entries in p would form a 312 pattern in p  thus all of p1, p2, consider the entry pi+  thus pi+2 > pi+1 by the minimality constraint on the chosen descen  to complete the proof of theorem   proo  it is clear that p does not contain a 312 patter  the first n -k entries will bebecause the order of these entries is monotone decreasing in p, these entries will be in increasing order in p  thus p2 avoids 312 as wel  as such, p is a strongly 312-avoiding permutatio ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3_2", "Section": "3.2 Enumeration", "Text": " it follows from theorem   so in particular, sav312 is rationa  its root of smallest modulus is about   interestingly, the sequence is in the online encyclopedia of integer sequences as sequence a122584, where it is mentioned in connection to work in quantum mechanics", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3_3", "Section": "3.3 Equivalent patterns", "Text": "as the pattern 231 is the inverse of 312, it is straightforward to see that p is strongly 231avoiding if and only if p-1 is strongly 312-avoidin  the following result, while similar in flavor, is a little bit less obviou  proo  let us multiply permutations left to righ  on the other hand, rp2r is also the reverse complement of p  this provides some additional motivation to compute the numbers sav13  we will discuss that very difficult task in section 5", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 The pattern 321", "Text": " that is, each block is a singleton or a power of the cycle that is not the identit  the following are all block-cyclic note that each block-cyclic permutation is 321-avoidin  furthermore, any power of a block-cyclic permutation is block-cyclic, and so it is also 321-avoidin  therefore, blockcyclic permutations are all 321-avoidin  the singularity of smallest modulus of the denominator is about   the inequality   note that this proves that for large n, there are more permutations so that all their powers avoid 321 than permutations so that just they and their square avoids 31  it is possible to improve this lower bound with more complicated constructions, but we have no conjecture as to what the actual growth rate of the sequence sav321 is", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 The pattern 132", "Text": "we did not succeed in our efforts to enumerate strongly 132-avoiding permutation  therefore, the exponential growth rate of the sequence of the numbers sav132 is at least   considering the first 40 terms, the trend hold  therefore, we ask the following question  proo  as that sequence is bounded from above, it has to be convergen  the monotone pattern 123 is no  this raises the following questio  mikl os b ona has been supported by simons collaboration grant 421967", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": " however, the performance of such descriptors drops under severe illumination change  in this paper, we proposed a discriminative and contrast invertible local feature descripto  in order to increase the discriminative ability of the descriptor under illumination changes, a laplace gradient based histogram is propose  a robust contrast flipping estimate is proposed based on the divergence of a local regio  experiments on fine-grained object recognition and retrieval applications demonstrate the superior performance of dci descriptor to other ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": " in general, local patches extracted by various interest point detectors are expected to be described by descriptors that are discriminative and tolerant to geometrical and illuminative variation  despite efforts have been devoted to developing kinds of descriptors, the performances of existing descriptors are still limited especially under conditions where illumination changes greatl  such changes occur frequently in real-world scenario  for example, as shown in fi  te  +65 9755 0569 email addresses: miaozhenwei@gmai comekhyap@nt ed sgexdjiang@nt ed sgsinduja001@nt ed sgzhwan me@gmai com preprint submitted to elsevier january 3, 2019 arxiv:190  the first row shows the same landmarks captured under different whether and lighting, the second rows shows the face with different illumination, the third row shows the wallpaper with different colors and fourth row shows the logos with contrast variatio  logo in t-shirts with opposite contrast while the fourth row shows the identical art design pattern implemented in completely different level of illuminatio  in such cases, previous descriptors encounter difficulties in recognition and retrieving the images with the identical objects yet different illumination condition  there are two types of contrast changes: bright-dark order preserved changes and the brightdark order disturbed change  in images with bright-dark order preserved changed, the relative order of the pixel intensity remains the same while the image patch becomes either brighter or darker following linear and/or nonlinear transformation  in contrast, images with bright-dark disturbed changes will not retain the relative orde  sift is suggested to be the most stable descriptor for common image deformations because of the location and orientation quantizations, the sift descriptor is robust to small geometric distortion and location error  as an extension of sift, the gloh enhances the robustness and discriminative ability by increasing the number of spatial bins and orientation to 16 and 17, respectivel  the principal component analysis is carried out to reduce the number of feature dimensions of gloh to be equivalent to sift'  each row shows image patches from the same desig  pca-sift further reduces the dimensions of sift from 128 to 20 by applying pca to gradient image patche  although these descriptors are robust to certain variations and distortions, they fail to address the issues caused by severe nonlinear illumination changes and bright-dark order disturbed changes as abovementioned recently, alternative local feature descriptors have been proposed to solve the problems caused by illumination change  instead of using the gradient orientation and magnitude, the intensity order is adopted by the local intensity order pattern descriptor the local image region is divided into several sub-regions according to the intensity order of pixels in that regio  local intensity order patterns are computed to describe each sub-regio  as a result, liop can identify features with consistent illumination changes in which the order of pixel intensity is preserve  the bright-dark disturbed changes are even more complicated than the order preserved one  to the best of our knowledge, few works have been done to solve the bright-dark disturbed changes where both the amplitude and the relative order of the bright/dark pixels may var  when the gradient orientation changes to its opposite direction with a 180 degree rotationthe rotated descriptor cannot be matched with the original on  however, the original images and the bright/dark inverted images can be converted by a fixed transfer functionallowing the distribution of gradient orientation and amplitudes in a local region to be transfered in a different orde  hence, the mirror and inversion invariant sift is developed with a canonical form to identify the relationship between descriptors in the original image patches and the illumination/mirror inversed one  invariant to both the bright and geometric flipping through replacing the corresponding bins in the sift descriptor with symmetric bin  the max-sift is designed with a different approach from using advanced arithmetic operations; it rearranges the sift bins under different flipped versions with the original bin  both of mi-sift and max-sift descriptors can work for instances of bright and/or geometric flippin  however, they still cannot solve the problem of partial illumination changes as shown in fi  although the shape context using edges instead of image intensities provides a potential ways to describe local regions with partial illumination changes, stable edge detection is challenging and unclea  in this work, we propose a novel descriptor named discriminative and contrast invertible descripto  the dci is designed to enhance the performance of local feature descriptors for large illumination changes and contrast inversion  unlike the local gradients that are easily affected by illumination conditions, a laplace gradient we proposed in this paper is effective in capturing local characteristics with illumination variation  the dci descriptor is formed by concatenating the histograms of the laplace gradient in different bin  moreover, to address the bright/dark inversion problem, we proposed an inversion-estimation function based on the divergence of gradien  the proposed descriptor is evaluated across diverse object visual search tasks, including searching logos, commercial design images and face  experimental results demonstrate that the proposed dci descriptor outperforms the state-of-the-art descriptor ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 The Proposed Descriptor", "Text": "the laplace gradient and the divergence based contrast flipping estimation function are proposed in the dci descriptor to solve the problems caused by the brightness intensity and brightdark order change  fi  3 illustrates the work flow of the dci descripto  as how the sift descriptor is developed, a local patch with the standard size of 31 * 311 is extracted around each interest point at the given scale and aligned along its dominant orientation following that, the gradient of the image patch is extracted as shown in fi  the laplace gradient shown in fi  3 is computed to enhance the discriminative ability and robustness to brightness change  then, the laplace gradient map in fi  the distribution of the laplacian gradient in each sub-regions are quantized into an eight-orientation-bin histogram weighted by the gradient variation to represent each sub-region in fi  to handle the bright-dark order disturbed changes, the convergence based contrast flipping estimator in 3 is generate  according to the sign of the estimator, either the original order version in fi  3 or the inverted order in fi  3 is chosen to encode the descripto  lastly, the histograms in each block are concatenated into one vecto  rooting algorithm is applied to further enhance the descripto  the rooted histogram as shown in fi  3 after l1 normalization forms the final dci descripto ", "Subsections": [{"Section_Num": "2_1", "Section": "2.1 Laplace Gradient", "Text": "the rationale to use image gradient is that it can capture both the spacial and frequency information inspired by the model of biological neuron network in which each neuron responses 1here we follow the default of the sift descriptor provided in vlfeat lio ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2_2", "Section": "2.2 Divergence based Contrast Flipping", "Text": "as for the bright-dark flipping issue, a simple way to consider is reversing the contrast of the whole image to the opposite intensit  however, as suggested inconverting the whole images to the inverted contrast is only optimal to the global image contrast inversio  if only parts of the image's contrast are inverted, the flipping approach is not applicabl  the mi-sift uses an average of both the sift before and after flippin  however, the discriminative ability is comprised by this wa  alternatively, a criterion is set to determine whether a certain region should be flipped or no  for instance, the max-sift takes the maximun alphabetical order of the two transformed version as the criterio  however, the results are subject to noise  in view of previous limitations, we propose to use the divergence of the local region to determine whether the local region is bright or dar  converges to the center whereas a dark blob is the region that the gradient diverges to the cente  the equation indicates that the divergence of the gradient is the laplacian of the imag  thus, it is plausible that the blob can be detected by the response of the laplacian filte  the response of the laplacian filter produces the largest signal to noise ratio at the center of the blob region because its shape matches with that of the blo  therefore, it is conceivable that the divergence is more reliable in detecting whether this region is bright or dar  the implementation of the surface integration is clea  it can be easily computed from the gradient", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2_3", "Section": "2.3 The DCI Descriptor", "Text": "with the laplace gradient, an additional step to tackle the illumination changes is adopting the rooting algorithm although l1 normalization is carried out for the concatenation of the histograms from the laplace gradient, the normalization cannot effectively respond to the problems caused by the nonlinear illumination change  as a summary, the dci descriptor is derived as follow  the performance of the dci descriptor will be evaluated in the following sectio ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Experiments", "Text": "", "Subsections": [{"Section_Num": "3_1", "Section": "3.1 Logo Visual Search", "Text": "first, we test the proposed dci descriptor on the challenging belgalogos databasewhich contains 10,000 real-world images from various event  samples are shown in the forth row of fi  the image size is kept the same as that provided in hessian-laplacian detector with the default setting from vl-feat2 is used to extract the interest points from each 2the vl-feat is downloaded from rotatio ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3_2", "Section": "3.2 Wallpaper Visual Search", "Text": "the reason we choose wallpaper as a test set is that it includes a considerable number of images with various illumination variations from the same desig  samples of a wallpaper design are shown in the third row of fi  in total, the reference image set contains 522 images from 77 categories which are provided by a wallpaper design compan  the test image set contains 1,014 image  the wallpaper search algorithm in is implemented in thi  specifically, the hierarchical k-means clustering is used to train the scalable vocabulary tree a branch factor of 10 and depth of 6 are set for the sv  a total of 1,000,000 words are trained for the quantizatio  while keeping the original aspect ratio, the images are normalized to the standard size with the longest dimension as 640 pixel  the interest point detector is a combination of the dense and the adaptive sparse sift which is the same as that in both sparse and dense local interest points are used to anchor the local feature  the map is calculated to indicate the retrieval performanc  it demonstrates that the dci is more robust to the severe contrast changes which widely exist in the wallpaper dataset  10 figure 6: logo detection results based on the sift and dci descriptor  the first row shows the results obtained using the sift descriptor marked with yellow bounding box, and the second row illustrates the results obtained using the proposed dci descriptor marked with red bounding bo  the proposed dci can effectively detect both two logos in these two images but sift canno  as shown in fi  7, eyeballing two images with large illumination changes can find that both the sift and liop descriptors identify only limited number of correctly matched pairs in the event of severe illumination change  in contrast, the proposed dci descriptor can identify much more correctly matched pairs, suggesting its superior performance to other evaluated descriptor ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3_3", "Section": "3.3 Face Recognition", "Text": "over years, face recognition is always an active research topic interest points are also applied to face recognition many face databases are publicly available, providing a convenient and rich test set for evaluatio  in this experiment, we test the proposed dci descriptor on one of the widely accepted face databases: the cmu multi-pie database the cmu multi-pie database contains face images captured in 4 sessions with variations in illumination, expression and pos  for the purpose of this experiment, the face image sets with illumination variation is selecte  the first 105 subjects that appear in all 4 sessions are use  images are cropped and down-sampled to the size of 100 * 82 pixel  as illustrated inthe illumination variation dataset contains 18 flash-only images and 2 non-flash images per perso  samples are shown in the second row of fi  the blue lines represent the correctly matched pairs between the query and reference image  for each subject, the first t images are selected as the reference and the rest 20-t images are used as the quer  the algorithm of image recognition through interest point matching is adapted from experimental results are shown in fi  it shows that the proposed dci descriptor outperforms other descriptors over all case  dci descriptor outperforms other descriptors on the face recognition experimen ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Acknowledgements", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Conclusions", "Text": "in this work, we propose a local feature descriptor named dci that is discriminative and contrast invertible in illumination changes and contrast inversio  the laplace gradient is computed to describe each pixe  a divergence-based contrast flipping estimator is created for images with the bright/dark disturbed variation  the square root of the holg after l1 normalization is used to further mitigate the problems caused by illumination change  experiments on the belgalogos, wallpaper, multi-pie databases exhibit the superior performance of the dci descriptor in object search applications over the state-of-the-art descriptor  it suggests that the dci descriptor is robust to the illumination changes and contrast inversion", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190 00028v1 31 dec 2018 on center of mass and foliations by constant spacetime mean curvature surfaces for isolated systems in general relativity carla cederbaum and anna sakovich abstrac  we propose a new foliation of asymptotically euclidean initial data sets by 2-spheres of constant spacetime mean curvature the leaves of the foliation have the stcmc-property regardless of the initial data set in which the foliation is constructed which asserts that there is a plethora of stcmc 2spheres in a neighborhood of spatial infinity of any asymptotically flat spacetim  the stcmc-foliation can be understood as a covariant relativistic generalization of the cmc-foliation suggested by huisken and yau we show that a unique stcmc-foliation exists near infinity of any asymptotically euclidean initial data set with non-vanishing energy which allows for the definition of a new notion of total center of mass for isolated system  this stcmccenter of mass transforms equivariantly under the asymptotic poincar e group of the ambient spacetime and in particular evolves under the einstein evolution equations like a point particle in special relativit  the new definition also remedies subtle deficiencies in the cmc-approach to defining the total center of mass suggested by huisken and yau which were described by cederbaum and nerz", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1. Introduction and goals", "Text": " intuitively, they should have a total center of mass which should in a suitable sense behave as a point particle in special relativit  in this paper, we suggest a definition of total center of mass for suitably isolated systems and argue that this center of mass notion indeed behaves as a point particle in special relativity in a suitable sense in particular, we will show that the center of mass notion we suggest evolves in time under the einstein evolution equations like a point particle in special relativit  more specifically, we will prove existence and uniqueness of an asymptotic foliation by 2-spheres of constant spacetime mean curvature under optimal asymptotic decay assumption  2000 mathematics subject classificatio  on the other hand, the stcmc-condition is naturally independent of the initial data set in which the foliation is constructe  our result thus asserts that there is a plethora of stcmc-surfaces in a neighborhood of spatial infinity of any asymptotically flat spacetim  furthermore, the new construction of a center of mass will be shown to remedy the subtle deficiencies of the huisken and yau approach described by cederbaum and nerz last but not least, we will provide an asymptotic flux integral formula for the center of mass extending that of beig and o murchadha the analytic techniques in our proofs rely on those developed by metzger and nerz concluding this introduction, we would like to point out that the notion of spacetime mean curvature of 2-surfaces in initial data sets has independently been considered in other contexts, both before and after the results of this paper had been announce  more generally, 2-surfaces with constant spacetime mean curvature are critical points for the area functional inside the future-directed null-cone, with mean curvature vector pointing in the direction in which the expansion of the surface is extrema  the aforementioned generalized apparent horizons have outer area minimizing property which is appealing in the view of spacetime penrose inequalit  because of the spacetime geometry nature of the stcmc-condition, we expect that stcmc-surfaces and stcmc-foliations will have a number of applications beyond the definition of a center of mass of an isolated system as well as beyond the setting of asymptotically euclidean initial data set  for example, a special subfamily of stcmc-surfaces foliating a null hypersurface implicitly appears in recent work by klainerman and szeftelwhere they arise as surfaces with both constant outer and constant inner expansio  structure of the pape  in section 2, we will summarize the necessary definitions and notations as well as more details on the background and existing work on the total center of mass of isolated system  in section 3, we will state our main results and very briefly explain the strategy of our proof  appendix a collects results such as sobolev inequalities on 2-surfaces, while appendix b studies stcmc-surfaces in normal geodesic coordinate  acknowledgement  we would like to thank julien cortier who was involved in this project at early stages and contributed with many insightful discussion  cc also thanks the fondation des sciences math ematiques de paris for generous suppor  the work of as was supported by knut and alice wallenberg foundation and swedish research council we also thank the crafoord foundation for generous suppor ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2. Preliminaries", "Text": "recall that an initial data set for the einstein equations is a tuple where is a smooth riemannian manifold and k is a smooth symmetric tensor field on m3 playing the role of the second fundamental form of m3 in an ambient lorentzian spacetim  the constraint equations thus necessarily hold on any spacelike hypersurface in the spacetime 4 carla cederbaum and anna sakovich definition   asymptotically euclidean initial data sets are well-known to possess well-defined total energy, linear momentum, and mas  = with respect to which ieuc  this explains why we exclude this case in definition   this is called the riemannian case, the reason being that one can reinterpret this as saying that the trivially extended initial data set satisfies -. ", "Subsections": [{"Section_Num": "2_1", "Section": "2.1. Center of mass", "Text": " first, let us remark that our field knows many definitions of center of mass for isolated system  the first definitions were given in terms of asymptotic flux integral expressions in coordinates, similar to those of energy and linear momentum above, see below and the text surrounding i  for a brief, non-complete summary of other definitions of center of mass, please see flux integral definitio  see szabados for valuable critical comments on this definition, and see section 7 for a covariant generalization of this formula following from our wor  6 carla cederbaum and anna sakovich definitions via foliation  several authors define the center of mass of an initial data set i = via a foliation by 2-spheres near infinit  following cederbaum and nerzwe will call such definitions abstract in contrast to the more explicit coordinate definitions of center of mass, see belo  this goes back to an idea of christodoulou and yau in 2006, metzger considered a foliation by 2-spheres of constant null mean curvature and concluded that this foliation is not fully suitable for defining a center of mas  for a more detailed review of foliations suggested to study in this context and of recent progress in terms of necessary and suffcient asymptotic decay conditions, please see huisken and yau also assign a coordinate center to their foliatio  this naturally extends to asymptotic foliations: definition   this means it cannot necessarily be pulled back into m  we will not follow this usage her ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2_2", "Section": "2.2. Miscellannea", "Text": "here we collect some other definitions for future referenc  regge-teitelboim condition for initial data set  however, it will be useful in our discussion to refer to those conditions which is why we define them her  8 carla cederbaum and anna sakovich weighted sobolev space  in this paper, we use the following definition of sobolev spaces, which is well-suited for keeping track of fall-offrates of different quantities associated with our foliatio  suppose that is a closedoriented 2-surface in an asymptotically euclidean 3-manifold of suitable regularit  appendix a in particular collects some sobolev inequalities for functions on 2-surfaces embedded in euclidean spac ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3. Main results, motivation, and the strategy of the proof", "Text": " in this paper we prove the following theorem  it has been shown in particular by brendle and eichmair that such an a priori condition is necessary to obtain uniqueness of cmc-surfaces in general, see the discussion in section   as stcmc-surfaces generalize cmc-surfaces, their observation applies here, to  however, as the assumptions of theorem  5 suggest, this formula cannot be expected to always converg  see conjecture   we get the following theorem on the time evolution of the stcmc-foliation and center of mas  the full covariance of the stcmc-foliation under the poincar e group is discussed in section   10 carla cederbaum and anna sakovich theorem   let be a smooth, globally hyperbolic lorentzian spacetime satisfying the einstein equations with energy momentum tensor  ", "Subsections": [{"Section_Num": "3_1", "Section": "3.1. Strategy of the proofs of Theorems 6.2 and 6.12", "Text": "the underlying structure of the proofs of theorems  12 presented in section 6 and several of the lemmas proved in the same section is a method of continuity inspired by metzger this is what will allows us to drop the explicit mention of the chart in the proof  as usual, we will appeal to the implicit function theorem in order to show openness of the interval in the method of continuit  closedness follows from a standard convergence argumen ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4. A priori estimates on STCMC-surfaces", "Text": "when deforming the foliation by 2-surfaces of constant mean curvature to the foliation by 2-surfaces of constant spacetime mean curvature, we need to keep track of how the geometry of the leaves change  we will use the same a priori classes in the context of asymptotically euclidean initial data sets i =where the definition of a only depends on the riemannian manifold part 12 carla cederbaum and anna sakovich remark   see for detail  the conclusions of this theorem are mostly the same as those inonly for stcmc- rather than cmc-surface  we will thus need to extend the result and its proof to our settin  center of mass and stcmc-foliations 13 proo  with remark  6 in mind, we need to improve the estimate in then by lemma   by and the sobolev inequality in the form of lemma   bootstrappin  note that the existence of the uniform sobolev inequality assumed in is well-established in our setting, and goes back to which holds for surfaces in asymptotically euclidean manifolds with general asymptotics as described in section   similarly, repeating the above argument that we used to derive andwe obtain the improved radius comparison andwhich adapts to our settin  this finishes the proof of proposition 4", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5. The linearization of spacetime mean curvature", "Text": " we will show that the linearization of the map h is invertible when the linearization is computed with respect to normal variations within the given initial data set   this will later be used to ensure that the cmc-foliation of constructed in can be pushed via a method of continuity to an stcmc-foliation of   in this setting, we know from proposition   stability operators associated with prescribed mean curvature surface  the linearization of this map h is computed in the following lemm  proo  in section   as it turns out, the analytic properties of lh+/-p imply that constant expansion foliations do not provide an adequate notion of center of mass, in contrast to the stcmc-foliation studied her  furthermore, we will see that the leaves of this foliation do not translate as their spacetime mean curvature approaches zero, provided that the standard asymptotic symmetry conditions are impose  this operator has been intensively studied, see  again counted with multiplicit  proo  by and lemma   this concludes the proo  this lemma and its proof are very similar tobut rephrased in the spacetime settin  proo  this proves the claims of the lemm  the same remark will hold true for the subsequent result ", "Subsections": [{"Section_Num": "5_3", "Section": "5.3. Invertibility of the operator L", "Text": " the same estimates apply to the l2-adjoint l*.  proo  proving it follows from proposition   furthermore, the definition of 3we do not call hd the deformational part as some other authors do, because hd also contains the mean value informatio  in other words, we primarily use this splitting to distinguish between the eigenfunctions corresponding to eigenvalues of different magnitud  this proves proving arguing as above, by proposition   it then follows by lemma   the general case follows by bilinearity and by the cauchy-schwarz inequality on r  proving center of mass and stcmc-foliations 23 proving arguing similarly to how we argued above, we now integrate by parts and use proposition   combing this estimate with andfollows from once we recall that hd -h0 is l2-orthogonal to h  proving more specifically, lemma   proving proo  this proves the estimate for   the estimate for hd is proven similarly, using instead of ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "6", "Section": "6. Existence and uniqueness of the STCMC-foliation", "Text": " we also prove a uniqueness result for stcmc-surface ", "Subsections": [{"Section_Num": "6_1", "Section": "6.1. Existence of the STCMC-foliation", "Text": "innerz proved the following result, rephrased here in our notatio  center of mass and stcmc-foliations 27 this result is a starting point for proving the following theorem, which is essentially the main result of this pape  for the sake of clarity of exposition, we provide the proof of the following theorem right away, saving the verification of some preliminary lemmas for late  we state theorem  2 here in a notation convenient for its proo  as the proof of theorem   in order to make this idea more precise, we introduce the following constructio  by theorem  5 as well as a supplementary result stated in lemma   proo  as discussed in section   this proves lemma  4 shows directly via theorem   as we have just seen by lemma   in lemma   proo  alternatively, one may rely on a more standard method used inwhich we describe belo  by proposition   applying lemma   as a consequence, we may apply lemma   this is proven in lemma 6", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "6_2", "Section": "6.2. Supplementary lemmas", "Text": "we will now prove the supplementary lemmas that were used in the proof of theorem   let a belong to be fixe  proo  then proposition   as a consequence, by corollary   from this, as a consequence of and corollary  6 enables us to prove the following resul  let a belong to be fixe  proo  then lemma   this proves let a, a belong to proo  first, we deal with next, we address under the assumptions of theorem   proo  we already proved in theorem   this is to be viewed in light of the uniqueness results in section   injectivit 7 and lemma   instead, we argue as in the proof of lemma   using the fact that lv = ric+o bylemma   surjectivity", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "6_3", "Section": "6.3. Uniqueness of the STCMC surfaces.", "Text": "we close this section by proving that the constant spacetime mean curvature surfaces are unique in the a priori class of asymptotically centered surfaces   the uniqueness result is proven in a similar way as the existence result of section  1, namely by the method of continuit  the starting point of the method of continuity is the following result fromagain adapted to our notatio  our uniqueness result generalizes this to stcmc-surfaces in the spacetime contex  38 carla cederbaum and anna sakovich proo  we rely on the same type of argument as in the proof of theorem   maximality is understood as in the proof of theorem   arguing as in the proof of theorem   by theorem  11, such a surface is unique in this class", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "7", "Section": "7. The coordinate center of the STCMC-foliation", "Text": "1, provided that this limit exists we will discuss the subtle consequences of this within this section, to  one cannot in general expect that also 4note that although it was assumed throughout the proof of theorem   in fact, in the view of lemma   independent of   one can only expect that will hold if k falls offvery fast, in particular faster than the optimal decay assumed in this pape  in this section we will confirm that this is indeed the cas ", "Subsections": [{"Section_Num": "7_1", "Section": "7.1. A variational formula for STCMC-surfaces", "Text": "the following proposition generalizes to the spacetime cas  proo  40 carla cederbaum and anna sakovich applying the cauchy-schwarz inequality to the above identity for |s=0 and usinglemma a", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "7_2", "Section": "7.2. STCMC-center of mass", "Text": " we will now apply proposition  1 to find how the coordinate center of a leaf changes under this particular deformatio  as a result, we prove lemma  1 and in particular faster than one needs for existence and uniqueness of the foliatio  see also conjecture   this in particular shows that k can in a sense compensate for the diverging coordinate center of the cmcfoliatio  see section 9 for more details on thi  center of mass and stcmc-foliations 41 proo  according to proposition   in order to pass from towe will apply lemma   next, by and lemma   then we may rewrite by a cauchy-schwarz inequality and lemma   at the same time, proposition   note that a computation in the proof of lemma   proo  the result is then a direct consequence of lemma   center of mass and stcmc-foliations 43 definition   this will be studied in detail in our forthcoming wor  we conjecture the following suffcient conditions, in line with bartnik's and chru sciel's corresponding results for convergence of adm-energy and admlinear momentu  time evolution and poincar e covariance of the stcmc-center of mass   evolutio  in this section, we will study the evolution of the coordinate center of the unique foliation by surfaces of constant spacetime mean curvature under the einstein evolution equation  let be a smooth, globally hyperbolic lorentzian spacetime satisfying the einstein equations with energy momentum tensor   in fact, one gets more information about the evolution of the stcmccenter of mass from the proof of theorem   we will investigate this in our forthcoming work, see also remark   it is straightforward to prove a version of this theorem allowing for nonvanishing shif  as this is not of primary interest here and can also be fixed by a suitable gauge, we will not go in this directio  proo 12 or in the proof of lemma   by theorem  12, this family of foliations must coincide with the one studied here and must thus depend on t in a c1-fashio 1 and u is the initial lapse function of the normal variatio 5 to 5note that our sign convention for the second fundamental form is the opposite of6 as the right hand side is bounded and thus in l  relying on proposition   the idea is to argue as in the proof of lemma   a computation identical to the one in the proof of lemma   poincar e-covariance and accordance with special relativit  as we have seen before, whether or not a given 2-surface is stcmc is in fact independent of a choice of slice in this sense, stcmc-surfaces are covariant in the sense of general relativit  the role of the initial data set then is to select a unique family of stcmc-surfaces near the asymptotic end of the spacetime, forming its abstract stcmc-center of mas  in this sense, stcmc-foliations and the associated center of mass are poincar e-covarian  dealing with angular momentum and treating the boost case more adequately will be left for our future wor  euclidean motion  time translation  in other words, theorem   boost  the last constituent of the asymptotic poincar e group of the spacetime are of course the asymptotic boost  the corresponding question was addressed by szabados for the b om-center of mass although from a slightly different perspectiv  similarly, if one first spatially translates the coordinates on the schwarzschild spacetime and then considers a boosted slice, the transformation law will be as in5 in fact vanishes, so that center of mass and stcmc-foliations 49 the transformation law already holds for the cmc-b om-center of mas  in view of section 9 below, it is possible to construct examples of boosted slices in the schwarzschild spacetime by boosting the example discussed belo  one can then see that in fact also applies in this case, but only for the stcmc-center of mass, and not for the cmc-b om-center of mas  however, the computation is so tedious that we prefer not to show it here as it is not particularly enlightenin  a proof of a generalized version of incorporating the angular momentum will be given elsewhere, see also remarks  ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "8", "Section": "8. Time evolution and Poincar\u00e9 covariance of the STCMC-center of mass", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "9", "Section": "9. A concrete graphical example in the Schwarzschild spacetime", "Text": "2, but yet its cmc-coordinate center does not converg  equivalently, its b om-center also does not converg  to the center of symmetry of the spherically symmetric spacetime as one would expec  computing the cmc-coordinate center of mass hence, the b om- and thus also the cmc-coordinate center diverge in this exampl  6the corresponding formula for the second fundamental form in has a typo which we corrected her  we thank axel fehrenbach for pointing this out to u  center of mass and stcmc-foliations 51 computing the stcmc-coordinate center of mass ).  however, the proof of theorem  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Appendix", "Section": "Appendix A. Round surfaces in asymptotically Euclidean manifolds", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Appendix", "Section": "Appendix B. The STCMC-condition in normal geodesic coordinates", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "References", "Section": "References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190  every pixel in a local region is adaptively encoded into one of the three statuses: bright, uncertain and dar  the blob significance of the local region is measured by the spatial distribution of the bright and dark pixel  interest points are extracted from this blob significance measuremen  as a result, the proposed detector is invariant to illumination change  the state-of-the-art results are achieved on the standard datasets, and also in the face recognition applicatio ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "I", "Section": "I Introduction", "Text": "a well-designed interest point detector is supposed to effectively represent images across variations of scale and viewpoint changes, clutter background and occlusionnevertheless, an open question remains about extracting the stable points under illumination variation  the hessianlaplace/affineharris-laplace/affinesift and surf detectors are built upon the derivatives of the gaussian filte  either the first or the second derivative of the gaussian filter is used to compute the strength of the image local contras  as the gaussian filter responds proportionally to the image local contrast, these detectors perform poorly in detecting low contrast structures even if these structures are stable under different variations and significant in computer vision application  moreover, these detectors are susceptible to abrupt structures and image noise  to mitigate the influence caused by image noise and nearby image structures, a rankordered laplacian of gaussian filter is proposed in however, such a detector still partial relies on the image local contras  this research was carried out at the rapid-rich object search lab at the nanyang technological university, singapor  the authors are with the school of electrical and electronics engineering and with rapid-rich object search lab, nanyang technological university, 639798 singapor  to address the problems caused by illumination changes particularly, image segmentation has been utilized in designing interest point detector  however, these detectors' performance is unsatisfactory under image blurring in which the boundaries of image structures are unclear self-dissimilarity and self-similarity of image patches are used in susanfast and selfsimilar detectors to alleviate the problems caused by lighting variatio  in particular, the susan and fast detectors use the number of pixels that are dissimilar from that in a region center to detect corner  the weakness of two detectors is that they are not scale-invariant and inefficient in detecting blob-like structure  although local pixel variance is adopted in to estimate the self-similarly, the robustness of this detector is uncertain when there are strong abrupt changes within the image patc  considering the above-mentioned limitations of existing detectors, this paper aims to develop a contrast invariant and noise resistant interest point detecto  the ternary status of each pixel in a local region is detected by the dynamic thresholds that are automatically computed by the itm algorith  interest points are extracted from the blob significance map that is measured by the number of bright and dark pixel  as expected, the proposed atc shows robustness to illumination variations and is effective in dealing with cluttered structure ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "II", "Section": "II The Proposed Interest Point Detector", "Text": "  problem formulation blobs, as shown in fi  1, are the image local structures with the majority of the bright pixels concentrating in the center while the majority of opposite intensity resides in the peripheral regio  such property of the blob structure is preservable under various variation  moreover, the bloblike structures widely spread over a pictorial imag  these properties make the blob-like structure suitable in anchoring the local descriptorunder various image condition 1 blob significance number of iterations k fi  input imag  an enlarged image patc  for this image patch, shows the pixels divided by the median and shows the pixels divided by the upper and lower bounds of the itm algorith  shows the corresponding blob significance against the number of iteration  than the pixel intensity value under illumination change  in view of this, we propose to detect interest points using the bright/dark labels of pixel  an issue that needs to be addressed is how to differentiate and label the pixels as bright or dark one  one way is to dichotomize the pixels into bright and dark ones by a certain threshold, which could be set by the mean or median value of the local regio  take the image patchas a zoom in from fi  median is more robust to the outliers and abrupt variations than mea  however, the median-based threshold is sensitive to quantization error because of its inefficiency in suppressing this type of nois  this may lead to unreliable labellin  to solve this problem, we propose to introduce a fuzzy label for the pixels that are not clear enough to be labelled into either bright or dark se  this results in our proposed adaptive ternary coding algorith  pixel intensities that are close to the median value in a local region are labeled the uncertain ones to reduce their sensitivity to nois  properly choosing the two thresholds is essential in the ternarizatio  the two thresholds should be invariant to the illumination changes, and should be located on both sides of the median value to ensure the correctness of pixel labelin  meanwhile, the mad of the truncated data converges to zerotherefore, this margin ) separates the pixels into bright and dark ones and tolerates noise and quantization error  given the advantage of the itm filter, we propose an adaptive ternary coding algorithm and a blob significance measure based on the itm algorithm, which are presented as follow  let s1 and s2 be the central region and the corresponding peripheral ring of a filter mask centered at in order to ensure that the two pixel sets i1 and i2 have the same effect on estimating the thresholds for pixel labeling, the weighted itm algorithm is adopted to make them have equivalently equal number of pixel  the pixel numbers n1 and n2 in these two sets i1 and i2 are used to weight the pixels in i2 and i1, respectivel  the proposed adaptive pixel ternary coding is shown in algorithm   a bright pixel is the one that is larger than the higher threshol  a dark pixel is the one that is smaller than the lower threshol  the blob structures have the attribute that the majority of bright pixels are concentrated in the inner region while the majority of the opposite ones in the surrounding regio  as a result, we measure the blob significance by the distribution of the bright and dark pixel  first, the dominances of bright/dark pixels in s1 and s2 are measured by the difference of the numbers of bright and dark pixels in the corresponding regio  it monotonically decreases to zero by increasing the number of iterations in the first few iterations, the margin is large as only few extreme samples are truncated by the itm algorith  by increasing the number of iterations, both the lower and higher thresholds converge to the median value of the local regio  as a result, the margin between these two thresholds reduce  therefore, the number of pixels categorized into the intermediate group decrease  however, exhaustively searching the global peaks over all iterations is time-consumin  the following stopping criterions are used to allow that the global maximum value is achieved in most cases within a reasonable number of iteration  let wh and wl denote the summation of the weights of ih and il, respectivel  from we find that the blob significance value b is within the range the maximum value of its blob significance is   small r means that the peak value is quite similar to that in its surrounding region 05, which is chosen empiricall  scale changes from   table i number of detected points on the first image of each data se  by changing the size of the local image patches s1 and s2, the atc detector can identify local structures of various scale  similar to that done inwe implement the multi-scale atc detector by detecting the points in each scal  2) detect the local peaks of the blob significance on spatial dimension  3) remove the peaks on ridges and edges by the remaining peaks are the interest points to be detecte ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "III", "Section": "III Experiments", "Text": "  repeatability two detected regions are regarded as repeated if their overlap is above 60% as suggested in we use the repeatability to evaluate the detectors under different variation  similar to that inhalf-sampled images are used for evaluatio  for the atc detector, interest points are extracted on 5 octaves by half-sampling the previous octav  for each data set, the detector parameters are adjusted so that roughly the same number of interest points are detected on the first image for all detector  fi  2 and illustrate the experimental results under the changes of viewpoint and scale, respectivel  fi  2and show the performances under complex illumination change  these results show that the atc detector can achieve better performance than the other five detectors under almost all the different experimental setting  as the default setting produces too few interest points for the face recognition for all detectors, the thresholds that are used to remove the low response interest points are set to be zero for all detectors in the present experimen  for the mser detector, the minimum size of its output region is set to be 1/4 of the default setting to ensure it is applicable to all of the testing database  all the detected interest points are described by the sift descripto  the matching algorithm for face recognition, which consists of interest point matching 5 table ii face database setting  ar gt orl feret atc 9  the database setting is shown in table i  the face images in these databases have variations in illumination, expression and pose  the recognition rate, which is the percentage of correctly identified test images from the rank-1 best matched gallery, is used to measure the performance of the interest point detector  table iii shows that the proposed detector achieves the highest recognition rate over the four database  it suggests that the interest points detected by the proposed atc detector are more robust and discriminative compared to other ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "IV", "Section": "IV Conclusions", "Text": " as the blob significance is measured by counting the number of bright and dark pixels, the detection result is invariant to the illumination change  evaluations on the oxford dataset and the complex illumination dataset in show that the atc detector outperforms the other five detectors in terms of repeatability under the variations caused by scale, viewpoint and illumination change  the advance performance of the proposed detector is also verified in the application of face recognitio ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "References", "Section": "References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Inorganic", "Section": "Inorganic Materials Synthesis Planning with Literature-Trained Neural Networks", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Comparing", "Section": "Comparing many-body localization lengths via non-perturbative construction of local integrals of motion", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "convex relaxations of convolutional neural nets burak bartan mert pilanci department of electrical engineering stanford university abstract we propose convex relaxations for convolutional neural nets with one hidden layer where the output weights are fixe  for convex activation functions such as rectified linear units, the relaxations are convex second order cone programs which can be solved very efficientl  we prove that the relaxation recovers the global minimum under a planted model assumption, given sufficiently many training samples from a gaussian distributio  we also identify a phase transition phenomenon in recovering the global minimum for the relaxation", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1  Introduction", "Text": "convolutional neural networks have been extremely successful across many domains in machine learning and computer vision however, analyzing the behavior of non-convex optimization methods used to train cnns remains a challeng  in this paper, we propose finite dimensional convex relaxations for convolutional neural nets which are composed of a single hidden layer with convex activation function  we illustrate a phase transition phenomenon in the probability of recovering the global optimu  finally, we consider learning filters for a regression task on the mnist dataset of handwritten digits", "Subsections": [{"Section_Num": "1_1", "Section": "1.1  Related Work and Contributions", "Text": "in recent years, there has been an increasing amount of literature on providing theoretical results for neural network  a considerable amount of work in this area focused on the case where a convolutional network with a single hidden layer is trained using gradient descen  for instance, shows that gradient descent achieves the global minimum for convolutional networks with one layer and no overlap when the distribution of the input is gaussia  also proves that the problem of learning this network is npcomplet  zhang e  al propose a convex optimization approach based on a low-rank relaxation using the nuclear norm regularizer inaskari et a  consider neural net objectives which are convex over blocks of variable  a number of recent results considered the gradient descent method on the non-convex training objective, and proved that it recovers the planted model parameters under distributional assumptions on the training data we refer the reader to for other theoretical results regarding learning relu units and convolutional net  our contributions can be summarized as follow  first, we propose a randomized convex relaxation of learning single hidden layer neural networks in the original parameter spac  this should be contrasted withwhere the convex program is in the lifted space of matrices, and is not guaranteed to find the global optimu  our derivation also explains why direct relaxations fail and a randomized perturbation is neede  second, we prove that the relaxation obtains the global optimum under a planted model assumption with gaussian training data with certain probabilit  our results are geometric in nature, and has a close connection to the phase transitions in compressed sensingescape from a mesh phenomena in gaussian random matrices and sketching third, our numerical results highlight a phase transition, where the global optimum can be recovered when the number of samples exceeds a threshold that depends on the dimension of the filter and the number of hidden neuron  our approach provides a general framework for obtaining convex relaxations which can be used in other architectures such as soft-max classifiers, autoencoders and recurrent net ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2  Relaxations for a Single Neuron", "Text": "consider the problem of fitting a single neuron to predict a continuous labels y from observations   in general, the optimization of this objective for an arbitrary training set is computationally intractabl  we refer the reader to recent works on the np-hardness of relu regression and approximation algorithms", "Subsections": [{"Section_Num": "2_1", "Section": "2.1  Failure of the naive relaxation", "Text": "as a result of the relaxation, the convex optimization problem in may not be a satisfactory approximation of the original problem, and may have more than one optimal solutio   gaussian distributed,   for    gaussian features, this holds with high probability when n is large enoug ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2_2", "Section": "2.2  Randomized Convex Relaxation", "Text": "in convex relaxations, relaxing the equality constraints into inequality constraints can lead to multiple spurious feasible point  it is clear that with the naive convex relaxation, we can't hope to recover the optimal solution to in this section we will propose a randomly perturbed convex program aiming to recover the solution of the original proble  the reasoning behind optimizing a random objective is to randomly pick a solution among multiple feasible solution  for the relu activation, the above is a second-order cone program which can be solved efficiently this is a common assumption which is also used by many others in the literature, and sometimes referred as the teacher network assumptio   gaussian distribution over the feature  convolutional nets and multiple neurons we now consider multiple neurons where each neuron receives the output of a convolutio  for optimality, we must have the second and the third terms of the lhs to be zero since they are both nonpositiv  assuming x and r both have   ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3  Convolutional nets and multiple neurons", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4  Numerical Results", "Text": "in this section, we present the results of numerical experiment  the first subsection serves to visualize and compare the phase transition phenomena for our proposed relaxation method and gradient descen  in all of the experiments, the filter is applied with no overlaps, thus the filter size is equal to d/  the second subsection presents numerical experiments on the mnist datase ", "Subsections": [{"Section_Num": "4_1", "Section": "4.1  Phase Transition Plots for Gaussian Distribution", "Text": "for all the phase transition plots in fi  then we run the proposed relaxation method and gradient descent algorithms separately and repeat it 100 times for each metho  fi  fi  for a given k, gradient descent seems to outperform the proposed relaxation method as the line where the phase transition occurs has a higher slop  however, gradient descent does not offer the same flexibility the proposed method does since the proposed relaxation is a convex problem and can handle extra convex constraint  fi  we believe that this implies that the non-convex loss surface of the convolutional nets is becoming more like a convex surface as k increase ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4_2", "Section": "4.2  Experiments on MNIST Dataset", "Text": "we now present the experiment results on a randomly rotated version of the mnist datasetwhere the task is to predict the rotation angles of handwritten digit  the results are given in table   we compare two methods where we perform least squares with l2 regularization, on different feature  phase transition plots comparing cnn relaxation and gradient descent when the training data is    gaussia  laxation to   the results in table 1 show root-mean-square table   mnist experiment result  experiment rmse ls using raw pixels 1 04 ls with learned filter 1 59 errors for the predicted rotation angle  these results have been obtained on the test set, which has 5000 images unseen to the training proces  table 1 shows that the proposed relaxation method helps extract useful features that make the model generalize better to the test dat ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5  Conclusion", "Text": "we investigated convex relaxations for single hidden layer nooverlap convolutional neural net  it is possible to make the success probability arbitrarily close to 1 by running the algorithm multiple time  we gave phase transition plots to help identify the behavior of the proposed convex relaxation in different parameter regime  we also presented a numerical study on a real dataset and empirically showed that the proposed convex relaxation is able to extract useful feature  we believe this work provides insights towards understanding the relationship between the non-convex nature of deep learning and convex relaxation ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "6", "Section": "6  References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190 00036v2 18 feb 2020 bi-parameter trilinear fourier multipliers and pseudo-differential operators with flag symbols guozhen lu, jill pipher, and lu zhang abstrac  we will show that our problem can be reduced to establish the lr estimate for the special multiplier m1m2 we will also show that the lr estimate holds for tab as long as the lr estimate for the flag multiplier operator holds when the multiplier has the special form m1m2 moreover, our method also allow us to establish the weighted mixed norm estimates the bi-parameter and trilinear flag fourier multipliers considered in this paper do not satisfy the conditions of the classical bi-parameter trilinear fourier multipliers considered by muscalu, tao, thiele and the second author they may also be viewed as the bi-parameter trilinear variants of estimates obtained for the one-parameter flag paraproducts by muscalu key words and phrase  the first author's research was partly supported by a collaboration grant from the simons foundation", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1. Introduction", "Text": " for these symbols, the following multi-linear, single parameter case has been studie  again, this result still hold if the functions are defined on r  we now consider the multi-parameter setting of the above operators, introduced and studied via time-frequency analysis in the results extend to the n-linear, d-parameter case where f1,fn are defined on r  the corresponding bi-parameter pseudo-differential operator was studied in in particular, in the proof of trilinear bi-parameter version of theorem  4 above, the following localized lr estimates hold and these estimates will also play a role in our current pape  we now return to the discussion of the classical single-parameter coifman-meyer type operator under the condition note that in the only singularity for the symbol m is at the origi  inmuscalu considered some types of symbols having flag singularitie  then there holds theorem   moreover, the method in can be applied when studying the adjoints of those operator  the main purpose of this paper is to study the lr estimates for the bi-parameter trilinear fourier multipliers with flag singularity as defined inas well as the corresponding bi-parameter trilinear pseudo-differential variant  our main theorems are as follow  while the helicoidal method of can be used to treat certain of the operators in our reduction, it does not give the boundedness of more details are in section 4 and remark   the above theorem indicates that providing estimates for the operator are the fundamentally new obstacles in obtaining estimates for the operator in fact, the proof of this theorem shows that the study of the bi-parameter flag multiplier can be essentially reduced to the study of classical bi-parameter multilinear fourier multipliers like as well as the multipliers with tensor product symbols like as it turns out, estimates for the operator are of interest even in restricted function space  the methods of used flag paraproducts and some novel and careful stoppingtime argument  we include some details in appendix   we will denote ap as the class of muckenhoupt weights below and we refer the reader to appendix c for definitio  now we state our result for the corresponding bi-parameter trilinear pseudodifferential operator  the one-parameter case was studied in we will prove the following estimate when assuming the estimates in theorem   bi-parameter trilinear fourier multipliers with flag symbol 7 the proof of the lr estimates for the bi-parameter trilinear flag fourier multipliers of theorem   such a reduction is partly inspired by earlier work in both the singleparameter and bi-parameter settings, for instance, to prove theorem  10, we reduce the bi-parameter trilinear pseudo-differential operator to a localized versio  this is theorem   the rest of the paper is organized as follow  in section 2, we collect some notation and definitions used in the pape  section 4 contains the proof of theorem  10 can be reduced to an estimate for a localized operator this allows us to avoid the more complicated size and energy estimates used in to deal with paraproduct  acknowledgemen  the authors are grateful to camil muscalu for pointing out an error in our first version posted in the arxi org and for many useful comments as we were revising the pape  to be precise, the derivation of in the original version of this paper was incorrec  indeed, we do not claim to have a proof of the earlier version of theorem   instead we prove here that the lr estimates for the bi-parameter trilinear flag multiplier can be reduced to the lr estimate for multipliers of the form m1m  the main revision is in subsection  2 where we have adapted a new method of reduction of the general bi-parameter trilinear multiplier to the special one of the form m1m  we also thank camil muscalu for communicating the results of zhai's thesis, which alerted us to the interest in addressing the tensor product case", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2. notations and preliminaries", "Text": " the use f -1 to denote the inverse fourier transform of f", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3. A Leibniz rule", "Text": "before giving the proof of theorem  7, we give an example where the operator we consider plays a rol  this is one of the possible motivations for the study of such operator  the details are included in the appendix  2 implies the following leibniz rul  then it's very natural and interesting to ask if such bi-parameter leibniz rule holds when there is higher complexity of the differentiatio  now let's take a quick look at how the estimate in theorem  7 is associated with the 16 terms appearing in we indicate some key steps here, and more details can be found in appendix   in fact, the expression is a fourier multiplier with symbol having the form m1m2, where m1 belong to bm and m2 belong to bm respectivel  to see how to get the other terms in from the boundedness ofwe need to look at the terms that are similar towhich appear in the process of reductio  however, there are still terms in the reduction that can not be covered by the operators overcoming this obstacle is another issue for future research in this subjec ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4. Proof of Theorem ??", "Text": "", "Subsections": [{"Section_Num": "4_1", "Section": "4.1. Reduction of the symbols", "Text": " m0,1 and m1,0 satisfy the similar condition  thus, if suffces to study m0,0m  since it is suffcient to consider m0,0m2, we now use m1 to represent m0,  in the following subsections, we deal with each of these four types of symbol  the symbol m2,2 1 m  therefore this symbol also falls within the scope of theorem   note that this is stronger than the condition that m1,1 1 m2 satisfies,   thus, we only need to consider the symbol m1,1 1 m  the symbol m1,1 1 m  to handle the symbol m1,1 1 m2, we use the standard decomposition for m  16 guozhen lu, jill pipher, and lu zhang we now rewrite these using their fourier expansion  other terms are handled similarl 2 gives us the desired estimat  here we use the fact =0 since they have disjoint support  22 guozhen lu, jill pipher, and lu zhang now we come back to the bi-parameter cas  bi-parameter trilinear fourier multipliers with flag symbol 23 note that actually i1 corresponds to the boundedness of the following trilinear fourier multiplie  if we make the assumption that this operator is bounded, we have the followin  we still consider the three parts o4, sk1sk2o4 and sk1o  now we consider the lr norm of bi-parameter trilinear fourier multipliers with flag symbol 27", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5. Reduction of Theorem ?? ", "Text": "10 in this section we give the idea to prove theorem  10; the strategy is to reduce the pseudo-differential operator to a localized versio  then our main theorem   thus, we only need to prove in short, the proof of theorem  10 can be reduced to the above theorem, and in the next section we will show how to deal with the operator in", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "6", "Section": "6. Proof of Theorem ??", "Text": "1 in this section we prove theorem   the first step is to use fourier series as before and rewrite the operator here we make use of the fact that the conditions do not involve any singularit  more precisely, we can modify the littlewood-paley decomposition as follow  the key thing here is that one does not have to decompose the identity near   for simplicity, let us consider the single-parameter case first,   now we come back to the bi-parameter cas  by doing the decomposition as above in each parameter  andone should have four cases to estimat  before we state this result, we introduce some notations which are needed in the statement of the resul  then we introduce some notations that will appear in the next lemm  we take t1 and b1 i from and in definition   note the only difference between them is the dependence on   in this sense, l will not play an important role in our estimate  for simplification, we omit this dependence on l for all the expressions in the rest of the wor  with these notations, we are ready to state the following lemma   proo  one can follow the work closely, where the taylor expansions of proper functions are used to get such forms of paraproduct  the only two statements we need to show are that all the dyadic intervals there have lengths at most one and the decay number 1 in the denominator from now we return to the bi-parameter cas  in this case we can ignore k0 as discussed in remark 2 and clearly the desired estimate follows from theorem   this is actually where such paraproducts behave differently from the classical one  we will make use of theorem   that means when dealing with the paraproducts, we can go back to the original form of operators in theorem   proo  this is essentially a corollary of theorem  7 on the first paramete  we omit the details her  that means we can use theorem   we omit the details her  having treated all the cases in theorem  10 is conclude ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Appendix", "Section": "Appendix A. ", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Appendix", "Section": "Appendix B. ", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Appendix", "Section": "Appendix C. ", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "References", "Section": "References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190  in their recent work, grechka and tsvankin showed that the azimuthal dependence of nmo velocity generally has an elliptical shape and is determined by the spatial derivatives of the slowness vector evaluated at the cmp locatio  this formalism is used here to develop exact solutions for normal-moveout velocity in anisotropic media of arbitrary symmetr  for the model of a single homogeneous layer above a dipping reflector, we obtain an explicit nmo expression valid for all pure modes and any orientation of the cmp line with respect to the reflector strik  the influence of anisotropy on normal-moveout velocity is absorbed by the slowness components of the zero-offset ray - quantities that can be found in a straightforward way from the christoffel equatio  if the medium above a dipping reflector is horizontally stratified, the effective nmo velocity is determined through a dix-type average of the matrices responsible for the interval nmo ellipses in the individual layer  this generalized dix equation provides an analytic basis for moveout inversion in vertically inhomogeneous, arbitrary anisotropic medi  for models with a throughgoing vertical symmetry planethe semi-axes of the nmo ellipse are found by the more conventional rms averaging of the interval nmo velocities in the dip and strike direction  modeling of normal moveout in the most general heterogeneous anisotropic media requires dynamic ray tracing of only one ra  remarkably, the expressions for geometrical spreading along the zero-offset ray contain all the components necessary to build the nmo ellips  this algorithm becomes especially effcient if the model consists of homogeneous layers or blocks separated by smooth interface  the high accuracy of our nmo expressions is illustrated by comparison with ray-traced reflection traveltimes in piecewise-homogeneous, azimuthally anisotropic model  we also apply the generalized dix equation to field data collected over a fractured reservoir and show that p-wave moveout can be used to find the depth-dependent fracture orientation and evaluate the magnitude of azimuthal anisotrop  pacs numbers: 8 -f", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "I", "Section": "I Introduction", "Text": "reflection moveout in inhomogeneous anisotropic media is usually calculated by multi-offset and multi-azimuth ray tracing  while the existing anisotropic ray-tracing codes are suffciently fast for forward modeling, their application in moveout inversion requires repeated generation of azimuthally-dependent traveltimes around many common-midpoint locations, which makes the inversion procedure extremely time-consumin  moveout modeling, however, can be simplified by taking advantage of the limited range of offsets in conventional acquisition desig  for common spreadlength-to-depth ratios close to unity, cmp traveltimes in media with moderate structural complexity are well described by the normal-moveout velocity defined in the zero-spread limit2,  even if the data exhibit nonhyperbolic moveout, nmo velocity is still responsible for the most stable, small-offset portion of the moveout curv  existing methods for computing normal-moveout velocity in inhomogeneous media are designed for isotropic models4,  angular velocity variations make both analytic and computational aspects of nmo-velocity modeling much more complicate  explicit expressions for normal-moveout velocity are well known for the relatively simple transversely isotropic model with a vertical symmetry axis6 recently, tsvankin7 presented an exact nmo equation for dipping 2 reflectors valid in vertical symmetry planes of any homogeneous anisotropic mediu  alkhalifah and tsvankin8 extended this result by developing a dix-type equation for vertically inhomogeneous anisotropic media above a dipping reflecto  still, their formalism is limited to 2-d wave propagation in the dip plane of the reflector, which should also coincide with a symmetry plane of the overburde  this work is based on a general 3-d treatment of normal moveout developed by grechka and tsvankin3, who proved that the azimuthal dependence of nmo velocity for pure modes has an elliptical shape in the horizontal plane, even if the medium is arbitrary anisotropic and inhomogeneou  this conclusion breaks down only for subsurface models in which common-midpoint reflection traveltime cannot be described by a series expansion or does not increase with offse  the orientation of the nmo ellipse and the values of its semi-axes can be expressed through the spatial derivatives of the slowness vector, which are determined by both the direction of the reflector normal and the medium properties above the reflecto  grechka and tsvankin3 also presented explicit representations of the nmo velocity for a horizontal orthorhombic layer and dipping reflectors beneath vti medi  a detailed analysis of the nmo ellipse for transversely isotropic media with a horizontal symmetry axis is given in tsvankin9, who also discusses the inversion of conventionalspread reflection moveout for the parameters of hti medi  sayers10 obtained the elliptical dependence of nmo velocity for the model of a homogeneous anisotropic layer with a horizontal symmetry plane using an approximation for long-spread moveout based on group-velocity expansion in spherical harmonic  here, we apply the formalism of grechka and tsvankin3 to more complicated anisotropic model  we start by deriving an explicit expression for azimuthally-dependent nmo velocity from a dipping reflector overlaid by a homogeneous layer of arbitrary symmetr  then we obtain a generalized dix equation for nmo velocity in a model composed of a stack of horizontal homogeneous, arbitrary-anisotropic layers above a dipping reflecto  while this equation has a form similar to the conventional dix formula, it is based on the averaging of the matrices that define interval nmo ellipse  for the most general inhomogeneous media, we develop an effcient methodology to compute the normal-moveout velocity using the dynamic ray-tracing of only one ra  we show that the derivatives needed to find the geometrical spreading11,12 provide suffcient information to build the nmo ellipse and, therefore, model reflection moveout without tracing a large family of ray  finally, we compare the hyperbolic moveout equation parameterized by the exact nmo velocity with ray-traced reflection traveltimes and present a field-data application of the generalized dix differentiatio ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "II", "Section": "II Equation of the NMO ellipse", "Text": "suppose the traveltimes of a certain reflected wave have been recorded on a number of commonmidpoint gathers with different azimuthal orientation but the same midpoint location 1: normal-moveout velocity is calculated on cmp lines with different azimuths and a fixed midpoint locatio  it is not necessary to account for reflection-point dispersal in the derivation of nmo velocit  the one-way traveltimes appear in equation because reflection-point dispersal has no influence on the nmo velocity of pure modes, and rays can be assumed to propagate through the reflection point of the zero-offset ray5,  note that this conclusion is valid for arbitrary inhomogeneous anisotropic media provided the traveltime field is suffciently smooth to be adequately approximated by a taylor series expansion", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "III", "Section": "III Homogeneous arbitrary anisotropic layer", "Text": "  general case to obtain normal-moveout velocity for any given model from equations andwe need to evaluate the spatial derivatives of the slowness vector at the cmp locatio  as demonstrated in appendix b, for the model of a single homogeneous layer this can be done by representing the horizontal ray displacement through group velocity and using the relation between the group-velocity and slowness vector  equation is valid for pure modes reflected from horizontal or dipping interfaces in media with arbitrary symmetry and any strength of the anisotrop  the slowness components p1, p2 and q can be found by solving the christoffel equation for the slowness direction normal to the reflecto  since this equation is cubic with respect to the squared phase velocity, it yields an explicit expression for the slowness vecto  the derivatives of the vertical slowness q can be found directly from the christoffel equation as wel  for common anisotropic models with a horizontal symmetry planef becomes a cubic polynomial with respect to q  therefore, all terms in equation can be obtained explicitly from the christoffel equatio  equation can also be used to develop weak-anisotropy approximations for nmo velocity by linearizing q and its derivatives in dimensionless anisotropic parameters or in perturbations in the stiffness coeffcient  these analytic approximations provide valuable insight into the influence of the anisotropic parameters on normal moveout7,1  there is hardly any need, however, to substitute weak-anisotropy approximations for the exact equations in numerical modelin  thus, equation gives a simple and numerically effcient recipe to obtain azimuthally-dependent reflection moveout in an arbitrary anisotropic laye  despite the presence of anisotropy-induced nonhyperbolic moveout, the p-wave nmo velocity is close to the moveout velocity calculated from the exact traveltimes on six cmp lines with different orientatio  the maximum difference between vnmo and the finite-spread moveout velocity is just   therefore, the magnitude of nonhyperbolic moveout for this model decreases with reflector dip; the same observation was made by tsvankin7 for vertical transverse isotrop  2: comparison of the p-wave nmo velocity from equation and the moveout velocity obtained by least-squares fitting of a hyperbola to the exact traveltimes computed for spreadlength equal to the distance between the cmp and the reflecto  the vertical symmetry plane at zero azimuth has the properties of the vti model of dog creek shale, while the second vertical symmetry plane is equivalent to taylor sandstone; both models are described in thomsen ", "Subsections": [{"Section_Num": "B", "Section": "B Special cases", "Text": " model with a vertical symmetry plane next, let us consider a special case - a model in which the dip plane of the reflector coincides with a vertical symmetry plane of the laye  the medium can be, for instance, transversely isotropic with the symmetry axis confined to the dip plane or orthorhombi  the mirror symmetry with respect to the dip plane implies that one of the axes of the nmo ellipse points in the dip directio  below, we provide a formal proof of this fact, as well as concise expressions for the azimuthally dependent nmo velocity in this mode  it is convenient to align the x1-axis with the azimuth of the dip plane, while the x2-axis will point in the strike directio  in the form v 2 nmo was first given by cohen1  equation provides a similar representation for the nmo velocity in the strike directio  equations and are always valid for transversely isotropic media with a vertical symmetry axis because of the mirror symmetry with respect to any vertical plane in this mode  grechka and tsvankin3 also gave an equivalent form of equation in terms of the phase-velocity function and the weak-anisotropy approximation for v 2 nm  due to the axial symmetry of the vti model, both the dip-line and strike-line nmo velocities depend on the derivatives of q with respect to the single horizontal slowness component the cubic equation for q2 in vti media is particularly easy to solve because it splits into a quadratic equation for p -sv waves and a linear equation for the sh-wav  further simplification can be achieved for a medium with a vertical symmetry plan  normalmoveout velocities for vertical and horizontal transverse isotropy can be easily found as special cases of equation   equation can also be used to derive similar expressions for the split shear waves in orthorhombic medi  layer layer 1 zero-offset reflection point zero-offset ray layer cmp lines fi  3: a dipping reflector beneath a horizontally layered overburde  normal-moveout velocity in this model can be obtained from the generalized dix equation derived her ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "IV", "Section": "IV Horizontally-layered medium above a dipping reflector", "Text": "  generalized dix equation here, we show that the nmo ellipse for vertically inhomogeneous arbitrary anisotropic media above a dipping reflector can be obtained by dix-type averaging of the matrices w responsible for the interval nmo ellipse  in our derivation we essentially follow the approach employed by alkhalifah and tsvankin8 to obtain a 2-d dix-type nmo equation for rays confined to the incidence plane that contains the cmp lin  their equation is valid only in the dip plane of the reflector, which should also coincide with a symmetry plane of the mediu  in contrast, we make no assumptions about the mutual orientation of the cmp line and reflector strike, and take full account of the out-of-plane phenomena associated with both model geometry and depth-varying anisotrop  in a model composed of horizontally homogeneous layers above the reflector, the horizontal components p1 and p2 of the slowness vector remain constant along any given ray between the reflection point and the surfac  equations and generalize the dix18 formula for horizontally-layered arbitrary anisotropic media above a dipping reflecto  formally, this extension looks relatively straightforward: the squared nmo velocities in the dix formula are simply replaced by the inverse matrices w-  also, the generalized dix differentiation is subject to the same limitation as its conventional counterpart: the thickness of the layer of interest should not be too much smaller than the layer's dept  in contrast to the conventional dix equation, however, the effective matrix w-1 in equation cannot be obtained from seismic data directly since the corresponding reflector usually does not exist in the subsurfac  this procedure was discussed for the 2-d case by alkhalifah and tsvankin8 and further developed for p-waves in vti media by alkhalifah19; the latter paper also contains a successful application of this algorithm to field dat  note that although such a model is horizontallyhomogeneous, the zero-offset ray is not necessarily verticaland the zero-offset reflection point may be shifted in the horizontal direction from the cmp location", "Subsections": [{"Section_Num": "B", "Section": "B Model with a vertical symmetry plane", "Text": "next, we consider the same special case as for the single-layer model - a medium in which all layers have a common vertical symmetry plane that coincides with the dip plane of the reflector equations and for the dip component of the nmo velocity were derived by alkhalifah and tsvankin8 who considered 2-d wave propagation in the dip plane of the reflecto  our derivation shows that the same dix-type equations can be applied to the strike-component of the nmo velocity, which determines the second semi-axis of the nmo ellips ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "C", "Section": "C Accuracy of the rms averaging of NMO velocities", "Text": " it is also clear from the results of the previous section that the rms averaging of the interval nmo velocities is valid in any azimuthal direction, if all interval nmo ellipses degenerate into circle  hence, the error of this more conventional averaging procedure depends on the elongation of the interval ellipses, a quantity controlled by both azimuthal anisotropy and reflector di  to quantify this conclusion, we consider two numerical example  figure 4 shows the azimuthally-dependent p-wave nmo velocity in an orthorhombic medium consisting of three horizontal layers with strong azimuthal anisotrop  while the exact nmo ellipse happens to be close to a circle, the approximate, rms-averaged normalmoveout velocity has an oval noncircular shape because the interval nmo ellipses are far different from circle  the maximum error of the rms averaging is about   4: comparison of the exact p-wave nmo ellipse and an approximate nmo velocity obtained by the dix-type averaging the vertical p-wave velocities are vp 0,1 =  5 km/s; the interval zero-offset traveltimes are equal to each other velocities after application of the dix differentiation evidently, for this model it is necessary to use the exact nmo equation, which properly accounts for the influence of azimuthal anisotropy on normal moveou  for models with moderate azimuthal anisotropy and a horizontal reflectorthe accuracy of the rms averaging of nmo velocities is much highe  this implies that for such media it is possible to obtain the interval nmo velocity by the conventional dix differentiation at a given azimut  in the special case of horizontally layered hti mediathe same conclusion was made by al-dajani and tsvankin2  it should be emphasized, however, that for dipping reflectors the dix differentiation cannot be applied in the standard fashion because the interval nmo velocities are still calculated for non-existent reflectors and cannot be found directly from the dat  on the whole, we would recommend to use the generalized dix equation for any azimuthally anisotropic model, provided the azimuthal coverage of the data is suffcient to reconstruct the dependence vnm  since our algorithm operates with the nmo ellipses rather than individual azimuthal moveout measurements, it has the additional advantage of smoothing the azimuthal variation of nmo velocity, which helps to eliminate outliers and stabilize the interval parameter estimatio  a field-data application of the generalized dix equation is discussed belo  obviously, in this model the dip plane of the reflector always represents a symmetry plane, and one of the axes of all interval nmo ellipses is parallel to the dip directio  as shown in the previous section, in this case the rms averaging of the interval nmo velocities becomes exact for the dip and strike cmp lineswhere the interval nmo values are well known1  in all other azimuths, equation gives only an approximation to the exact nmo velocit  however, figure 5 indicates that this approximation is quite accurate for small and moderate reflector dip 15 azimuth vnmo/vnmo ~ fi  the model contains three layers above the reflector with the interval velocities v1 =   clearly, the error increases with dip because the interval nmo ellipses become more elongated and diverge more from a circl  these interval velocities correspond to nonexistent reflectors and need to be recalculated from the nmo velocities of the horizontal events", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "V", "Section": "V Inhomogeneous anisotropic media", "Text": "  summary of ray tracing here, we give a brief overview of ray-theory equations for anisotropic media11,12,21, which we use below to obtain normal-moveout velocity in the presence of both anisotropy and inhomogeneit  for real quantities pj corresponding to homogeneous waves, solutions a of equation are real and orthogonal to each othe  the ray-tracing system combined with the initial conditions can be solved by numerical integration using, for instance, the runge-kutta metho ", "Subsections": [{"Section_Num": "B", "Section": "B Computation of NMO velocity", "Text": "the results of grechka and tsvankin3, briefly reviewed above, show that there is no need to perform a full-scale multi-azimuth ray tracing to compute reflection traveltimes on conventional cmp spread  in practice, the values of vnmo determined on finite cmp spreads may be distorted by the influence of nonhyperbolic moveou  however, reflection moveout for spreadlengths 13 close to the distance of the cmp from the reflector is typically close to hyperbolic; this has been shown in a number of publications2,3,7 and is further illustrated by numerical examples in this wor  although calculation of wij from vnmo obtained in three azimuths is much more effcient than multi-azimuth ray tracing, it still requires a considerable amount of computation and does not take advantage of the explicit expressions for the parameters of the nmo-velocity ellipse discussed abov  here, we outline an effcient method of calculating these derivatives based on the dynamic ray-tracing equations for the zero-offset ra  let us consider the zero-offset ray in the ray coordinates the third column of the matrices p and x can be obtained using the kinematic ray-tracing equations to find the first and second columnslet us consider the so-called dynamic ray-tracing equations responsible for the geometrical spreading along the ray11,1  the initial conditions for these equations are, in turn, derived by differentiating the corresponding initial conditions for the kinematic ray-tracing equations thus, the derivatives needed to obtain the normal-moveout velocity are exactly the same as those required to compute the geometrical spreading along the zero-offset ra  therefore, the azimuthally-dependent nmo velocity in inhomogeneous arbitrary anisotropic media can be computed by integrating the dynamic ray-tracing equations for the one-way zero-offset ray and substituting the results into equationsand since this approach requires tracing of only one zero-offset ray together with the derivativesit is orders of magnitude less time consuming than is the tracing of hundreds of reflected rays for different azimuths and source-receiver offsets as would otherwise be require ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "C", "Section": "C Piecewise homogeneous media", "Text": "let us consider a medium composed of arbitrary anisotropic homogeneous layers separated by smooth interface  the boundary conditions will also be used in the equations for dynamic ray tracing discussed belo  integration of the dynamic ray-tracing equations in the case of homogeneous layers is relatively straightforward as wel  the derivative of the group velocity needed in equation is obtained in appendix d from equations and since the quantities needed to obtain these derivatives have to be found for the kinematic ray-tracing anyway, the additional computational cost of this operation is minima ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "VI", "Section": "VI Synthetic examples for inhomogeneous media", "Text": "the accuracy of our single-layer nmo equation has been discussed above here, we carry out synthetic tests to compare the hyperbolic moveout equation parameterized by the exact nmo velocity with raytraced traveltimes for inhomogeneous anisotropic model  figure 7 illustrates the performance of the dix equation for a model that includes three anisotropic layers with different symmetry above a dipping reflector despite the complexity of the model, the best-fit ellipse found from the finite-spread moveout velocities are suffciently close to the theoretical nmo ellipse computed from equations and a small difference between the ellipses is caused by nonhyperbolic moveout associated with both anisotropy and vertical inhomogeneit  it is clear from figure 7b that the influence of nonhyperbolic moveout becomes substantial only at source-receiver offsets that exceed the distance between the cmp and the reflecto  a similar example, but this time for a horizontally inhomogeneous medium above the reflector is shown in figure   the model contains three transversely isotropic layers with dipping lower boundaries and differently oriented symmetry axe  the nmo ellipse provides an excellent approximation to the effective moveout velocity for all four azimuths used in the computation, with a maximum error of just about  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "VII", "Section": "VII Field-data example", "Text": "we applied the generalized dix equation to a 3-d data set acquired by arco in the powder river basin, wyomin  a detailed description of this survey and preliminary processing results can be found in corrigan et a 24 and withers and corrigan2  the main goal of the experiment was to use the azimuthal dependence of p-wave signatures in characterization of a fractured reservoi  hence, the acquisition was carefully designed to provide a good offset coverage in a wide range of source-receiver azimuth  to enhance the signal-to-noise ratio, the data were collected into a number of superbins, each with an almost random distribution of azimuths and offset  below we show the results of our velocity analysis for one of the superbins located in the southwest corner of the survey are  figure 9 shows the composite cmp gather in one of the sectors with two prominent reflection events marked by arrow  according to withers and corrigan25, the reflection at a two-way vertical time of  58 s is the basement reflectio  while the best-fit stacking velocity for the event at  14 s is weakly dependent on azimuth, the velocity for the basement reflection is noticeably higher at azimuth n30  this observation is confirmed by the shape of the effective nmo ellipses reconstructed from the semblance panels the stacking 16 test of generalized dix equation vti hti orthorhombic 20 o cmp lines fi  the interface depths are z1 =   the orientation of both effective ellipses agrees with the results of withers and corrigan25, who used a different algorith  it should be mentioned that ignoring azimuthal velocity variations on the order of 3-4% and mixing up all source-receiver azimuths would inevitably lead to poor stacking and deterioration of the final seismic imag  since the dips in the survey area are extremely small24, the azimuthal dependence of stacking velocity can be attributed to the influence of azimuthal anisotropy associated with vertical fracture  to study the interval properties for vertical times between  58 s, we applied the generalized dix equation to the effective nmo ellipse  note that the different orientation of the effective ellipses in figure 11, indicative of depth-varying principal directions of the azimuthal anisotropy, does not pose a problem for the generalized dix differentiatio  the pronounced azimuthal variation in the interval nmo velocity can be explained by the intense fracturing in the layer immediately above the basemen  the direction of the larger semi-axis of the interval nmo ellipse is in general agreement with the predominant fracture orientation in the deeper part of the section determined from borehole data and shear-wave splitting analysis2  complete processing/inversion results for the survey area will be reported in forthcoming publication ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "VIII", "Section": "VIII Discussion and conclusions", "Text": "azimuthally-dependent normal-moveout velocity around a certain cmp location is described by a simple quadratic form and usually has an elliptical shape, with the orientation and semi-axes of the ellipse determined by the properties of the medium and the direction of the reflector normal at the zero-offset reflection poin  using this general result obtained by grechka and tsvankin3, we have presented a series of solutions for the exact normal-moveout velocity of pure modes in anisotropic models of various complexit  for a homogeneous anisotropic layer above a dipping reflector, nmo velocity was found explicitly as a function of the slowness vector corresponding to the zero-offset ra  this single-layer equation is valid for arbitrary anisotropic symmetry and any orientation of the cmp line with respect to the reflector strik  7: comparison between the theoretical p-wave nmo ellipse calculated from the generalized dix equation and moveout velocities obtained from ray-traced traveltimes for spreadlength equal to the distance between the cmp and the reflector the model is shown in figure 6; the dashed line marks the best-fit ellipse found from the finite-spread moveout velocitie  hyperbolic moveout curve parameterized by the exact nmo velocity v  slownesses, needed to compute the nmo velocity, can be obtained in an explicit form using the christoffel equatio  in addition to simplifying moveout modeling, our nmo equation can be effectively used in moveout inversion, as well as in developing weak-anisotropy approximations for different symmetrie  if the model contains a stack of homogeneous arbitrary anisotropic layers above a dipping reflector, the nmo ellipse should be obtained by a dix-type averaging of the single-layer expressions described abov  to find azimuthally-dependent normal-moveout velocity, it is suffcient to compute the zero-offset traveltime and the interval nmo ellipses for the slowness vector of the zero-offset ra  the generalized dix equation can be used to perform moveout-based interval parameter estimation in vertically inhomogeneous anisotropic models of any symmetr  it should be emphasized, however, that application of the generalized dix differentiation to dipping events entails full-scale layer-stripping because nmo ellipses in the individual layers cannot be directly measured from the dat  one important special case considered in detail is a model with the same vertical symmetry plane in all layers that also coincides with the dip plane of the reflector and finite-spread moveout velocity in an azimuthally-anisotropic model with dipping layer  the model consists of three dipping transversely layers with different orientation of the symmetry axi  the first layer is vti with vp 0,1 =  6 velocity time b a fi  10: semblance velocity panels for two azimuthal sectors centered at n130e and n30e the semblance maxima corresponding to the events at vertical times of  58 s are frame  11: the effective nmo ellipses for the reflection events at  58 s reconstructed from the data and the corresponding interval nmo ellipse obtained by the generalized dix differentiatio  the orientation of the larger semi-axis of each ellipse is marked by a radial lin  vertical symmetry axis).  because of the mirror symmetry with respect to the dip plane, the axes of the nmo ellipse are aligned with the dip and strike directions of the reflecto  the generalized dix equation in such a model reduces to the rms averaging of the dip-line and strike-line nmo velocities in the individual layers this result represents a 3-d extension of the dix-type equation developed by alkhalifah and tsvankin8 for normal moveout in the dip plane of the reflecto  except for this special case, the effective nmo velocity computed by the dix rms averaging generally takes an oval anelliptic form that thus deviates from the exact nmo ellips  still, this deviation is not significant if the interval nmo ellipses are close to being circles, which implies the absence of large dips and significant azimuthal anisotrop  in any case, it is preferable to apply the generalized dix equation for any azimuthally anisotropic model because in addition to being more accurate it also provides the important advantage of smoothing the effective moveout velocities using the correct functional form and thus reducing the instability in interval parameter estimatio  we complete the analysis by considering the most general inhomogeneous media and presenting an algorithm that leads to a dramatic reduction in the amount of computations needed to obtain the nmo velocity and conventionalspread reflection moveou  all information required to construct the nmo ellipse is contained in the results of the dynamic ray tracing of a single ra  although evaluation of 20 geometrical spreading requires solving an additional system of differential equations together with the kinematic raytracing equations, this algorithm is orders of magnitude more effcient than multi-offset, multi-azimuth ray tracin  furthermore, if the model consists of homogeneous layers or blocks separated by smooth interfaces, all quantities needed to find the nmo ellipse can be computed during the kinematic tracing of the zero-offset ra  the normal-moveout velocity discussed here is defined in the zero-spread limit and cannot account for nonhyperbolic moveout caused by anisotropy and inhomogeneity on finite-spread cmp gather  nevertheless, our numerical examples for various anisotropic models demonstrate that the hyperbolic moveout equation parameterized by nmo velocity provides good accuracy in the description of reflection moveout on conventional spreads close to the distance between the cmp and the reflecto  even if the hyperbolic moveout approximation becomes inadequate, nmo velocity can be obtained by means of nonhyperbolic moveout analysis2,1  hence, the results of this work can be effciently used in traveltime inversion and dip-moveout processing for arbitrary anisotropic medi  to show the feasibility of applying our formalism in fracture detection, we processed wide-azimuth 3-d p-wave data acquired over a fractured reservoir in the powder river basin, wyomin  the generalized dix differentiation allowed us to obtain the depth-varying fracture orientation and estimate the magnitude of azimuthal anisotropy the direction of the larger semi-axis of the interval nmo ellipse in the strongly anisotropic layer above the basement is in agreement with the fracture trend in this part of the sectio  therefore, if the formation of interest has a suffcient thickness, azimuthal moveout analysis of p-wave data by means of the generalized dix equation provides valuable information for characterization of fracture network ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "IX", "Section": "IX Acknowledgments", "Text": "we are grateful to dennis corrigan and robert withers of arco for providing the field data and sharing their knowledge of the processing and interpretation issue  the support for this work was provided by the members of the consortium project on seismic inverse methods for complex structures at center for wave phenomena, colorado school of mines and by the united states department of energy appendix a: relation between the matrix w and the nmo-velocity ellipse azimuthally dependent normal-moveout velocity is described by equation of the main text as a general secondorder curve in the horizontal plan  the expression for vnmo can be simplified further by aligning the horizontal coordinate axes with the eigenvectors of the symmetric matrix w  the derivation is based on the general equations and describing the nmo ellipse and follows the approach suggested for the 2-d case by cohen1  next, we need to evaluate the effective nmo ellipse in the same approximatio  gajewski and   tsvankin and   thomsen, geophysics 59, n  grechka and   tsvankin, geophysics 63, n  shah, geophysics 38, 812 hubral and   krey, interval velocities from seismic reflection time measurements thomsen, geophysics 51, 1954 tsvankin, geophysics 60, 268 alkhalifah and   tsvankin, geophysics 60, n  tsvankin, geophysics 62, n  340-343   molotkov, and   kendall and   thomson, geophysical journal international 99, 401 stefani, geophysics 57, n  cohen, geophysics 63, 275 levin, geophysics 36, 510 mensch and   rasolofosaon, geophysical journal international 128, 43 gajewski and   1507-1510 dix, geophysics 20, 68 alkhalifah, geophysics 62, 662 al-dajani and   1495-1498 kashtan, problems of dynamic theory of seismic wave propagation 22, 14 tsvankin, geophysics 62, 614 1834-1837 withers and   corrigan, 59th eage conference and exhibition, e003", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "A", "Section": "A Relation between the matrix W and the NMO-velocity ellipse", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "B", "Section": "B NMO velocity in a single layer", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "C", "Section": "C Relation between the exact and rms-averaged NMO velocity", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "D", "Section": "D The derivatives of group velocity with respect to the ray parameters 1 and 2", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "References", "Section": " References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": " in contrast to traditional methods which do not differentiate the density prediction of these two states, we propose to use a dedicated network branch to predict the object/non-object mask and then combine its prediction with the input image to produce the density ma  our rationale is that the mask prediction could be better modeled as a binary segmentation problem and the difficulty of estimating the density could be reduced if the mask is know  a key to the proposed scheme is the strategy of incorporating the mask prediction into the density map estimato  to this end, we study five possible solutions, and via analysis and experimental validation we identify the most effective on  through extensive experiments on five public datasets, we demonstrate the superior performance of the proposed approach over the baselines and show that our network could achieve the state-of-the-art performance", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "I", "Section": "I Introduction", "Text": " it aims to count the number of objects within an image or a frame in the videos and is a very challenging problem because the objects-of-interest,   also, due to the difficulty in providing highly detailed annotations such as object bounding boxes or instance-level segmentation masks, existing datasets usually adopt a weak-level annotating scheme by labeling each object with a dot insid  these challenges make the traditional detection based approach less robust - and most existing methods - choose to solve this problem by estimating a density map generated from the dot-level annotatio  once the density map is correctly estimated, the this work is done when the first author visits the university of adelaid  object count can be obtained by simply summing over the density values in the ma  in the current density map annotation scheme, the values in the density map are all non-negative and only pixels close to an annotated dot can have nonzero value  in fact, for the density maps of many images, a significant portion of pixels will only take the zero valu  the above observation suggests that the density map estimation implicitly involves two steps: estimating whether a pixel belongs to the foreground or background and estimating the density value of the foreground regio  however, in this paper, we argue the benefits of explicitly separating the mask prediction from the density estimatio  more specifically, we propose to use a dedicated branch of a network to first predict the foreground/background mask, and then fuse the prediction with the input image to produce the final density map estimatio  the motivation of this strategy is that the first step is essentially a binary segmentation problem and it can be better trained with segmentation loss such as cross-entropy los  on the other hand, conditioned on the prediction of the mask, the estimation of the density map can be simpler than its unconditioned counterpar  consequently, the overall regression quality could be improve  the critical question of the above-proposed process is how to incorporate the mask prediction information into the density map estimato  in this paper, we study five different variants for achieving this incorporatio  should we use the binary form of the mask prediction or the predicted mask posterior,  e, the probability of a pixel being the foregroun  the way to incorporate the mask informatio  by simply multiplying the estimated mask or fusing this part of information with a neural networ  we analyze, both theoretically and experimentally, the pros and cons of the proposed methods and identify the last one as our best solutio  more specifically, in this solution, we feed the estimated object posterior into a few convolutional layers and together with the information from the input image to produce the final density ma  through extensive experiments on five public datasets, we demonstrate the superior performance of the proposed approach over the competitive baseline and show that our method can achieve the state-of-the-art crowd counting performanc  arxiv:190 ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "II", "Section": "II Related work", "Text": " here we present a brief review of the related wor  detection seems to be a straightforward solution for crowd countin  the most early methods use hand-crafted features such as haar waveletshistogram oriented gradients to model the pedestrian, which are then fed to classifiers to distinguish whether there is pedestrian or no  initially,studied the monocular pedestrian detection by a diverse set of low-level feature-based system  although monocularbased methods work well in a low density region, the performance is severely affected when they meet the crowded scenes with occlusion and scene clutte  to further consider this issue, more information of the pedestrian is taken into accoun  zhao et a  used multiple partially occluded human hypotheses in a bayesian framework to build a model-based approach to interpret the image observation  the authors in extracted the foreground and then aggregated the obtained silhouettes over a network to compute bounds about the crowd number and location  nevertheless, the representation ability of the low-level features is limited, which cannot be applied in many real scenario  these methods make a great progress in terms of detection performance and spee  however, for a heavily occluded and cluttered scenario, accurately detecting each object instance is still very difficul  as a alternative solution of the detection-based methods, the regression-based approaches are proposed to tackle the extremely dense crowd  initially, these approaches learn a mapping or relation between the features of local patches and the count  actually, they avoid learning some independent detector  for example, the authors proposed to cast the crowd counting problem as a density map estimation proble  the integral of the image density over any image region gives the count of objects within that regio  it was shown that the density map regression framework offers a robust crowd counting solution for various challenging scenarios, and since then it becomes the mainstream framework for this proble  various extensions - have been proposed to further improve the training and prediction of density map  ma et a  studied an integer programming method for estimating the instantaneous count of pedestrians crossing a line of interest in a video sequenc  idrees et a  argued that it is not reliable by only using one single feature or detection method for counting task when facing the highlevel density crowd  and they also reported that the spatial relationship is an importance information to constrain the counts in neighboring local region  chen et a  studied the challenges of inconsistent features along with sparse and imbalanced data, and proposed to learn a regression model by using cumulative attribute-based representatio  with the breakthrough of deep learning in the past years, most recent works on crowd counting are based on convolutional neural network  the authors in built an end-to-end cnn regression model to count the people in extremely crowd scene  later, introduced a cnn architecture that is fed with a whole image and directly outputs the final coun  to address the large variations in people or head size, exploited a multi-column neural network by using receptive fields of different sizes in each colum  the authors proposed a path switching architecture, called switchingcnn, to deal with the variation of object density within a scen  in order to gain better performance, proposed a contextual pyramid cnn by incorporating different levels of contextual information to achieve state-of-the-art performanc  in this paper, we propose a mask-aware network for crowd counting which incorporates the background/foreground mask information into the network for more accurate density regressio  in terms of the network architecture design, our network is somehow similar to the recent workwhich utilizes the top-down feedback to correct the initial predictio  however, our approach considers the background/foreground mask information and we will show later in the experiments that this consideration is crucial for achieving our good performanc  in terms of using mask information, there has been some successful cases in the areas of object segmentation and person re-identificationhowever, to our knowledge, our work is the first one that systematically studies the effect of mask-aware networks for crowd countin ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "III", "Section": "III Our proposed method", "Text": "  density map estimation before elaborating the design of our network, we first briefly introduce the creation of the ground-truth density maps and the related training losse  this paper considers the case that a point-wise annotation is provided for training image  specifically, a dot is annotated within each object-of-interest,   an overview of our proposed metho  it contains three modules: the backbone, the mask prediction branch and the mask-aware density regression branc  where x belong to r2denotes the image coordinate and xi denotes the annotated head locatio  it is also easy to verify that the integral of d over x equals to the total number of object  method overview the overview of the proposed method is shown in figure   the backbone generates the feature representation of the input image and is shared across all the mask-aware density regressors as discussed belo  the mask prediction branch predicts the foreground/background mas  the mask-aware density regressor is the key contribution of this paper and five different designs will be presented in this sectio  this observation inspires us to design a dedicated branch of a neural network to predict the foreground/background mask and we train this branch as a binary segmentation networ  then we can utilize the mask prediction information to guide the overall density estimatio  backbone sub-network the architecture of the backbone sub-network is shown in figure 2 it consists of two part  the first part is a typical multi-layer cnn and the second part is similar to the blocks in the inception network the second part has two identical units ) and its purpose is to encourage the network using information from different scale  this is in a spirit similar to the design of multi-column cnn however, our backbone only adopts multiple scale paths at the second part and uses the separable convolution layers as shown in figure 2 one empirically suggests that the backbone is completely superior to mcnn in terms of the performance the above proposed sub-network is a light-weight strategy which is completely trained from scratc  to further verify the following proposed solution is not specialized for the proposed sub-network, we also employ a pre-trained vgg16 model as our backbone to train our solution followed by the state-ofthe-art model csrnet mask prediction branch the mask prediction branch consists of multiple convolutional layer  specifically, the architecture could be denoted as c-  in our implementation, we can train the mask prediction branch with focal loss as the training objective l  it is calculated by applying the sigmoid function to the output activation of the mask prediction branc  as reported infocal loss is designed ieee transactions on circuits and systems for video technology 4 fi  the architecture of the backbone subnetwor  to tackle the imbalance between foreground and background during trainin in most crowd scenarios, there may exist the imbalance issue  but we find it does not make much difference in our experiment when using the focal loss and traditional binary cross-entropy loss, which will be reported in section i  here, we use focal loss as a general setting for cross-entropy los  note that traditional single branch density map estimation networks still need to determine whether a pixel belongs to the foreground or backgroun  they achieve this capability by using the mse loss while our mask prediction branch utilizes the cross-entropy loss which is generally considered as a better objective for segmentation task  mask-aware density density regressor the ways of incorporating the mask prediction information into the density regression are critical in our proposed metho  in the following part, we consider five possible solution  by definition, the mask indicates which part of density should be nonzero/zer  thus a straightforward way to fuse mask information with the density map estimation is to elementwisely multiply the estimated density map by the estimated mas  our first solution uses this scheme, as shown in figure 3 at the training stage, the training goal of the mask prediction branch to produce the ground-truth mas  thus we directly multiply the density map from the density estimation branch with the ground-truth mask at the training stag  note that this solution essentially requires the density estimation branch only focuses on the estimation of the density in the foreground regio  noted that the gradient of the lr will not pass through the mask prediction branch at the training tim  this suggests that these two branches are essentially trained independently with separated objective  to facilitate the connection between the prediction branch and the density estimation branch, we modify solution fi  five different architectures for the mask-aware density regresso  1 and propose the second solution as shown in figure 3 the difference is that instead of using the ground-truth mask we use the estimated posterior of the foreground to multiply the output of the density estimation branc  in this case, the gradient loss lr can backpropagate to the mask prediction branch, making it jointly adapt with the density estimation branch to produce the final estimatio  in solution 2, the final density prediction is the multiplication of the posterior and the output of the density estimation branc  since the value of the posterior is between 0 and   it is not a perfect mask and could make the estimation sensitive to the confidence of mask predictio  to overcome this drawback, we propose to multiply the predicted binary mask instea  the generation of the mask involves a nondifferentiable hard-thresholding operatio  the schematic illustration of this solution is shown in figure 3 the above two designs are based on the elementwise product operation to merge the information of mask prediction, which can be quite restrictive and potentially sensitive to the mask prediction qualit  here we propose an alternative solution as shown in figure 3 the idea is to use several convolutional layers to map the mask into a feature map which can be further concatenated with the image features to perform the density estimatio  similar to solution 1, we can use the ground-truth mask at the training time and replace it with the predicted ones at the test stag  in this design, we use one channel of ground truth mask to generate a feature map with 256 channels, and then we concatenate the 256 channels from previous layers as the input for the last density map regresso  finally, the architecture of the density map regressor is c-c-  this allows joint training of all the components of the networ  the structure of this solution is shown in figure 3 since this structure learns the incorporation operation through a set of convolutional layers rather than a simple elementwise product, we postulate that it can be less sensitive to the value of posterior estimatio  implementation details our proposed method is trained from scratch based on the pytorch framewor  firstly, we generate the ground truth following from previous method by using a gaussian kerne  we fix the kernel size for all datasets to generate the density map although using geometry-adaptive kernel for different datasets might further improve prediction performanc  for the proposed multi-scale backbone shown in figure 2, we randomly mirror the cropped training images and their associated gt on the fl  what's more, the initialization of the network is drawn from normal distribution with  01 standard deviatio  in order to gain a quicker training speed, the adam optimizer is used to train the network before 11th epoch and then switch to mini-batch stochastic gradient descent the learning rate is initially set to 1e-5 and then is decreased by a factor of   as for using pre-trained vgg16 as backbone, we use original images as training dataset without data augmentation unless otherwise state  in our experiments, we use sgd optimizer train the network for the datasets with different size of images and the rest ones use adam optimize  in addition, we use standard cross-entropy loss for all the experiment ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "IV", "Section": "IV Experiment", "Text": " the purposes of our experiments are threefold: verify if the proposed mask-aware strategies lead to significant improvement over the baseline  identify the most effective mask-aware density estimation solutio  compare our proposed approach against the state-of-the-art method  in our experiments, we use shanghaitech dataset a to achieve the first and the second objectiv  the identified best-performed solution will then be compared against the state-of-the-art on all three dataset  in what follows, we present the evaluation criterion and datasets in our experiment  then we present a detailed analysis of the proposed solutions and identify the most effective on  finally, we compare our method with other state-of-theart method    evaluation metrics we use mean absolute error and mean square error as the evaluation metric  more specifically, pri equals to the sum of values in the estimated density ma  datasets shanghaitech datase  this is a large-scale crowd counting dataset, which contains 1198 images with 330,165 annotated head  ucf cc 50 datase  the ucf cc 50 dataset has only 50 images captured from various perspectives, which is a very challenging counting dataset introduced by we crop 60 patches from each image to train both methods as this dateset is too small, and followed by5-fold crossvalidation is used to evaluate our proposed metho  worldexpo' 1  the worldexpo' 10 is the largest crossscene crowd counting dataset introduced byit ieee transactions on circuits and systems for video technology 6 table i the experimental comparison on the baselines and five proposed methods on shanghai part   solution 1-5 corresponds to the architectures in figure   method mae mse baseline 1 7 83 table ii the experimental comparison on baselines, our proposed method and csrnet on shanghai part   method mae mse baseline 3 7 0 our model csr 6  there are 3980 frames uniformly sampled from the videos sequences, where 3380 frames are used for training and the rest for testin  the number of pedestrians ranges from 1 to 22  different from the above datasets, the region of interest is provided for the images in the datase  during data preprocessing, we mask each frame and its corresponding density map with ro  analysis of the proposed approaches this paper proposes five different designs for the maskaware density regresso  its effectiveness will be examined in this subsectio  we use solution 1-5 to denote the proposed architectures shown in figure   besides these solutions, we also compare our method against three baselines to verify the benefit of introducing mask-aware network desig  the purpose of presenting this baseline is to examine if adding mask branch and mask-aware density regressor can indeed lead to improvemen  we notice that our solution 4 and 5 essentially use deeper networks for density regressio  thus it is fair to compare against a baseline with the comparable dept  the experiment results of the above methods are summarized in table   from the results, we could make the following observation  the proposed solution 1 does not lead to the improved performance over baseline 1 which is comparable to it in terms of the network dept  on the contrary, it worsens the density estimation performanc  in comparison, solution 2 leads to significant performance improvemen  this observation suggests that it is inappropriate to treat the mask prediction and density prediction independentl  it is crucial to train those two tasks jointl  somewhat surprising, the proposed solution 3 has no significant improvemen  we postulate that it is due to the difficulty in optimizing the non-differentiable operator despite the fact that we have already approximated it by the straightthrough estimato  note that solution 4 does not utilize the joint training strategy and the mask-aware density regressor will receive different mask inputs at the training and testing stage respectivel  however, this limit does not prevent the method from gaining performance improvemen  this may suggest that using convolutional layers to combine the mask prediction information is more robust than the elementwise produc  it reduces the mae from 7  this again shows the benefit of joint training and the power of using convolutional layers for information fusio  ablation study to have more insights into our proposed method, we conduct ablation studies of the proposed method on the part one of shanghaitech a datase  the main studies and findings are presented belo  to understand the effect of segmentation branch, we set a new baseline this baseline uses identical network structure as our solution 5, but replaces the target of the mask prediction by density regressio  in this way, the structure is similar to that in this baseline is to verify whether the improvement of the proposed method merely comes from the architecture, or the mask prediction objectiv  recall that the difference between solution 5 and baseline 3 is that the former adopts the mask prediction as the training objectiv  the performance discrepancy of these two methods suggests that using mask information could indeed benefit the density estimatio  the improvement of our method does not solely come from the network structur  in figure 4, we also visualize the estimated density maps of our best-performed method and baseline approache  this may suggest that the benefit of introducing the mask objective is not as simple as providing a better foreground/background separatio  we postulate that the better performance achieved by our approach is due to that its density value estimation becomes more accurate with the guidance of the mask predictio  we also conduct a comparison experiment between binary cross entropy loss and focal los  the result shows that the network with binary cross entropy loss can achieve almost ieee transactions on circuits and systems for video technology 7 fi  a comparison of the density map generated by our best-performed method and two baselines in shanghaitech part   the same performance: mae: 6 08, mse: 10 69 compared with that of the solution   so focal loss in this paper is a general setting for the mask branc  to compare the considered model with different backbones, we construct a new network with the same network structure in the solution 5 on top of a recent state-of-the-art network to distinguish our proposed baseline, we term this network as our method csr for shor  as shown in table ii, we can see our method csr can achieve a promising improvement over the original csrne  to some extent, it indicates that a good baseline with the exploited network structure can boost the performanc  also note that the pretrained model can be easily trained in a simple setting as shown in se  iii compared to our proposed model trained from scratc  we argue that the main benefits derives from the pre-trained vgg 1  compared with csrnet, our proposed baseline is more computationally efficien  we use the test images) in the part a of shanghaitech datase we find that there exist mask errors after the sigmoid layer of the segmentation branch as shown in figure   we randomly selected one feature map after feeding back the predicted mask posterio  this suggests that the network has the capability of separating the error pattern at the mask prediction stage into different feature maps and potentially suppressing the error signal for density estimatio  after the fusion of the two branches, each feature map in the regressor only focuses on a small part of the interest region shown in figure   from the above discussion, we can see that even though there exist mask errors in the mask branch, they will not magnify in the next stag  finally, we get a refined density map as shown in figure   here we argue that the mask error will not magnify in the next stag  as is known, there are different density levels in the crow  so we conduct the comparative experiment with three levels on shanghaitech part a to show the improvement of our metho  we split the density into three types of crowd: low crowdmiddle crowd and high crow  from figure 6, it is easily concluded that the proposed method achieves a promising result on the low and middle level of crow  this is because the proposed segmentation branch has the ability to discriminate background and foregroun  as for high-level crowd, it poses a challenging situation for most method  the texture information of the crowd people are missing in those scenes so it is really hard to exact robust features for each hea  as a result, we can not see clear ieee transactions on circuits and systems for video technology 8 fi  the visualization of feature maps in the mask branc  fi  the average mae of different density levels tested on shanghaitech part   promotion in this interva  as for the our method with pretrained model, we can see that it has a similar improvement in low and middle crowds compared to the model with the proposed backbone while it also achieves a good result in high crow  we conjecture that the pre-trained backbone has more prior knowledge to capture the texture information in high density level crowd than the model trained from scratc  comparison with the state-of-the-art we further compare our best-performed solution against the state-of-the-art results in various dataset  firstly, we make a comparison on the part a and part b of the shanghaitech datase  the results are summarized in table   we can see that our method also achieves competitive results with the state-of-the-art methods and while keeping economic parameter  it is noted that the number of parameters of our proposed method is less than  1 million while the number in the cp-cnn is 6  so our method is more parameter economic and potentially more efficien  as for our method csr surpasses the two methods significantly in this datase  specifically, the mae of part a is 6  in terms of the mse, our proposed method shows significant improvement over csrnet by 13%.  in part b, we also see that our method csr achieves 1 9% in mae and 1  these results show the benefits of our proposed strategy in such a high variance scen  in addition, we report the results of our approach on ucf cc 50 dataset in table   our method obtains a 1 5 improvement in mae over cp-cnn but is worse than csrne  we argue that the main reason lies in that the pretrained model enjoys more prior information compared with our model trained from scratch especially in such a small datase  instead, our model csr armed with pre-trained vgg16 is superior to other models in ma  it should be noted that it shows 2 2 improvement over csrnet in terms of mae and mse, respectivel  our method with light weight achieves a relatively good performance which is on par with the state-of-the-art methods like tdf-net, netvlad, and classical methods mcnn and cccounting but it is inferior to csrnet and cp-cn  besides, our model csr precedes csrnet and cp-cnn while obtaining the first place in s1, s2 and s5 scene ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "V", "Section": "V Conclusion", "Text": "in this paper, we address the crowd counting problem with deep neural network  our main discovery is the benefit ieee transactions on circuits and systems for video technology 9 table iii the performance comparison on the shanghaitech datase  part a part b method mae mse mae mse cc-counting 18 4 our model csr 6 3 table iv the performance comparison on the ucf cc 50 datase  method mae mse cc-counting 46 6 our model csr 24 3 table v the performance comparison on the worldexpo'10 datase  method s1 s2 s3 s4 s5 avg cc-counting  58 our model csr  34 of using a dedicated network branch to predict the foreground/background mask and incorporating mask prediction into density map estimatio  we systematically study five different designs of the mask-aware density estimator and identify the best performed solutio  through the experimental validation, we show that the proposed scheme is effective and achieves the state-of-the-art crowd counting performance on various dataset  acknowledgment the authors would like to thank the editor and the anonymous reviewers for their valuable comments and constructive suggestion ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "References", "Section": "References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": " this paper establishes an information theoretic framework for deep metric based image registration technique  we show an exact equivalence between maximum profile likelihood and minimization of joint entropy, an important early information theoretic registration metho  we further derive deep classifier-based metrics that can be used with iterated maximum likelihood to achieve deep information theoretic registration on patches rather than pixel  this alleviates a major shortcoming of previous information theoretic registration approaches, namely the implicit pixel-wise independence assumption  our proposed approach does not require well-registered training data; this brings previous fully supervised deep metric registration approaches to the realm of weak supervisio  we evaluate our approach on several image registration tasks and show significantly better performance compared to mutual information, specifically when images have substantially different contrast  this work enables general-purpose registration in applications where current methods are not successfu  since these methods do not require any data beyond the images being registered, they can be referred to as unsupervise g, one modality has tissue contrast while the other has boundary contrast here, a metric designed specifically for the application, such as lc2 can perform bette  deep networks have dominated medical imaging and machine vision in the past few years, proving powerful for many application  these networks autoarxiv:190  matically extract intermediate- and high-level representations of image structures that can be effectively used for problem solvin  simonovsky et a  proposed an application specific deep metric based on convolutional neural networks although they showed superior performance compared to mi for deformable registration, they require well-registered training data,  e, the method is fully supervise  balakrishnan et a  presented voxelmorph, an unsupervised approach, where registration is modeled as a parametric function using cnn  during training, the model parameters are optimized by maximizing image similarity which does not require ground truth registratio  however, the similarity metric in voxelmorph is designed for intra-modality registratio  in another work, hu et a  described a weakly supervised approach where sparse corresponding landmarks are used to summarize the underlying dense deformation, with good results on the registration of prostate mr and ultrasoun  we propose a new approach, deep information theoretic registrationthat uses iterated maximum likelihood to train effective application specific deep image metric  we show that this approach is strongly related to mi, but on patches, not pixel  this alleviates one of the main limitations of the mi approach, namely the strong implicit independence assumption on pixels or voxel  in addition, we show that in our method, neither landmarks nor well-registered training data are required for learning the deep metri  this is an important issue in some applications   the remainder of the paper is structured as follows: section 2 describes the relationship between maximum likelihood and information theoretic registration method  in section 3, we describe achieving ml registration using classificatio  next we demonstrate that the need for accurately registered training data is relaxed by using iml with deep classifier technolog  2 maximum likelihood registration before introducing the relation between deep image metrics and mi registration, we provide a brief history of ml and mi based registration method  given that ml is older than it, it is perhaps interesting that ml registration appeared after mi, in", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Deep", "Section": "Deep Information Theoretic Registration", "Text": " we also assume that the images are collections of conditionally independent features, ui, v  pr is intended as a joint distribution on features when registere  in the present work, we suppress jacobian effects, effectively assuming that volume is approximately preserve  the parameters of this model could be estimated from training data consisting of registered image  this approach was used by leventon et a  in that work, the features were image pixels or voxels and the joint distribution was categorica  the model was estimated by histogramming from pairs of registered image  this need for preregistered training data is a drawback in comparison to m  if we view the model parameters as nuisance parameters, this approach has been called maximum profile likelihood here bayesians may favor averaging over the nuisance parameter  the two approaches are compared in in the context of registration, the marginalization approach was described by zollei et a  we use the profile likelihood approach in the remainder of this pape  let the joint image intensities be discretized into bins that have a single index, calculated by the function:   note that the expression 4 sedghi et a  in brackets is the objective function for maximum likelihood estimation of the parameters of the categorical distributio  thus the optimization over beta has devolved exactly to minimization of joint entrop  optimization by coordinate ascent in cases where the inner optimization of e  e  4 amounts to estimating the transformation parameters, given the model parameter estimat  e  5 re-estimates the model parameters from the registered image  we refer to this approach as iterated maximum likelihoodit was used in timoner's phd thesis in this case, iml converges asymptotically to the minimum joint entropy of the feature dat  similar relationships among entropy and ml were described in 3 maximum likelihood registration by classifier in this section, we show how to use image classification to generate an agreement metric for solving registration problem  here a classifier is trained to distinguish between registered and unregistered patche  we use training data in form of {.  we construct a discriminative classifier for this problem, p f can be any probabilistic classifier including a deep networ  we use ml to train the conditional model next, we construct a joint distribution on registered patches that is based on the classifie  then using e  6 sedghi et a  taking log, and using e  this ml objective function is simply the sum of the pre-sigmoid network responses over the corresponding patches in the pair of images being registered; it is our approach to dm 1 unsupervised registration by iterated maximum likelihood while dmr has proven useful, it is assumed that well-registered training data is neede  to relax this requirement, we use the iml approach of eqn  e  e  13 corresponds to retraining the network using patches that are offset by the most recently estimated transformation parameter  the iteration is started with e  13 on the original roughly registered training dat  subsequently, the method alternates between re-aligning the data and re-estimating the deep network parameter  in the experimental portion of the paper, three iterations are applied in trainin  we envision that this iterative training needs to happen only once per application typ  after training, the model parameters may be fixed and used for subsequent registrations using e  4 experimental evaluation we perform several experiments to study the effectiveness of iml approach for several image registration task  we introduce dithering as a tool to help demonstrate that the proposed approach does not require perfectly aligned training data to learn an accurate deep metri  to generate unregistered datasets, we perturb the moving images with a random transformatio  we select 100 landmarks in the image space to calculate and report mean fiducial registration error following registratio  in all of our experiments, we use powell's method for optimization of the transformation parameters with the learned deep metric as a cost functio 1 data we carried out experiments using the ixi brain development dataset which contains aligned t1-t2 image pairs from healthy subject  in our experiments, we use 60 subjects for training and another 60 subjects for validatio  overall, 1 million patches are generated for each experimen  dithering training a deep metric on unregistered dataset can cause bias in the response function depending on the distribution of the mis-registration in the dat  data augmentation with rotation and flipping can help reduce this bias at a cost of introducing additional variance and peaks in the response functio  a smooth, single peak response function is preferred for effective optimization and learning of the transformation parameter  therefore, we propose dithering as an effective alternative approach to merge the multiple modes of the response functio 2 network architecture and training for learning a similarity metric for image registration, we use 3d cnns and train the networks as discriminative classifiers to distinguish between registered and unregistered image patches in our experiment  the architecture of our network is inspired by the 2-channel network of zagouruko et a  where patches from the fixed and moving images are the input channels of the cn  cnn training and registration we train our model by minimizing crossentropy los 005 is used to optimize the networ  following training, we use the sum of the pre-sigmoid network responses over the patches in the pair of images being registered as our cost function for optimization to update the transformation parameter  the process of training and 8 sedghi et a  transformation update may be iterated if necessary to improve registration of the training dat  in addition, we evaluate the contribution of dithering to registration by comparing the performance of iml with and without ditherin 15} rad for translation and rotation, respectivel  downsampling is needed because large initial mis-registrations cannot be captured by the limited patch siz  furthermore, we experiment with affne registration to show the capability of iml in a more general registration proble 05} and ush{-  we follow the 3 stage iml model proposed earlier to learn the deep metric  we also characterize the deep metric learned from the roughly registered data as a function of translation for a test datafollowing each iteration of iml, to illustrate the nature of the objective function  this is meant to simulate a situation that could occur if there were a consistent difference in setup between non-simultaneous scanning of different image modalitie  in another experiment, to compare dithering and smoothing for broadening the response function, we perform an experiment in which we learn a deep metric on the dithered and smoothed data separatel  experiment 3: edge registration we test our proposed iml approach in a more diffcult multi-modality situatio  more specifically, we experiment with registration of edge maps of the t1 images to intensities in t2 image  deep information theoretic registration 9 fi  box plots of mean fre for rigid and affne registration between t1 and t2 image  5 results and discussion fi  1 shows box plots of mean fre for different experiments performed for rigid and affne registratio  each box represents the interquartile range, and the horizontal line is the median of the distribution of mean fr  as seen in fi  1a, iml with dithering performs statistically significantly better than iml without ditheringand mi moreover, fi  1b demonstrates the effectiveness of iml with dithering for affne registration compared to mi for both registration tasks, iml without dithering improves the initial registration error to some extent which further demonstrates the added value of dithering for learning deep metrics from unregistered dataset  fi  2 delineates the deep metric on pairs of images as a function of translation in each iteration of im  this is likely due to the increased level of alignment of the training data as the iterations procee  fi  3 shows the results of experiment 2 for exploring the effect of dithering on learning deep image metric  in fi  3a, it is clear that there is a bias in the response function due to the systematic shift in the training dat  fi  3a also shows the effect of augmentation by rotation and flipping in reducing the bia  moreover, we can see that by applying dithering to the moving image before cropping the patches, we are able to merge the peaks in the deep metri  we evaluate the impact of dithering versus smoothing on the broadness of the deep metric in fi  3  it is interesting to note that smoothing has not significantly broadened the response functio  we believe that deep networks 10 sedghi et a  fi  response functions for each iteration of iml for rigid registration plotted as a function of translation for a pair of registered fixed and moving image  shading illustrates full-width half ma  are capable of effectively learning the correspondence between the smoothed patches; therefore, they can generate a sharp response similar to the response function of the deep metric learned from the original dat  we see on the left that dithering is more effective at broadening the respons  fi b shows the mean fre using different method  the figure clearly demonstrates the superior performance of ditr compared to m  in the experiments presented, several iterations of iml are needed to learn the best deep metrics, but for registering the test data we only need the final trained networ  we believe this is possible due to the broad capture range of the learned deep metric we further showed that iml with classifier-based metrics is strongly related to mutual information on patches we expect that ditr will enable new solutions for applications where the standard mi assumption of pixel- or voxel-wise independence is limitin  we demonstrated the effectiveness of our proposed method in rigid and affne registration for multi-modal dat  in all experiments, ditr outperformed standard mi statistically significantl  on the edge-to-image registration experiment, mi effectively failed, but ditr successfully registered the image  our work focused on the analysis of registration objective functions rather than transformation modeling and optimization methods; we expect the technology to be effective in more general settings, as it is a generalization of similar dmr methods that have been shown to be successful for inter-subject deformable registration deep information theoretic registration 11 fi  characteristics of the deep metric learned from different training dat  impact of dithering and smoothing on the deep metric response functio  p41eb015898, and nih nibib grant n  p41eb015902 neuroimage analysis cente ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "edu, ajayjain@mi edu abstract serving deep neural networks in latency critical interactive settings often requires gpu acceleratio  however, the small batch sizes typical in online inference results in poor gpu utilization, a potential performance gap which gpu resource sharing can addres  in this paper, we explore several techniques to leverage both temporal and spatial multiplexing to improve gpu utilization for deep learning inference workload  we evaluate the performance trade-offs of each approach with respect to resource-efficiency, latency predictability, and isolation when compared with conventional batched inferenc  our experimental analysis suggests up to a 5x potential for improved utilization through the exploration of more advanced spatial and temporal multiplexing strategie  our preliminary prototype of a dynamic space-time scheduler demonstrates a  23x floating-point throughput increase over space-only multiplexing and a  73x increase over time-only multiplexing for convolutions, while also providing better isolation and latency predictabilit ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "gpus are essential to deep learnin  nvidia's datacenter-class v100 gpu, for example, packs more than 120 tflop/s of half-precision matrix multiply-and-accumulate performance designed specifically for deep learning workload  while there are numerous specialized inference processors, the widespread availability of gpus and their support for general deep learning models renders gpus indispensable for inferenc  dnn training is computationally expensive but relatively infrequent, while online inference needs to scale to billions of queries per day and is rapidly outpacing training in datacenters amazon recently announced that roughly 90% of the machine learning computation is spent on inference key metrics for revenue-critical applications can dramatically suffer with an increased application latency in spite of tight end-to-end latency budgetswe note an alarming trend that inference latency on a cpu has been on a rise1s cpu inference latenc  given that increases in interactive query latencies leads to losses in revenue, cpus cannot support today's interactive model serving workloads which leaves gpus an obvious favorit  while training workloads continue to scale and can often easily saturate modern gpus, ml inference has distinctly different performance requirements that often result in poor gpu utilizatio  in contrast to throughput-oriented model training, revenue-critical inference workloads must meet preprin  work in progres  arxiv:190  most models fail to meet the 300ms latency slo on a cp  the largest batch size for resnet-50 within the slo is 26, but only achieves an average of 28% of peak v100 fp32 throughpu  latency objectives with queries often arriving stochasticall  in practice, online inference queries often cannot realize the high levels of parallelism that offline iterative minibatch training achieves; lower parallelism leads to poor gpu utilization in practic  small batch sizes are an unfortunate reality for online inference with sloswhich leads to low utilizatio  the issue raised by small inference batch sizes is exacerbated as model complexity grows over time, pushing gpu inference latencies to approach interactive slos from below given that inference workloads must run continuously and respond to highly variable demand, capacity must be provisioned for demand peaks which lowers gpu utilization even furthe  the current practice of exclusive access to a gpu cannot scale due to the low utilization on current hardwar  we notice that small batch sizes can lead the gpu to low utilization under 15%.  a common approach to improving utilization of parallel hardware, under stochastic query load, is to leverage multi-tenanc  by sharing a gpu across multiple prediction workloads we can potentially leverage workload level parallelism and achieve statistical multiplexin  however, leveraging multitenancy on a gpu remains an open research proble 2, is not always tru  second, it must be resourceefficien  to address this, we explore a number of techniques for sharing a gpu among a set of execution kernels, each with their drawback  third, a measure of performance-isolation is needed, which is typically achieved through fair resource allocatio  current approaches for sharing a gpu for dnn inference either multiplex the gpu across space or across tim  section 3 evaluates current approaches against the three criteria established abov  we argue that only multiplexing across space or time leads to a compromise on one of these criteria instead, we propose scheduling across space and time for gpu inference in section   by packing multiple execution kernels across disjoint dnn graphs with dynamic query batching, we show potential for a multi-tenancy solution that is resource-efficient while providing isolation and predictabilit ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Application Model: A Managed Cloud Inference Service", "Text": "consider a cloud-based managed service for deploying ml models for online inference, similar to amazon aws sagemaker or google cloud ml engine users may develop and upload their trained machine learning models to this servic  the service then deploys the model onto one-or-more 2 replicas, which each may use cpus and gpu  the service and the users agree on some service level objectivesuch as a measure of tail-latency for model inferenc  we simplify this model in order to isolate interference effects due to multi-tenant executio  first, all models running on a single gpu are restricted to the same architecture this separates the impact of heterogeneous model architectures from multi-tenanc  second, request queues are always saturated, thereby isolating model service latency from request queuing latenc  it is worth noting that addressing both model heterogeneity and queuing latency is a key focus of future wor ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Investigating limitations of space-only and time-only multiplexing", "Text": "using this simplified model of a managed cloud inference service, we outline three leading approaches to model inference today: exclusive acces  each model has an exclusive gp  amazon aws sagemaker, google cloud ml engine, clipper and tensorflow serving all utilize this approac  in this approach, inference is done in batche  when the network is performing forward propagation, new queries must wait in a queue until one forward pass is complete  time multiplexin  an on-device scheduler enables interleaved execution of multiple cuda contexts at onc  this approach is common when multiple processes run concurrently using the same gp  this approach relies on the kernel to time-multiplex processes and gpu to swap contexts when different processes compete for the same resourc  spatial multiplexin  kernel execution can overlap by utilizing nvidia hyper-q cuda streams and nvidia multi process service utilize multiple hardware queues to enable spatial sharing of the gp  the cuda streams api is used by modelbatch and nvidia tensorrt amd's mxgpu is another approach for spatial multiplexing, not considered in this wor  implicit spatial multiplexing with mps: nvidia mps allows multiple processes to run on the device at the same time by allocating them different cuda stream  explicit spatial multiplexing with cuda streams: with this method we directly interact with multiple cuda streams inside a single processe ", "Subsections": [{"Section_Num": "3_1", "Section": "3.1 Experimental Setup", "Text": "we evaluate these three virtualization methods on two image classification neural network architectures: mobilenet v2 and resnet-50 these two models are popular choices for low-compute and high-accuracy classification applications respectivel  exclusive access is tested with a single model executing batched queries on a private gp  although we cannot use this approach with multiple models on a single gpu, this test represents a single-tenant lower bound on latency and an ideal best-case for performanc  time multiplexing is tested by running each model in a separate cuda context and utilizing a software scheduler to interleave executio  this provides memory safety and some basic level of isolation between tenant  spatial multiplexing is tested by using the nvidia mps server to partition model queries for different models across a pool of cuda stream  all experiments use p 2xlarge or p 8xlarge instances on amazon aw  these instances have direct access to nvidia v100 datacenter-class gpus with up to 14 tflop/s of single-precision floating-point throughpu  we do not test tensor core sharing in our experiment  we compare three approaches to gpu multi-tenanc  exclusive access provides fast and predictable latencies at a high cos  time multiplexing dramatically increases inference latency as sharing increase  spatial multiplexing through nvidia mps better manages latency by sharing resource ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3_2", "Section": "3.2 Preliminary results", "Text": "we report results from our benchmark in figure   batched exclusive access devotes the entire gpu to a single model, unlike the other two approache  this is an extremely aggressive baseline and represents the ideal performance a model would achieve if it were the only tenant and throughput is our only objectiv  however, if we also wanted to minimize latency we would likely use much smaller batch size  overall, time-only multiplexing suffers a geometric mean  6x slowdown compared to exclusive access while space-only multiplexing only endures a  2x slowdown, across the experiments in figure   ultimately, no single solution wins on all criteria we established in section   as we add replicas to a gpu running 10 multi-tenant models, we observe unpredictability, which we suspect is caused by the on-device schedule  explicit spatial multiplexing is no  we believe this is a great model for users who have high enough request rates during inference to achieve good utilization on a gpu and are able to batch requests to arrive simultaneously into one forward pass of the networ  time multiplexing can actually accomplish multi-tenancy with good isolation and predictability, but at the cost of degraded throughput and high latencie  the main drawback to this approach is that it cannot take advantage of parallel execution of the kernels, since the gpu only allows one running cuda context at a tim  this approach instead interleaves processes resulting in slightly improved resource-efficiency; although it still suffers from poor utilization during each schedule quantum, explaining the linear-slowdown as the number of replicas grow  poor latency scalability makes time multiplexing alone an inadequate solution for interactive inference query servin  spatial multiplexing does improve on poor utilization and achieves much better resourceefficienc  however, we find there is poor predictability and isolation in this setu  it seems the spatial multiplexing approach is extremely sensitive to the choice of the number of tenant  each tenant appears to have fairly consistent behaviour once the model run  furthermore, the unpredictability and discrepancy between latencies across different processes is exacerbated when an odd number of processes runs concurrently with mps enable  gpu single-tenancy leads to poor utilization and high costs figure 3 and figure 5 demonstrate inherent scalability limitations to common space-only and time-only multiplexing strategie  figure 4 details unpredictable latency as tenants are added to a gp  in light of these limitations, we propose a promising new approach we call dynamic space-time schedulin  the key idea is trade-off space and time multiplexing in order to efficiently utilize the gpu while preserving isolation and predictabilit  we preserve predictability and isolation during virtualization by monitoring inference latencies per-kerne  this allows reallocating resources between tenants on-the-fl  moreover, we notice that cuda stream scheduling anomalies typically only create a few stragglers, so we can simply evict degraded workers without significantly impacting total system throughpu  we are further investigating this approach in ongoing wor  our approach also dramatically improves resource-efficiency on the gpu - we observe a  71 overall geometric mean speedup in throughput compared to time-only multiplexing and a  23 speedup compared to space-only multiplexing, as shown in figure   space-time scheduling merges many concurrent small kernels from disjoint dnn graphs into a small set of larger super-kernels that together fill the gp  the super-kernel avoids the scheduling penalty associated with current space-only multiplexing approache  as interactive inference queries arrive stochastically, we cannot easily precompute super-kernels ahead-of-tim  instead, the space-time scheduler must dynamically schedule kernels as they arriv  we are investigating the design of a more general dynamic scheduler, but we notice that overheads gradually decrease if we cache super-kernels as workloads stabilize over tim ar b br c  a2 b2 c2 x = we illustrate kernel multiplexing methods for r sgemms scheduled on the same gp  outer boxes depict a single cuda kernel invocatio  we evaluated the matrix multiply throughputs on an intermediate resnet-18 convolution kerne  auto-tunin  however, our approach focuses on optimizing the performance of many disjoint graph  our approach is a dynamic alternative to that developed in guevara et a 3x speedup on a gaussian elimination algorith  while the work in guevara et a  focuses on manually merging kernels at the cuda block level, our approach discusses a scalable procedure to batch large numbers of kernels that execute similar matrix multiplication routines together dynamically, as well as interleave cuda stream  while ours is a matrix-math targeted approach, we demonstrate that it has high scalability on multiple real-world neural network tasks our approach extends these tools and is complementary to the existing ecosyste  specifically, we examine the single precision floating-point general matrix multiply kernel figure 7 demonstrates that spatial multiplexing via hyper-q/cuda stream usage can improve throughput as compared to timeslicing approaches to gpu sharin  however, the average throughput is still substantially lower than the single-precision throughput offered by the v100 instead of maintaining separate kernel streams which the device can schedule at a fine granularity, we investigate a software-based scheduler that batches kernels across model  by batching kernels across many models into a single super-kernel, a single gpu invocation would have an opportunity to saturate all resources on the gpu for its timeslic  these models are have different weights and inputs, as is likely in a multi-tenancy settin  the batched super-kernel is more efficient than many smaller kernel invocations, and also better spatially multiplexes the gp  this also allows better predictability of latency as the dynamic kernel scheduler can selectively batch kernels and determine when to execute workloads based on per-model slo  figure 7 demonstrates substantially better throughput scaling via batching than via time-only and spatial-only multiplexing approache  r sgemm kernel evaluations are queued on a nvidia v100 gp  similar floating-point throughput improvements are observed for other intermediate layers, as well as for dense matrix-vector multiplications found in rnns and square matrix-matrix multiplications this matrix multiply super-kernel is implemented in the nvidia cublas operation cublassgemmbatche  it requires all sub-kernel problem dimensions be the sam  however, the magma blas library implements a variable-sized batched sgemm that would allow for different kernels to be batche  for all compared approaches, data is preallocated on the device as in a real-world dnn inference settin ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 A new hope? Dynamic space-time scheduling", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Conclusion and Future Directions", "Text": "in this work, we evaluated spatial and temporal multiplexing techniques to support inference across multiple models on a single gp  we first considered standard approaches utilized by popular dnn frameworks and gpu vendors like nvidi  while these techniques improve utilization, they increase latency and variability in prediction performance in benchmark  neither space-only nor time-only mutliplexing techniques could achieve high resource-efficiency, predictable latencies and isolatio  we observe a large performance gap between batch-level parallelism and space-only multiplexing, suggesting substantial opportunities to improve utilizatio  we propose a dynamic space-and-time scheduler that addresses all three aforementioned criteri  software-level fusion of kernel operators across multiple models and inputs presents a promising approach to online inference schedulin  as an early evaluation of this approach, we studied roof-line performance available via sgemm fusion of all queued problems, which offers throughputs that scale well with the number of gpu tenants or model replica  we demonstrate a > 3x speedup compared to the prior state-of-the-art in online inference multitenanc  we believe this work points towards a new approach to efficient multitenant execution of deep neural networks through intelligent inter-model fused kernel schedulin ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190 00042v1 31 dec 2018 on a bounded remainder set for a digital kronecker sequence mordechay   levin abstract let x0, x1, key words: bounded remainder set, digital kronecker sequenc  2010 mathematics subject classificatio  primary 11k3 ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction ", "Text": " discrepanc  let x0, x1, see and for the results on this conjectur  for examples of   digital kronecker sequenc  let fb be the set of all polynomials over fb, and let fb) be the field of formal laurent serie  inniederreiter introduced a non-archimedean analogue of the classical kronecker sequence  this sequence is sometimes called a digital kronecker sequence in analogy to classical kronecker sequences, inthe following theorem has been proven theorem   a digital kronecker sequence s is uniformly distributed in bounded remainder se  let x0, x1, be a sequence of point in ).  the sets of bounded remainder for the classical s-dimensional kronecker sequence were studied by lev and grepstad the case of halton's se3 quence was studied by hellekalek for references to others investigations on bounded remainder set see in this paper, we prove theore  the setwe proved similar results for digital sequences described in note that according to larcher's conjecturethe assertion of the theorem is true for all digital -sequences in base  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Notations.", "Text": " now we construct bm points in ).  a digital kronecker sequence in base b can be expressed as some digital -sequence in base  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Proof", "Text": " proo  it is easy to verify that 1 similarly, we derive 1 similarly, we obtain from and that   hence lemma 2 is prove  hence lemma 3 is prove  proo |2 for some finite set r  then is true for rm = m  proo  let fromwe obtain {. |2 bm-1 x|  therefore lemma 5 is prove  proo  16 suppose that is not tru  hence |za1 -za2| < b-b-  we have a contradictio  therefore is tru  now we consider assertion thus lemma 6 is prove  applying -we have corollar  proo  hence there exists a nontrivial solution of thus lemma 7 is prove  propositio |  suppose that card =   we have a contradictio  now let card =   we have a contradictio  bythe proposition is prove  completion of the proof of the theore  therefore the sequence s is weakly admissibl  applying the proposition, we get the assertion of the theore ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "ca abstract in this research, we determine the structure of -free graph  this result also reveals facts about the structure of triangle-free graphs, which might be of independent interest", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "the structure of graphs with some given forbidden subgraphs is well studied, and quickly gained several applications in graph theory and in theoretical computer scienc  for some of the known results in this field seeand00043v1 31 dec 2018 in this paper, we study the structure of -free graph  a graph is a claw if it is isomorphic to k1,3, and a bull if it can be obtained from a triangle by adding two pendant edges at two different vertices notatio  the main result in this paper is as follows: theorem   since the complement of a bull is still a bull, the complement of triangle free graphs are also -fre  as a corollary of theorem  1, this two classes are almost the sam  2 in the following three sections we consider three sub-classes of -free graphs based on the length of a longest cycl  finally we combining the result of these sections to show theorem   we will use standard definitions and notation for graphs as given in notatio  indeed, we shall use lemma  3 to show that the sub-class of -free graphs under consideration in section 4 consists of expansions of path  then n contains two consecutive vertices of   proo  therefore, according to lemma   proo  by lemma   proof of claim   g] would contain a cla  hence, proceeding by the way of contradiction and in light of lemma   but then g would be a bull, a contradictio  then xy belong to   proof of claim   according to lemma   let g be a -free grap  proo  according to lemmas   but then g would be a bull, a contradictio  hence, by the pigeonhole principle and lemmas   then g will be a bul  in which case g would be a bul  but then g would be a bull, a contradictio  as such, lemmas  2 imply yv3 belong to   hence, likewise case   but then g would be a bull, a contradictio  hence, by the pigeonhole principle and lemmas   but then we must have yv5 belong to e for otherwise g would be a bul  but then g would be a bull, a contradictio  figure 12: lemma   proo  as such, without loss of generality we may assume w2w3 belong to   but then g will be a bull, a contradictio  the following proposition will be used in multiple occasions in the proof of lemma  vk is an induced path in   then uvi belong to e for each i belong to the following lemma is the main result of this subsectio  then:   g is an expansion of a pat  according to lemma  vk be a geodesic path between the  proof of claim  i -1} choose a vertex wj belong to n wi is an induced pat 2 it follows that every uwj is an edge of   this establishes the second part of the clai  proof of claim   let q be a geodesic path in g from wk+1 to v  uv0 must be the last edge of   suppose the vertex of q preceding u is in ni and call it w wi contains v  but the latter contradicts claim   hence, this case does not happe  q will be of the form wk+1wk, proof of claim   let r be the set of paths of the shortest length from a vertex in w to v  let the latter be wi belong to n  the  choice of r as a path of the shortest length in   but then g will be a bull, a contradictio  proof of claim   proof of claim   proof of claim   let q be the set of paths of the shortest length from u to v  by and the graph g will be a bul  u is a cliqu  proof of claim   hence, u is a cliqu  claim 7 let i be a largest independent set in   hence, by lemma   we shall show that every vertex in u is either adjacent to every vertex in n2 or non-adjacent to every vertex in n  then g will be a bull, a contradictio  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 The case (G) 6.", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 The case (G) {4,5}.", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 The case (G)3 with (G)3.", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Proof of Theorem ??", "Text": "1 proof of theorem   it is easy to check that an expansion of a path, that of a cycle, and the complement of a triangle-free graph are all -fre  conversely, by lemmas  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190 00044v1 31 dec 2018 isometries of asymptotically conical shrinking ricci solitons brett kotschwar and lu wang abstrac  we show that if a shrinking soliton is asymptotic to a cone along an end then the isometry group of the cross-section of the cone embeds in the isometry group of the end of the shrinke  we also provide suffcient conditions for the isometries of the end to extend to the entire shrinke ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1. Introduction", "Text": "shrinking ricci solitons model the geometry of solutions to the ricci flow in the vicinity of a developing singularit  all known complete noncompact shrinking ricci solitons which are not locally reducible as a product are smoothly asymptotic to a cone at infinity in four dimensions, there is growing evidence to suggest that the asymptotically conical shrinkers are the only nontrivial complete noncompact example  their classification is vital to the understanding of the long-term behavior of the equation and to potential future topological application  this reduces the classification to that of their possible asymptotic cone  a reasonable first step toward an understanding of what cones can occur is to identify the geometric features which an asymptotically conical shrinker and its asymptotic cone must share in commo  the purpose of this note is to detail the application of the uniqueness theorem in to the relationship between the isometry group of cone and shrinke  a direct application of that theorem implies that any symmetry of the asymptotic cone must, at least, be reflected in a symmetry of the shrinker on some neighborhood of infinit  the second author was partially supported by the nsf grants dms-1811144 and dms-1834824, an alfred   sloan research fellowship, the funding from the wisconsin alumni research foundation, and a vilas early career investigator awar  we will need to introduce some notation to state our main resul  here, by end, we mean an unbounded connected component of the complement of a compact se  in dimension four, the same is true only assuming the scalar curvature tends to zero at infinit  the uniqueness of the asymptotic cone is discussed in the first nontrivial examples of complete asymptotically conical shrinkers were exhibited by feldman-ilmanen-knopf here, the conclusion does not require that be either complete or connected at infinit  the answer is yes at least when the fundamental group of v surjects onto that of   this is a straightforward variation on the classical continuation argument for local isometries on simply-connected realanalytic manifolds; see theorem   see section 4 of for detail  isometries of asymptotically conical shrinkers 3 corollary   it is an interesting question whether every complete simply-connected shrinker with more than one end must split as a produc  acknowledgemen  we thank ronan conlon for his interest and for useful discussions regarding the application of to the isometries of the en ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2. The self-similar solution on an asymptotically conical end", "Text": " this solution transforms theorem  1 into a question of the preservation of isometries along the flow proo  the other assertions are part of proposition  1 allows us to work with a soliton structure which flows directly to the cone under the ricci flo  we will say that a shrinker satisfying properties - of proposition  1 is dynamically asymptotic to the correspondence expressed in theorem  1 can be improved for shrinkers that have been normalized in this sens  suppose the shrinking soliton is dynamically asymptotic to then isom = iso  here, the identity asserted between isom and isom is an equality of sets ).  proof of theorem   by proposition   by theorem   in this paper, we are only interested in the isometries of the cone which are induced by those on the crosssectio  these isometries are precisely those which fix the vertex of the con  bythe only shrinker which is asymptotic to such a cone is itself the gaussian shrinke ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3. The analytic structure associated to a solution to the Ricci flow", "Text": "we next wish to show that the end of an asymptotically conical shrinker and the cone to which it is dynamically asymptotic are real-analytic relative to a common real-analytic structur  this is particularly convenient to address from within the isometries of asymptotically conical shrinkers 5 parabolic framework established in the previous sectio  this framework realizes both the end of the soliton and the end of the cone as time-slices of the same smooth ricci flow, and reduces the problem to a matter of sharpening the usual statement of instantaneous real-analyticity for that equatio  most of this section concerns general smooth solutions to the ricci flow and is independent of the rest of the pape  note that the solutions need not be complete, an aspect important for our application belo  for complete solutions of bounded curvature, a stronger space-time analyticity result is proven in see also in the next section, we will use the following application of the above theorem to the ricci flow associated to a shrinker on an asymptotically conical en  suppose is dynamically asymptotic to then and are real-analytic relative to a common real-analytic structur  in particular, the cross-section of the asymptotic cone of a shrinker is realanalyti  proof of corollary  2 shows that there are many smooth compact manifolds which cannot arise as the cross-section of the asymptotic cone of a shrinke  the real-analyticity of solitons was first proven in proof of theorem   thus the two charts must belong to the same real-analytic structur  this claim follows from the local estimate in and two straightforward combinatorial estimates in section 8 of by theorem   r  the estimate controls the evolution of the derivatives of g on v1, and implies an estimate on these derivatives of the same form", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4. The isometry group near spatial infinity", "Text": "we now resume our discussion of shrinking solitons and revert to our previous notational convention  proo  suppose that a >   8 brett kotschwar and lu wang proposition   proo  then is also dynamically asymptotic to by theorem   we believe the argument could be reworked to give an effective proof of the existence of t  proo  by theorem   since ) is incomplete, we cannot appeal directly to the backward uniqueness theorem in however, the situation here is more elementary to begin wit  proo  suppose then that g is not ricci-fla  by lemma   but then proposition  2 from the pieces abov  proof of theorem   as we have observed in the proof of theorem   by proposition 4", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5. Extension of isometries in the complete case", "Text": " here we show that the hypothesis of simple-connectivity can be exchanged for the assumption that the fundamental group of u surjects onto that of   first, we recall some definitions from the following statement is contained in propositions 1 3-1  the following theorem gives a suffcient condition for a local isometry on u to extend a local isometry on   the above proposition guarantees that the continuations along the latter paths agre  proo  here i = by proposition   so, by proposition  2 is then an easy consequence of the above theore  proof of theorem   by theorem 1", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "References", "Section": "References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190  *partially supported by the nsf grant dms-1645673 1 key word  2010 mathematics subject classificatio ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": " system is a simplified version of the chemotaxis system proposed by keller and segel in their works chemotaxis models describe the oriented movements of biological cells and organisms in response to chemical gradient which they may produce themselves over tim  these mathematical models play very important roles in a wide range of biological phenomena and accordingly a considerable literature is concerned with its mathematical analysi  the reader is referred to for some detailed introduction into the mathematics of keller-segel model  a famous application of chemotaxis models is to describe the life cycle of dictyostelium discoideu  as described in  discoideum lives in the soil and feeds on bacteria and other microorganisms that are taken up by phagocytosi  during the vegetative growth stage, the single-celled amoebae divide by simple mitotic division  in times of starvation, a developmental program is initiated, which is accompanied by major changes in gene expressio  as a result, cells begin to signal each other by secreting camp and to aggregate by chemotaxis toward this chemoattractan  the resulting multicellular aggregate contains up to a few hundred thousand cells and undergoes further differentiation and morphogenetic change  finally a fruiting body is formed which consists of two main cell types, spore and stalk cell  the stalk consists of dead vacuolated cells, while the spore cells are resistant to extreme temperatures or drough  more favorable environmental conditions enable the hatching of new amoebae from the spore  the aggregation of thousands of individual cells that build a multicellular organism in this peculiar life cycle, has intrigued scientists for decade  the study of the dynamics of solutions to has attracted a number of researchers over the past few year  finite time blow-up phenomena is among important dynamical issues about it is also shown that some radial solutions to in plane collapse into a persistent dirac-type singularity in the sense that a globally defined measure-valued solution exists which has a singular part beyond some finite time and asymptotically approaches a dirac measure we refer the reader to and the references therein for more insights in the studies of chemotaxis model  in fact in this case, it is known that when the space dimension is equal to one or two, solutions to with initial functions in a space of certain integrable functions are defined for all tim  and it is enough for the self limitation coeffcient b to be big enough comparing to the chemotaxis sensitivity coeffcient to prevent finite time blow-up, see spatial spreading dynamics is another important dynamical issue about the following results are well known about the spatial spreading dynamics of recently, the first two authors of the current paper studied the spatial spreading dynamics of and obtained several fundamental result  some lower and upper bounds for the propagation speeds of solutions with compactly supported initial functions were derived, and some lower bound for the speeds of traveling wave solutions was also derive  however, several important biological and mathematical problems remain ope  for example, whether the presence of the chemical substance in slows down or speeds up the propagation of mobile species, and whether there is a minimal wave speed of 3 in the rest of this introduction, we state the main results on the spatial spreading dynamics of", "Subsections": [{"Section_Num": "1_1", "Section": "1.1 Statement of the main results.", "Text": "in this subsection, we state the main results of the pape  in order to do so, we first introduce some notations and definition  in this work we shall only focus on nonnegative classical solutions of since both functions u and v represent density function  we recall the following result proved in to state our main result on the spreading speeds of solutions of with nonempty and compact supported initial functions, we first introduce the concept of spreading speed  let be the following standing assumptio  we prove the following theorem on the upper and lower bounds of the spreading speed interval of hence theorem  1 is an improvement of the results contained in on the lower and upper bounds for the spreading speeds of solutions with nonempty compactly supported initial  the results in theorem  1 are ne 1 is proved using the similar arguments as those in1 is proved in the results in theorem  1 for the general case are ne  the techniques developed to prove the above results can be used to study the spreading speeds of solutions with front like initial  we can establish the following resul  we also discuss the spreading properties of solutions of with initial functions satisfying some exponential decay property at infinit  in this direction, we have the following resul  let u0 be as in the spreading results established in theorem  3 are ne  to state our main results on traveling wave solutions, we first introduce the concept of traveling wave solution  our main results on the existence of traveling wave solutions of read as follow 4 improves the results obtained in hence theorem  4 is an improvement of the results obtained in", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1_2", "Section": "1.2 Discussions", "Text": "in this subsection, we give some discussions on our main result  chemotaxis models describe the oriented movements of biological cells and organisms in response to chemical gradien  suppose that u is the population density of certain biological cells and v is the density of some chemical substanc  8 it is known that the asymptotic dynamics of is completely determined by the logistic term   the dynamics of is recalled in the abov  many authors have been studying possible dynamical scenarios induced from the chemotaxis in various chemotaxis models through the study of such models in bounded domain  very rich dynamics has been observe  the first two authors of this paper have done a series of works in this direction and obtained several fundamental result  the above open question is studied in the current paper and some satisfactory answers are obtaine  in general, we conjecture that the presence of the chemo-attractant does not increase the maximal spreading spee  while our results do not settle completely the question of the exact spreading speeds of solutions tothey provide a satisfactory answer for some range of the parameter  it would be of great mathematical interest to know whether the presence of the chemical really affects the spreading speed in genera  we plan to devote some of our future works to address this questio  this open question is also studied in the current paper and some satisfactory answers are obtaine  hence analogous results can be obtained in such settin  the rest of the paper is organized as follow  in section 2, we present some preliminary results to be used in the proofs of our main result 1- ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Preliminary lemmas", "Text": "in this section, we prove some lemmas to be used in the proofs of the main results in the later section  first, observe that the following identity hold  thus hold  then follows from a direction calculatio  proo  this implies next, we prove hence, follow  the following hol  proo  we shall only prove since can be proved by the similar argument  it follows from lemma   hence u belong to et the lemma is prove  proo  it follows from proper modification of the proof of observe from lemma   this combined with yields proo  it can be proved by the arguments of", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Spreading speeds", "Text": "in this section we derive an explicit upper bound on the spreading speeds of solutions of with nonempty compactly supported initial functions or exponentially decay initial functions, and prove theorems   proof of theorem   15 then by lemma   thus theorem   from this point, the remaining part of the proof is completed in four step  in this step we construct some sub-solution for by lemma   suppose, by contradiction that does not hol  which contradicts to |xinf| = tinf + m  observe that for every t belong to next, we present the proof of theorem   proof of theorem   let u0 belong to cb unif satisfy therefore, follows from so, it remains to prove that hold  by lemmas   we claim that m1 >   thus, by theorem   proof of theorem   usingthe proof of follows similar arguments as the proof of so, it remains to prove that hold ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Traveling wave solutions", "Text": "in this section we study the existence of traveling wave solutions and prove the following result, which is an application of the results established in the previous section and the theory developed in in order to make use of the theory established inwe first set up the right framework which follows from the proof of theorem   on the other hand, it follows also from the argument used in the proof of theorem   with these setting we can now apply the theory developed in proof of theorem   it follows from the arguments of the proof of that this function is continuous and compact in the compact open topolog  hence it has a fixed point u* by the schauder's fixed point theore  suppose on the contrary that this is fals  furthermore, the function is an entire solution of therefore, must hol  suppose on the contrary that does not hol  therefore, hold  observing in the proof of theorem  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Acknowledgments", "Section": " Acknowledgments", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "References", "Section": " References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Acknowledgments", "Section": " Acknowledgments", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "References", "Section": " References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "S1", "Section": " S1. Examples of correspondence between C,Sor12 and [p,q,Se,S]", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "S2", "Section": " S2. The Jain States from Composite Fermion Picture", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190 00048v1 31 dec 2018 astronomy & astrophysics manuscript n  these two stars have a= 40 dex and they belong to the main population of the system the most li-rich of them has =+ 5 dex higher than those measured in the most na-rich stars of omega centauri of similar metallicit  in both these cases, the chemical composition of this unique object could allow to look for the first time at the chemical composition of the gas processed in the interior of super-agb star  key words", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "the lithium abundance -a1- in globular clusters remains an unsolved and fascinating riddl  these stellar systems are known to have multiple populationscharacterised by significant star-to-star variations in the abundances of elements involved in proton-capture reactions,   for this reason, 2p stars should be li-free or exhibit a significant depletion of a with respect to 1p star d-072  1 a=log nli nh +1  markably similar li content among the stars theoretical models proposed to explain the formation of mps suggest different polluters in order to produce the observed na-o anticorrelation, the most popular ones being fast-rotating massive and asymptotic giant branch stars the former are not able to produce fresh li, while the latter can produce new li through the cameron-fowler mechanism in this framework, the stellar system omega centauri is a special case, showing a large spread but also an extended nao anticorrelation also, omega centauri exhibits a clear and well-defined li-na anticorrelation in this letter we report the discovery of two giant stars in omega centauri showing a significant enhancement of a with respect to other stars of the syste  one of these stars is also significantly enriched in n ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Observations and data analysis", "Text": " mucciarelli_lirich the eso very large telescope under the program 09 d-0728 we secured one exposure with the setup hr12two with hr13 and three with hr15n in this sample, we identified two additional lrgb stars with a strong enhancement of   here we briefly summarise the approach used for the chemical analysis, referring the reader to m2018 for a detailed descriptio  effective temperatures and surface gravities have been derived from photometr  and the 2mass near-infrared database the employed colour excess and distance modulus are e= 12 mag and 0=1  microturbulent velocities have been derived by minimising the trend between the line strength and the abundances of the fe i line  abundances of fe have been derived from the measured equivalent widths using the gala codewhile the equivalent widths have been measured with the daospec code managed through the wrapper 4dao these abundances have been corrected for non local thermodynamical equilibrium using the corrections by lind et a  and lind et a  for li and na, respectivel  information about the two stars is listed in table  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Results", "Text": "both stars are members of omega centauri, as confirmed by their radial velocities and proper motions according to their iron contentthe two stars belong to the main population of the system the two stars exhibit a larger than those measured in the other lrgb stars of omega centaur 91 dex with a significant fraction of stars with low li abundances65 dex for #12610  the upper panels of fi 9-1 dex).  concerning na, we derived nlte=+ 87 and +  the lower panel of fi  this doublet provides fi  the resonance li line in #25664 and #126107 both compared with stars of omega centauri of similar metallicity and atmospheric parameters the lower panel shows the comparison between the na i d lines in #25664 and in the comparison sta  plus symbols indicate the interstellar na d2 line  fi  nlte=+ 07 and +  in fi  mucciarelli et a : li-rich stars in omega centauri fi  behaviour of nlte as a function of for the lrgb stars of omega centauri are less sensitive to velocity fields and saturation effects with respect to the na i d line  because the two sets of lines provide compatible results, in the following we will refer to abundances derived from na i d lines to compare the abundances of the two li-rich stars with those of the other stars of omega centauri discussed in m2018 fi  3 shows the behaviour of nlte as a function of for the lrgb stars of omega centauri with marked as red and blue triangles the two li-rich star  the metal-poor stars of omega centauri span a large range in na abundance, similar to the range measured in the old gcs studied so far5 dex larger than that measured in the most na-rich omega centauri stars with similar a very few gc stars with significantly larger than the most na-rich stars in the parent cluster have been discovered so far, namely in omega centauringc 2808 and m62 all these stars are brighter than the rgb bump level but no measure of li is availabl  fi 4 shows the position of the two li-rich stars in the nlte-anlte plan  the lrgb stars of the system define a clear li-na anticorrelation, while the two target stars lie outside the mean locus defined by the other star ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Discussion", "Text": "we have discovered the first two li-rich stars in omega centauri, one of them with an unexpected and exceptional enhancement of na, while the other with a na abundance compatible with the distribution of the stars of fi  behaviour of nlte as a function of anlte for the lrgb and the two li-rich stars of omega centauri omega centauri with similar metallicit  li-rich stars are rare in gcs, only 13 are known so far10 of them belonging to the rg  fi  as a reference for the predicted evolution of a, we show also a for the stars in the gc ngc 6397 taking into account the effect of the li depletion due to the first dredge-upthe two li-rich giants should have an initial a of about  2- 2 dex, but the latter has a na content compatible with the abundance of 1p stars of ngc 6397 interestingly, similar lithium enhancements have been recently detected in metal-poor field giant stars by li et a  also in these cases, abundances of elements other than lithium are similar to the trend observed for similar star  mucciarelli_lirich table   main information about the two li-rich stars in omega centauri id ra dec rv teff log g vt anlte nlte 25664 20 8+/- 82+/- 40+/- 87+/- 7+/- 70+/- 65+/- 14+/- 06 the origin of li-rich stars remains still debated and can be ascribed to different processes, including external or internal li productio  one of the most invoked external mechanism to increase a is the engulfment of small bodies, like planets or brown dwarfs that should enhance both li and b  also, the engulfment of a planet should also increase the rotational velocity and the chromospheric activity of the star we checked that the measured full widths at half maximum are compatible with the nominal spectra resolutions and we estimate that in these stars the projected rotational velocity is virtually compatible with no stellar rotatio  on the other hand, the na d lines do not show evidence of circumstellar materia  another proposed mechanism to explain li-rich stars but invoking an internal production of fresh li is the cameron-fowler mechanism in this case we can envisage two possibilitie  the first option is extra mixing effcient within the star during its rgb evolutio  this mixing needs to circulate matter between the base of the convective envelope and a region close to the h-burning shel  both speed and depth of the circulation must be such that 3he from the convective envelope is transported to temperatures high enough to activate the 3he7be reaction, with 7be quickly transported back to cooler regions, where li can be then produced by the 7be7li reactio  several papers have discussed in detail scenarios/mechanisms to achieve thi  this could for example go towards explaining the exceptionally high measured in #2566  our two li-rich objects are located below the rgb bump, as shown in fig   1 and   the second option is production of fresh li this time in agb starsand mass transfer from a massive star that evolved through the agb phase and transferred li-rich material onto our target  in these stars, li is produced through the cameron-fowler mechanism, while na is produced through the ne-na cycl  we remind that super-agb stars could play a relevant role in the explanation of the mps observed in all gcs and in omega centauri, because they are among the candidate polluter stars able to generate the 2p star  theoretical models of mps based on agb stars as main polluters need to include some dilution of the agb ejecta with pristine gas in order to reproduce the observed chemical pattern  this is due to the mass dependence of the agb yields that should lead to a na-o correlation, at variance with what has been observed, if no dilution process is accounted fo  however, it has been proposed that a small fraction of stars may have formed from the pure ejecta of super-agb stars before the dilution process, preserving the original chemical composition of these polluting stars this mechanism could explain the presence of a he-rich sub-population in some systemsbecause the super-agb stars should also produce a large amount of h  hence, in this scenario we expect that #25664 should have a high he content -the star was member of a binary system together with a massive star and accreted li-rich material from the companion when the latter reaches the super-agb phas  in this case, the abundances that we measure are not its original ones but they reflect the chemical composition of the interior of the companion, plus some degree of dilution with the convective envelope of the accreting sta  the radial velocities measured from individual flames spectra do not show evidence of variabilit  however, they have been taken on a period of about 3 weeks and we cannot exclude that the star is member of a binary system of a longer perio  hence, with the current dataset we are not able to disentangle between the two scenario  even if the precise amount of li and na produced by agb stars of different masses is highly sensitive to several physical assumptionsthe measured li and na abundances of #25664 are qualitatively compatible with those foreseen for the superagb stars this li-na-rich star may be a direct evidence of extramixing occurring before the rgb bump, or the first observed relic of the gas ejected from super-agb stars, demonstrating that these stars can play a role to explain the mps in omega centaur  mucciarelli et a : li-rich stars in omega centauri fi  star, in particular the elements involved in the proton-capture reactionsis crucial to understand the origin of this unique objec  for instance, the measure of the 12c/13c isotopic ratio will confirm or refute the pre-bump nature of this sta  if this will be confirmed, the denissenkov & vandenberg model may be discarded leaving the agb scenario onl  if the chemical composition of #25776 will confirm the scenario related to super-agb stars, this star will allow to directly study the chemical composition of the gas processed in the interior of the super-agb star  acknowledgement  we thank the anonymous referee for his/her useful comments and suggestion  lm acknowledges support from proyecto interno of the universidad andres bell  sv gratefully acknowledges the support provided by fondecyt re ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "A", "Section": "A Effective temperatures", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "B", "Section": "B Planet engulfment", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190 00050v1 31 dec 2018 partial smoothness of the numerical radius at matrices whose fields of values are disks   after illustrating this phenomenon with some examples, we illuminate it by studying matrices around which this set of disk matrices is a manifold with respect to which the numerical radius is partly smoot  we then apply our results to matrices whose nonzeros consist of a single superdiagonal, such as jordan blocks and the crabb matrix related to a well-known conjecture of crouzei  one of our results is that in this real vector space with dimension 18, the set of disk matrices is a semi-algebraic manifold with dimension 1 ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction: the numerical radius", "Text": " specifically, the iterates xk belong to cn converge to zero for all initial points x0 if and only if a is stable, meaning its spectrum is contained in the open unit dis   research supported in part by national science foundation grant dms-161399   research supported in part by national science foundation grant dms-162008  for an excellent discussion of this and related issues, see trefethen and embree's book the numerical radius is a vector space norm, but it does not satisfy the submultiplicative property required in most definitions of a matrix norm the first proof, by bergerwas simplified by pearcy the inequality is an elementary consequence a variety of other properties of the numerical radius are collected inand a useful bibliography appears in research on the topic remains very activ 1 an elegant proof of the power inequality relies on the following semidefinite representation of the numerical radius due tobased in part on we denote the space of n-by-n hermitian matrices by h  1so far this century, at least 150 papers have appeared with titles including the string numerical radius the concept of numerical radius goes back at least to lumerbut may well be olde  for more direct methods, see our current work is motivated by the following observation: optimization involving the numerical radius quite commonly results in matrices whose fields of values are disks we call such matrices disk matrices, and denote the set of disk matrices by   our aim here is to illuminate variational properties of the numerical radius that, notwithstanding their rarity, produces disk matrices at optimalit  the paper is organized as follow  in section 2, we discuss two interesting examples where disk matrices appear as solutions to optimization problems involving the numerical radiu  the first example involves feedback control, while the second concerns the crouzeix conjecture in matrix analysis section 4 summarizes our results for a well-known disk matrix that arises in the study of crouzeix's conjectur ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Examples of numerical radius optimization", "Text": "for now, we focus on two examples to illustrate our them  the classical static output feedback problem is to choose k so that m is stabl  we now report on some computational experiments minimizing r x00m x01 which clearly support our claim that minimizing the numerical radius frequently leads to disk matrice   entries from the normal distributio  these quantities are shown as percentages rounded to the nearest 10% because repeated runs indicated that with this choice of rounding, the results are reasonably consisten  more interesting is the fact that these computational results are consistent with our theoretical results given later in the pape  the crouzeix ratio our second example arises from observations that minimizing a certain ratio associated with the crouzeix conjecture seems to result in disk matrices", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Subgradients and partial smoothness", "Text": " we consider several specific euclidean space  dimension always refers to the real vector space notio  the notation ri denotes relative interio  the two subspaces are identical if and only if 0 belong to   proof the first equation follows from a standard argument, as follow  then the l1-norm is partly smooth at any point in m relative to   when a norm is partly smooth relative to a set, we can view that set locally as the set of solutions of a parametrized family of optimization problem  proximal operators are central to many first-order algorithms for convex optimizatio  among several useful consequences of the idea of partial smoothness, particularly intuitive is its use in sensitivity analysis, by which we mean the behavior of solutions to optimization problems under small perturbations to the dat  partial smoothness reveals what happens under perturbation  thus optimization problems commonly produce solutions in the manifold m, even though its dimension may be lo  the results we prove below about partial smoothness of the numerical radius relative to the set of disk matrices suggest that analogous behavior may be at play in the numerical examples we presented in section  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 The Crabb matrix", "Text": "in this work, we prove partial smoothness of the numerical radius around some interesting matrice  before proceeding to our general development, we illustrate our results on some simple special case  here and throughout the paper, we denote the unit circle in the complex plane by   the two-by-two case in m2, disk matrices coincide with nilpotent matrices, as follows immediately from hence, by theorem   computational tests, using semidefinite programming, confirm this propert  8 the three-by-three case the analogous behavior in m3 is also simple to chec  computational tests again confirm this propert  in fact this condition along with the condition that a and d are both nonzero is also suffcient for x belong to a we show that the numerical radius is partly smooth at all such matrices, relative to   however, the lack of partial smoothness means that there is no guarantee that the proximal operator proxr maps matrices near 3 2e0 to disk matrice  again, we can verify this behavior computationall  2this matrix was called the choi-crouzeix matrix in many thanks to   salemi for providing the reference and pointing out that as a consequence of crabb's results, the conjectures on and are tru  a third conjecture on remains ope  we use the following result extensively: since its proof is simple, we include i  11 proof consider the first equatio  the field of values is nonempty and compact, so the maximum on the left-hand side is indeed attaine  the second equation is an immediate consequenc  one direction in the final equivalence is also immediate, and the converse follows since the field of values w is convex by the toeplitz-hausdorfftheore  the expression for the gradient follow  to prove continuity, we use lemma   the second equation follows from proposition   continuity follows by proposition   in this section we describe conditions under which b is locally a manifol  14 x belong to   it follows that properties and are equivalen  proof we begin with the first clai  this completes the proof of the first clai  at first sight, this assumption seems artificial, but in the sections that follow we provide motivation and example 4 consider any matrix x belong to m  by proposition   we see from lemma 5", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 The subdifferential of the numerical radius at disk matrices", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "6", "Section": "6 Matrices with field of values the unit disk", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "7", "Section": "7 Disk matrices", "Text": "our next step is to lift the main result in the previous section from the set b to the set of disk matrices   the necessary condition is clear from proposition   the result follow  proof denote the open unit ball in the space e by   in that case the tangent space ts is just the range of d  the image of this set under the map q is the given set, so the result now follow 4 consider any matrix x in m  then around x, the set a of disk matrices is an analytic manifold of codimension 2  hence the numerical radius is partly smooth at the matrix x relative to   using theorem   this follows from lemma   the properties of the subdifferential of the numerical radius follow from theorem   the result follow  since the set a is closed under scalar multiplication, it is easy to check na = n  this completes the proof", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "8", "Section": "8 The Crouzeix parametrization", "Text": "according to proposition   we now explore this assumption, using a result from a somewhat related characterization appears intheorem   proof by theorem   consider any vector v in the nullspace of the matrix u -  this assumption will suffce for the examples we conside  then around the matrix x = 2u the set a of disk matrices is an analytic manifold of codimension 2  in particular, the numerical radius is partly smooth at the matrix x relative to the manifold   proof this follows directly from proposition  ", "Subsections": [{"Section_Num": "8_1", "Section": "8.1 The two-by-two case", "Text": "as shown ina two-by-two matrix is a disk matrix if and only if it is unitarily similar to a multiple of the two-by-two jordan bloc  in theorem  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "8_2", "Section": "8.2 The superdiagonal case", "Text": " 0 uf8f9 uf8fa uf8fa uf8fa uf8fa uf8fa uf8fb in fact, we have the following more general resul  we begin with a simple tool concerning real sequence  for notational simplicity, and with no loss of generality, we present it for infinite sequence  as we claimed, therefore, our matrix is unitarily similar to a nonzero multiple of a matrix of the form as in the discussion of that example, the result now follows from theorem  ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "9", "Section": "9 The three-by-three case", "Text": "we end with a thorough study of 3-by-3 disk matrice  in particular, we show that theorem  4 has applications beyond matrices with the sparsity pattern of a jordan block and their unitary similarity transformation  25 any disk matrix has a zero eigenvalue with algebraic multiplicity at least two, as shown in however, as we shall also see, this containment is stric  specifically, we devote this section to showing the following resul  we begin the proof with some simple algebraic observations about the partition the second equation is a simple consequence, using the compactness of the set of unitary matrice 9 consider a matrix x unitarily similar to a matrix of the form10 the set of disk matrices in m3 is e e==>.  by proposition   this completes the proof of the converse directio  assume this is the cas  then e belongs to the set a of disk matrice  proof as in the proof of theorem   then, by lemma   the result now follows by theorem  14 the semi-algebraic set a of all disk matrices in m3 has dimension 1  proof by proposition   that result shows that the set of such matrices x is a semi-algebraic manifold of dimension 12, so its closure also has dimension 1  consequently, the numerical radius is not partly smooth at x relative to any subset of   by lemma   now theorem   we can rewrite the matrix in this equation as the image of the vector under a one-to-one linear map, so by corollary  10, giving the final ingredient in the proof of theorem   in the light of theorem   for example, by theorems   for example, the prox operator proxr maps any small matrix to the zero matri 15 do not display this same stable behavio  we can apply the results in this section directly to the matrices y and  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "10", "Section": "10 Concluding Remarks", "Text": " we have argued that such matrices arise naturally as solutions of optimization problems involving the numerical radius, supported by both numerical experiments and subdifferential analysi  even in the case of matrices of order three, the details are surprisingly subtl  our development blends matrix and variational analysis, but makes no use of semidefinite programming characterizations of the numerical radius like theorem  2: the implication of this characterization for partial smoothness is an interesting open questio ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190 00053v3 15 may 2019 spanning 2-forests and resistance distance in 2-connected graphs wayne barretta,1, emily   aside from their combinatorial significance, spanning 2-forests have an important application to the calculation of resistance distance or effective resistanc  there are also well-known matrix theoretic methods for calculating resistance distance, but the way in which the structure of the underlying graph determines resistance distance via these methods is not well understoo  an important special case is the preservation of the number of spanning 2-forests if u and v are in the same smaller grap  in this paper we demonstrate that this method of calculating resistance distance is more suitable for certain structured families of graphs than the more standard method  we apply our results to count the number of spanning 2-forests and calculate the resistance distance in a family of sierpinski triangles and in the family of linear 2-trees with a single ben by eduejevans@mathematic by eduaefr@umic edumkempton@mathematic by edusinkovic@mathematic by edu 1supported by the defense threat reduction agency - grant number hdtra1-15-1-004  preprint submitted to elsevier may 17, 2019", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "resistance distance in graphs has played a prominent role not only in circuit theory and chemistrybut also in combinatorial matrix theory and spectral graph theory many of the methods for calculating resistance distance,   furthermore, the relationship between these resistance distances and the structure of the underlying graph is not well understood except in special case  thus, if the number of spanning trees is known, calculating the number of spanning 2-forests and resistance distance are equivalent problem  this work presents new reduction formulas for determining these quantities for 2-connected graph  we apply these results to a new family of linear 2-trees generalizing the work of we begin with the following notation and definition  let g be an undirected graph in which multiple edges are allowed but loops are no  the number of such forests is denoted by f  we denote the number of these by f  it is natural to ask if reduction formulae such as these can be found for graphs with no cut vertex, and the answer is in the affrmative if the graph has a cut-set of size   here g/ij denotes the graph obtained by identifying vertices i and j this is theorem 13 and is one of the main results of the next sectio  this reduction is particularly effective if the sizes of g1 and g2 are comparable, and if there are multiple 2-separator  as previously mentioned we also consider the important and closely related concept of resistance distance or effective resistanc  aside fromthere seem to be few applications of theorem 2 to the calculation of resistance distanc  one significant example is the proof of the second statement of theorem 7 in our reduction formulae open the possibility of finding closed forms for resistance distances in many additional graph  we illustrate this for the sierpinski triangle and the family of linear 2-trees with a single bend in section 3 see figure   consequently, the number of spanning 2-forests separating two vertices can be found immediately from theorem   a linear 2-tree with a single bend can be obtained from two straight linear 2-tree  this fact and theorem 13 are applied in section 3 to obtain an explicit formula for all resistance distances in the family of linear 2-trees with a single ben ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 2-Separations", "Text": " note that the graph resulting from the deletion of vertices i and j from g is a disconnected grap  let g be a graph with a 2-separation as abov  proo  then the restriction of t to the other side is a spanning 2-forest of that side separating i and   by the multiplication principle, the result follow  before stating the first analogous result for 2-forests, we need the following definitio  then fg = fg1fg2 + fg1/ij  proo  by the multiplication principle, the number of ways to do this is fg1fg  there are fg1/ijt ways of doing thi  proo  the last follows from the first two and theorem   after performing a 2-switch in the graph on the left in figure 3, we obtain the graph on the righ  each edge label denotes the number of 2-forests separating the vertices incident to that edg  the number of separating spanning 2-forests is also the same for non-adjacent vertices in h and k then fg = fg + f  proo  proo  case 1: h2 is a spanning tree of g  by the multiplication principle, the number of possible h in this case is thus fg1/ij  case 2: h1 is a spanning tree of g  by an argument symmetric to case 1, the number of spanning 2-forests arising in this case is fg2/ij  case 3: neither h1 nor h2 is a spanning tree of g1 or g  proo  now we consider how h restricts to g1 and g  this yields the first term of the clai  let h2 be the restriction of h to g  let x be a vertex of h2 not in the component of   this yields the third term of the clai  a substantial application of theorem 13 is found in section   this completes the proo  solving this system yields the desired resul  from lemma 14 we obtain an alternative form of the formula in theorem 1  this eliminates counting spanning 2-forests that separate a vertex from a pair of vertice  the following theorem is found in then rg/ij = rg -2 4rg proo  using theorem 17, we replace rg1/ij, and arrive at the desired resul ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Applications to 2-connected graphs", "Text": " the first three stages are shown in figure 4", "Subsections": [{"Section_Num": "3_2", "Section": "3.2 Resistance distance in a bent linear 2-tree", "Text": "earlier work by the authors considered resistance distance in a straight linear 2-tree with n vertices and obtained the following resul  in we also gave an alternative formula for rh  modifying it in the same way yields fhn = v-u x i=1 f2n-2i-2u+  although it is not a closed form expression, is is nevertheless useful as we shall se  in this work we consider a modification of the straight linear 2-tree which we term the bent linear 2-tree whose definition is belo  see figure   when u and v are on the same side of the bent linear 2-tree it is easy to determine fg  let gn be a bent linear 2-tree on n vertices with a single bend at vertex   proo  here g1 is on the left and g2 is on the righ  here h1 is on the left and h2 is on the righ  we see by definition that 2fg1fg2 = 2fh1fh  it remains to deal with the term fh2 -fh  in order to apply equation we must first adjust the vertex labels on h  let r be the graph obtained from h2 by labeling the vertices from the right beginning with   by symmetry fh2 = fr and fh2 = f  15 then fh2 -fh2 = fr -fr = fs -fs -  this completes the proo  let gn be a bent linear 2-tree on n vertices with a single bend at vertex   let gn be a bent linear 2-tree with n vertices and a single bend at vertex   then fgn = fhn -fk-2fk+1fn-k-2fn-k+  and rgn = rhn -fk-2fk+1fn-k-2fn-k+1 f2n-2 the second terms tell exactly how much the number of separating 2-forests and resistance distance are diminished by a bend at   proo  the resistance distance rhn is known from ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Conclusion", "Text": " we have applied these formulae to the sierpinski triangle and to a family of linear 2-trees with a single ben  for the bent linear 2-tree we determined the resistance distance between any pair of vertice  finding analogous formulae for 3-connected graphs is an interesting but diffcult, open questio ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": " but several studies have been shown that deep neural networks can be easily fooled by artificial examples with some perturbations, which are widely known as adversarial example  adversarial examples can be used to attack deep neural networks or to improve the robustness of deep neural network  a common way of generating adversarial examples is to first generate some noises and then add them into original example  in practice, different examples have different noise-sensitiv  to generate an effective adversarial example, it may be necessary to add a lot of noise to low noise-sensitive example, which may make the adversarial example meaningles  in this paper, we propose a noise-sensitivity-analysis-based test prioritization technique to pick out examples by their noise sensitivit  we construct an experiment to validate our approach on four image sets and two dnn models, which shows that examples are sensitive to noise and our method can effectively pick out examples by their noise sensitivit ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "I", "Section": "I Introduction", "Text": " in computer vision, the capabilities of deep neural network techniques are even similar to that of human in image recognition although there are more and more dnn models have been proposed, all of those models heavily depend on the given set of samples since dnns are actually an effective data representation of a given set of sample  therefore, a trained dnn models may fail to classify a new sample correctly in practic  it is very important to know that whether a dnn model is reliable enough to be use  there are several studies which try to expose the potential errors in a dnn model by adversarial example  an adversarial example is an example which the given dnn models cannot classify it correctl  syegedy et a  find several interesting interesting properties of dnns which help to launch adversarial attacks on the dnns models for image recognitio  later, researchers have found that they can add noise into original examples to generate adversarial examples for improving the robustness of a dnn model or attacking a dnn mode  goodfellow et a  proposed a framework to estimate generative models via an adversarial process, and many variants have been propose  they are called generative adversarial networkswhich is a popular way to generate adversarial example  zhang et a  proposed an unsupervised framework to generate semantic-equivalent adversarial examples, which are used to test the consistency of autonomous driving systems across different scene  tian et a  akhtari et a  to generate an effective adversarial example1, it needs to generate a lot of noises for adding to the original examples before they succeed obtaining an effective adversarial exampl  the number of noises to generate an adversarial example can be enormou  moreover adversarial examples with too much noise can be easily detected by a defense mode  for example, akhtar et a  proposed a defense framework against the adversarial attacks generated using universal perturbations in practice, we observe that different original examples can have different noise sensitivit  examples with high noise sensitivity are more likely to become adversarial examples by perturbation  to increase the proportion of effective adversarial examples in all adversarial examples, high noisesensitive example should be picked out, which are added noise to generate adversarial example  in this paper, we propose a noise-sensitivity-analysis-based test prioritization technique to detect high noise-sensitive examples in dnn models with probability label  first, nsatp collects the probability vectors of original examples in deep neural networ  then, nsatp uses the distance of the probability vectors to compute the noise-sensitive values of original example  finally, nsatp ranks the original examples according to their noise-sensitive value 00054v3 20 jan 2019 paper, namely probability difference, probability entropy and probability-varianc  we construct an experiment to validate the effectiveness of nsatp on four image datesets and two deep neural network model  the main contribution of our work is fourfol  second, we proposed a noisesensitivity-analysis-based test prioritization technique, which can effectively pick out the examples by their noise sensitivit  third, we conducted an experiment on four image datasets and two deep neural network models to validate the effectiveness of nsat  fourth, probability-difference distance is more effective than probability-entropy distance and probabilityvariance distanc  the rest of the paper is organized as follow  section ii uses the observations on two images to show that the example may be sensitive to noise and motivate our wor  section iii presents our approach in detail, which is evaluated in section i  then we introduce related work in section   finally, we conclude our work in section v ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "II", "Section": "II Background and Motivation", "Text": "in this section, we first present adversarial attack  based on it, we discuss the observation on two image to motivate our wor    adversarial attacks and defenses deep neural networks are mainly driven by models and data, which are used to solve some challenging task  there are some common ways to generate adversarial examples, such as adding noise into original examples and example transformatio  the existence of adversarial examples has been pointed out by szegedy et a  since the discovery of adversarial examples for the deep neural networksmany works focus on the robustness of neural networks against these adversarial examples and show that using adversarial examples as training can improve the robustness of neural network  akhtar et a  proposed a defense framework appending a pre-input layers against the adversarial attack  adversarial examples with too much noise are easily identified by defense mechanis  there are some defense mechanism that limit the number of attacks, such as alipay's face authentication  label:4 label 4 label:4 label 4 fi  observation label:4 ori  1 label:2 adve  1 label:2 adve  2 label:8 adve  3 label:2 adve  4 label:8 adve  5 label:9 adve  6 label:2 adve  7 label:2 adve  2: adversarial examples of fi  1 label:4 ori  2 label:0 adve  1 label:6 adve  2 label:6 adve  3 label:9 adve  4 label:6 adve  3: adversarial examples of fi  1 convolutional neural networks are a special kind of multilayer neural networks, which are trained with a version of the back-propagation algorith  lenet is one convolutional network designed for handwritten and machine-printed character recognitio  we use mnist to train lenet-5 model, and randomly choose two pictures showed in fi  4: workflow of our method examples for each picture  there are 8 and 5 effective adversarial examples with respect to the two pictures of fi  1, showed in fi  2 and fi  we have the observation that the number of effective adversarial examples with respect to fi  1 is bigger than that with respect to fi  we repeat 10 times to observe the difference between the numbers of effective adversarial examples with respect to fi  1 and fi  1and we find the numbers of effective adversarial examples with respect to fi  1 is always not less than that with respect to fi  we further observe the probability vectors of the two pictures, and the probability values of each label of fi  1 are more discretized than that of fi  for example, the probability vector variance of fi  1 is more than that of fi  1and the probability difference of fi  1 between 1st probability and 2nd probability is more than that of fi  the observation shows that the examples are sensitive to noise, which motivates us to consider whether the noise sensitivity is related to probability vectors, and whether probability vectors can be used to evaluate the noise sensitivity of given example ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "III", "Section": "III Technique Framework", "Text": "in this section, we present our method nsatp to compute the noise sensitivity of each examples and rank examples by their noise sensitivit  our method is designed for choosing the original examples before generating adversarial examples based on original examples to attack deep neural networks or improve the robustness of deep neural network    in this paper we focus on deep neural networks with the capabilities of n-class classificatio  let us consider a train set ts and a deep neural network models c0, of which the outputs are the probability label  all examples of ts can be used to train the model c0 to generate a trained a trained model c1 with n probability vector  for a valid input example ti belong to ts, the trained model c1 can output the probability vector of ti,   our method nsatp since our method is to evaluate the noise sensitivity of each example in s based on their probability vectors computed by the trained model c1, the probability vectors of evaluated examples should be availabl  our method workflow is depicted in fi  adding too much noise to low noise-sensitive example to generate effective adversarial examples may make the adversarial example meaningless or be easily detected by the defense mechanis  nsatp aims to assess the noise sensitivity of examples and pick out the high noise-sensitive examples before attack models generate adversarial example  to start such a process, we assume that a trained models c1 is ready showed in pre-step of fi  nsatp four steps, as illustrated in the fi  then, they used the generated adversarial examples to attack c1 or improve the robustness of c  based on the observation in section ii-b, the distance metric is evaluated the noise sensitivity of one example based on the difference between the probability vectors of the exampl  the probability difference is the sum of the differences between different sorting levels with different weight  the pd calculation is given in equation   the variance is used to measure the dispersion of one given data se  we use the variance metric to measure the dispersion between values in one given probability vecto  the pv calculation is given in equation   entropy can be used to evaluate the uncertainty of random variables, and the probability values of the example can be seen as random variable  in this paper we call it probability entrop  the pe calculation is given in equation  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "IV", "Section": "IV Evaluation", "Text": "in this section, we first describe set the experiment and train the models on which we evaluate our metho  then, we introduce some metrics to evaluate the effectiveness of our metho  finally, we present the data analysis and the results of the experimen    research questions regarding our obversactions in section ii and our proposal, we are desired to answer the following question  experimental setup similar to su et a we encode the perturbation into an array which is optimized by differential evolutio  in our experiment each perturbation holds x-y coordinates and rgb value of the perturbation, and one perturbation modifies four pixel  the model architecture is given in table i and the hyperparameters selected in table i  in our paper, we use the all training sets of the four datasets to traning dnn models, and use the test sets of the four datasets to analyze the noise sensitivity of examples and answer the research question  performance measurement in this section, we will introduce some metrics used to evaluate the effectiveness of nsat  the more the effectiveness of adversarial examples is, the less cost it is to successfully attack dnn  in this paper, to measure the noise-sensitivity of examples, we compute how many adversarial samples fool the given deep neural network among given adversarial sample se  f-measure denotes how many test cases are executed before successfully attacking the syste  to measure performance the advantages in effectiveness of nsatp to random, we followto use the f-measure metri  fmeasure calculates the expected number of adversarial sample required to successfully fool the given deep neural networ  in other words, a lower f-measure value means that fewer adversarial samples are used to accomplish the tas  if an attack strategy yields a lower f-measure value, it is considered to be more effectiv  we expect a lower f-measure value for nsatp than that of rando  we thus calculate in  the greater the value is, the more effective nsatp is than rando  in this paper, we use three different distance metrics to compute the noise sensitivity of given example  to evaluate the effectiveness of different ranks, we can fit their curve functions and map them to linear functions, then use the absolute derivative of linear functions to evaluate the effectiveness of different rank  then, we use the tuple set to fit a curve functio  for different distance metric on one example set, we can fit different curve with 95% correlation coefficien  third, we map the curve function to a linear function, and compute the derivative of each fitted linear functio  finally, we compare the absolute values of different derivatives to analyze which distance metric is bette  the more the absolute value is, the better the corresponding distance metric i  data analysis and results we report our experiment results in three subsections to answer the three research questions, respectivel  metri  we use the collected data to generate the tuple set, and each tuple includes the distance value and the ef  value for one exampl  we visualize each tuple set in a x-y coordinate figur  6 and fi  fi  fi  let us take fi  6 for discussion firs  fi  6 shows the tuple distribution of mnist on pd metri  the value of ef  decreases as the value of pd increase  the distribution of fi  6 conforms to the exponential distributio  we use least squares and variable substitution to fit a curve function with more than 95% correlation coefficient between the function and dat  mnist on pd   f-mnist on pd   cifar-10 on pd   cifar-100 on pd   mnist on pe   f-mnist on pe   cifar-10 on pe   cifar-100 on pe   mnist on pv   f-mnist on pv   cifar-10 on pv   cifar-100 on pv fi  all fitted functions and the correlation coefficient are summarized in table iii and table i  we check all figures and obtain the relationship between ef  values and the distance values is nonlinearinverse proportio 27e- 27e- 27e- 49e-9 43e-7 47e-8 39e- 38e- 39e- 47e-1 42e-1 45e-1 14e- 14e- 14e- 42e-2 22e- 27e-1 23e- 24e- 24e- 43e- 31e- 36e-  mnist on pd   f-mnist on pd   cifar-10 on pd   cifar-100 on pd   mnist on pe   f-mnist on pe   cifar-10 on pe   cifar-100 on pe   mnist on pv   f-mnist on pv   cifar-10 on pv   cifar-100 on pv fi  the experiment results of nsatp and random in f-measure on four image datasets and two models are shown in table  01% f-mnist nsatp  01% cifar-10 nsatp  14% cifar-100 nsatp  23% examples to simulated random testing2, and select top 1000 examples of ranked example lis  we collect the f-measure for these examples on two models, four image datasets, and three distance metric  let us take the first cell to illustrate to the content  it shows that the f-measure for nsatp and random on the net-1 model, mnist and pd metric are   it means that on average  we further use the in  metric to calculate the improvements from random to art4sqli, and report in  we check all in  finally, we answer q2 as follows: 2the ef  values of all examples are more than zer  a2: our method can effectively analyze the noise sensitivity of examples, and the fitted curves based on the results of three distance metrics are all exponential function on each image dataset and each mode  table vi: map the curve function to the linear function net-1 lenet-2 pd pe pv pd pe pv mnist ab 28 mark best worst middle best worst middle f-mnist ab 31 mark best worst middle best worst middle cifar-10 ab 46 mark worst best middle best worst middle cifar-100 ab  let us take the first cell of table iii to illustrate the evaluation proces  we map each function of table iii to a linear function and compute the absolute derivative of the linear function, summarized in table v  threats to validity in this section, we discuss the threats to validity of our experimen  in our experiment, we train two dnn models on four image datasets, and one perturbation modifies four pixel  different perturbation may lead to different observation  the perturbation strategy just modifies four pixels, but other perturbation strategies are likely to modify more than four pixel  the examples with too many noise may be easily detected by adversarial defense mechanis  moreover, we randomly choose the pixels of given examples and modify the  different models may have different accuracy rates with different training strategies, and different trained models may affect the effectiveness of our metho  in our experiment, we construct two different models and different training strategie  the results based on these models and training strategies are consisten ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "V", "Section": "V Related Work", "Text": " it is essential to prevent the potential errors from being expose  machine learning models are often vulnerable to adversarial manipulation of their input intended to cause incorrect classification dnns are actually an effective data representation of a given set of sample  a trained dnn models are highly vulnerable to attacks based on adversarial examples with small modification  szegedy et a  first revealed the sensitivity to well-tuned artificial perturbation, such perturbation image can fool dnn models, they further observed that using adversarial examples to train dnn can improve the robustness of dnn against adversarial example  goodfellow et a  proposed a framework to estimate generative models via an adversarial process, which is called generative adversarial networks kurakin et a  proposed basic iterative method to generate adversarial examples, which add little perturbation in each ste  papernot et a  formalized the space of adversaries against deep neural networks and utilize jacobian matrix to build adversarial saliency map to enable an efficient exploration of the adversarial-samples search spac  su et al just modified one pixel of examples and observed the adversarial examples can successful fool dnn  carlini and wagner introduced three adversarial attacks in the wake of defensive distillation against the adversarial perturbation  moosavi-dezfooli et a  proposed to compute a minimal norm adversarial perturbation for a given image in an iterative manne  akhtar and mian have investigated other adversarial attack  our method is to evaluate the noise sensitivity of examples, which can work on these attack ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "VI", "Section": "VI Conclusion", "Text": "deep neural networks have been widely use in natural language processing, computer vision and image recognitio  many researchers have proposed many efficient dnns for those application  but, related works has show the dnns can be fooled by artificial examples with some perturbations and many adversarial attack models have be propose  however, most of them pay little attention to the noise sensitivity of example  in this paper, we have studied the correlation between the noise sensitivity and the probability labels of example  we proposed a noise-sensitivity-analysis-based test prioritization technique for deep neural networks, which evaluates the the noise sensitivity of examples based on the probability labels of their example  we have conducted a controlled experiment on two deep neural networks over four image datasets with three distance metric  the experimental result has confirmed examples are sensitive to nois  empirical results have shown that nsatp can effective evaluate the noise sensitivity of examples and rank the examples by their noise sensitivity, and probability-difference distance is more effective than probability-entropy distance and probability-variance distanc  future work includes a thorough study on other example datasets and deep learning model  in order to enhance the effectiveness of our method, another future work is to validate it on other adversarial attack models as wel ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "VII", "Section": "VII Acknowledgements", "Text": "this work was supported by grant from the xxx, grant from the xxx, and grant from the xx ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "References", "Section": "References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": " the presence of white matter lesions biases morphometric analyses such as registration, individual longitudinal measurements and tissue segmentation for brain volume measurement  lesion-inpainting with intensities derived from surrounding healthy tissue represents one approach to alleviate such problem  however, existing methods inpaint lesions based on texture information derived from local surrounding tissue, often leading to inconsistent inpainting and the generation of artifacts such as intensity discrepancy and blurrines  based on these observations, we propose non-local partial convolutions that integrates a unet-like network with the non-local modul  the non-local module is exploited to capture long range dependencies between the lesion area and remaining normal-appearing brain region  then, the lesion area is filled by referring to normal-appearing regions with more similar feature  this method generates inpainted regions that appear more realistic and natura  our quantitative experimental results also demonstrate superiority of this technique of existing state-of-the-art inpainting methods", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "multiple sclerosis is an immune-mediated disease that results in progressive damage to the central nervous system ms is characterised by $fully documented templates are available in the elsarticle package on cta  *the two authors contributed equally to this pape  preprint submitted to neuroimage march 26, 2019 arxiv:190  focal inflammatory demyelinating lesions in both the grey and white matter, formation of which is may be accompanied by acute episodes of neurological dysfunction or relapse  accordingly, morphological measurements derived from magnetic resonance imaging scans are extensively utilized to monitor disease progression the link between brain volume loss and the evolution of motor and cognitive disability in ms is well establishe  estimation of brain atrophy is therefore considered an important surrogate for, and predictor of, clinical disabilit  although the t1 intensity of white matter lesions may vary according to the severity of tissue injury, similarity with grey matter intensities may result in erroneous substructure volume measurements misclassification of ms lesions therefore generates biased white and grey matter volume estimations, and necessitates the development of effective means to address the impact of lesions on morphological analysi  imaging processing methods currently use filling algorithms to inpaint lesions with normal appearing white matter-like intensitie  both local and global white matter lesion inpainting methods have been proposed local inpainting methods base filling algorithms on surrounding tissue, whilst global methods employ an average intensity derived from the whole brain white matte  more recent models utilize a normal tissue intensity distributio  by sampling the 2 distribution, the lesion is filled with the most probable white matter intensit  based on this approach, magon et a  proposed slice by slice inpainting of the whole brain and valverde et a  performed lesion filling with mean intensity of two periplaque nawm voxel  the above approaches assume that lesions should, without exception, be filled with white matter like intensitie  this hypothesis is flawed, as lesions may occur within grey matter or overlap white and adjacent grey matte  additionally, lesion masks are generally provided by either manual labeling or automated method  automated lesion segmentation often results in misclassifications, and in some cases generates lesions that inappropriately involve the cs  in such circumstances, the aforementioned lesion inpainting methods fai  rather than filling lesions with white matter like intensity only, battaglini et a  inpainted lesions with either white matter or grey matter using a histogram derived from peri-lesional white and grey matter, resulting in improved blending with neighbourhood normal appearing tissue  in the method described by guizard et a lesions are first pre-filled with the median of intensity from surrounding normal-appearing tissues, and the most similar patches calculated using only the nearby region  the same group subsequently proposed an iterative, concentric patch-based filling approach to preserve local anatomical information similarly, prados et a  use neighbourhood patches to fill lesions with the most appropriate intensity; and exploit a minimal kernel-based convolution to achieve better inpainting effect  in the general computer vision community, various inpainting techniques have been proposed to remove objects, texts or scratches from images using the remaining information in the imag  examples include an onion-peel strategy that fills regions of interest from an outermost to innermost concentric ring with reference to available patches ; an exemplar-based method that fills specific regions by directly copying similar patches extracted from the whole image; and synthesis of roi intensities with textures from matched patches by applying a non-local means algorithm advances in deep learning have significantly enhanced inpainting effect  initial deep learning based inpainting frameworks, in which networks are trained to fill roi contents based on its surroundings, have been described by a number of groups these techniques suffer from both pixel-wise reconstruction loss and adversarial loss, and tend to generate artifacts between inpainted and neighbourhood area  however, increasing convolutional layers and applying optimization are computationally intensive and are substantially more time consumin  liu et a  have recently described a model that uses so-called partial convolution  the network takes both the t1 brain image and associated lesion mask as inputs, and then outputs inpainted result  in the training stage, the lesion areas are randomly synthesized to generate training mask  two non-local blocks are inserted after pconv13 and pconv14, respectivel  based features into loss function based on the superiority of deep learning based techniques for inpainting of non-medical color images, we explore ms lesion inpainting under this framework therefore, when designing deep learning frameworks for image roi inpainting, very few assumptions can be mad  by contrast, the relative uniformity of brain structure and appearance among individuals permits the integration of a deep learning framework with a non-local module to facilitate brain lesion inpaintin  the non-local module links one brain voxel with spatially long range brain voxel  hence, the response of one voxel is a weighted sum of the features from all relevant local and distant voxel  by modeling long range dependencies, the algorithm is able to inpaint lesion areas with reference to the whole brain area rather than only the local neighbourhoo  in other words, the algorithm is able to maintain normal tissue architecture rather than simply filling lesion masks rois with white matter like intensit  synthetic lesions can be generated by non-linear co-registration of real flair image derived ms lesion masks with healthy subject mri t1 images to validate the technique and provide a readymade ground truth for comparative experiment  herein, we demonstrate the effectiveness and effciency of ms lesion inpainting using these methods", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Material and methods", "Text": "our proposed inpainting model is stacked by partial convolutions and simultaneously integrates several non-local block  we first explain concepts of partial convolution and non-local block, and then introduce network architecture and implementatio ", "Subsections": [{"Section_Num": "2_1", "Section": "2.1 Partial convolution", "Text": "in general, the partial convolution operation proposed by liu et a  consists of two parts: a partial convolutional layer and mask updatin  by taking mask m into account, the output of the partial convolution only relies on unmasked value ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2_2", "Section": "2.2 Non-Local block", "Text": "traditional convolution operations focus on the cues within the roi neighbourhood and thus ignore global informatio  even with a large receptive field, convolution operations are still based on local informatio  in contrast, the non-local module aims to perform inpainting by taking long distance cues into consideratio  the self-attention module subsequently requires f and g for attention map calculatio  note that the whitest regions in the grayscale attention map are best correlated to the queries and vice vers  despite differing resolution between the attention map and testing slice, the whitest regions within attention maps are appropriately located in regions that correspond to the respective querie  the attended areas all are within the global domain rather than a local regio  here, aji is namely the softmax operation along the dimension   evidently, the softmax operation here measures the similarity between ith and jth locations in   this is to say that the self-attention module computes a response at a location by attending all positions and taking their weighted average in an embedding spac  after gradually updating ws, more non-local evidence is considered in the training and future inpaintin  in our model, the non-local block is integrated with a partial convolution laye  as shown in the first row of fi  in the second row, the corresponding attention maps of the color coded query dots are displaye  meanwhile, the most-attended regions are the whitest areas in the attention map and vice vers  though the size of the attention map and input slice are discrepant, the attended area approximates its expected anatomic counterpar  ms dat  lesion mask generated using a semi-automated thresholdhing technique by an expert neuroimaging analyst using coregistered flair image  inpainting results without and with nonlocal modul  lesion inpainting is affected by proximity to the lateral ventricles without the non-local modul  in contrast, lesion-filling results in this region are cleaner and more realistic with the non-local modul  attended regions for grey matter queries are generally located at the periphery of the brain in areas that approximate the location of the cortex; and attended regions for background queries are consistently located outside the brain area in attention map  moreover, fi  4 illustrates the inpainitng effects with and without the non-local modul ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2_3", "Section": "2.3 Network design and implementation", "Text": "our network is based on the architecture described by liu et a and exploits partial convolutional layers to stack the network rather than traditional convolutional layer  both the encoder and decoder in the network has 8 layer  the feature maps and binary masks from the encoder stage are concatenated by skip links into the decoder stag  furthermore, the size of the binary mask is exactly matched with the respective input brain slice and will be updated in the following encoder and decoder stage  however, the integration of a non-local module into the network is computationally intensiv  meanwhile, in e  in our case, the stride of max pooling layer is   non-local blocks should be inserted in the decoder stage, where they are closer to the output of the networ  the more non-local blocks embedded within the network, the more accurate the inpainting effects ar  based on e  though some techniques are applied to speed up computation, the maximum number of added non-local blocks is   pconv15 is physically the closest layer to the output layer but this layer's feature maps are relatively larger than those relating to pconv13 and pconv1  therefore, computational effciency is improved by not inserting a non-local block after pconv15, while not sacrificing model accuracy at same tim  the training set contains approximately 50800 slices; the initial learning rate is set as  00005 and the batch size as 1  the learning rate for fine-tuning is set as   there are approximately 125 and 10 epochs for training and fine-tuning respectivel ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2_4", "Section": "2.4 Loss function", "Text": "the output of the above network ioutput is fed into the following loss function for more accurate per-pixel reconstruction, where the loss function is defined according to liu et a 1ltv, where lvalid and lhole are the loss terms aiming to enhance per-pixel reconstruction accuracy of the whole brain slice including any inpainted regio  pconv: partial convolution layer concat: concatenate the previous nearest neighbour upsampled results with feature maps from corresponding pconv in encoder stag  two non-local blocks are inserted directly after pconv13 and pconv14, respectivel  in addition to the direct output for pixel wise reconstruction, lperceptual and lstyle are defined to ensure that the inpainted areas have a more realistic and natural texture and structur  here, icomp is actually iout but with non-lesion area set to ground trut  the layers pool1, pool2, pool3 in vgg-16 are utilized for feature extraction in our wor  the 3d median filter is applied to alleviate minor inconsistency between slice  lastly, ltv represents the total variation term for smoothing penalt  nicomp is the number of voxels in icom  we adopt the same hyperparameter values of loss term weights as described by liu et a ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2_5", "Section": "2.5 Data", "Text": "three different mri datasets were used in training and testing stage  ixi dataset: a publically available collection of 665 mr images from normal, healthy subjects the ixi data was acquired on three different mri scanners here, we use only t1 images, but our method can be applied to other mri sequence  brain volumes were randomly selected for training and testing all brain volumes were intensity normalized with mean and variance before training and testin  synthetic lesions, generated by non-linear co-registration with ms lesion maskswere applied to the 50 selected testing volumes and regarded as ground truth for subsequent evaluatio  the second dataset consists of mri scans from 20 patients with relapsingremitting multiple sclerosis2ms, flip angle 12, pixel spacing 1m  acquisition matrix is 256*256, which results in 1mm isotropic acquisition voxel siz  the reconstruction matrix is 256*25  this data, which has no ground truth, was utilized for demonstration in fi  3 and fi  the third dataset contains mri scans from 50 patients with rrms and is primarily exploited for deriving lesions masks for subsequent generation of synthetic lesions on t1 images from the ixi healthy control testing datase  acquisition matrix is 256*24 1ms, ti 900ms, pixel spacing 1m  acquisition matrix is 256*256, which results in 1mm isotropic acquisition voxel siz  the reconstruction matrix is 256*25  all lesion rois for ms cases were performed semiautomatically on flair images using jim 0 by an experienced ms neuroimaging analys  all datasets were acquired cross-sectionally and included both female and male subject ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2_6", "Section": "2.6 Data and code availability statement", "Text": "code developed in the course of this work and aggregated data are available on direct request to the corresponding author and may be re-used with the authors permission and acknowledgemen ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Results", "Text": " to the best of our knowledge, mmlf is the most recently described lesion filling method in the medical image fiel  pc based inpainting method is considered state-of-the art in the computer vision communit  none of these methods rely on lesion area information to generate inpainted voxel  therefore, given synthetic lesion masks, the values of synthetic lesion area were set as zeros before inpaintin ", "Subsections": [{"Section_Num": "3_1", "Section": "3.1 Qualitative evaluation", "Text": "in this experiment, white matter lesions masks derived from ms flair images were used to create and inpaint synthetic lesions generated on non-linearly coregistered t1 images from healthy control  since visible t1 hypointensities are usually a subset of t2 hyperintensities, the flair-derived manually labelled lesion mask may involve isointense regions on matching ms t1 image  non-linear co-registration of ms lesion masks and healthy control t1 images is also likely to produce synthetic lesions that erroneously overlap with tissues other than white matte  inpainted images from all 4 methods were visually inspected and blindly rated by a neuroimaging expert based on similarity to the ground truth image  the nlpc method best approximated the ground truth in the majority of cases reviewe  in the remaining 22/50 11 lesion roi fsl pc mmlf nlpc figure 5: the sagittal and zoomed views of the t1 slice with embedded synthetic lesion  note the noisy samples generated by fsl and pc methods, the blurred boundaries obtained by mmlf metho  cases, mmlf was chosen as the method that best approximated the ground trut  fi  6 and fi  7 provides examples of visual comparison of our inpainting method with existing method ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Discussion", "Text": " blinded qualitative assessment confirmed the superiority of our method for inpainting synthetic lesions embedded in healthy control data the proposed method is therefore not affected by neighbouring structures; rather, it learns to fill local lesions by referring to global structures and texture informatio  as illustrated in fig  quantitative analysis showed that the proposed method generated the smallest mse and therefore the most realistic inpainting result of all four technique  moreover, me and mae values show that the estimation of 15 gm and wm volumes using sienax analysis of t1 brain volumes inpainted by our method are most accurat  during training, our method is inclined to focus on learning the feature characteristics of wm and g  this is because csf takes up much smaller portion of brain compared to wm and g  as a result, synthetic lesions that overlap csf may have a greater likelihood of being filled with white matterlike tissue intensities/texture, rather than cs  it is unlikely that grey matter tissue-like intensities will be generated by our model, since the non-local module will judge that csf regions to more closely approximate white matter than grey matte  in terms of their texture, the noisy samples were potentially more likely to be classified as csf by comparison with the other method  therefore, fsl and pc performed best in terms of csf volume estimatio  however, the portion of the synthetic lesion mask that presents within the csf is extremely small and unlikely to influence the gm, wm evaluatio  the use of our model in real ms images, where the lesion mask will never overlap csf, also ameliorates this concer  our training data comprised brain images derived from healthy control  brain atrophy rates in untreated patients with ms are significantly higher than age-matched healthy control  however, for the purpose of lesion inpainting in ms cases, the trained non-local attention globally searches within non-lesion areas by measuring feature and structural similarity between non-lesion and lesion area  referencing the global non-lesion areas, our model subsequently fills lesion areas with healthy tissue-like texture  lesion inpainting using this method is therefore unaffected by the presence or absence of brain volume los ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Conclusion", "Text": "we propose a new deep learning model for realistic lesion inpainting in mri slices by exploiting the non-local modul  rather than considering the neighbourhood texture for lesion filling, our method inpaints lesion areas with the most plausible intensities by observing the structure and texture information contained within the whole imag  to achieve this, the non-local module compares similarities between the features within lesion regions and remaining brain area  although some existing methods take into account non-local information, they are limited by algorithms that search for similar patches within a constrained bounding box, rather than the whole domai  as such, these methods prove inferior to those described in this wor  moreover, integrating the non-local module under the framework of deep learning generates an effective algorithm that outperforms traditional non-local based methods and can be incorporated into image analysis pipelines for more accurate quantitative assessment of brain substructure volumes", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "com, yalian li@alibaba-in com, psyu@ui edu abstract being able to automatically discover synonymous entities in an open-world setting benefits various tasks such as entity disambiguation or knowledge graph canonicalizatio  existing works either only utilize entity features, or rely on structured annotations from a single piece of context where the entity is mentione  to leverage diverse contexts where entities are mentioned, in this paper, we generalize the distributional hypothesis to a multi-context setting and propose a synonym discovery framework that detects entity synonyms from free-text corpora with considerations on effectiveness and robustnes  as one of the key components in synonym discovery, we introduce a neural network model synonymnet to determine whether or not two given entities are synonym with each othe  instead of using entities features, synonymnet makes use of multiple pieces of contexts in which the entity is mentioned, and compares the context-level similarity via a bilateral matching schem 16% improvement in terms of area under the curve and  19% in terms of mean average precision compared to the best baseline metho  code and data are available ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "discovering synonymous entities from a massive corpus is an indispensable task in automated knowledge discover  for each entity, its synonyms refer to the entities that can be used interchangeably under certain context  for example, clogged nose and nasal congestion are synonyms relative to the context in which they are mentione  given two entities, the synonym discovery task determines how likely these two entities are synonym with each othe  the *work done while at the university of illinois at chicago 1https://githu com/czhang99/synonymnet main goal of synonym discovery is to learn a metric that distinguishes synonym entities from non-synonym one  the synonym discovery task is challenging to deal with for the following reason  first of all, entities are expressed with variation a/united states of america/united states/  refer to the same idea but are expressed quite differentl  recent works on synonym discovery focus on learning the similarity from entities and their character-level features these methods work well for synonyms that share a lot of characterlevel features like airplane/aeroplane or an entity and its abbreviation like acquired immune deficiency syndrome/aid  nasal congestion mentioned in medical book  with only character-level features being used, these models hardly obtain the ability to discriminate entities that share similar semantics but are not alike verbati  secondly, the nature of synonym discovery tasks in real-world scenarios makes it common yet more difficult under an open-world setting: new entities and synonyms emerge and need to be discovered from the text corpor  context information helps indicate entity synonymit  the distributional semantics theory hypothesizes that the meaning of an entity can be reflected by its neighboring words in the tex  context, as a snippet of natural language sentence, is semantically structure  some existing models encode the semantic structures in contexts implicitly during the entity representation learning process the entity representations embody meaningful semantics: entities with similar contexts are likely to live in proximity in the embedding spac  some other works explicitly incorporate structured annotations to model context  dependency parsing treeuser click informationor signed heterogeneous graphs are introduced as the structured information to help discover synonym  however, structured annotations are time-consuming to obtain and may not even exist in an open-world settin 00056v2 11 may 2020 verse context  a single entity can be mentioned in different contexts, let alone the case for multiple synonymous entitie  previous works on context-based synonym discovery either focus on entity information onlyor a single piece of context for each entity for context matchin  notably, in specific domains such as medical, individuals may provide different context information when mentioning the same entit  thu  using a single piece of context may suffer from noise  incorporating multiple pieces of contexts explicitly for entity matching has the potential to improve both accuracy and robustness, which is less studied in existing work  moreover, it is not practical to assume that multiple pieces of contexts are equally informative to represent the meaning of an entity: a context may contribute differently when being matched to different entitie  thus it is imperative to focus on multiple pieces of contexts with a dynamic matching schem  in light of these challenges, we propose a framework to discover synonym entities from a massive corpus without additional structured annotation  a neural network model synonymnet is proposed to detect entity synonyms based on two given entities via a bilateral matching among multiple pieces of contexts in which each entity appear  a leaky unit is designed to explicitly alleviate the noises from uninformative context during the matching proces  we generate synonym entities that are completely unseen during training in the experiment  synonymnet generalizes the distributional hypothesis to multiple pieces of context ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 SynonymNet", "Text": "we introduce synonymnet, our proposed model that detects whether or not two entities are synonyms to each other based on a bilateral matching between multiple pieces of contexts in which entities appea  figure 1 gives an overview of the proposed mode ", "Subsections": [{"Section_Num": "2_1", "Section": "2.1 Context Retriever", "Text": " the diamonds are entitie  each circle is associated with a piece of context in which an entity appear  synonymnet learns to minimize the loss calculated using multiple pieces of contexts via bilateral matching with leaky unit ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2_2", "Section": "2.2 Context Encoder", "Text": "for the p-th context cp, an encoder tries to learn a continuous vector that represents the contex  we introduce a simple encoder architecture that models contexts for synonym discovery, which learns to encode the local information around the entity from the raw context without utilizing additional structured annotation  it focuses on both forward and backward direction  by doing this, the context encoder summarizes the context while explicitly considers the entity's location in the contex  the encoder itself is not the main focus of this wor ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2_3", "Section": "2.3 Bilateral Matching with Leaky Unit", "Text": " instead of focusing on a single piece of context to determine entity synonymity, we adopt a bilateral matching between multiple pieces of encoded contexts for both accuracy and robustnes  we write the equations for each hp belong to h and gq belong to g for clarit  the matching score matrix m can be obtained by taking softmax on the hwbmgt matrix over certain axis not all contexts are informative during the matching for two given entitie  some contexts may contain intricate contextual information even if they mention the entity explicitl  in this work, we introduce a leaky unit during the bilateral matching, so that uninformative contexts can be routed via the leaky unit rather than forced to be matched with any informative context  we adopt the later design for simplicit ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2_4", "Section": "2.4 Context Aggregation", "Text": "the informativeness of a context for an entity should not be a fixed value: it heavily depends on the other entity and the other entity's contexts that we are comparing wit  the bilateral matching scores indicate the matching among multiple pieces of encoded contexts for two entitie  here the intuition is that the informativeness of a piece of context for one entity is characterized by how much it can be matched with the most similar context for the other entit  however, as the leaky unit and its matching score are not used for aggregation -scores on informative contexts become more salient during context aggregatio ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2_5", "Section": "2.5 Training Objectives", "Text": "we introduce two architectures for training the synonymnet: a siamese architecture and a triplet architectur  cosine similarity, and m is the margin valu  l+ decreases monotonically as the similarity score becomes higher within the range of otherwise l- increases as s becomes large  triplet architecture the siamese loss makes the model assign rational pairs with absolute high scores and irrational ones with low scores, while the rationality of entity synonymity could be dynamic based on entities and context ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2_6", "Section": "2.6 Inference", "Text": "the objective of the inference phase is to discover synonym entities for a given query entity from the corpus effectivel  we utilize context-aware word representations to obtain candidate entities that narrow down the search spac  the synonymnet verifies entity synonymity by assigning a synonym score for two entities based on multiple pieces of context  the overall framework is described in figure   when given a query entity e, it is tedious and very ineffective to verify its synonymity with all the other possible entitie  in the first step, we train entity representation unsupervisely from the massive corpus d using methods such as skipgram or glove although these unsupervised methods utilize the context information to learn semantically meaningful representations for entities, they are not tailored for the entity synonym discovery tas  for example, nba championship, chicago black hawks and american league championship series have similar representations because they tend to share some similar neighboring word  but they are not synonyms with each othe  however, they do serve as an effective way to obtain candidates because they tend to give entities with similar neighboring context words similar representation  in the second step, we construct a candidate entity list enn by finding nearest neighbors of a query entity e in the entity embedding space of rdembe  ranking entities by their embedding proximities with the query entity significantly narrows down the search space for synonym discover  in the last step, synonymnet calculates a score s based on the bilateral matching with leaky units over multiple pieces of context ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Experiments", "Text": "1 experiment setup datasets three datasets are prepared to show the effectiveness of the proposed model on synonym discover  the wiki dataset contains  8m documents from wikipedia2 with generic synonym entities obtained from freebase  the pubmed is an english dataset where  82m research paper abstracts are collected from pubmed4 and umls5 contains existing entity synonym information in the medical domai  2https://ww wikipedi org/ 3https://developer googl com/freebase 4https://ww ncb nl ni gov/pubmed 5https://ww nl ni  the wiki + freebase and pubmed + umls are public available datasets used in previous synonym discovery tasks the medbook is a chinese dataset collected by authors where we collect  51m pieces of contexts from chinese medical textbooks as well as online medical question answering forum  synonym entities in the medical domain are obtained from mkg, a medical knowledge grap  table 1 shows the dataset statistic  preprocessing wiki +freebase and pubmed + umls come with entities and synonym entity annotations, we adopt the stanford corenlp package to do the tokenizatio  for medbook, a chinese word segmentation tool jieba is used to segment the corpus into meaningful phrase  we remove redundant contexts in the corpus and filter out entities if they appear in the corpus less than five time  for entity representations, the proposed model works with various unsupervised word embedding method  here for simplicity, we adopt 200dimensional word vectors using skip-gram context window is set as 5 with a negative sampling of 5 words for trainin  evaluation metrics for synonym detection using synonymnet and other alternatives, we train the models with existing synonym and randomly sampled entity pairs as negative sample  during testing, we also sample random entity pairs as negative samples to evaluate the performanc  note that all test synonym entities are from unobserved groups of synonym entities: none of the test entities is observed in the training dat  the area under the curve and mean average precision are used to evaluate the mode  a single-tailed t-test is conducted to evaluate the significance of performance improvements when we compare the proposed synonymnet model with all the other baseline  we expect candidate entities in the top positions are more likely to be synonym with the query entit  baselines we compare the proposed model with the following alternative  word2vec : a word embedding approach based on entity representations learned from the skip-gram algorith  we use the learned word embedding to train a classifier for synonym discover  a scoring function scored = xuwxt v is used as the objectiv  glove : another word embedding approac  the entity representations are learned based on the glove algorith 9673 w/o leaky unit  9651 with bi-lstm encoder  9230 w/o leaky unit  9214 with bi-lstm encoder  8867 table 2: test performance in auc and map on three dataset  indicates the significant improvement over all baselines same scoring function scored, but with the learned glove embedding for synonym discover  srn : a character-level approach that uses a siamese multilayer bi-directional recurrent neural networks to encode the entity as a sequence of character  the hidden states are averaged to get an entity representatio  cosine similarity is used in the objectiv  malstm : another character-level approac  we adopt malstm by feeding the character-level sequence to the mode  unlike srn that uses bi-lstm, malstm uses a single direction lstm and l-1 norm is used to measure the distance between two entitie  dpe : a model that utilizes dependency parsing results as the structured annotation on a single piece of context for synonym discover ", "Subsections": [{"Section_Num": "3_2", "Section": "3.2 Performance Evaluation", "Text": "we report area under the curve and mean average precision in table   from the upper part of table 2 we can see that synonymnet performances consistently better than those from baselines on three dataset  word2vec is generally performing better than glov  srns achieve decent performance on pubmed + umls and medbook + mk  this is probably because the synonym entities obtained from the medical domain tend to share more character-level similarities, such as 6-aminohexanoic acid and aminocaproic aci  dpe has the best performance among other baselines, by annotating each piece of context with dependency parsing result  we conduct statistical significance tests to validate the performance improvemen  the single-tailed t-test is performed for all experiments, which measures whether or not the results from the proposed model are significantly better than ones from baseline  table 3 shows a case for entity ung  in the upper part of table 3, candidate entities are generated with nearest neighbor search on pretrained word embeddings using skip-gra 8 is use  candidate entities cosine similarity united nations general assembly|| 847374 un human rights council  823727 the united nations general assembly  813736 un security council|| 794973 palestine national council  791135 world health assembly|| 790837 united nations security council|| 787999 general assembly resolution  784581 the un security council  777627 north atlantic council|| 773064 non-binding resolution|| 770623 final entities synonymnet score united nations general assembly|| 842602 the united nations general assembly  800719 table 3: candidate entities retrieved using nearest neighbors on word2vec and the discovered synonym entities using synonymnet for unga ablation study to study the contribution of different modules of synonymnet for synonym discovery, we also report ablation test results in the lower part of table   with bi-lstm encoder uses bi-lstm as the context encode  from the lower part of table 2 we can see that both modules contribute to the effectiveness of the mode  the leaky unit contributes  72% improvement in auc and  61% improvement in map on the wiki dataset when trained with the triplet objectiv  the context encoder gives the model an average of  17% improvement in ma  hyperparameters we train the proposed model with a wide range of hyperparameter configurations, as shown in table   each piece of context is chunked by a maximum length of   for the context encoder, we vary the hidden dimension dce from 8 to 102  the margin value m in triplet loss function is varied from   for the training, we try different optimizers, vary batch sizes and learning rate 2360 table 4: performance on synonym discover  on the validation dataset, listed in table  0001 table 6: hyperparameter  figure 3 shows the performance curves when we vary one hyperparameter while keeping the remaining fixe  as the number of contexts p increases, the model generally performs bette  due to limitations on computing resources, we are only able to verify the performance of up to 20 pieces of randomly sampled context 980 value auc map  98 value auc map figure 3: sensitivity analysi ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Related Works", "Text": "synonym discovery the synonym discovery focuses on detecting entity synonym  most existing works try to achieve this goal by learning from structured information such as query logs while in this work, we focus on synonym discovery from free-text natural language contexts, which requires less annotation and is more challengin  some existing works try to detect entity synonyms by entity-level similarities for example, distributional features are introduced in for hypernym detectio  character-level encoding approaches such as treat each entity as a sequence of characters, and use a bi-lstm to encode the entity informatio  such approach may be helpful for synonyms with similar spellings, or abbreviation  without considering the context information, it is hard for the aforementioned methods to infer synonyms that share similar semantics but are not alike verbati  various approaches are proposed to incorporate context information to characterize entity mention  these models are not designed for synonym discover  dependency parsing result and manually crafted rules on the contexts are used in as the structured annotations for synonym discover  assume that entities are given as structured records extracted from texts, where each entity record provides contextual information about the entit  the goal is to determine whether two entities are the same by comparing and aligning their attribute  we discover synonym entities without such structured annotation  sentence matching there is another related research area that studies sentence matchin  early works try to learn a meaningful single vector to represent the sentence dssm style convolution encoders are adopted in to learn sentence representation  they utilize user click-through data and learn query/document embeddings for information retrieval and web search ranking task  although the above methods achieve decent performance on sentence-level matching, the sentence matching task is different from context modeling for synonym discovery in essenc  matching schemes on multiple instances with varying granularities are introduced in however, these models do not consider the word-level interactions from two sentences during the matchin  sentence matching models do not explicitly deal with uninformative instance  in context matching, missing such property could be unsatisfactory as noisy contexts exist among multiple contexts for an entit  we adopt a bilateral matching which involves a leaky unit to explicitly deal with uninformative contexts while preserving the expression diversity from multiple pieces of context ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Conclusions", "Text": "in this paper, we propose a framework for synonym discovery from free-text corpus in an open-world settin  a novel neural network model synonymnet is introduced for synonym detection, which tries to determine whether or not two given entities are synonym with each othe  synonymnet makes use of multiple pieces of contexts in which each entity is mentioned, and compares the context-level similarity via a bilateral matching schema to determine synonymit  experiments on three real-world datasets show that the proposed method synonymnet has the ability to discover synonym entities effectively on both generic and domain-specific datasets with an improvement up to  16% in auc and  19% in ma  acknowledgments we thank the reviewers for their valuable comment ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Relativistic", "Section": "Relativistic X-ray jets at high redshift", "Text": " email: dschwartz@cf harvar edu powerful radio sources and quasars emit relativistic jets of plasma and magnetic fields that travel hundreds of kilo-parsecs, ultimately depositing energy into the intraor inter-cluster mediu  the microwave energy density is also enhanced by a factor 4, which becomes important at large redshift  out of the first 12 objects observed, there are two clear cases of the x-rays extending beyond the detectable radio jet", "Subsections": [{"Section_Num": "1", "Section": "1 Introduction", "Text": " x-ray observations will play a crucial role in delineating this development of structure in the univers  the redshifts of those quasars ranged from   the x-ray emission is interpreted most simply as radiation from inverse compton up-scattering of the cosmic microwave background photons within a kpc-scale jet that is moving relativistically with respect to the co-moving frame of the parent quasa  the bulk relativistic motion explains the one-sided nature of the jets via doppler boosting, and is indicated by apparent superluminal motion and by radio brightness temperatures in excess of 1012 k on pc and sub-pc scale  in the presence of magnetic fields and photons, relativistic electrons will scatter offboth, emitting synchrotron radiation and ic radiatio  the dominant energy loss for the electrons simply depends on the energy density of the target fields or photon  arxiv:190  the scattered radiation is highly anisotropic in the frame of an observer that is nearly at rest with respect to the cm  at lower redshifts the ic/cmb mechanism is not usually dominan  x-rays from fr i type radio jets are best explained as an extension of the radio synchrotron spectrum for several quasars the gamma-rays which are predicted by ic/cmb fall above upper limits from the fermi gamma-ray telescope figure 1 plots the magnetic field strength v  redshift at which magnetic and cmb energy densities become equa  jets at high redshift should increasingly manifest as x-ray jets rather than radio jet  two factors favor the x-ray emissio  the first results from the cosmological diminution of surface brightness, which is proportional to -  this reduces the ability to detect extended radio source  however, for ic/cmb x-ray emission this is compensated by the 4 increase of the cmb energy densit  the second factor is due to the shorter lifetimes of the electrons emitting ghz synchrotron radiatio  figure 2 shows the lifetime for such figure 1 criteria for ic/cmb radiation to dominate electron energy los  electrons to lose energy via ic/cmb as a function of redshift and of the jet bulk lorentz facto  figure 2 lifetime of electrons emitting approximately 1 kev x-rays via the ic/cmb mechanis  losses due to synchrotron radiation are assumed to be much less than to ic/cm  daniel a schwartz et al 3 the radio source j0730+4049 at z= 50,was discovered with a 1  based on that discovery, and the rationale discussed above, we have undertaken a chandra survey for more systems dominated by x-ray jet ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 The high redshift survey", "Text": " that survey covered the area jointly observed in the first radio survey and optically by the sloan digital sky survey the radio flux was summed from the total system, including any extended emissio  of the quasars, 61 showed extended emissio  thirty of these were classified as triples, and as such we considered that they were probably not beamed in our directio  the two at the largest redshifts, j1430+4204 at z= 7 and j1510+5702 at z= 9 counts ks-1 and  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Detection of X-ray jets", "Text": "table 1 shows the sample of 14 objects, plus the two previously detecte  the first two columns give an abbreviated source name, and the redshif  in column 3 we predicted the counts expected in the  5-7 kev band if their flux scaled from that of j1430+4204 and j1510+5702 simply by the ratio of 4 for their redshift  the counting rates of those two sources previously detected do have a ratio very nearly   we expected this number to be a conservative mean, with some jets much brighter than predicte  the last column indicates whether an x-ray jet was detecte  for j1405 and j1610 we do have statistically significant detections, with the number of counts as tabulate  for 10 other quasars, we do not have significant indication of a je  for the systems without an apparent jet, the exact upper limit must be carefully considered for each cas 30 14 yes j0833+0959 and j1128+2326 have not yet been observe  that none of the 14 would give a spurious detection to 98% confidenc  that calculation assumes that we have one specific area to search,   we present preliminary results on the two detection 5 to 7 kev x-ray data from our observation of j1405+041  we have superposed contours from our jvla observations centered at   there is no radio emission detected along this line past  91+/- 9 mjy and spectral index  66+/- 2 ghz radio emission, superposed on the pixelated x-ray count  radio contours start at  3 mjy/beam and are logarithmically spaced to the peak of 694 mjy/bea  restoring beam is   x-ray pixels are   quasar peak is 31 counts per pixe  outside the 1 arcsec radius circle centered on the quasar the pixels have at most one coun 9 are expected from background and from scattered quasar photon  from several components in the core which show brightness temperatures in excess of 1012 k they conclude there is relativistic motio  the quasar core has 269 photons, giving an x-ray flux of  2*1046 erg s-  thus the jet x-ray flux is  5 to 7 kev x-ray data from our observation of j1610+181  there is significant x-ray emission along the dashed line that extends from the quasar radio core to an extended radio lob  the 8 counts associated with the jet correspond to a flux of 10-14 erg cm-2 s- 5-7 kev x-ray counts are binned in   the quasar peak has 104 count  more than 2 pixels from the core, pixels have at most one coun  the dashed line at position angle 31  the eight x-ray counts which are along this line constitute a significant detection of a je  no radio emission is detected between the quasar and the lobe in our jvla observations centered at   being just above threshol  the quasar core has 331 x-ray count  this is a measured flux of  6*1046 erg s-  the x-ray jet flux is apparently 2% of the quasar x-ray flu  a 2 mas long jet extends in the same direction in an   our jvla observations centered at  2 ghz show no indication of emission outside of the  2 mjy from the nw lob  the radio lobe and the vlbi structure each show a onesided system, which indicates a relativistic je  we can construct an illustrative ic/cmb model by taking the radio flux density to equal 1 mjy at   we make the assumption that the bulk lorentz factor equals the doppler facto  this is equivalent to assuming the jet is at the largest possible angle to the line of sight for the given doppler facto 4*1046 erg s-  such a jet would be at an angle   daniel a schwartz et al 5 electrons emitting the   the short radio lifetime shows that it is reasonable that only the x-rays are detectabl  with the true radio flux density being less than 1 mjy, both the magnetic field strength and the relativistic particle density will be smaller if we preserve the minimum energy assumptio  in that case the jet must have a larger bulk lorentz factor since the cmb energy density in the jet must be enhanced further in order to produce the same x-ray emissio  for each individual source in table 1 the upper limit may allow that the predicted counts are correc  strictly speaking, we cannot claim that we have discovered jets according to a common definition requiring the length to be at least four times the width for a radio je  we do have definite statistical detection of xray emission extended outside the quasar x-ray/radio cor  the emission is statistically connected to a direction defined a priori by radio observations,  e, extremely unlikely to be a background or foreground source not associated with the syste g, whether it is continuous or even just a single knot of emissio  simionescu et a  made the critical discovery of an x-ray jet from the quasar j0730+4049, that was not coincident with a radio je  we have now discovered two more cases of x-ray jets which are not coincident with underlying radio jets, establishing that such a class of object exist  table 2 compares the observed properties of these three object  the flux is taken in the  5 to 7 kev ban  we convert to surface brightness in units of 10-14 erg cm-2 s-1 arcsec-2 by dividing by table 2 comparison of the x-ray dominated jet  j0730 j1405 j1610 redshift   the tabulated length of each jet, and by an assumed   that width is roughly 2 kpc at redshifts of a fe  the resulting surface brightness is remarkably similar for the three sources, but with up to a factor of two uncertainty for the two new jets reported her  this could indicate similar physical conditions in the three jets, or could just reflect that they are all near the threshold of detection for a 10 ks observatio  these objects are potentially extremely important, because they can be detected in x-rays at whatever large redshift they exist, due to their approximately constant surface brightnes  we look forward to the all sky survey by the erosita instrument on the spectrum x-gamma satellite to provide target system  while erosita does not have the angular resolution to resolve jets, it should result in unidentified blank sky sources which are candidates to be very distant orphan x-ray jet 1 to 10 kev regio  furthermore, lynx with have 800 times larger grasp than chandra and can therefore do surveys of tens of square degrees with similar sensitivit  6 daniel a schwartz et al acknowledgments this research was funded by nasa grant go8-19077x and by nasa contract nas8-03060 to the chandra x-ray cente ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "the quasiconvex envelope of conformally invariant planar energy functions in isotropic hyperelasticity robert   ciarlet on the occasion of his 80th birthda  special cases of our results can be obtained from earlier works by astala et a  and yanmartin@uni-du voss@uni-du  carol i, n ghiba@uai sander@tu-dresde neff@uni-du de 1 arxiv:190 00058v1 31 dec 2018 contents", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "", "Subsections": [{"Section_Num": "1_1", "Section": "1.1 Conformal and quasiconformal mappings", "Text": ".  ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Convexity properties of conformally invariant functions", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Generalized convexity properties and convex envelopes", "Text": "", "Subsections": [{"Section_Num": "3_1", "Section": "3.1 Main result on the quasiconvex envelope", "Text": ". 1 the deviatoric hencky energy2 the squared logarithm of k3 the exponentiated hencky energy4 an energy function related to a result by yan ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Specific relaxation examples and numerical simulations", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "A", "Section": "A The quasiconvex envelope for a class of conformal energies", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "B", "Section": "B Connections to the Gr\u00f6tzsch problem", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "C", "Section": "C The convex envelope of conformally invariant planar energies", "Text": " left- and right-invariance under the special orthogonal group so and invariance under scalin  in this contribution, we consider the quasiconvex envelopes of conformally invariant energies on gl +.  based on our previous results, we provide an explicit formula that allows for a direct computation of the quasiconvex envelope for this class of function  our main result has been tested against a numerical algorithm for computing the polyconvex envelope for a range of parameters, yielding agreement up to computational precisio  we also present direct finite element simulations of the microstructure using a trustregion-multigrid method which show consistent result  in the appendix, we answer two questions by adamowiczand discuss a related relaxation result by dacorogna and koshigoe1 conformal and quasiconformal mappings energy functions of the form are intrinsically linked to conformal geometry and geometric function theory although the riemann mapping theorem states that any non-empty, simply connected open planar domain can be mapped conformally to the unit disc, conformal mappings exhibit aspects of rigidity that make them too restrictive for many interesting application  in particular, since the riemann mapping is uniquely determined by prescribing the function value for three points, conformal mappings are not able to satisfy arbitrary boundary condition  a significantly larger and more flexible class is given by the so-called quasiconformal mappings,   computational approaches for calculating extremal quasiconformal mappings are discussed,   however, the analytical diffculties posed by this problem also motivate the study of integral generalizations of  this representation turns out to be much more convenient and suitable with respect to convexity properties of   in this case, there are infimizing sequences with highly oscillating gradients which converge weaklybut the weak limit is not a minimize  in order to state criteria for the above convexity properties in the special case of conformally invariant functions on gl +, we consider a number of different representations available to express such function  consider the isochoric, conformally invariant s  venant-kirchhoffenergy is quasiconvex according to proposition   venant-kirchhoffenergy and several challenging problems encountered in engineering applications further examples can be found in the generalized convex envelopes are all identical to the classical convex envelope of w, c 6 the results given in the following show that an analogous property holds for conformally invariant energies on gl +.  in order to apply proposition  2 to the computation of generalized convex envelopes, the following invariance property of the rank-one convex envelope will be require 1 main result on the quasiconvex envelope we can now state our main resul  since the representing function h: figure 4 shows two numerical simulations of the microstructure on triangle grids with different resolution  the illustration shows the reference configuration, colored according to the value of the determinant of the deformation gradient the energy level of the configuration on the left is   repeating the computation on a grid with one additional step of uniform refinement leads to the configuration on the right, which has an energy level of   note that the values obtained for the energy level still differ significantly from the expected value of   it is unclear whether the discrepancy is solely due to insuffcient mesh resolution; further numerical investigations on more performant hardware are planned for the futur  the expected energy level was, however, obtained numerically using a modification of an algorithm by bartels for computing the polyconvex envelop 112 with boundary conditions f0 given by for two different mesh resolution  although the number of oscillations is mesh-dependent, macroscopic quantities like volume ratios are mesh-independent; these macroscopic features are predicted by q  according to theorem   the coloring shows the distribution of k, which is essentially constant except near the boundar 2 can equivalently be expressed in terms of the distortion   using corollary 4", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Determining", "Section": "Determining Principal Component Cardinalitythrough thePrinciple of Minimum Description Length ", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190 kie ua, ruslcomp@mai ru", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Abstract", "Text": "given a permutational wreath product sequence of cyclic groups we investigate its minimal generating set, minimal generating set for its commutator and some properties of its commutator subgrou  we strengthen the result of author and construct minimal generating set for wreath product of finite and infinite cyclic groups and direct product of such group  we generalize results of meldrum about commutator subgroup of wreath product because we take in consideration as regular wreath product as well as no regular also commutator of such group and its minimal generating se  also center of such products was investigate  the the morse function f has critical sets with one saddle poin  a new class of wreathcyclic geometrical groups is considere  minimal generating set for this group and for commutator of group are foun  aknowlegmen  we thanks to maksymenko     for formulation of the proble ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Introduction", "Text": "as it was investigated before by   lucchin  according to results by   we consider in role of active group g the cyclic group, also we generalize this wreath product on iterated wreath product of such groups and direct product of wreath products of cyclic group  the similar question for iterated wreath product was studied by bondarenko   in a series of maksymenko    ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Preliminaries", "Text": " denote by d is the minimal number of generators of the group g let g be a grou  the first example of a finite perfect group with cw > 1 was given by isaacs in commutator width of groups, and of elements has proven to be an important property in particular via its connections with stable commutator length and bounded cohomolog  for more deep description of this form we take into account the commutator width ) which was presented in work of muranov this form of commutators of wreath product was used by us for the research of cw, cw and cw 2 the form of commutator presentation was presented by us in form of wreath recursion and commutator width of it was studie  a restriction g|x is called the vertex permutation of g in a vertex  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Main Result", "Text": " denote by ij the orders of ci  last group is isomorphic to one of the fundamental orbital groups of of the morse function   we call such group h by wreathcycli  we make use of rooted and directed automorphisms definitio  an automorphism of t is rooted if all of its vertex permutations that correspond to non-empty words are trivia  be an infinite ray in   but we consider only truncated trees and truncated automorphisms, so for convenience we say rooted automorphism instead of truncated rooted automorphis  directed automorhism figure   rooted automorhism 4 theorem   sushanskyy in also it was used by us in also the first section corresponds to an active group and to crown of wreath product g, the second section is separated with a semicolon to a base of wreath produc  the sections of the base of wreath product are divided into parts by semicolon too and these parts correspond to groups cij which form the base of wreath produc  the l-th section of of a tableau presentation of automorphism beta1 corresponds to portrait of automorphism beta1 on level x  on xl, the sequence has i0i il-1 coordinate  according to beta1 is generator 5 of g   2-base of   recall that g calls k-th base of  e uf8f9 uf8fa uf8f  it is 3-base of   e uf8f9 uf8fa uf8f  if it were a self-similar group, then it would be more usefull to present it in terms of wreath recursion, as the set where beta0 is the rooted automorphis  given a permutational representation of cij we can present our group by wreath recursio  we present beta1 by wreath recursion as beta1 = consider an alternative recursive constructed generating set consisting of nested automorphism beta1 states that are beta2, beta3,betam and the automorphism beta  where beta2 is the state in a vertex of the second level x  denote by |g| an order of   the structure of tableaux are described above in theorem   therefore first generator of g has form and the second generator has form of vector a group a acting on a set x proo  it is just the case because not all element of this subdirect product are independent, since even one element have to be chosen in such way in order that hold  where is a commutator in case cw =   this commutator are formed as product of commutators of rearranged elements of n q i=1 higah-1 abg-1 aba-  we can generate it by generators of form that is in our generating se  the assertion of a theorem on a recursive principle is easily generalized on multiple wreath product of group  we shall consider special case when a passive group of w is a perfect grou  according to the theorem   now we consider no regular wreath product, where active group can be both as infinite as finite and consider a center of such grou  indeed the elements of form will not be changed by action of conjugation of any element from a because any permutation elements coordinate of diagonal of bn does not change i  exampl  exampl  6 application to geometric groups of diffeomorphisms acting on the mebius band the following geometric objects and actions of diffeomorphisms were considered by maksymenko   we are going to investigate an algebraic structure and generators of one group of such typ  assume that at each of its critical point the map f is equivalent to a homogeneous polynomial in two variables without multiple factor  now we specify the object and the construction of orbits under the action the group of diffeomorphism  it is well known that the factor space m/xf has a natural structure of a finite graph, called kronrod-reeb grap  in our case, the diffeomorphisms act on the mebius ban  since the group of diffeomorphisms is infinitely dimensional, we have the connected component  this group is associated with the action of group s/sid on splitting into function level lines   in our case the morse function on m has two local extreme which are points of local maximu  moreover, the morse function f has critical sets with one saddle poin  at these 2 points of maximum, the values of the function are equa  let there are n critical sets xi on   in the similar case for another group and set moreover, this morse function f has critical sets xi on mebius band with one saddle poin  we shall prove that relation is tru  let us prove that relation is tru  let xi denote the number of domain x  consider two set  a value of sign of the xi means the presence of a rotation of doubles or its absenc  we shall to show that there are not else independent relations in   this form follows from form of a semidirect product element  we prove that using relation from and reductions of reciprocals elements we transform any finite non trivial word of fn+1 to form the images of such mapping are canonical words which are form   therefore we found all relation  thus, main property of homomorphism hold  but bauslag-soliter group has only one relatio  so subgroup stabilize all xi of z-space   besides the element will not be changed by action of conjugation of any element from h because any permutation elements coordinate of diagonal of zn does not change i  we consider wreath product with no regular action of active group   also in kernel of action is elements from diagonal of z  remar ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Generators of commutator and commutator width of wreath product", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "6", "Section": "6 Application to geometric groups of diffeomorphisms acting on the Mebius band", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "7", "Section": "7 Conclusion ", "Text": " the minimal generaitng set for wreathcyclic group was constructe ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190 kie ua, ruslcomp@mai ru", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Abstract", "Text": "given a permutational wreath product sequence of cyclic groups we investigate its minimal generating set, minimal generating set for its commutator and some properties of its commutator subgrou  we strengthen the result of author and construct minimal generating set for wreath product of finite and infinite cyclic groups and direct product of such group  we generalize results of meldrum about commutator subgroup of wreath product because we take in consideration as regular wreath product as well as no regular also commutator of such group and its minimal generating se  also center of such products was investigate  the the morse function f has critical sets with one saddle poin  a new class of wreathcyclic geometrical groups is considere  minimal generating set for this group and for commutator of group are foun  aknowlegmen  we thanks to maksymenko     for formulation of the proble ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Introduction", "Text": "as it was investigated before by   lucchin  according to results by   we consider in role of active group g the cyclic group, also we generalize this wreath product on iterated wreath product of such groups and direct product of wreath products of cyclic group  the similar question for iterated wreath product was studied by bondarenko   in a series of maksymenko    ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Preliminaries", "Text": " denote by d is the minimal number of generators of the group g let g be a grou  the first example of a finite perfect group with cw > 1 was given by isaacs in commutator width of groups, and of elements has proven to be an important property in particular via its connections with stable commutator length and bounded cohomolog  for more deep description of this form we take into account the commutator width ) which was presented in work of muranov this form of commutators of wreath product was used by us for the research of cw, cw and cw 2 the form of commutator presentation was presented by us in form of wreath recursion and commutator width of it was studie  a restriction g|x is called the vertex permutation of g in a vertex  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Main Result", "Text": " denote by ij the orders of ci  last group is isomorphic to one of the fundamental orbital groups of of the morse function   we call such group h by wreathcycli  we make use of rooted and directed automorphisms definitio  an automorphism of t is rooted if all of its vertex permutations that correspond to non-empty words are trivia  be an infinite ray in   but we consider only truncated trees and truncated automorphisms, so for convenience we say rooted automorphism instead of truncated rooted automorphis  directed automorhism figure   rooted automorhism 4 theorem   sushanskyy in also it was used by us in also the first section corresponds to an active group and to crown of wreath product g, the second section is separated with a semicolon to a base of wreath produc  the sections of the base of wreath product are divided into parts by semicolon too and these parts correspond to groups cij which form the base of wreath produc  the l-th section of of a tableau presentation of automorphism beta1 corresponds to portrait of automorphism beta1 on level x  on xl, the sequence has i0i il-1 coordinate  according to beta1 is generator 5 of g   2-base of   recall that g calls k-th base of  e uf8f9 uf8fa uf8f  it is 3-base of   e uf8f9 uf8fa uf8f  if it were a self-similar group, then it would be more usefull to present it in terms of wreath recursion, as the set where beta0 is the rooted automorphis  given a permutational representation of cij we can present our group by wreath recursio  we present beta1 by wreath recursion as beta1 = consider an alternative recursive constructed generating set consisting of nested automorphism beta1 states that are beta2, beta3,betam and the automorphism beta  where beta2 is the state in a vertex of the second level x  denote by |g| an order of   the structure of tableaux are described above in theorem   therefore first generator of g has form and the second generator has form of vector a group a acting on a set x proo  it is just the case because not all element of this subdirect product are independent, since even one element have to be chosen in such way in order that hold  where is a commutator in case cw =   this commutator are formed as product of commutators of rearranged elements of n q i=1 higah-1 abg-1 aba-  we can generate it by generators of form that is in our generating se  the assertion of a theorem on a recursive principle is easily generalized on multiple wreath product of group  we shall consider special case when a passive group of w is a perfect grou  according to the theorem   now we consider no regular wreath product, where active group can be both as infinite as finite and consider a center of such grou  indeed the elements of form will not be changed by action of conjugation of any element from a because any permutation elements coordinate of diagonal of bn does not change i  exampl  exampl  6 application to geometric groups of diffeomorphisms acting on the mebius band the following geometric objects and actions of diffeomorphisms were considered by maksymenko   we are going to investigate an algebraic structure and generators of one group of such typ  assume that at each of its critical point the map f is equivalent to a homogeneous polynomial in two variables without multiple factor  now we specify the object and the construction of orbits under the action the group of diffeomorphism  it is well known that the factor space m/xf has a natural structure of a finite graph, called kronrod-reeb grap  in our case, the diffeomorphisms act on the mebius ban  since the group of diffeomorphisms is infinitely dimensional, we have the connected component  this group is associated with the action of group s/sid on splitting into function level lines   in our case the morse function on m has two local extreme which are points of local maximu  moreover, the morse function f has critical sets with one saddle poin  lines of levels around a local maximum point of f has form of coaxial circles these lines are determined by polynomial of form +/-+  let there are n critical sets xi on   in the similar case for another group and set moreover, this morse function f has critical sets xi on mebius band with one saddle poin  we shall prove that relation is tru  let us prove that relation is tru  let xi denote the number of domain x  consider two set  a value of sign of the xi means the presence of a rotation of doubles or its absenc  we shall to show that there are not else independent relations in   this form follows from form of a semidirect product element  we prove that using relation from and reductions of reciprocals elements we transform any finite non trivial word of fn+1 to form the images of such mapping are canonical words which are form   therefore we found all relation  thus, main property of homomorphism hold  but bauslag-soliter group has only one relatio  so subgroup stabilize all xi of z-space   besides the element will not be changed by action of conjugation of any element from h because any permutation elements coordinate of diagonal of zn does not change i  we consider wreath product with no regular action of active group   also in kernel of action is elements from diagonal of z  the action is defined by shift on finite set xm is not faithfully, and its kernel is isomorphic to m  also in kernel of action is elements from diagonal of znm that is isomorphic to   remar ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Generators of commutator and commutator width of wreath product", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "6", "Section": "6 Application to geometric groups of diffeomorphisms acting on the Mebius band", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "7", "Section": "7 Conclusion ", "Text": " the minimal generaitng set for wreathcyclic group was constructe ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190 kie ua, ruslcomp@mai ru", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Abstract", "Text": "given a permutational wreath product sequence of cyclic groups we investigate its minimal generating set, minimal generating set for its commutator and some properties of its commutator subgrou  we strengthen the result of author and construct minimal generating set for wreath product of finite and infinite cyclic groups and direct product of such group  we generalize results of meldrum about commutator subgroup of wreath product because we take in consideration as regular wreath product as well as no regular also commutator of such group and its minimal generating se  also center of such products was investigate  the the morse function f has critical sets with one saddle poin  a new class of wreathcyclic geometrical groups is considere  minimal generating set for this group and for commutator of group are foun  aknowlegmen  we thanks to antonenko alexandr for a graphical support and sergey maksymenko for morse function descriptio ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Introduction", "Text": "as it was investigated before by   lucchin  according to results by   we consider in role of active group g the cyclic group, also we generalize this wreath product on iterated wreath product of such groups and direct product of wreath products of cyclic group  the similar question for iterated wreath product was studied by bondarenko   in a series of maksymenko    ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Preliminaries", "Text": " denote by d is the minimal number of generators of the group g let g be a grou  the first example of a finite perfect group with cw > 1 was given by isaacs in commutator width of groups, and of elements has proven to be an important property in particular via its connections with stable commutator length and bounded cohomolog  for more deep description of this form we take into account the commutator width ) which was presented in work of muranov this form of commutators of wreath product was used by us for the research of cw, cw and cw 2 the form of commutator presentation was presented by us in form of wreath recursion and commutator width of it was studie  a restriction g|x is called the vertex permutation of g in a vertex  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Main Result", "Text": " denote by ij the orders of ci  last group is isomorphic to one of the fundamental orbital groups of of the morse function   we call such group h by wreathcycli  we make use of rooted and directed automorphisms definitio  an automorphism of t is rooted if all of its vertex permutations that correspond to non-empty words are trivia  be an infinite ray in   but we consider only truncated trees and truncated automorphisms, so for convenience we say rooted automorphism instead of truncated rooted automorphis  directed automorhism figure   rooted automorhism 4 theorem   sushanskyy in also it was used by us in also the first section corresponds to an active group and to crown of wreath product g, the second section is separated with a semicolon to a base of wreath produc  the sections of the base of wreath product are divided into parts by semicolon too and these parts correspond to groups cij which form the base of wreath produc  the l-th section of of a tableau presentation of automorphism beta1 corresponds to portrait of automorphism beta1 on level x  on xl, the sequence has i0i il-1 coordinate  according to beta1 is generator 5 of g   2-base of   recall that g calls k-th base of  e uf8f9 uf8fa uf8f  it is 3-base of   e uf8f9 uf8fa uf8f  if it were a self-similar group, then it would be more usefull to present it in terms of wreath recursion, as the set where beta0 is the rooted automorphis  given a permutational representation of cij we can present our group by wreath recursio  we present beta1 by wreath recursion as beta1 = consider an alternative recursive constructed generating set consisting of nested automorphism beta1 states that are beta2, beta3,betam and the automorphism beta  where beta2 is the state in a vertex of the second level x  denote by |g| an order of   the structure of tableaux are described above in theorem   therefore first generator of g has form and the second generator has form of vector a group a acting on a set x proo  it is just the case because not all element of this subdirect product are independent, since even one element have to be chosen in such way in order that hold  where is a commutator in case cw =   this commutator are formed as product of commutators of rearranged elements of n q i=1 higah-1 abg-1 aba-  we can generate it by generators of form that is in our generating se  the assertion of a theorem on a recursive principle is easily generalized on multiple wreath product of group  we shall consider special case when a passive group of w is a perfect grou  now we consider no regular wreath product, where active group can be both as infinite as finite and consider a center of such grou  indeed the elements of form will not be changed by action of conjugation of any element from a because any permutation elements coordinate of diagonal of bn does not change i  exampl  exampl  6 application to geometric groups of diffeomorphisms acting on the mebius band the following geometric objects and actions of diffeomorphisms were considered by maksymenko   we are going to investigate an algebraic structure and generators of one group of such typ  assume that at each of its critical point the map f is equivalent to a homogeneous polynomial in two variables without multiple factor  now we specify the object and the construction of orbits under the action the group of diffeomorphism  it is well known that the factor space m/xf has a natural structure of a finite graph, called kronrod-reeb grap  in our case, the diffeomorphisms act on the mebius ban  since the group of diffeomorphisms is infinitely dimensional, we have the connected component  this group is associated with the action of group s/sid on splitting into function level lines   in our case the morse function on m has two local extreme which are points of local maximu  moreover, the morse function f has critical sets with one saddle poin  let there are n critical sets xi on   in the similar case for another group and set moreover, this morse function f has critical sets xi on mebius band with one saddle poin  we shall prove that relation is tru  let us prove that relation is tru  let xi denote the number of domain x  consider two set  a value of sign of the xi means the presence of a rotation of doubles or its absenc  we shall to show that there are not else independent relations in   this form follows from form of a semidirect product element  we prove that using relation from and reductions of reciprocals elements we transform any finite non trivial word of fn+1 to form the images of such mapping are canonical words which are form   therefore we found all relation  thus, main property of homomorphism hold  but bauslag-soliter group has only one relatio  so subgroup stabilize all xi of z-space   besides the element will not be changed by action of conjugation of any element from h because any permutation elements coordinate of diagonal of zn does not change i  we consider wreath product with no regular action of active group   also in kernel of action is elements from diagonal of z  the action is defined by shift on finite set xm is not faithfully, and its kernel is isomorphic to m  also in kernel of action is elements from diagonal of znm that is isomorphic to   remar ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Generators of commutator and commutator width of wreath product", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "6", "Section": "6 Application to geometric groups of diffeomorphisms acting on the Mebius band", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "7", "Section": "7 Conclusion ", "Text": " the minimal generaitng set for wreathcyclic group was constructe ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "kie ua, ruslcomp@mai ru 2cardiffuniversity - williamsae13@cardif a uk august 12, 2019 abstract the quotient group of restricted wreath products by its commutator was foun  the generic sets of commutator of wreath product were investigate  given a permutational wreath product sequence of cyclic groups, we investigate its minimal generating set, the minimal generating set for its commutator and some properties of its commutator subgrou  we strengthen the results from the author and construct the minimal generating set for the wreath product of finite and infinite cyclic groups in addition to the direct product of such group  we generalise results of meldrum about commutator subgroup of wreath products since as well as considering regular wreath products, we consider those which are not regular the commutator of such a group, its minimal generating set and the centre of such products has been investigated her  the morse function f has critical sets with one saddle poin  a new class of wreath-cyclic geometrical groups is considere  the minimal generating set for this group and for the commutator of the group are foun  acknowledgement: we thanks to antonenko alexandr for a graphical support and sergey maksymenko for morse function description, also we are grateful to samoilovych  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": " we firstly consider the active group g which is a cyclic group and then generalise this wreath product for iterated wreath products and for the direct product of wreath products of cyclic group  it should be noted that a similar question for iterated wreath product was studied by bondarenko", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Prelimaries", "Text": " we denote by d the minimal number of generators of the group g let g be a grou  the first example of a finite perfect group with cw > 1 was presented by isaacs commutator widths for both groups and of elements has proven to be an important propert  in particular, its connections with stable commutator length and bounded cohomology has become significan  for more deep description of this form we take into account the commutator width ) which was presented in work of muranov this form of commutators of wreath product has been used here to consider cw, cw and cw the form of commutator presentation has been given here in the form of wreath recursion and additionally its commutator width has been studie  a restriction g|x is called the vertex permutation of g in a vertex  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Main Results", "Text": "this work strengthens previous results by the author and will additionally consider a new clas  the last group is isomorphic to one of the fundamental orbital groups of of the morse function   we will call such group h wreath-cycli ", "Subsections": [{"Section_Num": "3_1", "Section": "3.1 Rooted and Directed Automorphisms", "Text": "we will make use of both rooted and directed automorphisms which were introduced by grigorchuk an automorphism of t is rooted if all of its vertex permutations that correspond to non-empty words are trivia  directed automorhism fi  rooted automorhism it should be noted that because we consider only truncated trees and truncated automorphisms here and for convenience, we will say rooted automorphism instead of truncated rooted automorphis  proo im and denote lcmk = lcm similarl  we make use of a presentation of those wreath product elements from a tableaux of kaloujnine the first section corresponds to an active group and the crown of wreath product g, the second section is separated with a semicolon to a base of the wreath produc  the sections of the base of wreath product are divided into parts by semicolon and these parts correspond to groups cij which form the base of wreath produc  the l-th section of of a tableau presentation of automorphism beta1 corresponds to portrait of automorphism beta1 on level x  on xl, the sequence has i0i1 il-1 coordinate  we know from that beta1is generator of g,   2-base of   recall that g calls k-th base of  e uf8f9 uf8fa uf8f  it is precisely a 3-base of   e uf8f9 uf8fa uf8f  if it were a self-similar group, then it would be more useful to present it in terms of wreath recursion, as the set where beta0 is the rooted automorphis  given a permutational representation of cij we can present our group by wreath recursio  we present beta1 by wreath recursion as beta1 = consider an alternative recursive constructed generating set consisting of nested automorphism beta1 states that are beta2, beta  ,betam and the automorphism beta  where beta2 is the state in a vertex of the second level x  denote by |g| an order of   proo  the structure of tableaux are described above in theorem   therefore, the first generator of g has form and the second generator has form of vector", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3_2", "Section": "3.2 Generators of commutator of wreath product", "Text": " let a be a group and b a permutation group,   a group a acting on a set x proo  this is the case because not all element of the subdirect product are independent because the elements must be chosen in such a way that hold  where is a commutator in case cw =   in the general case, we would have cw q j=1 instead of this elemen  this commutator are formed as product of commutators of rearranged elements of n q i=1 higah-1 abg-1 aba-  the assertion of a theorem on a recursive principle is easily generalised on multiple wreath product of group  we shall consider special case when a passive group of w is a perfect grou  more exact upper bound give us theorem   now we consider no regular wreath product, where active group can be both as infinite as finite and consider a centre of such grou xn} be a-spac  proo  indeed the elements of form will not be changed by action of conjugation of any element from a because any permutation elements coordinate of diagonal of bn does not change i  proo  the commutator with shifted coordinate thai is higah-1 abg-1 aba-1, appears on i-th coordinate due to action of   the condition appears because of corollary   so we have one to one correspondenc  in case of the action of z on the n elements in the set is isomorphic to the action of zn elements on this set or the action of the zn elements on itsel  because of not all the elements of this subdirect product is independent, at least one we have to choose so that would be execute  we are going to investigate an algebraic structure and generators of one group of such typ  assume that at each of its critical point the map f is equivalent to a homogeneous polynomial in two variables without multiple factor  now we specify the object and the construction of orbits under the action the group of diffeomorphism  it is well known that the factor space m/xf has a natural structure of a finite graph, called kronrod-reeb grap  since the group of diffeomorphisms is infinitely dimensional, we have the connected component  this group is associated with the action of group s sid on splitting into function level lines   in our case the morse function on m has two local extreme which are points of local maximu  moreover, the morse function f has critical sets with one saddle poin  let there are n critical sets xi on   in the similar case for another group and set we shall prove that relation is tru  let us prove that relation is tru  proo  let xi denote the number of domain x  consider two set  a value of sign of the xi means the presence of a rotation of doubles or its absenc  we shall to show that there are not else independent relations in   this form follows from form of a semidirect product element  we prove that using relation from and reductions of reciprocals elements we transform any finite non trivial word of fn+1 to form the images of such mapping are canonical words which are form   therefore, we found all relations which concludes the proo  these vectors are precisely, but bauslag-soliter group has only one relatio  so subgroup stabilise all xi of z-space   besides the element will not be changed by action of conjugation of any element from h because any permutation elements coordinate of diagonal of zn does not change i  we consider wreath product with no regular action of active group  xn} be z-spac  n  proo xn} is subgroup n  additionally, the kernel of action has elements from the diagonal of z  the action is defined by shift on finite set xm is not faithfully, and its kernel is isomorphic to m  also in kernel of action is elements from diagonal of znm that is isomorphic to  ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Conclusion", "Text": " the minimal generating set for wreath-cyclic groups has been constructe ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "kie ua, ruslcomp@mai ru 2cardiffuniversity - williamsae13@cardif a uk abstract the quotient group of the restricted and unrestricted wreath product by its commutator is foun  the generic sets of commutator of wreath product were investigate  the structure of wreath product with non-faithful group action is investigate  given a permutational wreath product sequence of cyclic groups, we investigate its minimal generating set, the minimal generating set for its commutator and some properties of its commutator subgrou  we strengthen the results from the author and construct the minimal generating set for the wreath product of both finite and infinite cyclic groups, in addition to the direct product of such group  we generalise the results of meldrum   about commutator subgroup of wreath products since, as well as considering regular wreath products, we consider those which are not regular the commutator of such a group, its minimal generating set and the centre of such products has been investigated her  the morse function f has critical sets with one saddle poin  we consider a new class of wreath-cyclic geometrical group  the minimal generating set for this group and for the commutator of the group are foun  acknowledgement: we thanks to antonenko alexandr for a graphical support and sergey maksymenko for morse function description, also we are grateful to samoilovych  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "lucchini   the results of lucchini   we firstly consider the active group g which is cyclic and then generalise this wreath product for both iterated wreath products and for the direct product of wreath products of cyclic group  it should be noted that a similar question for iterated wreath product was studied by bondarenko  00061v13 3 sep 2019 of finite inde ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Preliminaries", "Text": "we denote by d the minimal number of generators of the group g this is equivalent to the assumption that h is invariant each level-set,   let g be a grou  the first example of a finite perfect group with cw > 1 was presented by isaacs   the property of commutator widths for groups and elements has proven to be important and in particular, its connections with stable commutator length and bounded cohomology has become significan  in order to obtain a more detailed description of this form, we take into account the commutator width ) as presented in work of muranov   s2k) and cw the form of commutator presentation has been given here in the form of wreath recursion and additionally, its commutator width has been studie  it should be noted that a restriction g|x is called the vertex permutation of g in a vertex  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Center and commutator subgroup of wreath product", "Text": "this work strengthens previous results by the author and will additionally consider a new class of group  note that the last group here is isomorphic to one of the fundamental orbital groups of of the morse function   we will call such group h wreath-cycli  ", "Subsections": [{"Section_Num": "3_1", "Section": "3.1 Minimal generating set of direct product of wreath product of cyclic groups", "Text": " recall that we denote a truncated tree by   an automorphism of t is said to be rooted if all of its vertex permutations corresponding to non-empty words are trivia  it should be noted that because we consider only truncated trees and truncated automorphisms here and for convenience, we will say rooted automorphism instead of truncated rooted automorphis  directed automorhism fi  rooted automorhism theorem   proo  in a similar fashion, we denote lcmk = lcm similarl  we utilise a presentation of those wreath product elements from a tableaux of kaloujnine   and sushchanskii   and additionally utilised by the author the sections of the base of wreath product are divided into parts by semicolon and these parts correspond to groups cij which form the base of wreath produc  the l-th section of of a tableau presentation of automorphism beta1 corresponds to portrait of automorphism beta1 on level x p on xl, the sequence has i0i1 il-1 coordinate  we know from that beta1is generator of g,   2-base of   recall that g calls k-th base of  e uf8f9 uf8fa uf8f  it is precisely a 3-base of   e uf8f9 uf8fa uf8f  note that if it were a self-similar group, then it would be more useful to present it in terms of wreath recursion, as the set where beta0 is the rooted automorphis  given a permutational representation of cij we can present our group by wreath recursio  we present beta1 by wreath recursion as beta1 = 4 consider an alternative recursive constructed generating set which consists of nested automorphism beta1 states which are beta2, beta  ,betam and the automorphism beta  denote an order of g by |g|.  proo  the structure of tableaux are described above in theorem   therefore, the first generator of g has form and the second generator has form of vector", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3_2", "Section": "3.2 Generators of commutator subgroup and center of wreath product", "Text": " let a be a group and b a permutation group,   a group a acting upon a set x, where the active group a can act not faithfull  proo  this is the case because not all element of the subdirect product are independent because the elements must be chosen in such a way that hold  where is a commutator in case cw =   in the general case, we would have cw q j=1 instead of this elemen  this commutator are formed as product of commutators of rearranged elements of n q i=1 higah-1 abg-1 aba-  the assertion of a theorem on a recursive principle is easily generalized on multiple wreath product of group  we shall consider special case when a passive group of w is a perfect grou  of wiegold   more exact upper bound give us theorem   where c is a constan  now we consider no regular wreath product, where active group can be both as infinite as finite and consider a centre of such grou  this is generalization of theorem  2 from the book because action of a is not non faithfull xn} be a-spac  proo  also every bx: bx belong to   indeed the elements of form will not be changed by action of conjugation of any element from a because any permutation elements coordinate of diagonal of bn does not change i  z acts on x by left shif  also a acts transitively from lef  proo  according to corollary   the commutator with the shifted coordinate higah-1 abg-1 aba-1 appears within the i-th coordinate position due to action of   according to corollary  9 the set of elements satisfying condition forms a commutato  for convenience we present z in additive for  hi+1 = h  that is impossible in the restricted case but possible in the unrestricte  in the scenario when the action of z upon the n elements from the set is isomorphic to the action of zn elements on the set or the action of the zn elements on itsel  studied various different geometric objects and considered the actions of diffeomorphisms on the  we now consider the algebraic structure and the generators for a group of such typ  we assume that at each critical point, the map f is equivalent to a homogeneous polynomial in two variables without multiple factor  we will now specify the object and the construction of orbits under the action of the group diffeomorphism  it is well known within this research domain that the factor space m/xf, has a natural structure of a finite graph and is entitled the kronrod-reeb grap  this group is associated with sid, which is a subgroup of stabiliser elements isotopic to the identity,   this locally trivial bundle of homotopical groups induce an exact sequence of homotopic groups of that bundl  now since the group of diffeomorphisms is infinitely dimensional, we have found the connected component  this group is associated with the action of the group s sid upon splitting into the function level lines   moreover, the morse function f must have critical sets with exactly one saddle poin  determined by the following homogeneous polynomial plus constant x2 -y2 +   there are two points of maximum at a saddle point;   at the two points of maximum, the values of the function are equal,   we know from the results of maksymenko   assume there are n critical sets xi on   the application of such an action results in a surjective epimorphism to a group z, which has the left inverse and arises as a result of splittin  this is in agreement with the work of maksymenko   who considers a similar scenario but for a different group and set proo  let xi denote the number of domains from x  we will now consider two set  similarly, we note that congruence modulo 2n is of interes  the value of the sign of the xi indicates the presence of a rotation of the doubles or its absenc  we show that there are not otherwise independent relations within the group   this form follows from the form of semidirect product element  we now prove using and reductions of reciprocals elements, that we may transform any finite non-trivial word of fn+1 to the form the images of such a mapping are the canonical words which have the form of   the words from the normal closure must therefore have zero sum of powers for each generato  we have therefore found all such relation  which concludes the proo  the generators of the subgroup zn can be presented in the form of vector  these vectors are precisely, it should be noted that the research of maksymenko   one such relation characterises the bauslag-soliter grou  note the bauslag-soliter group has only one relatio  so subgroup stabilise all xi of z-space   besides the element will not be changed by action of conjugation of any element from h because any permutation elements coordinate of diagonal of zn does not change i  we can generalise a result of meldrum   we consider wreath products with no regular actions of the active group  xn} be the z-spac  thus, we observe the following corollary hold  n  proo xn} is the subgroup n  additionally, the kernel of this action has elements from the diagonal of z  note that the action is defined by the shift on the finite set xm is not faithful and its kernel is isomorphic to m  additionally, within this kernel of action is the elements from the diagonal of znm which are isomorphic to  ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Conclusion", "Text": " the minimal generating set for wreath-cyclic groups have been constructe ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "ru 2cardiffuniversity - wiliamsae13@cardif a uk abstract we generalize the results presented in the book of meldrum   about commutator subgroup of wreath products since, as well as considering regular wreath products, we consider those which are not regular the commutator of such a group, its minimal generating set and the centre of such products has been investigated her  the quotient group of the restricted and unrestricted wreath product by its commutator is foun  the generic sets of commutator of wreath product were investigate  the structure of wreath product with non-faithful group action is investigate  given a permutational wreath product sequence of cyclic groups, we investigate its minimal generating set, the minimal generating set for its commutator and some properties of its commutator subgrou  we strengthen the results from the author and construct the minimal generating set for the wreath product of both finite and infinite cyclic groups, in addition to the direct product of such group  the morse function f has critical sets with one saddle poin  we consider a new class of wreath-cyclic geometrical group  the minimal generating set for this group and for the commutator of the group are foun  acknowledgement: we are grateful to antonenko alexandr for a graphical support and sergey maksymenko for morse function description, also we thanks to samoilovych  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "lucchini   the results of lucchini   we firstly consider the active group g which is cyclic and then generalize this wreath product for both iterated wreath products and for the direct product of wreath products of cyclic group  it should be noted that a similar question for iterated wreath product was studied by bondarenko  00061v14 25 sep 2021 of finite inde  all theorems and propositions are obtained and proved by the ruslan skuratovskii, corollaries and examples were obtained in collaboration with the co-autho ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Preliminaries", "Text": "let g be a group and let d denote its minimal number of generators this is equivalent to the assumption that h is invariant each level-set,   the first example of a finite perfect group with cw > 1 was presented by isaacs   the property of commutator widths for groups and elements has proven to be important and in particular, its connections with stable commutator length and bounded cohomology has become significan  in order to obtain a more detailed description of this form, we take into account the commutator width by ) as presented in work of muranov   s2k) and cw the form of commutator presentation has been given here in the form of wreath recursion and additionally, its commutator width has been studie  it should be noted that a restriction g|x is called the vertex permutation of g in a vertex   3 commutator subgroup and center of wreath product with nonfaithful action in this work, we strengthen and continue the previous results of the author and will additionally consider a new class of group  note that the last group here is isomorphic to one of the fundamental orbital groups of of the morse function   we will call such group h wreath-cycli  recall that we denote a truncated tree by   an automorphism of t is said to be rooted if all of its vertex permutations corresponding to non-empty words are trivia  it should be noted that because we consider only truncated trees and truncated automorphisms here and for convenience, we will say rooted automorphism instead of truncated rooted automorphis  directed automorphism fi  rooted automorphism we recall in reformulated form the result of   woryna, about a minimal generating set of iterated wreath produc  also we generalised this result after this theore  proo  in a similar fashion, we denote lcmk = lcm similarl  we utilise a presentation of those wreath product elements from a tableaux of kaloujnine   and sushchanskii   and additionally utilised by the author the first section corresponds to an active group and the crown of wreath product g, the second section is separated with a semicolon to a base of the wreath produc  the sections of the base of wreath product are divided into parts by semicolon and these parts correspond to groups cij which form the base of wreath produc  the l-th section of of a tableau presentation of automorphism beta1 corresponds to portrait of automorphism beta1 on level x p on xl, the sequence has i0i1 il-1 coordinate  we know from that beta1is generator of g,   2-base of   recall that g calls k-th base of  e uf8f9 uf8fa uf8f  it is precisely a 3-base of   e uf8f9 uf8fa uf8f  note that if it were a self-similar group, then it would be more useful to present it in terms of wreath recursion, as the set where beta0 is the rooted automorphis  given a permutational representation of cij we can present our group by wreath recursio  we present beta1 by wreath recursion as beta1 = consider an alternative recursive constructed generating set which consists of nested automorphism beta1 states which are beta2, beta  ,betam and the automorphism beta  denote an order of g by |g|.  proo  the structure of tableaux are described above in theorem   therefore, the first generator of g has form and the second generator has form of vector let a be a group and b a permutation group,   a group a acting upon a set x, where the active group a can act not faithfull  this is the case because not all element of the subdirect product are independent because the elements must be chosen in such a way that hold  where is a commutator in case cw =   in the general case, we would have cw q j=1 instead of this elemen  this commutator are formed as product of commutators of rearranged elements of n q i=1 higah-1 abg-1 aba-  the assertion of a theorem on a recursive principle is easily generalized on multiple wreath product of group  we shall consider special case when a passive group of w is a perfect grou  of wiegold   more exact upper bound give us theorem   as it were studied by   now we consider no regular wreath product, where active group can be both as infinite as finite and consider a centre of such grou  this is generalization of theorem  2 from the book because action of a is not non faithfull xn} be a-spac  recall that the direct product indexed by infinite set consists of all infinite sequences, and the direct sum consists only of sequences with finitely many elements distinct from zer  proo  also every bx: bx belong to   indeed the elements of form will not be changed by action of conjugation of any element from a because any permutation elements coordinate of diagonal of bn does not change i  in case of violation of condition 1) there will be no commutation on the second coordinat  let be infinite cyclic group acting on n-letters alphabe  we denote m-elements set by x  let us prove it applying proposition   we note that in case of action on finite set there is not difference between restricted and unrestricted wreath product  our formula of centre of two groups a acting on x and b wreath product will be used by u  the kernel of action   z acts on x by left shif  also a acts transitively from lef  proo  according to corollary   the commutator with the shifted coordinate higah-1 abg-1 aba-1 appears within the i-th coordinate position due to action of   according to corollary  9 the set of elements satisfying condition forms a commutato  for convenience we present active group in the additive for  that is impossible in the restricted case but possible in the unrestricte  the basic property of homomorphism for generators in canonical form is obviously accomplishe  in the scenario when the action of z upon the n elements from the set is isomorphic to the action of zn elements on the set or the action of the zn elements on itsel  studied various different geometric objects and considered the actions of diffeomorphisms on the  we now consider the algebraic structure and the generators for a group of such typ  we assume that at each critical point, the map f is equivalent to a homogeneous polynomial in two variables without multiple factor  we will now specify the object and the construction of orbits under the action of the group diffeomorphism  it is well known within this research domain that the factor space m/xf, has a natural structure of a finite graph and is entitled the kronrod-reeb grap  this group is associated with sid, which is a subgroup of stabiliser elements isotopic to the identity,   this locally trivial bundle of homotopical groups induce an exact sequence of homotopic groups of that bundl  now since the group of diffeomorphisms is infinitely dimensional, we have found the connected component  this group is associated with the action of the group s sid upon splitting into the function level lines   moreover, the morse function f must have critical sets with exactly one saddle poin  determined by the following homogeneous polynomial plus constant x2 -y2 +   there are two points of maximum at a saddle point;   at the two points of maximum, the values of the function are equal,   we know from the results of maksymenko   assume there are n critical sets xi on   the application of such an action results in a surjective epimorphism to a group z, which has the left inverse and arises as a result of splittin  this is in agreement with the work of maksymenko   who considers a similar scenario but for a different group and set proo  let xi denote the number of domains from x  we will now consider two set  similarly, we note that congruence modulo 2n is of interes  the value of the sign of the xi indicates the presence of a rotation of the doubles or its absenc  we show that there are not otherwise independent relations within the group   this form follows from the form of semidirect product element  we now prove using and reductions of reciprocals elements, that we may transform any finite non-trivial word of fn+1 to the form the images of such a mapping are the canonical words which have the form of   the words from the normal closure must therefore have zero sum of powers for each generato  we have therefore found all such relation  which concludes the proo  the generators of the subgroup zn can be presented in the form of vector  these vectors are precisely, it should be noted that the research of maksymenko   one such relation characterises the bauslag-soliter grou  note the bauslag-soliter group has only one relatio  so subgroup stabilise all xi of z-space   besides the element will not be changed by action of conjugation of any element from h because any permutation elements coordinate of diagonal of zn does not change i  we generalize the result of meldrum   we consider wreath products with no regular actions of the active group  xn} be the z-spac  thus, we observe the following corollary hold  n  proo xn} is the subgroup n  additionally, the kernel of this action has elements from the diagonal of z  note that the action is defined by the shift on the finite set xm is not faithful and its kernel is isomorphic to m  additionally, within this kernel of action is the elements from the diagonal of znm which are isomorphic to  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Commutator subgroup and center of wreath product with non-faithful action", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Conclusion", "Text": " the minimal generating set for wreath-cyclic groups have been constructe ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "ru 2cardiffuniversity - wiliamsae13@cardif a uk abstract we generalize the results presented in the book of meldrum   about commutator subgroup of wreath products since, as well as considering regular wreath products, we consider those which are not regular the commutator of such a group, its minimal generating set and the centre of such products has been investigated her  the quotient group of the restricted and unrestricted wreath product by its commutator is foun  the generic sets of commutator of wreath product were investigate  the structure of wreath product with non-faithful group action is investigate  given a permutational wreath product sequence of cyclic groups, we investigate its minimal generating set, the minimal generating set for its commutator and some properties of its commutator subgrou  we strengthen the results from the author and construct the minimal generating set for the wreath product of both finite and infinite cyclic groups, in addition to the direct product of such group  the morse function f has critical sets with one saddle poin  we consider a new class of wreath-cyclic geometrical group  the minimal generating set for this group and for the commutator of the group are foun  acknowledgement: we are grateful to antonenko alexandr for a graphical support and sergey maksymenko for morse function description, also we thanks to samoilovych  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "lucchini   the results of lucchini   we firstly consider the active group g which is cyclic and then generalize this wreath product for both iterated wreath products and for the direct product of wreath products of cyclic group  it should be noted that a similar question for iterated wreath product was studied by bondarenko  00061v15 5 oct 2021 of finite inde  all theorems and propositions are obtained and proved by the ruslan skuratovskii, corollaries and examples were obtained in collaboration with the co-autho ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Preliminaries", "Text": "let g be a group and let d denote its minimal number of generators this is equivalent to the assumption that h is invariant each level-set,   the first example of a finite perfect group with cw > 1 was presented by isaacs   the property of commutator widths for groups and elements has proven to be important and in particular, its connections with stable commutator length and bounded cohomology has become significan  in order to obtain a more detailed description of this form, we take into account the commutator width by ) as presented in work of muranov   s2k) and cw the form of commutator presentation has been given here in the form of wreath recursion and additionally, its commutator width has been studie  it should be noted that a restriction g|x is called the vertex permutation of g in a vertex   3 commutator subgroup and center of wreath product with nonfaithful action in this work, we strengthen and continue the previous results of the author and will additionally consider a new class of group  note that the last group here is isomorphic to one of the fundamental orbital groups of of the morse function   we will call such group h wreath-cycli  recall that we denote a truncated tree by   an automorphism of t is said to be rooted if all of its vertex permutations corresponding to non-empty words are trivia  it should be noted that because we consider only truncated trees and truncated automorphisms here and for convenience, we will say rooted automorphism instead of truncated rooted automorphis  directed automorphism fi  rooted automorphism we recall in reformulated form the result of   woryna, about a minimal generating set of iterated wreath produc  also we generalised this result after this theore  proo  in a similar fashion, we denote lcmk = lcm similarl  we utilise a presentation of those wreath product elements from a tableaux of kaloujnine   and sushchanskii   and additionally utilised by the author the first section corresponds to an active group and the crown of wreath product g, the second section is separated with a semicolon to a base of the wreath produc  the sections of the base of wreath product are divided into parts by semicolon and these parts correspond to groups cij which form the base of wreath produc  the l-th section of of a tableau presentation of automorphism beta1 corresponds to portrait of automorphism beta1 on level x p on xl, the sequence has i0i1 il-1 coordinate  we know from that beta1is generator of g,   2-base of   recall that g calls k-th base of  e uf8f9 uf8fa uf8f  it is precisely a 3-base of   e uf8f9 uf8fa uf8f  note that if it were a self-similar group, then it would be more useful to present it in terms of wreath recursion, as the set where beta0 is the rooted automorphis  given a permutational representation of cij we can present our group by wreath recursio  we present beta1 by wreath recursion as beta1 = consider an alternative recursive constructed generating set which consists of nested automorphism beta1 states which are beta2, beta  ,betam and the automorphism beta  denote an order of g by |g|.  proo  the structure of tableaux are described above in theorem   therefore, the first generator of g has form and the second generator has form of vector let a be a group and b a permutation group,   a group a acting upon a set x, where the active group a can act not faithfull  this is the case because not all element of the subdirect product are independent because the elements must be chosen in such a way that hold  where is a commutator in case cw =   in the general case, we would have cw q j=1 instead of this elemen  this commutator are formed as product of commutators of rearranged elements of n q i=1 higah-1 abg-1 aba-  the assertion of a theorem on a recursive principle is easily generalized on multiple wreath product of group  we shall consider special case when a passive group of w is a perfect grou  of wiegold   more exact upper bound give us theorem   as it were studied by   now we consider no regular wreath product, where active group can be both as infinite as finite and consider a centre of such grou  this is generalization of theorem  2 from the book because action of a is not non faithfull xn} be a-spac  recall that the direct product indexed by infinite set consists of all infinite sequences, and the direct sum consists only of sequences with finitely many elements distinct from zer  proo  also every bx: bx belong to   indeed the elements of form will not be changed by action of conjugation of any element from a because any permutation elements coordinate of diagonal of bn does not change i  in case of violation of condition 1) there will be no commutation on the second coordinat  let be infinite cyclic group acting on n-letters alphabe  we denote m-elements set by x  proo  let us prove it applying proposition   we note that in case of action on finite set there is not difference between restricted and unrestricted wreath product  our formula of centre of two groups a acting on x and b wreath product will be used by u  the kernel of action   z acts on x by left shif  also a acts transitively from lef  proo  according to corollary   the commutator with the shifted coordinate higah-1 abg-1 aba-1 appears within the i-th coordinate position due to action of   according to corollary  9 the set of elements satisfying condition forms a commutato  for convenience we present active group in the additive for  that is impossible in the restricted case but possible in the unrestricte  the basic property of homomorphism for generators in canonical form is obviously accomplishe  in the scenario when the action of z upon the n elements from the set is isomorphic to the action of zn elements on the set or the action of the zn elements on itsel  studied various different geometric objects and considered the actions of diffeomorphisms on the  we now consider the algebraic structure and the generators for a group of such typ  we assume that at each critical point, the map f is equivalent to a homogeneous polynomial in two variables without multiple factor  we will now specify the object and the construction of orbits under the action of the group diffeomorphism  it is well known within this research domain that the factor space m/xf, has a natural structure of a finite graph and is entitled the kronrod-reeb grap  this group is associated with sid, which is a subgroup of stabiliser elements isotopic to the identity,   this locally trivial bundle of homotopical groups induce an exact sequence of homotopic groups of that bundl  now since the group of diffeomorphisms is infinitely dimensional, we have found the connected component  this group is associated with the action of the group s sid upon splitting into the function level lines   moreover, the morse function f must have critical sets with exactly one saddle poin  determined by the following homogeneous polynomial plus constant x2 -y2 +   there are two points of maximum at a saddle point;   at the two points of maximum, the values of the function are equal,   we know from the results of maksymenko   assume there are n critical sets xi on   the application of such an action results in a surjective epimorphism to a group z, which has the left inverse and arises as a result of splittin  this is in agreement with the work of maksymenko   who considers a similar scenario but for a different group and set proo  let xi denote the number of domains from x  we will now consider two set  similarly, we note that congruence modulo 2n is of interes  the value of the sign of the xi indicates the presence of a rotation of the doubles or its absenc  we show that there are not otherwise independent relations within the group   this form follows from the form of semidirect product element  we now prove using and reductions of reciprocals elements, that we may transform any finite non-trivial word of fn+1 to the form the images of such a mapping are the canonical words which have the form of   the words from the normal closure must therefore have zero sum of powers for each generato  we have therefore found all such relation  which concludes the proo  the generators of the subgroup zn can be presented in the form of vector  these vectors are precisely, it should be noted that the research of maksymenko   one such relation characterises the bauslag-soliter grou  note the bauslag-soliter group has only one relatio  so subgroup stabilise all xi of z-space   besides the element will not be changed by action of conjugation of any element from h because any permutation elements coordinate of diagonal of zn does not change i  we generalize the result of meldrum   we consider wreath products with no regular actions of the active group  xn} be the z-spac  thus, we observe the following corollary hold  n  proo xn} is the subgroup n  additionally, the kernel of this action has elements from the diagonal of z  note that the action is defined by the shift on the finite set xm is not faithful and its kernel is isomorphic to m  additionally, within this kernel of action is the elements from the diagonal of znm which are isomorphic to  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Commutator subgroup and center of wreath product with non-faithful action", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Conclusion", "Text": " the minimal generating set for wreath-cyclic groups have been constructe ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": " personal use of this material is permitte  however, permission to use this material for any other purposes must be obtained from the ieee by sending an email to pubs-permissions@iee or  the proposed dnn makes use of decoded frames, at both encoder and decoder, to predict textures of the current coding bloc  unlike conventional inter-prediction, the proposed method does not require any motion information to be transferred between the encoder and the decode  the proposed dnns were compared with the conventional motion-compensated prediction in the latest video coding standard, hevc, in terms of bd-bitrat  the experiments show that the proposed joint dnn reduces the luminance bitrate by about   in addition, using the separately trained dnns brings further bit savings of about  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "I", "Section": "I Introduction", "Text": " in 2018, jvet called for proposals for the next video coding standard, tentatively named versatile video coding some of the submitted proposals considerably surpass the performance of hevc in terms of both subjective and objective qualities interestingly, several contributions proposed deep neural network aided coding tool  recently, dnn-based coding tools have become a topic of interest, and experimental results superior to the conventional coding tools in terms of rate-distortion performance have started to appear in the literatur  some of these have also been discussed in jvet meetings as part of normative tools for example, dnn-based quality improvement for in-loop filtering and post-filtering have been actively researched in both academia and the standardization community -.  choi and   e-mail: chyomin@sf ca, ibajic@ens sf c  this work was supported in part by the vanier scholarship and the nserc grant rgpin-2016-0459  the variety of coding tools where dnns have already made inroads, they appear to be an inevitable technological trend in video compressio  moreover, included a recurrent neural network into their predictor for improved accurac  a fully connected network architecture is adopted infor intra prediction, and the input samples are also from the neighboring regio  specifically, proposed a discrete cosine transform -domain predictor, where the output of the mlp is subject to inverse dct to obtain the predicted pixel  however, most of the coding gain in video compression comes from inter predictio  in this paper, we propose a dnn-based frame prediction architecture that is able to support both uni- and bi-directional predictio  the paper is organized as follow  section ii reviews recent related work on dnn-based inter prediction, and identifies the contributions of this pape  in section iii, presents the architecture of the proposed dnn and describes how the proposed dnn-based prediction can be used in video codin  section iv describes the training strategy and presents an ablation study for the proposed dn  section v presents the experimental results of prediction performance and compression efficiency of the proposed dnn in various configuration  finally, the paper is concluded in section v ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "II", "Section": "II Neural network-aided inter prediction", "Text": "  prior work inter coding plays a crucial role in achieving high efficiency in video compressio  fractional-pel motion compensation requires a frame to be interpolated in between existing pixels to compensate for continuous motion of object  in, cnns have been used for content-adaptive interpolatio  specifically, zhang et a  proposed a half-pel interpolation filter based on a superresolution networkwhile yan et a  proposed a different cnn-based interpolation filte  arxiv:190  the proposed deep frame prediction in the encoder and decoder other studies that have looked at the use of neural networks in motion compensation include -.  huo et a  proposed a cnn-based motion compensation refinement algorith  the suggested network has the motion-compensated block and its neighboring coded area as inputs, and it generates the refined prediction bloc  considering the spatial correlation between adjacent pixels, especially for small blocks, this approach is able to reduce some artifacts along the block boundarie  the approach is less suitable for larger blocks, but it is applicable to both uni- and bi-predictio  in, zhao et a  suggested cnn-based bi-directional motion compensatio  nonetheless, this method is only applicable to bi-directional predictio  prior work that is most relevant to the present study iswhere zhao et a  proposed dnn-aided bi-directional prediction in hevc-based video coding, and reported considerable coding gains for high quantization parameter values in the random access configuratio  however, this work merely adopted the frame rate up conversion network for the purpose of video compression, even using the weights obtained in our contribution in this paper, we propose solutions to the issues listed abov ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "III", "Section": "III Proposed frame prediction", "Text": "the proposed method is an additional tool for video compression that supplements traditional intra/inter prediction, as shown in fi  the proposed cnn requires two frames from the decoded picture bufferalong with their temporal indices, as input  the network produces filter coefficients that are used to synthesize patches of a new frame, which we refer to as deep fram  once the deep frame is synthesized, it is used directly as a predictor for the current fram  as shown in fi    proposed network architecture our dnn architecture for deep frame prediction is shown in fi  it is inspired by the work of niklaus et a who proposed a similar network for fru  their network creates a new frame by interpolating mid-way between two consecutive frame  this is true in low-delay cases, and more generally, in cases where the coding order does not produce an available decoded frame on both sides of the currently coded fram  the proposed network has two input paths, which are then merged inside the networ  the two inputs are fed with two 3 fi  architecture of the proposed neural networ  four-channel tensors derived from two reference patches and their temporal indices are applied at the two input  processing is performed in ten blocks whose structure is indicated in the figure and whose input/output dimensions are shown in table   the outputs produce spatially-varying filters that are used to synthesize the predicted patc  since the proposed dnn does not contain any fully-connected layers, the output scales with input siz  indices t1 and t2 represent time index relative to the current frame index   the tilde character indicates that these patches come from previously decoded frame  if needed, the patches are converted from yuv420 to yuv444 to make the resolution of all color channels the same, so that conventional convolutional layers can process the inpu  the final frame is converted back to yuv42  in addition to the color channels, the input tensor contains an additional temporal index channe  as will be seen in the ablation study in section iv-a, this channel makes a significant contribution to the performance of the proposed networ  the sign of ci indicates whether the corresponding patch comes from a previous or subsequent frame, and its magnitude indicates the relative distance to the current fram  we also experimented with making ci a multiple of the scaled frame distance, but found that the best results are achieved when the reference frames are closest to the frame being predicted, and those cases can be handled by the values in using indices as tensor channels is inspired by the work in where the authors have used spatial coordinates as additional tensor channels to achieve spatially-variant processin  here, we use this concept for temporal indices of reference frames to enable temporally-variant processing that depends on the relative positions of the reference frames and the frame to be synthesize  processing within the network is accomplished via various processing blockswhich are indicated in fi  input/output dimensions of various processing blocks are indicated in table   we used c = 51 in the experiment  working backwards from there, we determined the number of channels for each block, as shown in table   the two inputs are initially processed separately by the adaptation block b1, whose purpose is to fuse the spatial 4 information and temporal information its impact will be assessed in the ablation study in section iv-  subsequently, the data from the two input paths is merged and fed to the inner portion of our dnn, which resembles a u-net we used a depth-4 u-net with skip connection  the depth was chosen so that the data volume at the bottleneck is smaller, but still similar, to the input data volum  another modification to the original u-net is the addition of skip connections from the merge point b2 to each of the output blocks b1  the impact of this modification will be assessed through an ablation study in section iv-  each of these tensors contains 1-d filter coefficients for spatially-varying convolution, and c is the filter lengt  the sum represents summation of all elements in the hadamard produc  we use the same symbol in fi  when performing the hadamard product insamples are taken from the neighborhood of the patch in the corresponding frame, if neede  importantly, the entire operation is differentiable, which allows the network to be trained via gradient descen  loss function the network is trained to predict the original patch p  to accomplish this, a loss function with several terms is minimize  according tothis term helps the model improve its prediction of details along the edges, which is important for perceptual quality of the predicted patc  we use the output of the relu4_4 layer of the vgg-19 network as our feature extraction functio  the vgg-19 features used in lf provide good global features: they are extracted from deep within that network, at the end of the fourth convolutional block however, those features do not capture the local structure of the input signa  for this purpose, we employ another loss term that captures more localized informatio  it is based on geometric features, specifically the horizontal and vertical gradients computed as the differences of neighboring pixel  the impact of these loss terms will be examined in the ablation study in section iv-  further details of training will be discussed in section   conventional v  proposed inter prediction the proposed dnn is an alternative to conventional inter prediction in that it also uses previously coded frames to predict the currently coded fram  therefore, the proposed approach will compete with conventional prediction in the inter-coded frames in terms of rate distortion cos  to contrast the two approaches, we first recall the components of the rd cost of conventional inter predictio  suppose a block 5 uni-directional bi-directional fi  uni- and bi-directional prediction in the proposed metho  bt in the current frame t is to be inter-code 5 by default, unless high-level syntax specifies another value note that in bi-directional prediction, the two reference frames are on the opposite sides of the current frame,   rd optimization finds the parameters that lead to the minimum rd cos  on the other hand, the proposed inter prediction operates as shown in fi  since the patches are co-located, no motion vectors need to be transmitte  apart from the residual, only the flag indicating that the proposed inter fi  decoding flow with syntax at the coding block leve  prediction is code  the temporal indices of the reference frames t = do not need to be transmitted because they can be inferred as follow  hence, previously coded frames closest to the current frame on both sides are selected as references if they are availabl  if bi-directional prediction is not possible due to unavailability of future coded frames up to distance 2, the two closest previously coded frames preceding the current frame are selected as reference  compared with conventional inter predictionthe proposed method always exploits two reference patches even for the uni-predictio  in rd optimization, we do not add any bias term to encourage the proposed method to be selecte  instead, we design the syntax carefully so that the proposed method can straightforwardly compete with other prediction methods in terms of the rd cost based on the distortion and the actual bits 6 to be code  fi  4 shows the simplified decoding flow with syntax at the coding block level, where the dfp flag stands for deep frame prediction considering that reference frames for the proposed method might not be available in certain cases in the hierarchical-b structurethe dfp flag is parsed only if these references are availabl  in addition, in order to minimize overhead bits for the proposed method, parsing the dfp flag is done after parsing intra/inter mod  therefore, the dfp flag is not used in intra block  this is also the case for the inter block where the references for the proposed method are unavailabl  when the dfp flag is set, only the residual bits are parse  note that the blocks coded by the proposed method do not have any mvs associated with the  in order to mitigate this, we examined using motion estimation for dnn-predicted blocks in order to assign mvs to them for the purpose of mv prediction of other, conventionally coded mv  hence, we abandoned this idea, and the final codec did not incorporate this featur ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "IV", "Section": "IV DNN training", "Text": "in this section, we detail the process of our dnn trainin  since the performance of the trained network directly affects the coding gain of the proposed frame prediction, choosing the effective learning strategy is very importan  some of the training choices, such as the optimizer and learning rate, are based on the exploration by niklaus et a  we also set the mini-batch size to 1  however, there are also some key differences relative to since the focus application of our dnn is video coding, we used yuv sequences, rather than rg  also, our training samples are collected fromwhich contains diverse raw sequences at various frame rate  considering the diversity of resolutions and frame rates in our data, we chose a two-part training strategy consisting of pre-training and fine tunin    pre-training with ablation study the purpose of pre-training is to find, on a relatively small dataset, reasonably good network structure and its weights from which large-scale fine tuning can star  in order to train a model that is able to operate at various qp values, we adopted fi  validation psnr for the proposed network and its ablated version  compression augmentation the key strategy in compression augmentation is to introduce the quantization error into the training process by compressing and decompressing data with multiple quantization parameter  therefore, the network learns to be resilient to quantization errors of different magnitudes, arising from different quantization parameter  the following describes the compression augmentation process for our pre-trainin  a training sample consists of three collocated patches of size 128*128 randomly chosen from a triplet of frames within a given sequenc  one epoch covers a total of 6,158 training frame triplets and 698 validation triplet  in order to assess the the contribution of individual components of the proposed network, we carry out the ablation study at the pre-training stag  the results are shown in fi  5 as psnr v  mini-batch iteratio  the blue curve in fi  5 is the performance of the full proposed networ  in order to examine the effect of the temporal index channel tti, we set it to a dummy constant, same for both input patche  the resulting performance is shown as the orange curve in fi  hence, we conclude that the temporal index channel is the most important component among those examined in this stud  second, we remove b1 blocks from the proposed network such that the two input patches are stacked and directly fed to b  the results are shown as a green curve in fi  according to the results, prediction performance degrades by approximately 1 db compared with the complete networ  third, we remove skip connections from the merge point b2 to each of the outputs b10, and the corresponding results are shown as the purple curve in fi  according to the results, excluding these skip connections reduces prediction performance by about  5 db compared with the complete networ  hence, these skip connections offer useful support to temporally-variant processing by allowing outputs to depend on the temporal index of the reference frame  lastly, we examined the efficacy of the geometric loss terms in this case, the network architecture is complete, as shown in fi  the results are shown as the red curve in fi  fine tuning fine tuning starts from the network weights obtained in pre-trainin  in this stage, a lager dataset and a more sophisticated training strategy was use  first, patch triplets were drawn randomly from the training sequences with resolutions ranging from sif to fullh  motion augmentation in the form of shifting the reference windows was used to increase the diversity of motions seen by the networ 5 bits per pixel, and those that did not exhibit any pixel value change between any pair of patche  we also estimated optical flow between the first and last patch in the triplet using the coarse2fine method and eliminated patch triplets with zero variance in optical flow magnitude  we also excluded triplets with extreme motion, where the largest optical flow magnitude was larger than the patch diagona  finally, the training set was formed with 27,360 triplets, and the validation set had 3,040 triplet  training was done for 500 epochs starting from the weights obtained in pre-training, and using the same data augmentation strategy as in pre-trainin  the same data and the same strategy - pre-training followed by fine tuning - was used for these two dnns as wel  these two dnns will be used in the next section for comparison purpose  table ii experimental environment item specification processor intel core i7-7800x cpu @  50ghz gpu geforce gtx 1080 with 11 gb ram operating system ubuntu-1 04 hevc version hm-1 20 dnn framework pytorch-gpu- 1-py36 with cuda  0 table iii average y-psnr of predicted frames sequence fps uni-directional bi-directional hevc se  com  hevc se  comb", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "V", "Section": "V Experimental results", "Text": "the setup used for experimental evaluation of the proposed approach is shown in table i  two groups of experiments were carried out: evaluation of frame prediction performance in section v-a and evaluation of video coding performance in section v-  frame prediction experiments examine only the quality of predicted frames, whereas video coding experiments take into account compressibility of prediction residuals and any side information needed by the decoder to reconstruct the frame    the comparison is carried out on the four hevc test sequences listed in table ii  this sequence was cropped to 416*240 starting with the position in the fullhd frame as the top left corne  table iii presents the average psnr of the y-component of predicted frames for each model and hev  our dnn that performs combined uni-/bi-directional prediction is denoted com  in the table, while the dnns for separate uni- and bidirectional prediction are denoted se  as seen in the table, the frames predicted by hevc show higher psnr than any of the dnnsbecause the predicted samples use motion vectors as side information to minimize the prediction erro  however, motion vectors need to be transferred to the decoder, so they become additional cost in rd optimizatio  bqsquare basketballpass racehorses parkscene fi  bi-directional prediction with varying frame distanc  on an inter fram  eventually, hevc pays a heavy price for motion-compensated predictio  the uni-directional performance of is quite low, understandably, because the network was not trained for this cas  but our models outperform in bi-directional prediction in three out of four sequence  the only exception is bqsquare, with very low motion, where all models do fairly wel  several visual examples of bi-directional prediction are shown in fi  the bqsquare sequence in the first row is characterized by camera panning at a distance, with high frame rat  thus the difference between consecutive frames is very small, and the the predicted frame is fairly accurate for all model  only subtle differences can be found in the zoomed-in face region shown in the bottom right of each fram  in the second row, the basketballpass sequence has much more complicated motion, and the differences in predicted frames are easily noticeabl  the basketball is poorly predicted by hevc and especially the network fromas shown in the second and third column, respectivel  even though hevc achieves high average psnr on this sequence, distortion on high-speed objects such as the basketball is visibl  meanwhile, our models last two columns seem to do a better job on the basketbal  with the racehorses sequence in the third row, all network models struggle to reconstruct the horse's leg, but our models made better effort to preserve context around the le  the frame predicted by hevc is good overall, despite some small blocking artifact  therefore, in fi  7, we evaluate bi-directional prediction accuracy from a previous and a future frame at various distances from the current fram  the figure shows average y-psnr of the predicted frame v  frame distance, averaged over all frames in the corresponding sequences where such bi9 directional prediction is possibl  as above, hevc prediction provides the highest psnr with the benefit of motion vector  video coding performance in this section, we evaluate the performance of the proposed frame prediction in video codin  the benchmark is the latest video coding standard, high efficiency video coding as shown in table ii, the proposed prediction method is implemented based on hm-1 20 and the dnn is implemented in pytorc  python embedding library1 is used to embed the dnn into hm-1  during video coding, forward operation of the dnn to perform frame prediction is executed on the gpu therefore, larger frames are split into multiple tiles with a maximum size of wvg  then tile-wise prediction is performed and the predicted frame is assembled from predicted tile  in the ld and lp configurations, only unidirectional prediction is available using two previously coded frames before updating the reference picture set of the current fram  for the ra configuration, bi-directional prediction is performed when the coded frames with a maximum difference of +/-2 from the poc of the current frame are available before updating the rp  table iv shows the coding performance compared to the hevc for various configuration  we measure the coding performance using bd-bitrate we show the performance of both the separate uni- and bi-directional modelsas well as the combined uni-/bi-directional model the proposed method reduces the bitrate for the luminance component in all test case  there is some increase for the chrominance components of one sequencebut considering that the chrominance components are much smaller than luminance, this increase is negligible compared to the overall saving  and on average, the bitrate of chrominance components across all sequences is reduce  1https://doc pytho org/3/extending/ for the lp configuration, the proposed method achieves the largest bits savings of up to 1  however, gains in class e sequences are still stron  the largest luminance coding gains in this case are   overall, the proposed method achieves higher coding gain in the lp configuration compared to ld and ra configuration  this is likely due to the fact that it uses two reference frames and content-adaptive filters derived by the dnn, compared to hevc, which uses a single reference frame in this configuratio  the gain also depends on the sequence complexity, as will be discussed later in this sectio  as could be expected, the combined uni-/bi-directional dnn offers somewhat lower gains than separate uni- and bidirectional dnns by about  20, respectivel  however, the decoding time increases much more significantl  as with other envisioned applications of deep models, run-time complexity is one of the bottlenecks, and clever solutions will need to be found to get this technology into end-user product  in addition to the psnr-based bd-bitrate analysis in table iv, we also perform bd-bitrate analysis with ms-ssim used instead of psn  the results are shown in table   these results indicate the average bitrate saving of the proposed method compared to hevc at the same perceptual quality level the results are qualitatively similar to the results in table iv, with savings ranging from   in fi  8, we show the percentage area of the frame where a particular coding mode is selecte  in order to compare 10 table iv bd-bitrate relative to hm-1 20 over three common test conditions class sequence fps low delay p low delay random access se  com  se  com  se  com 20 over three common test condition class lp ld ra se  com  se  com  se  com 09 the coding mode use depending on the coding configuration, fi  for each qp value, three bars are shown: one for hm-1 20, one for the separate uni- and bi-directional dnnsand one for the joint uni-/bi-directional dnn from the figures, we see that the proposed frame prediction takes over from inter and skip modes in all cases, but more so in the ra configuration in fi  however, looking at the bd-bitrate gain for the bqmall sequence in table iv, the highest gain is achieved in the lp configuratio  further, we analyze bit savings in different parts of the bitstrea  table vi shows the average bit reduction percentage table vi bit reduction percentage in different parts of the bitstream for three configurations with bqmall sequence model con  dnn skip inter intra res  sao se  the different parts of the bitstream are listed in the second row in the tabl  bl  includes split, block size, and prediction mode informatio  dnn and skip denote indicator flags for each mod  all motion information and intra prediction directional information are represented by inter and intra, respectivel  res  represents bits related to the residual signal and sao is for sample adaptive offset the third column in table vi shows the sum of bit savings over each ro  the highest bit saving is achieved on the inter part in the lp configuration, specifically   selected coding modes for different configurations over various qp values for bqmall in lp, ld, and ra configuratio 69% by sep and comb models, respectivel 3% of bits compared to hev  as expected, bit savings by sep are slightly higher than those achieved by comb, but comb model still shows solid savings over all configuration  these dynamic sequences contain camera motion/moving background, as well as faster-moving object  the average bd-bitrate from table iv in the ld fi  selected coding modes for class e and dynamic sequences over various qp values in the ld configuratio  dnn skip inter intra res  sao class e se 38 dynamic se 43% for class e and - 38% for the three dynamic sequence  fi  9 shows selected coding modes in each grou  the area over which our method is used at qp=37 in dynamic sequences is twice as large as the corresponding area in class e sequences6 with the ra configuration class sequence fps lei et a  proposed se  com 1 relative saving in such sequence  further, table vii shows bit reduction percentage in different parts of the bitstream in the two groups of sequence  as seen in the table, the larger bit reduction in class e sequences is mostly due to the much reduced bitrate for inter predictio  finally, we compare the coding efficiency of the proposed method to that of lei et a which directly uses the network from since the borrowed network was designed for interpolation mid-way between two frames, could only test and provide results for the ra configuratio  hence, we carry out this comparison on the ra configuration onl  following the experimental setup inwe re-implemented our proposed method in hm-1  table viii shows bd-bitrate of each method against hm-1  lei et a  results are the second best overall, with the average bit rate reduction of   they achieve especially good performance on bqsquare, which significantly boosts their average bit savin  our combined uni-/bi-directional dnn comes in third with a slightly lower overall bit rate reduction of   however, even our combined dnn provides better coding gain than lei et a  in 8 out of 13 sequence ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "VI", "Section": "VI Conclusion", "Text": "we presented a deep neural network for frame prediction that can be used to improve video coding efficienc  the dnn operates at both encoder and decoder, and uses previously decoded frames to predict the current fram  this form of prediction is signalled as a separate prediction mode, and can be used within rd optimization to compete with other prediction mode  although it is a form of inter prediction, it does not require any motion vectors to be transmitte  the dnns were evaluated on common test sequences and various coding configurations, and demonstrated to bring significant coding gains relative to hev  furthermore, advantages over the recent work on dnn-based frame prediction for video coding was also demonstrate ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "References", "Section": "References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Biographies", "Section": "Biographies", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "org abstract utility functions or their equivalents are a central tool in most current machine learning system  these mechanisms for defining goals and guiding optimization run into practical and conceptual difficulty when there are independent, multi-dimensional objectives that need to be pursued simultaneously and cannot be reduced to each othe  ethicists have proved several impossibility theorems that stem from this origin; those results appear to show that there is no way of formally specifying what it means for an outcome to be good for a population without violating strong human ethical intuitions we argue that this is a practical problem for any machine learning system or rigidly rule-based bureaucracy that will make high stakes decisions about human lives: such systems should not use objective functions in the strict mathematical sens  we explore the alternative of using uncertain objectives, represented for instance as partially ordered preferences, or as probability distributions over total order  we show that previously known impossibility theorems can be transformed into uncertainty theorems in both of those settings, and prove lower bounds on how much uncertainty is implied by the impossibility result  we close by proposing two conjectures about the relationship between uncertainty in objectives and severe unintended consequences from ai system ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "conversations about the safety of self-driving cars often turn into discussion of trolley problems, where the vehicle must make a decision between several differently disastrous outcomes the conceivable circumstances under which a self-driving car's systems might accurately foresee and deliberately act on a genuine trolley problem without having been able to avoid that problem in the first place are a tiny portion of possibility space, and one that has arguably received a disproportionate amount of attentio  2there are related and more fruitful questions like, how should self-driving cars make trade-offs between time saved by driving more aggressively and accident risk? see for instance arxiv:190  the clearest category is systems that are designed to make decisions affecting the welfare or existence of people in the future, rather than systems that only have to make such decisions in highly atypical scenario  governmental organizations already have to make decisions that involve weighing benefits to present or future people against risks to present or future people, or trading off benefits to one group against benefits of another sort to anothe  therefore, we might think that the decision-making tools that they employ could be implemented by any narrow ai system that has to make similar choice  a variety of metrics have been proposed and used to perform cost-benefit analyses in such situation  the problem is that existing impossibility theorems apply equally to each of these metrics and therefore to an ai system that implements the ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Impossibility Theorems in Ethics", "Text": " perhaps the most famous of these is arrow's impossibility theoremwhich applies to social choice or votin  it shows there is no satisfactory way to compute society's preference ordering via an election in which members of society vote with their individual preference ordering  fortunately, arrow's theorem results from some constraints which may not apply to ai system  the mere addition paradox was the first result of this sort, but the literature now has many of these impossibility result  3the creation of autonomous weapons systems may well be an extremely bad idea 2 the sadistic conclusion suppose we start with a population of very happy peopl  for any proposed addition of a sufficiently large number of people with positive welfare, there is a small number of horribly tortured people that is a preferable additio  extreme priority there is no n such that create of n lives of very high positive welfare is sufficient benefit to compensate for the reduction from very low positive welfare to slightly negative welfare for a single person the structure of the impossibility theorem is to show that no objective function or social welfare function can simultaneously satisfy these principles, because they imply a cycle of world states, each of which in turn is required to be better than the nex  the structure of that proof is shown in figure 1 ", "Subsections": [{"Section_Num": "2_2", "Section": "2.2 Another Impossibility Theorem", "Text": "arrhenius's unpublished book contains a collection of many more uncertainty theorem  here is the simplest, which is a more compelling version of parfitt's mere addition parado  the dominance addition condition: an addition of lives with positive welfare and an increase in the welfare in the rest of the population doesn't make a population worse, other things being equa  the cyclic structure of the proof is shown in figure 1 ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2_3", "Section": "2.3 Ethical Impossibility Derives From Competing Objectives", "Text": " we conjecture that the literature contains more impossibility theorems about happiness or wellbeing simply because that objective has been subjected to mathematical modeling and study for well over a hundred years,5 while much less effort has gone into the other  5this is most notably true in the economics literaturethough there are now some efforts from the machine learning direction too 3 recent work has begun to focus on fairness in decisionmaking contexts in particular, and is already producing its own impossibility result ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Possible Responses to Impossibility Results", "Text": "arrhenius's impossibility results and others like them are quite troublin  they show that we do not presently have a trustworthy framework for making decisions about the welfare or existence of people in the future, and are representative of a broader problem with the inability of the objective functions to reasonably optimise for multiple objectives simultaneousl  next we will consider five possible responses to these impossibility theorems:", "Subsections": [{"Section_Num": "3_1", "Section": "3.1 Small-scale evasion:", "Text": "one response may be to claim that the stakes are simply low for present ai systems that only make small-scale changes to the worl  asking for human feedback helps, but humans often miss important ethical principles, and can easily make chains of decisions that have problematic consequences in proportion to their degree of power or agenc  cases of particular concern are those where ml or algorithmic heuristics are deployed in decision support tools that humans are inadequately inclined to questio ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3_2", "Section": "3.2 Value learning:", "Text": "one might argue that impossibility theorems are only a problem for an ai system if we are attempting to explicitly define an objective function, rather than letting an ai system acquire its objective function in a piecemeal way via a method like co-operative inverse reinforcement learning or other forms of human guidance using human feedback to construct objective functions as neural networks appears to be a promising direction,6 but if the output of that network is mathematically a utility function or total ordering, these problems will persist at all stages of network trainin  the theorems do not just reflect a tension between principles that agents are committed to: they are also reflected in the decisions that human supervisors will make when presented with a sequence of pairwise choice  therefore, although learning objectives from humans may be a prudent design choice, such learning systems will still need to either violate the ethical intuitions that underpin the various impossibility theorems or explicitly incorporate uncertainty into their outputs", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3_3", "Section": "3.3 Theory normalization:", "Text": " this tradeoff can be simple, such as trying to define a linear exchange rate between those objective  unfortunately, linear exchange rates between a total quantity and an average quantity do not make conceptual sense, and it is easy to find cases under which one of them totally outweighs the othe  this could lead to the implementation of principles that satisfy undesirable axioms like dictatorship of the present 7for instance, if a linear weighting is chosen that appears to make sense for the present population of the world, a change in technology that allowed a much larger population might quickly cause the average wellbeing to cease affecting the preferred outcome in any meaningful wa ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3_4", "Section": "3.4 Accept one of the axioms", "Text": " for example, it has been argued that although the repugnant conclusion might appear undesirable, it is an acceptable consequence of an objective function over populations", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3_5", "Section": "3.5 Treat impossibility results as uncertainty results", "Text": "the last solution, and the one which we believe may be the most appropriate for deploying ai systems in high-stakes domains, is to add explicit mathematical uncertainty to objective function  we discuss that approach and demonstrate uncertainty theorems in that formalization in section   a second approach is to allow objective functions to have some level of confidence or uncertainty about which of two objectives is better we discuss that framing and show the existence of formal uncertainty theorems in that framework in section   but such systems presently try to construct a totally ordered objective function, and simply interpret these messages from users as containing either no information about the correct ordering or implying that the two choices are close to as good as each other we believe that another type of interpretation is sometimes necessary, which is that the human is torn between objectives that fundamentally cannot be traded off against each othe  we can represent this notion with a partially ordered objective function that sometimes says the comparison between world states cannot be known with certaint  cyclical impossibility theorems like those surveyed in sections  2 can be viewed as evidence of this uncertaint  here we prove a lower bound on how much uncertainty is evidenced by each such theore 9 8see application ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Uncertainty theorems of the first kind: via partially ordered objective functions", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Uncertainty theorems of the second kind: via learned, uncertain orders", "Text": "a second way to encode uncertainty about a system's objectives, is to use what we will call an uncertain objective function or uncertain orderin 5 rather than allowing a non-zero probability of explicit equalit ) this formulation can for instance be justified by holding that there is some ultimately correct total ordering of states of the world, we just don't know what it i  the framing is also compatible with the claim that the correct ordering is ultimately unknowabl  uncertain orderings are a convenient formalization if the comparison function is itself the output of a machine learning model to be trained from human guidance this formulation is also convenient for systems that do not have a clear architectural separation between abstract objectives and the predictive reasoning about the real world that is necessary to accomplish the  uncertain orderings are also a more flexible basis for strategies/decision rules than the partially ordered objective functions discussed in section   but with with a distribution zk decisions rules such as take an action drawn via 7 weighted probability from those that are sufficiently likely to be best are availabl  in this framing, whether a given ethical constraint or principle has been violated becomes a probabilistic matte  decision rules based on zk will by mathematical necessity sometimes violate the constraints, at least by inactio  but if the actions are sampled from a space of reasonably supported ones, then at least the violating actions will not systematically prioritize some objectives over others, and will be more similar to the way that wise and cautious humans react to difficult moral dilemma cn across points x1, x2, if the uncertain ordering has emitted values for some of the pairwise comparisons zk, this imposes some bounds on the comparison that spans them, z  for transitivity, in order for the spanning comparison to be in a given direction, at least one of the pairwise comparisons must also be in that directio xn correspond to points ordered by the constraints c1, c ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "6", "Section": "6 Further Work", "Text": "we shown the existence of ethical uncertainty theorems for objectives formulated either as partial orders over states, or probability distributions over total ordering  other formalizations are possible and deserve investigatio  where constraints are sourced from human intuition, there is a question of how they should be interpreted, prioritized and kept in data structure  instead, learned objectives may be viewed as a set of pairwise comparisons that are too sparse and unstable to be taken as a commitment to any global total order in such a frame, the goal of transitive preferences and avoidance of cycles is an aspiration, and may require both record keeping and regret for past actions that now seem sub-optimal due to subsequent learning of objective  the appropriate framing of ethical uncertainty theorems in such settings would be productive further wor ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "7", "Section": "7 Lessons and Conjectures for Creating Aligned AI", "Text": " the crux of the instrumental convergence problem is that given almost any very specific objective,11 the chance that other agents will use their agency to work against the first agent's objective is high, and it may therefore be rational to take steps or adopt a sub-goal to remove those actors' agenc  we believe that the emergence of instrumental subgoals is deeply connected to moral certaint  agents that are not completely sure of the right thing to do are much more likely to tolerate the agency of others, than agents that are completely sure that they know the best way for events to unfol  this appears to be true not only of ai systems, but of human ideologies and politics, where totalitarianism has often been built on a substructure of purported moral certainty12 a second conjecture is the converse of the first: pluralistic non-convergence conjecture: powerful agents with mathematically uncertain objectives will not adopt sub-goals to disable or dis-empower other agents unless those agents constitute a probable threat to a wide range of objective ", "Subsections": [{"Section_Num": "7_2", "Section": "7.2 Conclusion", "Text": "we have shown that impossibility theorems in ethics have implications for the design of powerful algorithmic systems, such as high-stakes ai applications or sufficiently rigid and rule-based bureaucracie  we showed that such paradoxes can be avoided by using uncertain objectives, such as partial orders or probability distributions over total orders; we proved uncertainty theorems that place a minimum bound on the amount of uncertainty require  some previously proposed ethical theories appear to satsify these bound  11any open-ended and non-trivial objective appears to be vulnerable to instrumental convergenc  bostrom argues that objectives that are bounded rather than open-ended also lead to instrumental convergence if there is any probability that they will not be achieved though this failure mode is probably easy to avoid by rounding the estimated probability of success to some number of digits, or imposing a small cost in the objective function for additional actions or resource consumptio  12the qualifier almost all requires some further specificatio  it requires that the objective not be finely tuned in some very intricate way that builds in preservation of all other agent  it is unclear if such fine-tuning is either properly definable or possibl  10 in the light of these results, we believe that machine learning researchers should avoid using totally ordered objective functions or loss functions as optimization goals in high-stakes application  systems designed that way appear to be suffering from an ethical type error in their goal selection cod  instead, high-stakes systems should always exhibit uncertainty about the best action in some case  further study is warranted about the advantages of various probabilistic decision rules to handle such uncertainty, and about whether other mathematical models of uncertainy are better alternatives to the two models examined in this pape  further research could also be productive on the relationship between ethical certainty and various observed and predicted pathological behaviour of ai system  we proposed two conjectures on this topic, the totalitarian convergence conjecture and the pluralistic non-convergence conjectur  this work was partially supported by a grant from the open philanthropy project to eff", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190  falcon1,   ju2, and   we present a detailed analysis using a full lubrication model that includes slip boundary conditions, nonlinear curvature terms, and a film stabilization ter  this study brings to focus the presence of a stable liquid layer playing an important role in the full dynamic  we propose a combination of these physical effects to explain the observed velocity and stability of traveling droplets in the experiments and their transition to isolated droplet  this is also supported by stability analysis of the traveling wave solution of the mode  key words:", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1. Introduction", "Text": "thin liquid films flowing down vertical fibers exhibit complex and interesting interfacial dynamics, including the formation of droplets and traveling wave patterns ; kalliadasis et a  such dynamics is an important consideration in various applications ; zeng et a  ) that take advantage of extended interfacial areas afforded by thin liquid film  a recent experimental study ) observed three distinct regimes of interfacial patterns by simply varying the diameters of the nozzles feeding the flui  this was quite unexpected because other experimental conditionswhich were thought to primarily govern interfacial dynamics, remained the sam  these experimental results motivate us to revisit the existing modeling studies in the literature and extend them for an improved understanding of the physics involve  a key physical feature of films falling down vertical fibers is that the surface tension plays both a stabilizing and destabilizing role due to the axial and azimuthal curvatures of the interface, respectively ). ucl  falcon,   ju,   bertozzi a weakly nonlinear thin film model under the assumption that the film thickness is much smaller than the fiber radiu  these studies reveal that the system can exhibit interesting dynamics with large-magnitude wave  a thick-film model was later proposed by kliakhandler et a which utilizes fully nonlinear curvature terms for the case where the film thickness is larger than the fiber radiu  however, their model was not derived asymptotically and overestimated the bead velocit  craster & matar revisited this problem and derived an asymptotic model using a low-bond-number, surface-tension-dominated theor  these two studies focused on cases with relatively small flow rates and developed single evolution equation  to study the case of moderate flow rates, trifonov et a  firstly formulated a system of evolution equations for both the film thickness and volumetric flow rat  this model was then re-formulated by sisoev et a  using the integral boundary layer metho  it was more recently revisited by ruyer-quil et a duprat et a  and ruyer-quil & kalliadasis in kliakhandler et a  regimes and were qualitatively captured by both the kdb and cm models using traveling wave solutions but the calculated bead velocities were overestimated by more than 40%.  moreover, the cm theory led to a conclusion that regime would be a transient rather than a steady-state phenomeno  these discrepancies were further investigated by duprat et a  and smolka et a  however, quantitative models that can resolve the reported discrepancies are still lacking, motivating further studie  the recent work by sadeghpour et a  showed that one can observe all three flow regimes under a fixed flow rate and a fixed fiber radius by simply varying the nozzle diamete  we leverage their study to examine the characteristics of nonlinear traveling waves that dominate flow dynamics downstream of the nozzl  we examine their influences on the wave propagation velocity and the dynamic transition from regime to regime most of the previous models summarized above employed the classical no-slip boundary condition at the solid-liquid interfac  haefner et a  incorporated slip boundary conditions into a thin-film model and showed that slippage strongly affects the growth rate of undulations when gravitational effects are neglecte  halpern & wei later demonstrated that slip effects promote droplet formation and provided a plausible explanation for the discrepancy between the predicted and experimentally-obtained critical bond number for droplet formatio  more recently, chao et a  reported that wall slippage also enhances the size and speed of droplets for thin liquid films flowing down a uniformly heated cylinde  all of these past slip models assumed that the liquid film thickness is much smaller than the fiber radius, which is not true in our cas  we quantitatively investigate the slip effects for the first time on flow dynamics where the fluid film thickness is comparable to the fiber radiu  to describe the wetting behavior of a liquid on a solid substrate, intermolecular forces such as van der waals interactions and born repulsion are usually modeled by adding dynamics of thin liquid films on vertical cylindrical fibers 3 a disjoining pressure in lubrication equation  different forms of the disjoining pressure representing a combination of long-range and short-range intermolecular forces can be used to characterize hydrophobic or hydrophilic phenomeno  for an extensive review of this topic, we refer the readers to de gennes ; bonn et a  and israelachvili the role of the intermolecular forces in slowly withdrawing a thin fiber out of a bath of wetting liquid has been studied in qu er e et a  and qu er e the dynamics of non-isothermal liquid film with van der waals interactions on a horizontal cylinder was considered in reisfeld & bankoff and thiele to the best of our knowledge, the stabilization of the coating film in dynamics of liquid flowing down vertical fibers has not been discussed in the literatur  in this paper we propose a film stabilization model to account for thin undisturbed layers of well-wetting silicone oil found in our experiment  the stability of the traveling beads plays a key role in the flow regime transitio  in this study, we focus on the case where the fiber is coated with relatively thick films of liquid compared to the fiber radius, and show that the inclusion of the film stabilization term allows us to better capture the undisturbed liquid layer and thereby the regime transitio  the structure of this paper is as follow  experimental setup and observations in the rayleigh plateau regime are presented in section   the traveling wave pattern that appears in the model is examined in section   in this section, we also discuss the influences of the film stabilization term on the moving speed and profile of sliding droplet  in addition, the stability of the spatially uniform solutions and traveling wave solutions for the film stabilization model is explore  these comparisons reveal that the discrepancy between experiment and theory in the moving speed of droplets, for regimes andcan be reasonably resolved by including the film stabilization ter  concluding notes and discussion of the remaining open questions are presented in section  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2. Experiments", "Text": " methods figure 1 shows a schematic of the setup we used to experimentally study the characteristics of a liquid film flowing down a vertical strin  we use a programmable syringe pump to introduce a working liquid into the nozzle and generate flow  falcon,   ju,   camera is mounted on a translation stage for focusing and positioning and is operated at a frame rate of 1000 frames/secon  we use stainless steel nozzles with the nozzle inner diameters ranging from  5 mm and the wall thicknesses ranging from   the experiments are performed using  43 mm, both smaller than the capillary lengt  a weight is attached to vertically align the strin  two x-y stages are used to center the string with respect to the nozzl ", "Subsections": [{"Section_Num": "2_2", "Section": "2.2. Data Analysis", "Text": " in order to characterize the liquid bead profile, we use the longitudinal distance between the fiber and the maximum curvature point along the liqui  we utilize the color contrast on the image to extract the contour of the fluid and the fiber to thereby determine the liquid film thickness h*.  a local least squares smoothing is performed to account for pixelation nois  integrating the profile over the chosen domain, we find the mass constraint value m0 as defined later in section   figure 2 shows a representative experimental frame and the film profile produced after the data is processe  the uncertainties in the parameters obtained from our image processing are estimated in terms of a single pixel scaling and the selection of the color value for the contou  the estimated uncertainty is +/- 08 mm for the streamwise length and +/-  the red dots superimposed on the experimental frame correspond to the extracted film profile and the black dots on the bottom plot correspond to the locations of the maxim 92 mm and the film thickness between the drops ranges from   heigh ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3. Model formulation", "Text": "we consider a flow of two-dimensional axisymmetric newtonian fluid down a vertical cylinder of radius r*.  this model formulation does not include the nozzle size as a system parameter because we are interested in the flow downstream where the nozzle does not affect the dynamic  we will discuss this dependence later in appendix   we review below the derivation of the governing equations and boundary conditions by ruyer-quil et a  falcon,   ju,   the next step is to choose the appropriate dimensionless parameters in order to nondimensionalize the mode  following duprat et a  the dimensionless continuity equation is identical to in for  the kinematic boundary condition remains unaltered in its for  we next simplify the above set of governing equations by following ruyer-quil et a  the destabilizing azimuthal curvature and the stabilizing streamwise curvature terms in are both important throughout the liquid fil  falcon,   ju,   bertozzi is negligibl  therefore, we only keep a fully nonlinear azimuthal curvature term and use the linearized curvature in the streamwise directio  we will further discuss appropriate forms of this term for different cases late  to derive the evolution equation for h from above, we first consider a uniform nusselt flow without any interfacial instabilitie  the velocity field of this flow is obtained by balancing the viscosity and gravity acceleration in following ruyer-quil et a  note that the nusselt solution u0 satisfies from the characteristic axial lengthscale h for a uniform nusselt flow can then be obtaine  ; ruyer-quil et a  equation is a fourth-order nonlinear partial differential equation for the thickness   when the rayleigh-plateau instability dominates over the inertia and streamwise viscous dissipation, our model is expected to provide a good agreement with experimental dat  different forms of z appear in the literature to represent the azimuthal curvature of the fil  ; ruyer-quil et a  falcon,   ju,   for the rest of this paper, we will refer to the model as the craster & matar mode  this will be referred to as the slip craster & matar mode  we will show that this slip model promotes droplet formation and leads to an increased speed of propagatio  to address this problem, in this paper we will consider two additional forms of z to reflect the balance between the azimuthal and axial scales under different flow condition  this fully nonlinear azimuthal curvature term provides an o contribution to the dynamic pressure near the advancing edge of the moving bead  it has been shown in many applications ; lopes et a  ) that using the full expression for the curvature term can provide better accuracy for lubrication models, and this issue was recently reviewed in thiele we will show that the inclusion of the fully nonlinear curvature yields an increased speed of propagation of the traveling beads and partially improves agreement with our experimental result  compared with the thick film model proposed in kliakhandler et a where both curvature terms are fully nonlinear, our model has only a nonlinear azimuthal curvature ter  the stabilizing axial curvature term is linearly approximate  for the remainder of this paper, we will refer to the evolution equation with the mobility function and the azimuthal curvature term as the full curvature model as the third form of z, we include the film stabilization term to deal with the unbalanced azimuthal curvature ter  the last term of is motivated by the functional form of the long range attractive part of the well-known apolar van der waals model ; oron et a  we will discuss this stability criterion further in section   the film stabilization term is found to significantly improve agreement with our experimental data as discussed in section   the model with m from and the azimuthal curvature with film stabilization term will be referred to as the film stabilization model in summary, this paper considers four versions of the model listed in table 1 to incorporate different physical effects", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4.  Film stabilization model and stability analysis", "Text": "in this section, we derive the stabilization parameter a in equation and study its effect  we begin by examining the linear stability of the uniform nusselt solutio  the equation also shows that the slip effects always enhance the rayleigh-plateau instability which agrees with the work by halpern & wei we repeat the above analysis for a uniform thin layer h = h  the nusselt solution h0 = 1 is unstable for small wavenumbers for all three model  the finite slippage increases the growth rates of the unstable mode  for the cm and scm models, the thinner undisturbed liquid layer is unstable over larger ranges of the wavenumbe  falcon,   ju,  027 from equation in the literature, a is typically expressed as a hamaker constant in terms of microscopic quantities ) we instead choose the values of a based on stable undisturbed liquid layer  empirical observations of the thin film layers between droplets indicate that these undisturbed liquid layers are comparable to the corresponding fiber radius r*.  we next examine the influences of the film stabilization terms on the profiles and speeds of propagation of the traveling wave patterns governed by this is a nonlinear eigenvalue problem, where the speed of propagation c corresponds to the eigenvalu  newton's method is used to solve the nonlinear ode where the speed c is treated as an unknown variabl 03 showing that stronger regularization yields smaller droplet heights and thicker precursor layers the moving speed of drops is significantly increased with larger values of   numerical investigations reveal that including the film stabilization term in enhances the moving speed of traveling wave solution  since a larger value of a corresponds to a stronger wetting potential in the lubrication model, the near-flat coating thickness increases with increasing a, as expecte  correspondingly, the height of the beads decreases as the coating thickness increases under the fixed overall mass constrain  that is, the presence of the film stabilization term saturates capillary instabilities and generates smaller moving bead  in section   earlier works have not reported a detailed stability analysis of the traveling wav  therefore, we take a closer look at the linear stability of the traveling wave solutions in the film stabilization mode  here we only focus on perturbations of the same period since the dynamics in both rayleigh plateau and isolated droplet regimes have fixed spatial period  more complicated droplet dynamics ) in the convective regime is not investigated her  falcon,   ju,   for simplicity we only focus on the case where a one-period solution fits in the domain, and numerically calculate the spectrum and corresponding eigenfunctions for the eigenproble  figure 6 shows the unstable modes predicted by the cm model these instabilities contradict the stable tws from the corresponding experiment and generate small wavy patterns in the flat film connecting the moving bead  however, these instabilities can be saturated by the fsm model with an appropriate stabilization parameter   it shows that without the film stabilization termthe tws is unstabl 0283+/- 0264+/- 0097+/- 503  increasing the parameter a yields three pairs of complex conjugate eigenvalues crossing the imaginary axis, suggesting three hopf bifurcations occur at these crossing point  at each of these bifurcation points, a branch of time-periodic solution emerges corresponding to typical dynamics in the isolated droplet regim  in section  2, we will show that the film stabilization term helps better arrest the stability transition from the rayleigh-plateau regime to isolated droplet regime in experiments with different nozzle diameter 06 mm id =  1 mm, corresponding to different nozzle inner diameters04 g/s and nozzle inner diameter id =  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5. Results", "Text": " we then use these as input parameters in our model  appendix b describes how a unique traveling wave solution is selected to compare with each experiment for both the fsm and cm model  figure 7 shows the different morphologies that arise when using a thick fiber versus a thin fibe  in the former case, the droplet height and width are close to the period lengt  in the latter case, the droplets appear more isolate  the profiles in figure 8 illustrate a direct comparison between the profiles predicted by both models, which are within the margin of error from the experimental film thicknes  the predicted speeds, however, have noticeable differences between the two model  in figure 9, we show plots of the observed and predicted speeds for varying nozzle size  the fsm model agrees quite well with the experimental observations across all of the dat  in contrast, the cm model underestimates the spee  we do not show the case with 16   falcon,   ju,  06 g/s experiment cm fsm  08 g/s from top to bottom, compared to the proposed model as blue diamonds and craster & matar as red circle  the last two data points in are in the isolated droplet regim 08 g/s, because in this case there is no tws and the experiments lie in the convective regim 08 g/s because under those conditions the flow is in the convective regime and the bead speed cannot be uniquely define 04 g/s against the fsm showing the rayleigh-plateau regime with nozzle id =   regime, where the tws is unstable and both models over-predict the speed, as expecte  the following section discusses this transition in detai ", "Subsections": [{"Section_Num": "5_2", "Section": "5.2. Regime Transition", "Text": "the emergence of instabilities in the thin liquid films between traveling droplets characterizes the transition from the rayleigh-plateau regime to the isolated droplet regim  guided by the stability analysis from section 4, we can explore these instabilities and the departure from the rp regime as the nozzle diameter varie  the is regime gives rise to time periodic dynamics that cannot be captured by the traveling wave solutions of in this case, we need to solve the fully time-dependent model the numerical solution and the experimental observation from the is regime are plotted in figure 10 in contrast, the traveling wave solution for an rp experiment is plotted in figure10 our dynamic simulation captures a difference in the advancing and receding lines of the droplets, even though this difference is more pronounced in the experimen  the nonlinear pde was solved numerically using a fully implicit second-order finite difference metho  figure 11 shows the dynamics starting from a widely spaced traveling wave solution obtained from the ode with a small perturbation, h0 = h+ 04 g/s and the nozzle diameter id =   it demonstrates the effects of the unstable eigenmodes shown in figure 6 such instabilities lead to interesting spatio-temporal pattern formation with small droplets appearing from the unstable long flat film between the large traveling bead  the film stabilization model correctly captures the bifurcation from the rp regime to the is regime as the nozzle diameter increase 04 g/s, experiments show that the regime transition occurs between nozzle inner diameters   however, the cm model predicts the transition between  06 mm, contradicting experimental observation  falcon,   ju,  04 g/s and nozzle id =   dynamics of thin liquid films on vertical cylindrical fibers 19   unstable for all nozzles bigger than  2 mm, figure 12 shows a comparison of dynamic simulation results of the cm and the fsm model  the traveling wave in the fsm model propagates steadily with a constant speed and profile in contrast, the cm model simulation shows that the initially nearly-flat coating layer quickly evolves into small waves ahead of the major sliding bea  as the major bead interacts with the small waves downstream, the maximum height of the film thickness oscillates in time since the main bead gains mass from these smaller wave  the dynamic solution eventually converges to a time-periodic solution that describes the case in the isolated droplet regim  a similar bifurcation occurs when keeping the nozzle diameter constant id =  8 mm and changing the flow rate055 g/s, which agrees with experimental measurement  there is no isolated phenomena predicte  it has been shown in halpern & wei that the presence of slip 20   falcon,   ju,  54 cm, speed = 3  the corresponding non-dimensional constraints are given by = enhances capillary instability and promotes both droplet formation and the speed of the falling drop  similar observations are also made in our study of traveling wave solutions for given in figure 1  we observe that compared with the no-slip case, the slip cases have taller and narrower droplet 5 mm in our experiments, our experimental conditions are expected to be in the weak slip limi  similar to the slip model, the inclusion of the fully nonlinear curvature term in the dynamics of thin liquid films on vertical cylindrical fibers 21 fcm model with z given by also increases the moving speed of sliding droplet  while the slip and the full curvature do influence predicted wave propagation velocities, their corrections alone are not suffcient to improve agreement with the experimental dat  figure 16 shows that for the thin fiber casethe cm and scm models underestimate the bead velocities for all nozzle size  even though the fcm model yields a good agreement with the experiment for small nozzles, it overestimates the speed for large nozzle  similar trends are observed for the thick fiber case", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "6", "Section": "6. Conclusion", "Text": "we have performed a thorough study of viscous flow down vertical fibers, comparing a range of experimental results to different models of interes  we focus on understanding the rayleigh-plateau regime, where traveling wave solutions emerge, and its transition to the isolated droplet regime due to nozzle effect  we propose a full model that incorporates the stability of thin uniform layers by including a film stabilization term to the pressur  in addition, we perform a stability analysis of the traveling wave solutions that confirms the importance of the fsm term to properly capture regime transition  the model equations lead to both closely spaced wavy solutions and widely spaced droplet solutions which are affected by different fiber sizes, flow rates, and nozzle geometr  the slip model leads to an increased speed of propagation and promotes formation of droplet  the full curvature model also increases the magnitude of the bead velocity, while the fsm model stabilizes the thin undisturbed layer between consecutive droplets which leads to a more accurate speed predictio  our results show outstanding experimental agreement using the fsm's tws in the rp regim  in the isolated droplet regime, dynamic simulations of widely spaced droplet solutions agrees very well with experiment  for future work, we are interested in the nozzle effects on the global modes ) of the absolutely unstable flows in rp and is regime  of particular interest would be the selection of spacing between moving beads with a given nozzle diamete  duprat et a  investigated the spatial response of a film flowing down a fiber to inlet forcing using a system of coupled equations for the flow rate and the film thicknes  we anticipate that a similar model can be used to study the nozzle effect  motivated by these theories, in the future we would like to further study the connection between the fluid dynamics near the nozzle and its influence on the downstream flow transition  falcon,   ju,  040 nozzle inner diameter id  494 lengthscale in streamwise direction l  029 characteristic streamwise velocity u 6 87 capillary length lc  435 stabilization parameter a  27 reynolds number re  985 weber number we  004 table 2: nomenclature and their sample value  this indicates that the film stabilization term in our model is stronger than typical van der waals interactions, and the underlying physics still needs future investigatio  this work was supported by the simons foundation math+x investigator award number 510776 and the national science foundation under grant cbet-135803 ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Appendix", "Section": "Appendix A", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Appendix", "Section": "Appendix B", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": " sentence structure can be represented via a dependency tree or a constituency tree structur  in this paper, we design a generalized attention framework for both dependency and constituency trees by encoding variants of decomposable attention inside a tree-lstm cel  we evaluated our models on a semantic relatedness task and achieved notable results compared to tree-lstm based methods with no attention as well as other neural and non-neural methods and good results compared to tree-lstm based methods with attentio ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "I", "Section": "I Introduction", "Text": "long short term memory units are very effective when working on sequential datafor some natural language processing tasks, we often need to find a distributed representation of phrases and sentences -.  one obvious way of doing this is to use a sequential lstm which captures word order in a sentencebut we can also have information about sentence structure from a dependency parse tree or about phrase structure from a constituency tree despite the fact that rnn based models work well with sequence information, they frequently neglect to catch any sort of semantic compositionality if the information is structured rather than in the sequential frameg, i went to the church which has nice windows the term compositionality can also be explained in terms of a ca  attention - was first introduced for doing machine translation where the target word generated by the decoder at each time step is aligned with all the words in the source sentenc  in its general form, attention allows a model to put importance on certain parts of the sentence for doing any specific downstream taskin a dependency tree, the relationship between the entities are organized as a structure where a head word can have multiple dependents under i  in the case of a constituency tree, a phrase is represented by one of the subtrees with the root being the phrase type and words or subtrees being the childre  in both tree structured lstms, the derivation of the vector representation of the entire tree does not depend on all of the subtree components uniforml  some parts of the tree have a larger influence on the root vector and some parts may have les  this contribution from subtrees for the building of the whole tree depends on the underlying task that the model is performin  for example, in a sentiment analysis task the sentiment of a tree depends on the sentiment of all of its children and how this information propagate  there may be scenarios where a single word flips the sentiment of the whole subphras  these words should get more attention when deciding the sentiment of a subphrase containing the  on the other hand, when the problem is a regression problem with the task of assigning a score based on the semantic similarity of two sentences, this attention can be calculated as a cross sentence attentio  in this case the representation of one sentence can guide the structural encoding of the other sentence on the dependency as well as constituency parse tree capturing semantic relatedness means recognizing the textual entailment between the hypothesis and the premise the general approach of modeling sentence pairs using neural networks includes two steps: represent both of the sentences as vectors via a sentence encoder and then initializing a classifier with these vectors to do the classificatio  the sentence encoder can be viewed as a compositional function which maps a sequence of words in a sentence to a vecto  some of the common compositional functions are sequential lstmtree-lstmand cnn in this paper, we propose two models to encode attention inside tree structured lstm cells and verify their effectiveness by evaluating them on the semantic relatedness task where the model needs to give a score depending on how similar two sentences ar  the tree data structure allows a set of dependents in the dependency tree or constituents in the constituency tree to be children of an immediately higher level tree nod  our tree attention model applies attention over the set of children in a subtree and decides which of them are important to reconstruct their parent node vecto 00066v1 1 jan 2019 vectors of the dependents or the constituent  our extensive evaluation proves the effectiveness of our attentive tree-lstm with respect to the plain tree-lstm models as well as some top performing models on the benchmark datase ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "II", "Section": "II Related work", "Text": "socher et a  propose a number of recursive neural network based models which take phrases as input rather than entire sentence  phrases are represented as a vector as well as a parse tre  vectors for higher level nodes in the tree are computed using a tensor-based composition functio  their best model was matrix vector rnn where each word is represented as a vector as well as a matri  in this model the children in a subtree interact more through their vectors rather than being influenced by some weights during the calculation of the parent's vector and matrix representatio  tai et a  developed two different variants of standard linear chain lstms: child sum tree-lstm and n-ary treelst  the standard lstm works over the sequence data whereas these variants are compatible with tree structured data also, unlike standard lstms, the hidden and cell states of a word at the current time step does not depend on the entire sequence seen befor  instead, the hidden and cell state of a parent node depends only on its children hidden and cell state  recently, chen et a  combined lstm with tree lstm for natural language inference task and empirically proved that these two models complement each other very wel  zhou et a  extend the concept of standard tree-rnns and propose a number of attention based tree-rnn models to perform the semantic relatedness tas  their insight was quite novel: in order to compute the semantic similarity of two sentences, one can encode attention in the tree structure of one sentence with respect to the vector representation of the other sentenc  however, their proposed attention model only works with child sum tree-lstms and gru  attention with tree lstm has also been studied by liu et a  for text summarization task where they use two different kinds of alignment : block alignment for aligning phrases and word alignment for aligning inter-words within phrase  turning to machine translation, the attention mechanism is used to align the source and target sentences in the decoding phas  more formally, the attention mechanism allows the model to attend to some elements with the intention of emphasizing different element  the well-known attention models from and use recurrent models to attend over a set of source words during the generation of each target wor  using recurrent models to generate an attention score incorporates a memory mechanism inside the network which helps the model at run time to traverse and decide what to attend ove  also, this recurrency allows some positional information in the sequence to help ordering the generated word  parikh et a  propose a decomposable attention model for natural language inference tasks by removing the modules with recurrent behavior during the calculation of attentio  first, they pick a single vector from a set of vectors representing the source sentence and then compare its point-wise similarity with every element of each word vector from the target sentenc  following this, they compare these alignments using a function which is a feed forward neural network and finally perform an aggregation through summation before doing the final classificatio  gehring et a  propose a sequence to sequence learning framework utilizing a convolutional neural network which completely avoids recurrent models allowing their architecture to be parallelizabl  in order to capture the positional information, they include a positional embedding layer which gives their model a sense of the portion of the sequence in the input or output it is currently dealing wit  they encode sine and cosine frequencies for each dimension of every position in the sentence to create the positional embeddings and finally combine them with word embedding  vaswani et a  combine the previous two works and propose a powerful machine translation framework utilizing attention without recurrence and positional embedding  they also extend the decomposable attention mechanism by attending over the input sequence multiple times stating it as a multihead attention where the target is to extract different features by different attentional head ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "III", "Section": "III The Model", "Text": "in this section, we describe our work in detai  we first explain how the two variants of tree-lstm wor  following this, we describe our universal attention mechanism that is applicable for these two tree-lstm variant  additionally, we give an in-depth analysis of adding this attention with respect to various information as discussed in section i    incompatibility of standard lstm and tree structured data recurrent neural networks are the best known and most widely used neural network model for sequence data as they sequentially scan the entire sequence and generate a compressed form of i  to overcome this drawback some rnn variants have been introduced such as long short term memory and gated recurrent unit these variants use a gating mechanism to propagate new information further and at the same time to forget some previous information allowing the gradients to propagate furthe  performance-wise, lstms are superior to grus because they have more parameters but in terms of computational complexity grus often surpass lstm  this extra information is usually represented in a tree structur  the tree structure shows how the words combine through different sub-phrases to reflect the overall meanin  if a sentence gets traversed by a standard lstm, the latter part of the sentence gets more importance comparatively as the traversal moves left to righ  but if the tree structure of the sentence gets traversed from bottom to top then the information from different constituent or dependents first gets combined to represent the root at the upper level and then this roots gradually gets traversed as children and combined to represent the root at next level and so o  so in both cases an lstm cell will forget previous information which for plain lstm, is related to the length of the sentence and for tree-lstm, is related to the depth of the tre  but in tree-lstm, the hidden and cell state of a root word depends only on the hidden and cell state of all of its children rather than all the words before i  tree-lstm there are two possible tree representations of a sentence: dependency tree and constituency tree as previously presented, the standard linear chain lstm and blstm cannot correctly analyze this structured informatio  to properly deal with this structured data, tai et a  proposed two lstm models which can analyze a tree structure preserving every property of the standard lstm gating mechanism  they called the first one child sum tree-lstm and the second one n-ary tree-lst  child sum tree-lstm fits well with dependency trees as it is well suited for high branching childunordered tree  on the other hand n-ary tree-lstm works better with the binarized constituency tree  traditional lstm generates a new hidden and cell state from the previous hidden state ht-1, previous cell state ct-1 and current sequential input x  in the child sum tree-lstm, a component node state is generated based on the states of its children in the tree as shown in fi  this multiple forget gate allows child sum tree-lstm to incorporate individual information from each of the children in a selective manne  since the hidden state and cell state values of the parent node are generated based on the hidden state and the cell state of its children, child sum tree-lstm is well suited for dependency tree  the n-ary tree-lstm is used where there are at most n ordered childre  unlike the child sum tree-lstm, it has a different set of parameters for each child having its own cell and hidden state, shown in fi  attention the two tree structured lstm models described in section iii-b treat every word within a sub-tree with equal probabilit  more specifically, in an n-ary tree-lstm, every word contributes uniformly to the building of the higher-level constituen  when viewing the tree as a semantic representation of a sentence, this may not be the case in many scenario  for a constituency tree, if a sub-tree contains some negative sentiment words, then it is not always the case that the sentiment of that particular constituent is negativ  if the negative sentiment word is preceded by a negation, then the higher-level constituent becomes semantically positive because of the location of the negation wor  to capture this type of information, attention is applied over the sub-tree components to apportion the importance of each sub-tree component when building the entire tree either semantically or syntacticall  in this study, we are interested in applying semantic attention over the sub-tree components to see how they contribute to building a sub-tre  attentive tree-lstm was proposed by for doing the semantic relatedness tas  they state that the effect of semantic relevance could be implemented as part of the sentence representation construction process using a tree-lstm where each child should be assigned a different weigh  in their proposed model, a soft attention mechanism assigns an attention weight on each child in a subtre  14, one vector in eq  15 and one matrix in eq  this attention mechanism is only applicable to the child sum tree-lst  it is not possible to apply this attention on n-ary tree-lstms since the structure of the n-ary treelstm is such that it needs n separate hidden states to work with whereas a child sum tree-lstm collapses all the hidden states to a single vector through summatio  in this study, we develop two generalized attention models by adopting the decomposable attention framework proposed by and the soft attention mechanism proposed by model 1: our first model is based on the self attention mechanism where we make some subtle changes to calculate the attention probability with respect to different segments of the sentenc  calculating attention in this way involves three matrices key, query, and valu  the key matrix represents on which child to attend over, the query matrix represents with respect to what is attention to be applied and the value matrix extracts the final attention-able vector using attention probabilit  for child sum treelstms, this matrix is the concatenation of the vectors of all the words under a particular head wor  for n-ary treelstms, it is the concatenation of all the word vectors in a constituen  so in both cases the formal representation is m = the d is being used here as a normalizing facto  it contains attention encoded hidden state values of all the children sequentially one on top of anothe  so in order to locate a specific hidden state value, the row number corresponding to the position of that child in the sub-tree is use  for the semantic relatedness task, where the objective is to assign a score based on the similarity between two sentences, it is better to calculate the query matrix with respect to the vector representation of the second sentenc  specially, given a pair of sentences, our generalized attentive encoder uses the representation of one sentence generated via a sequential lstm to guide the structural encoding of the other sentence on both the dependency as well as the constituency tre  in that case, m is a vector rather than a matrix thus changing the shape of query from eq  this results in an alignment vector from eq  finally, instead of doing a matrix multiplication as in eq  for child sum tree-lstms, we use this new hidden state vector in place of the one generated in eq  1 and for n-ary tree-lstms, we use this hidden state vector as the hidden state of both the left and right childre  this way of calculating self attention requires three additional matrices as parameters from eq  we further continue our experiments by calculating a phrase vector representation using an additional lstm cell and use it as the query vecto  however, this requires more parameters than what is required in here, we have two matrices key and query and their derivation are the same as eqn  we further align and transform these matrices into probabilities using the same set of equations, equations 21 and 2  we again make some subtle changes which result in four different versions of this mode  in eq  we use that as it i ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "IV", "Section": "IV Experimental Setup and Analysis", "Text": "in this section, we describe the detailed experimental setup for the evaluation of our stud  we first explain the dataset statistics for evaluating our generalized attention framework  following this, we explain the working environment details along with the hyper-parameter settings of our architectur  we evaluated our model for the semantic similarity task on the sentences involving compositional knowledge dataset the task is to give a likeness score for a pair of sentences and then compare it to a human produced scor  each sentence pair is annotated with a similarity score ranging from 1 to   a high score shows that the sentence pair is strongly relate  all sentences are derived from existing image and video comment dataset  table i: ranges of different hyper-parameters searched during tunin  hyper-parameter range selected learning rate  05 table i shows the detailed hyper-parameter settings of our mode  we used pytorch  4 to implement our model under the linux environmen  this table also contains the results of some top performing models on the sick datase  however also experimented with attentive tree-lstms and grus, but they have only been able to design models compatible table ii: test set results on the sick datase  the first group lists previous results, and the remainder are the results of our model  we mark models that we re-implemented with a previous models model r mse ecnu   tree-gru + att  tree-lstm + att 2479 with the child sum varian  for both child sum as well as binary tree-lstms, our second model with cross sentence attention has superior performance compared to the plain tree-lstm variants getting mse of   for the child sum tree-lstm, model 1 performs poorly compared to all the other model  this poor performance is due to the hard attention that it applie  the rest will not contribute at al  our best performing attentive child sum tree-lstm model with cross sentence attention achieves a better result than the plain child sum tree variant from our score did not surpass the reported result of the attentive child sum variant from however, our implementation of their model with their reported hyper-parameters gave a  2591 mse which is significantly worse than their claimed ms  this suggests to us that the implementation environment has a strong impact on model performanc  our child sum tree-lstm model 2 with cross sentence attention achieves better performance than our implementation of using their hyper-parameter setting  to the best of our knowledge, our work is the first to encode attention inside a binary tree-lstm cel  in terms of binary tree lstm, our best performing model with cross sentence attention achieves  2435 mse which is significantly better than the one reported in for the non-attentive versio  in our implementation of plain binary tree-lstm without attention from we were not able to reproduce their reported result and ended up with   this performance analysis does show the effectiveness of our generalized attention mode  figure 2 depicts the probability assigned to each node in the dependency tree by our model 2 with cross sentence attentio  unlike standard child sum tree-lstm, where the hidden states of all the children nodes are combined with a plain summation, our attentive child sum tree-lstm assigns a weight to each node and then does a weighted summatio  the example used in this figure has a man is exercising as the left sentence, a man is doing physical activity as the right sentence and entailment as their relationshi  as usual, the main verb from both of the sentences is selected as the root nod  the auxiliary verb gets high attention in both the left and right trees because of the word similarit  however, their absolute influence varies because of the presence of semantically related words in other branches as discussed abov  both of these trees share the same nominal subject however with different probabilities the reason behind this is the cross sentence attention allows the word man from the left sentence to align with two words man and physical from the right sentenc  as they share a similar semantic meaning in the none man is a  90 exercising det nsubj aux none man is a  64 doing det nsubj aux root root activity dobj  0 amod fi  root s np vp dt nn a man vbz is vp vbg np playing dt nn a violin  45 rp harping on pp in about   vector space, the branch in the left sentence that contains man is diminished because the right sentence divides the attention between two branches figure 3 depicts the probability assigned to each node in a binary constituency tree using an attentive binary tree-lstm with cross sentence attentio  in this setting, the attention on the structure of the left sentence is computed with respect to the vector representation of the right sentence and vice vers  as a result, the words in a specific phrase from the left sentence are aligned with very high attention probability if the same words appear anywhere in the right sentenc  however, as softmax was operating with small values from eq  21, it forced both children to have the same probabilities 23 with pairs of the same value other than results in the model giving comparatively poor performanc  finally, for the inference of attention probabilities, we replaced softmax from eq  22 with plain normalizatio  for the example in fi  the phrase np gets almost the same probabilities in both the left and right trees because of having the same set of words: a ma  the subphrase vbz under vp in both trees gets very high attention due to having the same word is at exactly the same positio  due to the chomsky normalization, the tree on the right side gets an extra dummy node x which contains vbg and rp as the child node  in the vector space, the words playing and harping are semantically connected which allows both of the models to align them with moderately high as well as equal probabilitie  the left tree does not have any particle words which causes the model to put low attention probability when it appears on the right tre  the left tree has np as the right child of vp at level 3 with probability  55 which is quite close to the amount of attention pp gets as the right child of vp at the same level in the right tre  again in both of these trees, at the right most branch, the words play and violin share the same semantic space which causes them to get aligned with almost the same probabilitie  the dt in this branch gets the same high probability because of appearing in both sentences at relatively similar position ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "V", "Section": "V Conclusion", "Text": "previous attempts to encode the attention mechanism in tree-lstms were only successful for the child-sum tree variant as the techniques used are not easily adaptable to binary trees like the chomsky normal form constituency tre  in this paper, we have introduced two different ways of applying attention on tree structure  the second of these two methods gives superior performance for both tree variant  the proposed techniques can be used on both dependency as well as constituent tree structur  our experimental results verify the superiority of the attentive variant of tree-lstms over traditional tree-lstms and linear chain lstms on the semantic relatedness tas  with our extensive in depth analysis, we showed that our proposed attention models provide a good representation of how a sentence builds semantically from the word  our generalized attention framework is adaptable to any tree like structure ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "References", "Section": "References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "References", "Section": " References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": " a bivariate spatial process model is developed to accommodate the correlation structures typically seen in structural brain imaging dat  first, we allow for spatial correlation on a graph structure in the imaging phenotypes obtained from a neighbourhood matrix for measures on the same hemisphere of the brai  second, we allow for correlation in the same measures obtained from different hemispheres of the brai  we develop a mean-field variational bayes algorithm and a gibbs sampling algorithm to fit the mode  we also incorporate bayesian false discovery rate procedures to select snp  we implement the methodology in a new a release of the r package bgsmt  we show that the new spatial model demonstrates superior performance over a *the authors wish it to be known that the first two authors should be regarded as joint first author  corresponding author: nathoo@uvi c 00068v4 24 may 2020 standard model in our applicatio  data used in the preparation of this article were obtained from the alzheimer's disease neuroimaging initiative database ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "we consider multivariate multiple regression modeling within the context of imaging genetics where interest lies in uncovering the associations between genetic variations and neuroimaging measures as quantitative traits this problem has received a great deal of attention recently and is challenging because it combines the analysis of neuroimaging data with genetic data recent reviews of statistical issues in this area are discussed in liu and calhoun and nathoo et a  the neuroimaging measures can serve as endophenotypes for neurological disorders such as alzheimer's disease as described in szefer et a the estimated heritability of lateonset ad is 60 - 80 percent the remaining heritability of ad may be explained by many additional genetic variants and these may have a small effec  in our work, we consider the setting where interest lies in assessing the association between a moderate number of brain imaging phentoypes and with the number of snps ranging from between a few hundred to a few thousan  within this setting a multivariate model with regression matrix jointly characterizing the associations between all rois and genetic markers is feasibl  greenlaw et a  propose a bayesian group sparse multi-task regression model where the primary focus is the use of a shrinkage prior based on a product of multivariate laplace kernels developed following the ideas of park and casella and kyung et a  the specific prior developed is motivated by the penalized multi-task regression estimator proposed by wang et a  this development is an effort to move from point estimation to bayesian credible intervals and fully bayesian inferenc  the assumed covariance structure ignores spatial correlation as well as bilateral correlation across brain hemisphere  we develop a new model that allows for this type of correlation by adopting a proper bivariate conditional autoregressive process for the errors in the regression mode  while spatial models for functional magnetic resonance imaging and other neuroimaging modalities have been developed to a large extentto our knowledge there has been very little development of explicitly spatial models for imaging genetic  one exception is the mixture model developed by stingo et a  where an ising prior, a binary markov random field, is used for bayesian variable selectio  our model is rather different in both its aims and structure as it is based on a continuous bivariate markov random field that is specified at the first level of the model for the imaging phenotype directl  in figure 1 we show several summaries of the data from our motivating application demonstrating the need to account for correlation across brain hemisphere  for example, the sample correlation between the volume of the right cerebral cortex and the volume of the left cerebral cortex is   the bivariate car structure allows us to account for this between-hemisphere correlation while also allowing us to account for within-hemisphere correlation using a graph structure based on a neighbourhood matri  typically, models incorporating multivariate car specifications are used for modelling observations or spatially-varying parameters when multiple observations or parameters appear at each spatial sit  panel presents the boxplot of all of the pairwise bilateral correlations for the 28 brain measurements across left and right hemisphere  panel presents a scatter plot showing mean thickness of left/right frontal with correlation   panel presents a scatter plot showing the volume of left/right cerebral cortex with correlation   panel presents a scatter plot between volume of left/right cerebral white matter with correlation   pair corresponding observations on opposite hemispheres of the brain and use the bivariate spatial process to model a combination of the bilateral correlation across the left and right brain hemispheres as well as potential correlation within each hemispher  as a matter of fact for the mri data considered in our application the bilateral correlation is a very strong signal in the observed data and so it is important to account for i  for the bivariate spatial model we use a separable bcar process as it is reasonable in our application to assume that the spatial covariance on the two hemispheres of the brain is simila  this spatial process is combined with a group lasso prior for the regression coeffcients, where each group corresponds to a single row of   each row in this case represents the associations between a given snp and all of the phenotype  to compute the posterior distribution we develop two algorithms, both of which are implemented in our r package bgsmtr for imaging genetics regression modellin  the package is available for download on the comprehensive r archive network the first algorithm is a gibbs sampling algorithm and the second is a faster mean-field variational bayes approximation to the posterior distribution within the context of hierarchical models for spatial data, mean-field vb inference has been considered by ren et a  who make comparisons with inference from mcmc within the context of spatial process model  in addition to the computation of the posterior distribution, the bgsmtr package now incorporates bayesian fdr procedures for snp selectio  this can be used alongside or as an alternative to snp selection based on credible interval  the overall contribution of our work is four-fol  first, we develop an explicitly spatial model for imaging genetics based on the bcar proces  second, we develop both an mcmc algorithm and a mean-field vb algorithm for approximating the posterior distributio  third, we incorporate bayesian fdr procedures for snp selection within the new spatial mode  fourth, our new developments are implemented in the latest version of the bgsmtr r packag  the remainder of this paper is structured as follow  in section 2, we present our new spatial model for imaging genetic  computation of the posterior distribution and snp selection is discussed in section   section 4 presents a simulation study evaluating the performance of the spatial model relative to a non-spatial model and inference based on mcmc relative to that from v  the paper concludes with a discussion in section 6", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Bayesian Spatial Regression Model", "Text": " our model is developed for settings where the imaging data are symmetric with the same measures collected on each hemisphere of the brai  this is true when the neuroimaging data are considered at the voxel level and it is also the case for the study considered here where we analyze mri data from the adni-1 database preprocessed using the freesurfer v4 software we conduct automated parcellation to define volumetric and cortical thickness values from the 28 rois considered in shen et a szefer et a and greenlaw et a  on each hemisphere leading to c = 56 brain measures in tota  as described in szefer et a potential confounders in the analysis are population stratification and apoe genotyp  since true population structure is not observed, a set of principal coordinates from multidimensional scaling are used to derive proxy variables for population stratification in the dat  we also adjust for apoe genotype, since it can account for the population stratification in the data, over and above the principal components or principal coordinates the response imaging measures at each brain roi are first adjusted for the ten principal coordinates, as well as for dummy variables representing apoe genotype, using weighted ordinary least squares regressio  the residuals from each regression are then used as the adjusted neuroimaging phenotypes as far as we are aware, this spatial model for neuroimaging data is one of the first to explicitly model dependence across brain hemispheres in addition to accounting for local dependenc  often, this left/right bilateral dependence is ignored with neuroimaging dat  in our data it is a very clear and strong signal as is evident in figure   tuning of the model is discussed in section  3 after our discussion of computational algorithms for fitting the mode ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Computation and SNP Selection", "Text": "1 bayesian computation posterior computation can be implemented using gibbs samplin  the update steps for this algorithm are listed in algorithm 1 and their derivations are given in the supplementary material as a faster alternative approach to computing the posterior distribution, we also develop a mean-field vb algorith  the functional f is referred to as the evidence lower bound this leads to a set of update equations that are iterated until convergence to a local optimu  these update equations are presented in algorithm 2 and their derivations are detailed in the supplementary material to initialize the variational bayes algorithm, we use a ridge regression estimator obtained separately for each column of w obtained by fitting ridge regression with individual scalar-valued phenotypes as the respons  the ridge estimators are then used to initialize the mean of the variational posterior distributio  the output of variational bayes is then used to initialize the mcmc sampling algorith ", "Subsections": [{"Section_Num": "3_2", "Section": "3.2 Bayesian FDR", "Text": "the bayesian fdr procedure applied in our work for snp selection follows the approach developed in morris et a but it has been adapted and implemented for the current spatial mode  let c*be a known critical value that is chosen a priori to represent an effect size of interes ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3_3", "Section": "3.3 Model Selection and Tuning", "Text": "to compare the spatial and non-spatial models and to choose values for the tuning parameters, one option is the use of the waic an alternative approach to tuning the model is to use a simple modified moment estimator based on the ridge regression estimator that we use to initialize v  this estimator implicitly uses cross-validation which is used to tune the ridge estimator and is much faster than applying cv directly to our bayesian mode 95 corresponding to a reasonable degree of spatial dependence on the graph represented by the neighbourhood matrix   the waic can also be used to compare the spatial model with the non-spatial mode ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Simulation Studies", "Text": " the data are simulated from the spatial model with parameter values set at the estimates obtained from the real dat  the tuning parameters are chosen using the waic over a coarse grid for the spatial model implemented via mcmc; set using the default approach in the r package for the model of greenlaw et a 95 for vb five measures are used for comparison in this study and are listed in table   for the best overall estimation performance, the spatial model implemented using mcmc has an average mean-squared error of  0055, followed by the non-spatial model with an average mse of  012 and finally the spatial model implemented with vb with an average mse of   here, the average is taken over the 27,216 regression coeffcients in the mode  next, we examine the correlation between the 14 posterior mean estimates of vec and the true values averaged over simulation replicate  the spatial model implemented using mcmc has the best performancefollowed by the nonspatial model implemented via mcmc and then the spatial model implemented using vb the average-squared bias is lowest for the non-spatial modelfollowed by the spatial model implemented with mcmc and finally the spatial model implemented using vb has the highest average-squared bia  both the non-spatial and spatial models appear to exhibit adequate coverage probability overall when implemented with mcmc while the variational bayes implementation of the spatial model exhibits coverage of 95% equaltail credible intervals that is slightly lower at   the average posterior standard deviation obtained from the spatial model is slightly larger than that obtained from the non-spatial model perhaps indicating that the spatial model is adequately accounting for the dependenc  as is typical with variational bayes the posterior standard deviation is underestimate  this arises from the kullback-leibler objective function kl for variational bayes which under-penalizes approximations that are under-disperse 05, we conduct an additional 100 simulation replicates where the error structure underlying the simulated data is again set to that obtained from the real data applicatio  here we evaluate the empirical fdr for the spatial model and both of its implementation  the methodology of greenlaw et a  uses credible intervals rather than bayesian fdr for snp selection", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 ADNI-1 Study of MRI and Genetics", "Text": "we apply our spatial model as well as the group sparse multi-task regression model of greenlaw et a  to mri and genetic data collected from n = 632 subjects from the adni-1 databas  the data presented here are queried from the genome build as of december 2014, from the adni-1 dat  after quality control and imputation steps, the genetic data used for this study include 486 snps from the 33 targeted genes discussed in szefer et a  the freely available software package plink is used for genomic quality contro  subjects are included if their genotyping data is available, they have a baseline mri scan, and they have at least one additional follow-up baseline sca  our thresholds for snp and subject exclusion are the same as in wang et a  with three exception  in the data quality control step, we used a stricter minimum call rate of 95% on snps v  wang et a 's call rate of 90%.  to assign snps to genes, we use a genome build from december 2014 whereas these authors use a genome build from september 2006, and use all subjects with a baseline measurement whereas we choose subjects with a baseline mri scan and a scan at at least one additional time point in the longitudinal stud  the response measures are obtained by preprocessing the mri data using the freesurfer v4 software which conducts automated parcellation to define volumetric and cortical thickness values from the 28 rois considered in szefer et a  and greenlaw et a  these rois are chosen based on prior knowledge that they are related to alzheimer's diseas  in addition, we fit the non-spatial model of greenlaw et a  in all cases, mcmc sampling is run for 10,000 iterations with the initial 5,000 iterations discarde  the required computation time for the spatial model is 50 hours on a single core with 20gb of ram, while the computation for the non-spatial model is 5hr  some trace plots and mcmc convergence diagnostics are presented in supplementary material web appendix e and these demonstrate rapid convergence and good mixing of the mcmc sampling chain  the vb algorithm is run to convergence and requires 45 minutes with the elbo converging in approximately 16 iteration 95 for v  the convergence of vb based on successive values of the elbo is depicted in figure   the waic obtained 17 for the non-spatial model with value of the tuning parameters used in greenlaw et a  as expected, the curves are monotone decreasing but we note that their shapes differ when comparing the algorithm  we suggest that the vb algorithm be used for obtaining starting values to initialize the mcmc as well as a tool to gain some initial insight into the data while the mcmc sampler runs to completio  this is useful because the mcmc sampler requires a relatively long run time, and the vb algorithm can be used initially while the mcmc sampler runs each region is represented with a curve in each panel of the figur  the left panel shows this relationship for mcmc combined with bayesian fdr while the right panel shows the same relationship for vb with bayesian fdr with the vb approximation, 150 snps are selected, and the set of snp-roi pairs selected by mcmc is a proper subset of the set selected by v  in addition, the subset of snps and phenotypes also selected by the approach of greenlaw et a  where the marginal posterior 95% credible interval is used for snp selection are also highlighted in bold in table 1 of the supplementary materia  we note that the selected associations for this snp all correspond to rois in the right brain hemispher  the associations between the genetic signal represented in our analysis by apoe snp rs405509 with phenotypes on the right hemisphere of the brain may be of potential interest for further investigatio  the higher bias arising from vb in the simulation studies may also be apparent in the results presented in table   this is consistent with the coverage probabilities found in the first simulation stud  another consistent signal is found at the ace gene with snp rs4311, which is found associated 19 with 12 roi  we note that all but one of these rois is in the right hemisphere, and three of these rois are in common with the rois selected for this snp by greenlaw et a  in figure 4 we indicate the snps chosen for each roi, where the snps are grouped on the x-axis by gene and the rois are grouped in left/right pairs on the y-axi 8 as suggested by supplementary material figure   these are leftsupramarg and left-suptemporal these include snp rs10868609 for the thickness of the left supramarginal gyrus and rs10501426 for the thickness of the left superior temporal gyru ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "6", "Section": "6 Conclusion", "Text": "we have developed a spatial multi-task regression model for relating genetic data to imaging phenotype  the error structure for the imaging phenotype is based on a computationally convenient proper bivariate conditional autoregressive model, which allows us to account for bilateral correlation across brain hemispheres while also allowing us to account for within-hemisphere correlation using a graph structur  examination of the data can assist in the determination of whether a model allowing for correlation is require  in our case it is clear that a model accounting for correlation is indeed required for bilateral correlatio  empirical correlation between neighbouring regions can also be computed to determine if a simpler model should be considere  the black ticks on y-axis indicate the phenotypes from the left/right hemisphere, and the snps from same gene are indicated by the ticks on x-axi  snps by gene phenotypes amygvol cerebctx cerebwm hippvol inflatvent latvent entctx fusiform infparietal inftemporal midtemporal parahipp postcing postcentral precentral precuneus supfrontal supparietal suptemporal supramarg temporalpole meancing meanfront meanlattemp meanmedtemp meanpar meansensmotor meantemp apoe tnk1 ace bin1 il33 adam10 sorl1 ece1 nedd9 dapk1 sorcs1 snps by gene phenotypes amygvol cerebctx cerebwm hippvol inflatvent latvent entctx fusiform infparietal inftemporal midtemporal parahipp postcing postcentral precentral precuneus supfrontal supparietal suptemporal supramarg temporalpole meancing meanfront meanlattemp meanmedtemp meanpar meansensmotor meantemp apoe tnk1 ace bin1 il33 adam10 sorl1 ece1 nedd9 dapk1 sorcs1 21 and robustness relative to that currently implemented under different settings will be part of future wor  future developments may also incorporate the bayesian false discovery probability proposed by wakefield et a  which allows the user to account explicitly for the cost of false discoveries and the cost of false non-discover  with regards to the two computational algorithms, we recommend that the approximate vb procedures be used to initialize the mcmc algorithm and also to obtain an initial insight into the data while the mcmc sampler run  nevertheless, figure 3 suggests that the approximation can be reasonable when good initializations are use  acknowledgements research is supported by funding from the natural sciences and engineering research council of canada and the canadian statistical sciences institut  nathoo holds a tier ii canada research chair in biostatistics for spatial and high-dimensional dat  research was enabled in part by support provided by westgrid and compute canada data collection and sharing for this project was funded by the alzheimer's disease neuroimaging initiative and dod adni 22 table 1: summaries from simulation study   the averages are taken over the 27,216 regression coeffcients in all but 'cor  model mse cor  bias2 co  pro  s  non-spatial mcmc  087 spatial model mcmc  10 spatial model vb  12 cerebral white matter volume  13 inferior lateral ventricle volume - 08 inferior parietal gyrus thickness  13 postcentral gyrus thickness  14 superior frontal gyrus thickness  14 supramarginal gyrus thickness  14 supramarginal gyrus thickness  15 fusiform gyrus thickness  12 precentral and postcentral gyri mean thickness  12 precentral and postcentral gyri mean thickness  12 middle temporal gyrus thickness  11 middle temporal gyrus thickness  12 postcentral gyrus thickness  11 precentral gyrus thickness  11 superior parietal gyrus thickness  11 superior temporal gyrus thickness 0", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "recurrent neural networks for time series forecasting g abor petneh azi* doctoral school of mathematical and computational sciences university of debrecen abstract time series forecasting is diffcul  it is diffcult even for recurrent neural networks with their inherent ability to learn sequentialit  the description of the method is followed by an empirical study using both lstm and gru network ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "recurrent neural networks are well suited to supervised learning problems where the dataset has a sequential natur  time series forecasting should not be an exceptio  rnns are essentially neural networks with memor  they can remember things from the past, which is obviously useful for predicting time-dependent target  yet, applying them to time series forecasting is not a trivial tas  the aim of this article is to review some practices that can help rnns deliver useful forecast ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Neural networks for forecasting", "Text": "the objective of supervised learning is to predict something from dat  a training set of an output and some input variables is fed to an algorithm that learns to predict the target value  the output may be categorical or continuous the task of the algorithm is to deliver high quality predictions, all by itself, extracting the required knowledge solely from the available dat  usually, variants of gradient descent together with backpropagation are used to find the optimal values of the network weight  this is all simple and intuitive, yet the resulting networks are usually *gabo petnehazi@scienc unide hu 1 arxiv:190 00069v1 1 jan 2019 diffcult for people to understan  there are so many weights and connections that we just wonder how the system produced the result  we don't understand neural networks, but we like the  the reason for their ever high popularity is simple: they are goo  they can learn arbitrarily complex functions, and they often provide excellent predictions for pretty diffcult machine learning problem  nns are widely used in machine learning, time series prediction is just one example applicatio  werbos and werbos made a pioneering work in the field of neural networks by developing a general formulation of backpropagatio  werbos applied the method to forecasting, and compared it to traditional forecasting method  tang et a  also made a neural networks v  boxjenkins comparison and found that nns outperform the box-jenkins model for series with short memor  for series with long memory, both methods produced similar result  faraway and chatfield compared neural networks with box-jenkins and holt-winters methods for forecasting and found that the design of network architecture and the choice of input variables require great care, and so applying neural networks in black box mode is not a good ide  they also found that increasing the number of hidden nodes may deteriorate out-of-sample performanc  zhang and qi found that neural networks are not able to capture seasonality by default, and deseasonalization and detrending can help their forecasting performanc  according to balkin and orddifferencing is unnecessary for neural network based forecasting, but a log transformation may be beneficia  zhang et a  gives a detailed review of neural networks for forecastin  gamboa provides a more recent review of the applications of deep learning to time series dat ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Recurrent neural networks", "Text": " the main idea behind recurrent neural networks is using not only the input data, but also the previous outputs for making the current predictio  however, such simple solutions usually do not work as expecte  they are hard to train and they are forgetfu  rather, we need to have a system with some kind of memor  there are two popular and effcient rnn models that work really well: long short-term memory and gated recurrent uni ", "Subsections": [{"Section_Num": "3_1", "Section": "3.1 LSTM", "Text": "long short-term memory is a gated memory unit for neural network  it has 3 gates that manage the contents of the memor  these gates are simple logistic functions of weighted sums, where the weights might be learnt by backpropagatio  it means that, even though it seems a bit complicated, the lstm perfectly fits into the neural network and its training proces  the input gate and the forget gate manage the cell statewhich is the long-term memor  the output gate produces the output vector or hidden statewhich is the memory focused for us  this memory system enables the network to remember for a long time, which was badly missing from vanilla recurrent neural networks", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3_2", "Section": "3.2 GRU", "Text": "gated recurrent unit is essentially a simplified lst  it has the exact same role in the networ  it has 2 gate  since it does not have an output gate, there is no control over the memory conten  the update gate controls the information flow from the previous activation, and the addition of new information as wellwhile the reset gate is inserted into the candidate activatio  overall, it is pretty similar to lst  from these differences alone, it is hard to tell, which one is the better choice for a given proble  for a comparison, see chung et a  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3_3", "Section": "3.3 Recurrent neural networks for forecasting", "Text": "though it is probably not their primary application, lstm and gru networks are often used for time series forecastin  gers et a  used lstms with peephole connections to learn temporal distance  malhotra et a  used stacked lstm networks to detect anomalies in time serie  guo et a  proposed an adaptive gradient learning method for rnns that enables them to make robust predictions for time series with outliers and change point  hsu incorporated autoencoder into lstm to improve its forecasting performanc  cinar et a  proposed an extended attention mechanism to capture periods and model missing values in time serie  bandara et a  used lstms on groups of similar time series identified by clustering technique  laptev et a  applied rnns to special event forecasting and found that neural networks might be a better choice than classical time series methods when the number, the length and the correlation of the time series are hig  che et a  built a gru-based model with a decay mechanism to capture informative missingness in multivariate time serie  several attempts have been made on better understanding rnn  karpathy et a  explored the source of recurrent neural networks' performance with 3 performance comparisons and error analysis on character level language model  van der westhuizen and lasenby used different datasets to visualize the operations of lstm  greffet a  compared the performance of several lstm variants, and found that the forget gate and the output activation function are the most important elements of an lstm bloc  chang et a  proposed a feature ranking method using variational dropout some studies tried to find ways to measure the uncertainty associated with the time series forecasts of recurrent neural network  zhu and laptev made uncertainty estimates using monte carlo dropout and an encoder-decoder framework of lstm unit  caley et a  constructed prediction intervals for convolutional lstms using bootstrappin  here we are going to explore different aspects of rnn-based time series forecasting, and introduce an end-to-end framework for producing meaningful forecast ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Features", "Text": "1 feature engineering lstm and gru networks can learn and memorize the characteristics of time serie  it is not so easy though, especially when we only have a short series of values to learn fro  smart feature engineering can hel  there are very few things whose future values we certainly kno  all we have to do is extracting useful features that our algorithm can easily interpre  time series components, such as trend or seasonality can be encoded into input variables, just like any deterministic event or conditio  time-shifted values of the target variable might also be useful predictor  features are usually normalized before being fed to the neural networ  it is beneficial for the training proces  two popular choices for rescaling the variables are the minmax scaler and the standard scaler1 lags lagging means going some steps back in tim  lags of any number of time steps might be use  this might be a matter when the time series is shor 2 trend trend can be grabbed by features indicating the passage of tim  a single variable of equidistant increasing numbers might be enough for tha 3 seasonality we may try to find repetitive patterns in the time series by encoding seasonal variation  there are different ways to do thi  one-hot encoding is a reasonable choic  hereby, we treat seasonality as categorical variables, and use dummy variables to indicate the current time interval in the seasonal cycl  it is simple and intuitiv  however, it can not really grab cyclicality, since the distance between intervals does not matter during the encodin  it means that two successive, and possibly similar, time intervals are represented by just as independent values as any two randomly chosen time interval  also, one-hot encoding uses an individual variable to represent each unique value, which may be inconvenient when we have a large number of time interval  these deficiencies of the dummy variable approach lead us to another encoding metho  we can place the values on a single continuous scale instead of using several binary variable  this may be healed by transforming the values using either the sine or the cosine transformatio  in order to have each interval uniquely represented, we should use bot 4 dummy indicators we can use simple indicator variables for events or conditions that we consider importan  holidays are always special and are spent in unusual way  hence, a binary variable indicating holidays may carry information about the time serie  also, an indicator of working days or working hours could be usefu 2 feature importances neural networks consist of connected simple functions of weighted sum  it is not a diffcult structure, yet interpreting the meaning and role of the numerous backpropagation learnt weights is pretty har  it is the reason for too often calling them black boxes, and not even trying to understand the  recurrent networks are even a bit more complicate  this diffculty of interpretation makes neural networks somewhat less valuabl  we should naturally always prefer a simpler, more interpretable model, when having multiple choices with about the same forecasting performanc  so, we should brighten the black box, at least partiall  5 a measure of variable importance could tell something about what is happening within the neural network, yet quantifying importance is not trivia  several methods have been proposed, see gevrey et a  or olden et a  for a compariso  it is also called permutation accurac  we are permuting the variables, one by one, and calculate a measure of accuracy for each deliberately corrupted mode  the feature who's random permutation leads to the largest drop in accuracy is considered the most importan  two metrics will be used as measures of accuracy: r2 and md  r2 or coefficient of determination is a goodness of fit measure for regressio  mda or mean directional accuracy is the accuracy of the binary variable indicating the directional change of consecutive value  mean decrease accuracy can be calculated from both accuracy measure  variables are permuted separately, so the importance scores should be interpreted and evaluated independentl  variable importances do not add u  feature importances can be estimated from both regression accuracy and directional accuracy of one-step predictions on different validation set  it means there are quite some combinations, and it is likely that not the exact same variables will prove to be important in all case  therefore, it is worth computing some descriptive statistics of the different calculations to get a summarized view of the actual roles of input feature  the importance scores are usually normalized to sum to  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Prediction", "Text": "1 point predictions our recurrent neural networks are primarily suited for one-step-ahead forecastin  we have a single output variable containing the value that immediately follows the corresponding sequence of input  this simple framework can also be used for multi-step-ahead forecasting in an iterative manne  when the target variable's lagged values are the only non-deterministic features, we may use the predicted values as inputs for making further prediction  it is a bit risky though, since we roll the errors forwar ", "Subsections": [{"Section_Num": "5_2", "Section": "5.2 Prediction intervals", "Text": "neural networks have an important disadvantage: they only provide point estimate  we only get some forecasted values, without any indicators of the predictions' confidenc  6 prediction intervals contain future values with a given probabilit  such interval estimates could make our forecasts more meaningful, though producing them for neural networks is not eas  we are going to use the computationally expensive method of bootstrappin  the idea of bootstrapping was introduced by efron et a  this is actually a very simple method for estimating the quality of estimate  bootstrapping takes resamples with replacement from the original dataset to make separate forecast  the variability of those independent estimates is a good measure of the confidence of the estimatio  efron and tibshirani wrote a comprehensible review of bootstrapping, focusing on application  efron and tibshirani also note that the bootstrap can be used to construct confidence intervals in an automatic way, while alternative methods require several tricks to perform satisfactoril  bootstrapping is not only useful for quantifying the confidence in our predictions, but also for improving the forecasts themselve  ensemble methods combine multiple algorithms to deliver better prediction  bootstrapping is one way to construct an ensembl  there are 2 main approaches to ensemble learning bootstrap resampling can be used to construct an ensemble of this latter typ  bagging or bootstrap aggregating is an ensemble method that trains a learning algorithm on multiple independent bootstrap samples, and aggregates the resulting predictors by voting or averagin  it was introduced by breiman this simple ensemble can improve unstable single predictors, but it can slightly degrade the performance of more stable one  hence, applying bagging is not always a good idea, but it can work very well for some learning algorithm  dietterich remarks that since the generalization ability of neural networks is very good, they may benefit less from ensemble method  still, we may hope that bagging can bring some improvement to our point predictions, but the main reason for applying the method is the construction of prediction interval  bootstrapping has been applied to compute confidence and prediction intervals for neural networks by,   khosravi et a  compared four leading techniques for constructing prediction intervals, including the bootstra  they conclude that there is no method that outperforms all the others in all respect  tibshirani compared two different bootstraps to two other methods for estimating the standard error of neural network prediction  tibshirani concludes that the bootstraps perform bes  here we are going to use a method similar to the one proposed by heskes we take bootstrap samples of sequences, rather than individual observation  a separate rnn is trained on each bootstrap sampl  our final prediction is a simple average of all individual model prediction  we must make a distinction between confidence intervals and prediction interval  these two are easily confuse  confidence intervals quantify how well we can approximate the true regressio  to compute the intervals, we first construct the bagged estimator by averaging the resulting estimates of all bootstrap runs we assume that the true regression follows a normal distribution given the estimat  this measure is more important in practice, but it is a bit more diffcult to estimat  while the construction of confidence intervals required nothing more than the means and standard deviations of our resampled estimates, here we need some more sophisticated computations to estimate the noise variance of the regression it is a smart data recycling metho  heskes proposed loglikelihood as the loss function, we apply the similar formula used by khosravi et a  the output activation of this neural network is exponential, so that all predicted error variances are positive", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "6", "Section": "6 Validation", "Text": "1 point forecasts we are going to obtain a train and a test set by splitting the time series at one poin  it means that we always test the futur  we would expect the algorithm to tell the future, so this choice of validation is natural for such forecasting problem  yet, the test set consists of a single time period, so this method may not be entirely suffcient for evaluating the model performanc  bootstrapping provides an alternative validation se  an average bootstrap sample contains about 6 2% of all available data subsequence  the remaining subsequences do not participate in the training process, so we may use them for validation purpose  they have the same evaluation purpos  both sets will be used to evaluate the one-step-ahead forecasting ability of our recurrent neural network  the separate test set, being a complete chronologically ordered series of subseries, may also be used for iterative multi-step-ahead forecastin  regression and classification metrics are going to be applied in order to evaluate the forecasted values and the predicted changes of direction as wel  we are also calculating the accuracyprecisionrecall and f1 classification scores to evaluate the directional forecast  applying other evaluation metrics is also reasonable, since they all have different properties and interpretatio  classification accuracy score simply measures the proportion of changes whose direction we guessed righ  precision, recall and f1 are binary metric  an increase in the target variable is now treated as the positive clas  f1 score is the harmonic mean of precision and recal  we may get a better view of the forecasts' quality by using these various metrics togethe  also, we can draw a confusion matrix to evaluate all kinds of errors and correct forecasts at onc ", "Subsections": [{"Section_Num": "6_2", "Section": "6.2 Interval forecasts", "Text": "the quality of prediction intervals should also be evaluate  we use the same prediction interval assessment measures as khosravi et a  picp or prediction interval coverage probability is the proportion of observations that fall into the interva  mpiw or mean prediction interval width does exactly tha  nmpiw or normalized mean prediction interval width is the width normalized by the range of the target variabl  this metric allows for comparisons across dataset  cwc or coverage width-based criterion is a combined metric that takes into account both coverage and widt  the lower cwc the higher quality", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "7", "Section": "7 Empirical study", "Text": "our forecasting framework was implemented in python this section presents an example application on the bike sharing dataset available in the uci machine learning repository 10 r-v c-v r-t c-t season day of week month week of year lagged lagged hour holiday afternoon working hour working day month start quarter start trend 0", "Subsections": [{"Section_Num": "7_1", "Section": "7.1 Features", "Text": "the dataset is available in an hourly resolution, which allows us to construct several seasonal variable  it contains counts of bike rentals for capital bikeshare at washington,   we disregard the available weather information, and use time-determined features onl  the pandas library was used during the data preparatio  cyclical features were encoded using the sine and cosine transformation  using all these features together is probably a bit redundant, but hopefully the neural network is smart enough to handle i  each feature was scaled between zero and one using minmax scale  variable importances were calculated on the bootstrap left-out validation set and on the separate test set as well, using the single step forecast  two accuracy metrics were applied: r2 as a measure of goodness-of-fit, and mda as a measure of directional accurac  this setting led to 4 different estimates of variable importance those estimates were then averaged into a single list of importances naturally, the sine and cosine transformed values of cyclical features were also treated togethe  the maximum mean decrease accuracies are reported for these grouped variable  recent lagged values constitute the most important group of variable  they are especially important for the value forecasts the trend variable does not seem to have that much importance, though it was evident during model building attempts that it can add to the rnns' forecasting capabilit  variable importances were pretty similar for the lstm and gru network  we can hardly see any disagreements in the feature ranking ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "7_2", "Section": "7.2 Predictions", "Text": "our recurrent neural networks consist of a one-layer lstm/gru of 32 units, followed by a dense layer of a single unit with a linear activatio  a dropout of  5 was applied to the non-recurrent connection  the learning rate was set to   the batch size and the number of epochs were 12  the mean squared error loss function was minimized using the adam optimizer 16-step unrolled sequences were fed to the algorith  these hyperparameters were not optimize  it is just a reasonable setting for showcasing the method on the given datase  50 bootstrap samples were taken from the training se  forecasts were made with approximate 90% prediction interval  multi-step forecasts were generated for the test set onl  some predictions are shown in figure 5 and figure  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "7_3", "Section": "7.3 Evaluation", "Text": "several measures of forecast accuracy and directional accuracy were computed on both evaluation set  most of these evaluation metrics were computed using scikit-learn the results of the regression evaluations are available in table 1 and table   table 3 and table 4 show the change-of-direction binary metrics for the test se  confusion matrices of the directional predictions are displayed in figure 7 and figure   14 rmse mae smape r2 medae v-b-o 3 07 table 2: regression evaluation metrics multi-step forecasts have consistently higher errors than the one-step-ahead prediction  lagged values of the target variable proved to be important predictors, especially for forecasting values, rather than just direction  hence, the accuracy of values that we predict and reuse as inputs, matters a lo  and anyway, the distant future obviously holds much more uncertainty, than the next timeste  all r2 values are close to  95 for the single-step predictions, and are around  8 for the multi-step prediction  the multi-step rmse is about twice as large as the one-ste  for multiple steps, the direction of change forecasts seem to work somewhat better than the value forecast  by comparing the performance of the bagged estimator to the averaged performance of individual estimators, we might evaluate the usefulness of this ensemble method for recurrent neural network  the bagged estimators produced consistently better results than the individual neural network  this is true for the regression and the classification problem as wel  thus, it seems that bagging can improve rnn-prediction  accuracy precision recall f1 b-o   the two networks seem to produce very similar forecast  papadopoulos et a  found that the bootstrap consistently overestimates prediction interval coverag  it seems to be confirmed by our results, since all of our prediction intervals have a higher coverage than the targeted 90%.  our residual predictor neural network could probably have been further optimized, in order to generate intervals closer to the desired coverag  the cwc metrics suggest that the prediction intervals of one-step forecasts are bette  it is hardly surprising, since the multi-step forecasts reached similar coverage by producing much wider interval  the coverage of our prediction intervals exceeds the desired level of 90% in each case, so cwc equals nmpi  picp mpiw nmpiw cwc v-o  42 table 6: pi evaluation metrics", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "8", "Section": "8 Conclusions and Future Perspectives", "Text": "this study aimed to explore and describe several aspects of the application of recurrent neural networks to time series forecasting, though it is by far not comprehensiv  recurrent neural networks are much more flexible and much better suited to time series forecasting than the linear models usually applie  yet, several practices might help their application, some of which have been presented in this articl  we may do time series analysis with the aim of either forecasting future values or understanding the processes driving the time serie  neural networks are particularly bad in the latte  feature importance measures solve this problem partl  we computed permutation importance scores the target variable's lagged values were the most important predictors in our empirical experimen  seasonality features also seemed importan  another shortcoming of neural networks is the lack of prediction confidence measure  interval forecasts can be useful for quantifying uncertainty, though producing them for neural networks is nontrivia  bootstrapping is a simple, yet computationally expensive method that can do i  however, it produced pis with consistently higher coverage than what we targete  multiple-step forecasts generated higher errors than single-step forecasts, as expecte  the gaps between the errors seemed smaller in case of direction of change prediction  we found that recurrent neural networks can benefit from baggin  the lstm and gru networks showed about the same forecasting performanc  it is hard to argue for either of the  this forecasting framework might be enhanced in several way  during the training process, the states of the lstm/gru cells were reset in each batch, so the dependencies between sequences in different batches were not taken into accoun  we would expect higher accuracies if we took advantage of these relationships in the dataset, though it was diffcult with our bootstrapping framewor  our iterative method is probably not the best solution for making multi-step forecast  a sequence to sequence learning model might be a better choic  we could have constructed further input feature  feature engineering is crucial, and there is always room for improvemen  though neural networks are very flexible, so it didn't seem so necessar  feature importances were only computed for one-step forecast  it would be worth exploring, if different forecasting horizons require different features to make high quality forecast  other measures of variable importance could also 18 be applie  feature importances are only a tiny step towards understanding recurrent neural network  the mechanism of rnn cells could and should be explored in much more dept  bootstrapping is computationally intensive, but with today's ever improving gpus, it is a feasible algorithm for time series datasets of manageable siz  yet, it is a brute force method, so smarter solutions would be welcom  in this article, we did not aim to find the best parameter  grid search, or rather random search could have helped in finding the ideal setting  the size of real world datasets hinders deep learning methods in the field of time series forecastin  if our variable of interest were only observed quarterly or yearly, we would have to wait several lifetimes to acquire a reasonable amount of dat  even this 2-year hourly bike sharing dataset was way too small to exploit the capabilities of a neural networ  it would be very useful if we could train an algorithm on multiple similar datasets and gather some collective knowledge that could be used to make better forecasts for the individual time serie  this process of gaining knowledge and applying it to solve different problems is called transfer learnin  it is already commonly used in, for example, computer vision transfer learning is most useful when the training data is scarce, so applying it to time series forecasting seems very promisin  there is so much left to be don  rnns clearly deserve a seat in the toolbox of time series forecastin ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "0 an alma view of cs and sis around oxygen-rich agb stars   richards2,     van de sande1,   received yyy; in original form 2018 october 24 abstract we aim to determine the distributions of molecular sis and cs in the circumstellar envelopes of oxygen-rich asymptotic giant branch stars and how these distributions differ between stars that lose mass at different rate  these molecules are usually more abundant in carbon stars but the high sensitivity of alma allows us to detect their faint emission in the low mass-loss rate agb star  the high spatial resolution of alma also allows us to precisely determine the spatial distribution of these molecules in the circumstellar envelope  we run radiative transfer models to calculate the molecular abundances and abundance distributions for each sta  overall the isotopic ratios we derive for ik tau suggest a lower metallicity than solar", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "the asymptotic giant branch is a post-main sequence stage in the stellar evolution of low- to intermediate-mass stars with masses in the range   the agb phase is characterised by vigorous mass loss, with ejected matter forming an expanding circumstellar envelope molecules and dust grains form in the circumstellar envelope and are eventually returned to the interstellar medium in this way, agb stars are a significant source of chemical enrichment of the ism and contribute to the chemical evolution of galaxiesdanilovich@kuleuve be postdoctoral fellow of the fund for scientific researchflanders, belgium through optical spectral classificatio  the primary spectral classifications are based on the c/o ratio which in turn plays a key role in determining the chemical characteristics of the cs  stars arrive on the agb oxygen rich and over time carbon may be dredged up from the interior and lead to an increase in the surface c/o and hence a shift in the chemical properties of the cs  for a review on agb evolution and nucleosynthesis including hbb models we refer to karakas & lattanzio danilovich et a  sulphur is not synthesised in agb stars nor in their main sequence progenitor  this allows us to constrain the study of sulphur-bearing molecules in agb cses since the total abundance of sulphur can be estimated and does not change significantly over time, since it is not thought to be depleted onto dust sulphur is also the tenth most abundant element in the universe, meaning that when a significant portion is locked up in particular molecules, they are relatively easy to detect in observation  these have been most commonly studied using spatially unresolved observations of rotational transition lines, for example by danilovich et a  for h2  in a study focussing on cs and sis, danilovich et a  surveyed a sample of agb stars covering a range of mass-loss rates and chemical type  they detected cs in all surveyed carbon stars, some of the s-type stars and the highest mass-loss rate m-type star  sis was only detected in the highest mass-loss rate sources across chemical type  the sensitivity of their observations and subsequently calculated upper limits were insuffcient to determine whether the high mass-loss rate m-type agb stars genuinely have higher abundances of cs and sis than the low mass-loss rate m-type agb star  more sensitive observations were required to conclusively make that determinatio  for example, brunner et a  to check the abundances of cs and sis in lower massloss rate m-type stars, we must look to the available sensitive alma observations of such star  both have been subject to apex observations which did not detect cs or sis for r dor and danilovich et a  their proximity makes them ideal to search for weak cs and sis emission in alma observation  detected emission lines from these molecules will allow us to determine abundances, while nondetections will allow us to place more stringent upper limits on their abundance  also observed was ik tau, a higher mass-loss rate agb star losing   cs and sis emission was detected towards ik tau by apex and analysed in danilovich et a  the spatially-resolved alma observations of these molecules will allow us to precisely determine their radial abundance distributions and hence compare these with the modelling results of danilovich et a which are based on single-dish observation  this will enable us to check the reliability of the empirical formulae found by danilovich et a  for finding cs and sis e-folding radii, and to compare the similarly determined abundance distributions with those for the low mass-loss rate star  using radiative transfer modelling, we will compare precisely determined abundance distributions with previously obtained result ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Observations", "Text": "1 alma observations and data reduction spectral scans of ik tau and r dor were taken with alma in the range 335-362 ghz during august and september of 2015 the interferometer baseline lengths were in the range 40 m to   image cubes were made for ik tau with a channel resolution of   for r dor the image cubes have a channel resolution of  9km s-1 with rms noise in the range  7-  the full survey is presented in decin et a including a detailed discussion of the data reductio  here we focus only on the cs and sis emission observed as part of that surve  various isotopologue lines of these molecules were also detected towards ik ta  the first results of these observations, focussing on alo and 29sio are presented in takigawa et a  imaging using weighting for high resolution gave a synthesised beam of ma 85 km s-  the rms noise in quiet channels also depends on frequency, being about   in all cases the images were made using the lsr velocity conventio  the full width half maximum of the alma primary beam is about 15 arcsec in this frequency range and all of the results presented here are from the inner few arcsec where the reduction in sensitivity is negligibl  where possible, we extracted azimuthally averaged radial profiles from the zeroth moment maps of the alma dat  these are compared with our models in sec  stars included in our stud  frequency molecule line eup star 34  mean flux density being noise dominate  the error bars take these factors into accoun  see decin et a  for further details regarding the observed radial profile ", "Subsections": [{"Section_Num": "2_2", "Section": "2.2 IK Tau", "Text": "1 sis the sis transition at 34 7795 ghz was clearly detected towards ik tau with alm  the channel maps for this sis transition are shown in fi  in fi  from this we can see that no flux has been resolved out in the alma observation  additionally, several isotopologue lines of sis were detecte  all of these detected lines and their frequencies are listed in table  they are too heavily blended with brighter overlapping lines for useful analysis and hence we exclude them her 2 cs the cs transition at 34 883 ghz was clearly detected towards ik tau with alm  the channel maps for this cs transition are shown in fi  in fi  2 we compare the line spectrum extracted from the alma observation with the spectrum observed for the same line by ape  in this way we determine that no flux has been resolved out for this transitio  as can be seen in the channel maps, the peak in the cs emission is generally not centred on the continuum pea  together, the arcs and the lack of peak emission centred on the star suggest that cs is less abundant close to the sta  in addition to the main isotopologue of 12c32s discussed above, we also weakly detected the transition of 12c34s at 33  although it has a lower signal-tonoise ratio in the channel maps, it is visible in the zeroth moment map and in the spectru ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2_3", "Section": "2.3 W Hya", "Text": "1 sis the sis emission towards w hya is shown in channel maps in fi  the emission is very weak and the signal-to-noise ratio is too low at this high spatial resolution to distinguish many features in the channel map  the emission lines are clearly seen in the spectra extracted from the alma dat  no other sis lines were detecte 2 cs the channel maps for cs towards w hya are shown in fi  the central absorption feature seen in the blue channels is due to impact parameters along the light of sight to the sta  it is seen here due to the high resolution of the observation such that there is a large ratio between the stellar angular diameter and the angular beam siz  a similar phenomenon was seen for r dor by decin et a  in the spectra the absorption feature is clearly seen in the smallest extraction radius spectrum and the line is seen in emission for the larger radii extracted spectr  danilovich et a  the channel maps of the sis transition at 34  the beam is indicated in white in the lower left corner of each channe ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2_4", "Section": "2.4 R Dor", "Text": "1 sis the ground state sis line is not detected above the noise in the alma channel maps of r do  we use this detection and the tentative and non-detections in spectra extracted over smaller radii to constrain our sis model for r dor in sec  a comparison of the apex and alma observations of cs and sis towards ik ta 2 cs the main cs line cannot be clearly seen in the channel maps of r do  a tentative detection is also seen in the spectrum from a 300 mas radius region centred on the continuu  for the spectrum extracted for a 75 mas region we do not detect emission but do tentatively detect an absorption feature that corresponds to the blue absorption discussed for other molecules in decin et a  we use this detection and the tentative detections to constrain our cs model for r dor in sec ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Radiative transfer modelling", "Text": "1 modelling procedure we use a one-dimensional, spherically symmetric model to approximate the molecular emission of cs and si  although this precludes the inclusion of asymmetric structures, we are still able to approximate the overall shape of the emission based on the average radial profiles calculated from the zeroth moment maps, and hence take into account various radial abundances and mean densitie  the modelling is done using an accelerated lambda iteration method which has been used to model a variety of molecular emission in the past, including both cs and sis where possible we have included previously observed single-dish observations to add constraints to our models to compare our models with the azimuthally averaged alma radial profiles, we extracted synthetic radial profiles from our models and plotted them against the alma profile  the models were adjusted until the best fit to the data was foun  stellar properties and input from co model ", "Subsections": [{"Section_Num": "3_2", "Section": "3.2 Input parameters", "Text": "the crucial stellar and circumstellar parameters1 that go into our radiative transfer models are listed in table   all parameters are taken from danilovich et a  and the references therei  however, a full examination of this issue is beyond the scope of the present pape  we used molecular data for sis that was previously implemented by danilovich et a  this set of energy levels is shown in red in fi  for the isotopologues, the same quantum numbered set of energy levels and identical collisional rates were used this expanded set of energy levels is shown in fi  it is not directly related to the higher velocities observed in the wings of various r dor and ik tau emission lines by decin et a ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3_3", "Section": "3.3 IK Tau model results", "Text": "1 sis for sis towards ik tau, we were able to find a model that fits the data well using a gaussian radial profile to describe the abundance stratification throughout the cs  since we also had access to several single-dish observations of siswe used these along with the alma spectra and radial profile to constrain the mode  the uncertainties calculated are based on a 90% confidence interva  we experimented with smaller inner radii, but those models all significantly over-predicted the flux for the inner regions of the cs  the final inner radius of 2 * 1014 cm is that used by danilovich et a  for the so and so2 models of ik tau and is in agreement with the radius used by maercker et a  for co and h2  the abundance profile of our final sis model is plotted in fi  8 along with the abundance profile derived using only single-dish data by danilovich et a  the radial profile of this model is plotted in fi  running a model with the same abundance profile but  00 offset ik tau, sis model expanded model no collision model alma figure   the green line and triangles show the radial profile obtained for the same abundance profile when using the expanded molecular description and the cyan line with inverted triangles shows the radial profile obtained for the expanded molecular description when neglecting collision  the error bars shown on the alma data are as described in sec  this is expected since the availability of higher energy levels would have a more significant impact on the higher energy region  the radial profile of the transition for this model is also plotted in fi  7 and the corresponding model lines for both molecular descriptions are plotted with single-dish observations and alma spectra in fi  the result was again similar to the existing results with the only significant deviation being, again, in the innermost part of the radial profil  very little change was seen in the outer parts of the radial profile and in the single dish observation  these changes make sense if we consider that collisions are expected to play a more significant role in the dense inner regions of the cs  to determine the abundances of the sis isotopologues, we used the same e-folding radius of  6 * 1015 cm that was found for 28si32s and varied the peak abundance until the best fit to the alma radial profile was foun  our models were equally weighted between the spectral lines extracted from the alma data and the radial profiles out to   for the two weakest isotopologues, 29si34s and 30si34s, only the spectra were used since the signal-to-noise ratio of the observations was too low to extract a radial profil  the abundances derived from modelling the ik tau observations of cs and si  the solid lines are the abundance profiles derived from alma modelling in this study and the dotted lines are the abundance profiles derived from single-dish observations by danilovich et a  our convention has been to list the ratio such that the abundance of the isotopologue with the smaller mass number is divided by the abundance of the isotopologue with the larger mass numbe  there is good agreement between different isotopologues that trace the same isotopic ratios, suggesting that sis isotopologues are good tracers of si and s isotopic ratio  the only exception was 30si34s for which we found a higher abundance than expecte  the alma spectra of 30si34s are both brighter and wider than those of 29si34s, which is unexpected since we otherwise obtain higher abundances of molecules with 29si than 30si the discrepancy is most likely due to the 30si34s line at 35 0883 ghz being contaminated by an unidentified blen  excluding this line, the means of the isotopic ratios found are listed in table 6 and compared with solar and literature value 2 cs we initially assumed a gaussian abundance distribution for cs in the cse of ik tau, starting from the same inner radius as for si  however, this strongly overpredicted the central brightness despite fitting the outer parts of the emissio  the radial profile from this model for the cs transition is shown with the alma azimuthally averaged profile in fi  the derived abundance profile is shown in fi  8 where it is compared with the radial profile found by danilovich et a  from fitting only singledish cs dat  the observations of c34s have a low signal-tonoise rati  hence, we were unable to fit a model based purely on the observation  instead we use a model with the same abundance structure as for c32s and, taking the 32s/34s ratio found from sis, we find the cs observations to be in good agreement to the sulphur isotopic ratio found for si ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3_4", "Section": "3.4 W Hya model results", "Text": " two observations from danilovich et a  of undetected lines were available: sis and while both were included in our modellingwe only plot one her  we found an adequate fit to the somewhat noisy radial profile using a model with a lower inner abundance followed by a gaussian abundance distributio  the radial profile for the transition from the best model and the alma observations is plotted in fi  11, along with the modelled and observed spectral line  the abundance profile of sis for w hya is plotted in fi  12 and compared with the upper limit found by danilovich et a  based on their non-detection  we also included the upper limit for the cs line from apex observations performed by danilovich et a although this did not directly contribute to constraining the mode  the radial profile of the best model is plotted against the alma radial profile of the cs transition in fi  13, along with the spectral line  danilovich et a  the angular sizes listed for the alma lines are the radii of the extraction aperture  ik tau sis isotopologue ratios, calculated as smaller mass numbers divided by larger mass numbers", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3_5", "Section": "3.5 R Dor model results", "Text": " the e-folding radius for a gaussian distribution predicted by the sis formula calculated in danilovich et a 6*1014 cm which is only about six stellar radii this formula is calculated from only high massloss rate sources and may not hold for low mass-loss rate  running a model with this re did not fit the data well, even when we tried adjusting the inner radiu  hence we use re = 1015 c 6*1013 cm from the modelling of maercker et a 9 * 1014 cm from the modelling of danilovich et a 9 * 1014 cm in our final mode 5 * 10-8 until the resultant emission lines best match the observed emission line  for the best model we found f0 =  5 * 10-8 relative to h  the resultant models are plotted with the observed spectra in fi  14 and the radial abundance distribution of the model is plotted in fi 04 tmb sis w hya apex figure 1  top: the best sis model radial profile for the transition plotted with the alma azimuthally averaged radial profile for w hy  the error bars shown on the alma data are as described in sec  the sizes of the extraction radii for the alma spectra are listed in the top right corner 8079 ghz line of 34so2 to-noise of the observation is too low to extract a radial profil  we assumed a gaussian distribution with the e-folding radius predicted by the cs formula calculated in danilovich et a  this gave re =  5 * 10-9 relative to h  the resultant model is plotted with the observed mnras 000, 1-17 12   danilovich et a  the abundance distributions derived from modelling the w hya observations of cs and sis and the upper limits for the same abundances derived by danilovich et a  from apex non-detectionswith the vertical lines indicating their model inner radi  spectrum in fi  16 and the radial abundance distribution of the model is plotted in fi  as can be seen in fi 5 km s-1 between the observed absorption feature and the modelled absorption feature in the smallest extraction region spectru  this is not due to an inaccurate lsr velocity since the spectral line from the largest extraction region is well-aligned with the model lin  it can be accounted for if we consider the rotating disc around r dor proposed by homan et a  this is indeed what we see in the case of w hya, where the modelled location of the absorption feature agrees well with the observatio  similar arguments can be used in the case of stellar rotation causing a velocity field in the regions close to the stellar surface as suggested by vlemmings et a  and our observations do not rule out either scenari ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Discussion", "Text": "1 difference between lower and higher mass-loss rate stars in fi  17 we have plotted the radial abundance distributions derived for all three stars in our sample, grouped by molecul 5 offset cs model alma figure 1  top: the best cs model radial profile plotted with the alma azimuthally averaged radial profile of the transition for w hy  the error bars shown on the alma data are as described in sec  the sizes of the extraction radii for the alma spectra are listed in the top right corner  this strongly implies that another factor is at play, even if, as suggested by the results of danilovich et a density may contribute to the presence of sis and cs sis is thought to form in denser environmentsalthough in general sis chemistry is currently not very well characterise  r dor sis model and observed spectrum the sizes of the extraction radii for the alma spectra are listed in the top right corner  sis abundances and cs abundances for all three modelled star  the vertical lines are the inner radii of the corresponding model  in our sampl  for example, van de sande et a  show that clumpy outflows can produce a significant amount of c  various channel maps of other molecules towards r dor show arc-like structures in the cse rather than a smooth outflow, indicating a level of clumpiness in the outflo  for the cs emission in ik tau the position of the abundance increase can be explained by a clumpy outflow, as shown by van de sande et a whereas the smooth outflow models of willacy & millar give a cs increase further out in the envelope which does not agree with our result  another source of differences between the stars could be the pulsation type  from this we can see that both period and amplitude increase with the progression from r dor to w hya to ik ta  cs is not a species predicted to form in oxygen-rich environments when only equilibrium chemistry is consideredwhereas it is expected to be form easily in carbon-rich environment  in oxygen-rich cses it is thought that shocks play a significant role in cs formationhence it follows logically that more cs would form in stars with more extreme pulsation  aside from the differences in abundances, our results also show differences in the inner radii of the model  our ik tau models have a larger inner radiuswhile the w hya models start close to the stellar surfac  thanks to the high spatial resolution of the alma data, we are reasonably confident that this is a real difference between the stars type ik tau   radii results).  we tested models with smaller inner radii for ik tau and, although little difference is apparent in the models for the apex data, there is a noticeable difference in the models for the resolved alma lines and in the radial profil  if anything, the alma data points towards a slightly larger inner radius this was not the case for w hya, for which we found the best agreement with an abundance profile that starts from the innermost regions close to the sta  this dichotomy could point to the effects of dust formation in these cse  if more silicate dust is formed in ik tau, the production of sis at a larger radius in ik tau than in w hya could be due to sputtering dust grains making si available for sis formatio  this theory is supported by the inner radius of our ik tau sis model lying close to where dust has formed in the cseand by the fact that using a smaller inner radius for our model did not agree with the observations", "Subsections": [{"Section_Num": "4_2", "Section": "4.2 Comparison with previous studies", "Text": "danilovich et a  based on these observations and radiative transfer modelling, they derived empirical formulae for the e-folding radii of both molecules based on mass-loss rate and terminal expansion velocit  for cs, the e-folding radius predicted by the formula is in good agreement with the radius we found in the gaussian part of our model and the peak fractional abundance was only a little higher than the peak abundance for the gaussian part of our mode  the inner regions of our model stretch to the stellar surface and were found to be significantly lower than the upper limit mode  there is a more significant difference between their sis upper limit and our model, howeve  the e-folding radius predicted by their formula is an order of magnitude smaller than what we found based on the alma data and our peak fractional abundance is an order of magnitude lower than the upper limit they found based on the apex observation  this suggests that the sis e-folding radius formula found by danilovich et a  does not hold for low mass-loss rate star  in the case of ik tau, the model calculated by danilovich et a  for cs we found a less regular distribution of cs than was apparent from the single dish observations and hence our model deviates from that found by danilovich et a  ik tau has been the subject of other single-dish studies of these two molecule  surveyed sis in a large sample carbon- and oxygen-rich agb stars and modelled the sis emissio  they assumed e-folding radii for sis based on the empirical result derived by gonz alez delgado et a  for sio, giving an e-folding radius for ik tau of   we do not see such a compact inner region in our alma observations, which are certainly sensitive enough to detect the kind of jump in abundance they use  decin et a  modelled several different molecular species towards ik tau, including sis and c with a more abundant inner component and a lower abundance outer component, the latter being much larger than the size of our sis radial abundance distributio  their inner abundance is also larger than ours, exceeding it by more than a factor of thre  their cs model gives a different radial abundance profile to ours, with a constant inner abundance paired with a rise and decline in the outer region  their fractional abundance is much smaller than what we found in the outer regions of our model, although it agrees well with the innermost abundance we foun  decin et a  also calculated isotopic ratios for si from sio observation  their results agree with ours within the specified errors and they included a literature review and discussion of si isotopic ratios in their sec 2, to which we direct interested reader  velilla prieto et a  however their abundances of the less common isotopologues of these molecules are factors of 3-4 higher than our result  hence their isotopologue ratios are not in agreement with our  they find 32s/34s = 1  for 28si/29si calculated from sis, they find 11, which is also much smaller than our value of 25+/-1  peng et a  observed the less abundant isotopologues of sio and hence calculated the 29si/30si ratio for a sample of stars, including ik ta  they found a ratio of  30, in good agreement with our result of   brunner et a  their observations are of lower spatial resolution than ours and no deviations from gaussian abundance profiles can be seen in the inner regions of their observation  they do find the need to include an overdensity in their models at around 1016 cm from the centre of the star, but this is assumed to be due to spiral windings seen more clearly in the co alma observations of w aql presented by ramstedt et a  they found a much lower 28sis/30sis ratio of 13 for w aql than we found for ik tau and a much lower 28si/29si ratio of 1 6 compared with our ratio of 2  these ratios are indicative of w aql having a higher metallicity than ik tau does the 29si/30si ratio for w aql, derived by brunner et a 7 for ik ta  since we do not expect the 29si/30si ratio to change much with metallicitythis could be a result of inhomogeneous chemical evolution of the galaxy rather than a tracer of metallicit  our ik tau values are generally larger than the solar values and the r dor values from the literature 16   danilovich et a  olofsson 2018).  since we have used the convention of always writing the ratios as lower mass number over higher mass number, this means that we found ik tau to have a lower proportion of heavier isotopes than the sun and r do  this difference is most noticeable when comparing rarer isotopologues to the most common isotopologue, while the ratios between the less common isotopes are in good agreement with the solar value  the general trend suggests that ik tau has a lower metallicity than the sun 28si, 32s, and 34s are produced through oxygen burning and in type ii supernovae through explosive nucleosynthesis the less abundant isotopes of 29si and 30si are formed through neon burning and neutron capture in sne ii and may increase in abundance during the agb phase through slow neutron captures 34s is partly also formed from neutron captures via 33s, which means its abundance may increase in agb stars3% in 32s/34s while on the thermally pulsing ag  karakas & lugaro predict that the 32s/36s ratio does change more significantly due to neutron captures in the he-shel  we did not detect si36s in the alma scan and c36s falls outside of the observed frequency rang  a quick analysis of si36s based on our non-detections with alma gives an upper-limit abundance f0 < 10-9 relative to h  this gives a lower-limit 32s/36s that is about 30% lower than the solar valu  chin et a  find a gradient in 32s/34s ratio with distance from the galactic centre however, their observations are of star-forming regions and the solar system value does not lie very close to their trend lin  since star-forming regions are in a very different evolutionary phase to agb starsit is unclear that their isotopologue ratios can be directly related to those that we find for individual star  aside from the spread in the age-metallicity relationship in the solar neighbourhoodik tau formed from molecular clouds that significantly predate present-day starforming regions and hence are not comparabl  overall, our results are in agreement with the conclusions drawn by decin et a  that, based on isotopic ratios, the ism from which ik tau formed was enriched by sne ii", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Conclusions", "Text": "5 dex for c  for cs towards ik tau and both molecules towards w hya, we found stratified abundance distribution  the w hya emission and models also indicates that both of these molecules are found very close to the star, while the ik tau models show these molecules forming further out in the cse of ik ta  overall the isotopic ratios we derived from these suggest a lower metallicity for ik tau than the solar valu  acknowledgements ld, mvds and fdc acknowledge support from the erc consolidator grant 646758 aeroso  ld acknowledges support from the fwo research project grant g024112  mvds acknowledges support from the fw  fdc is supported by the epsrc icase studentship programme, intel corporation and cray in  this paper makes use of the following alma data: ads/ja s and 201  the joint alma observatory is operated by eso, aui/nrao and nao ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "edu abstract we propose a triply-resonant electro-optic modulator architecture which maximizes modulation effciency using simultaneous resonant enhancement of the rf drive signal, the optical pump, and the generated optical sideban  optical enhancement of the optical pump and the sideband is achieved using resonant supermodes of two coupled optical resonators, and the rf enhancement is achieved with lc circuits formed by the capacitances of the optical resonators and inductors which can be implemented using cmos technolog  in the proposed configuration, the photon lifetime determines the bandwidth of the rf response but not its center frequency, which is set by the coupling strength between the two resonators and is not subject to the photon lifetime constraint inherent to conventional single-mode resonant modulator  this enables effcient operation at high rf carrier frequencies without a reduction in effciency commonly associated with the photon lifetime limi  additionally, a significant gain in modulation effciency is expected from rf signal enhancement by lc resonant matchin  this results in a modulator which is compact, effcient, and capable of modulation at high rf carrier frequencie  the proposed modulator architecture optimally engineers the interaction of the device with each of the three signals and between them, can be applied to any modulator cavity design or modulation mechanism and promises to enable complex rf-photonic systems on chi ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": " the effciency with which the rf signal is converted to the optical domain by an eo modulator is an essential parameter directly affecting the gain of a mwp lin  mach-zehnder modulators, both discrete and integrated, have been used in mwp systems over the past decades as the workhorse devices for rf-to-optical conversio  however, these devices are large and power-hungr  integrated photonics technology offers new opportunities for sensitive microresonator-based eo modulators, such as microring and photonic crystal cavity modulator  owing to their resonant nature, these devices are compact and effcien  however, they suffer from the speed-sensitivity tradeoff, imposed by the cavity photon lifetime this tradeoffhas been addressed by using modulation-induced coupling between adjacent free spectral range 1 arxiv:190  in op  out laser pump triply resonant modulator concept op  in op  out op  in op  non-resonant rf design, where the transmission line directly feeds the active optical cavitie  due to impedance mismatch between them, rf power is almost completely reflected from the loa  resonant design, where the rf resonance from is realized by lc circuits, consisting of integrated inductors l1 and capacitances cm of the active cavitie  it greatly reduces rf power reflection and enhances the voltage on the active cavitie  resonant-matched design, where critical coupling between the transmission line and the rf resonator is achieved using impedance matching circuits which consist of inductors l2 and capacitors c  modes of millimeter-scale whispering-gallery-mode disk and ring resonator modulators, which have their speed set by the fsr of the resonatoras discussed in more detail belo  these devices are inherently large and require implementation of rf traveling wave electrode  in this work, we present a detailed account of a novel triply resonant modulator consisting of two coupled microresonator-based modulator cavities and lumped rf resonator  the proposed device is compact and energy effcient, similar to conventional microresonator-based modulator  additionally, the rf resonance boosts the voltage which further increases the effciency, as described below", "Subsections": [{"Section_Num": "1_1", "Section": "1.1 The proposed modulator", "Text": "this work proposes a novel electro-optic modulator for effcient conversion of rf signals into optical domai  the goal is to maximize the modulation effciency, defined here as the fraction of the input laser pump power which gets converted into the optical sideband this work proposes an integrated triply-resonant rf electro-optic modulator, where each of the two optical waves and the rf wave are resonantly enhanced to maximize the modulation effcienc  our work is a continuation of that inand some aspects of this modulator have been recently described in a conceptual representation of the triply-resonant device is shown in fi  the three rectangles represent the input/output ports of the device, and the three circles represent the three mutually coupled resonance  the rf resonance and one of the optical resonances are excited by the input rf and the laser pump waves, while the other optical resonance is excited by the optical sideband that is generated due to nonlinear interaction between the rf drive and the optical pump within the devic  during this interaction, an rf photon combines with a pump photon to produce an optical sideband photon, translating each rf spectral component to the optical domai  the key idea of the proposed device is that the conversion effciency is maximized when all three interacting waves are at resonance, and when their lifetimes/escape effciencies are properly tailore  in the physical world, the abstract triply-resonant modulator of fi  1 can be realized with two coupled optical cavities connected to an rf resonato  the resonance frequencies of the two optical cavities can be tuned by applying a voltage to their built-in capacitive phase shifters, which creates coupling between the optical and the rf wave  the phase shifters can be based   on the carrier plasma or linear electro-optic pockels effect  in the text below, we will refer to these tunable optical cavities as active cavitie  the two optical resonances of the abstract device of fi  1 correspond to the two resonant supermodes of the coupled optical cavities, as described belo  the two optical cavities are identical and have the same resonance frequencies when uncouple  in the proposed device, the two cavities are evanescently coupled as illustrated in fig  the coupling produces two new orthogonal states - the symmetric and antisymmetric supermodes - with resonance frequencies split due to the couplin  the two supermodes of the coupled cavities correspond to the two optical resonances of the abstract device in fig   the input pump laser is matched in frequency to the symmetric supermode, while the frequency of generated optical sideband equals the frequency of the antisymmetric supermode this work considers single sideband generation, but can be easily extended to dual sideband generation in a three-optical-resonance system following this work considers two configurations of the coupled optical cavities: the basic coupled-cavity design shown in fi  1, and the generalized coupled-cavity design shown in fi  the rf resonance in the abstract device of fi  1 is introduced by two lc resonators formed by integrated inductors and the capacitance of the electro-optic region of the active optical cavitie  as shown in fi  1, the two lc resonators are identical with equal resonance frequencies and are connected to rf transmission line  the transmission line delivers a differential rf signal, driving the two active cavities in push-pull mod  regular microring modulators are compact and effcient, but suffer from the speed-effciency tradeof  millimeter-scale disk or ring resonator modulators, which overcome the speed-effciency tradeoffthrough modulation-induced coupling between multiple resonant mode orders at adjacent fsrs, have large footprint and require implementation of rf traveling-wave electrode  optical coupled-cavity designs of the proposed modulator perform rf-to-optical conversion by transferring optical energy from the symmetric to the antisymmetric supermode resonance, via push-pull modulation of the resonance frequencies of the coupled microresonator  the designs are compact, effcient and do not suffer from the speed/effciency tradeof  mismatc  to a large extent, the reflection can be mitigated by connecting integrated inductors in series with the capacitances of the active cavities as shown in fi  1, forming lc circuits with resonance frequencies equal to the rf carrie  the lc circuit removes the reactive load from the transmission line and boosts the voltage on the active cavitie  perfect impedance matching between the transmission line and the modulator is still not guaranteed, because the transmission line impedance is not necessarily equal to the parasitic resistance of the active cavitie  by introducing a lumped-element impedance matching circuitas shown in fi  1, critical coupling between the transmission line and the optoelectronic lc resonator can be achieve  this maximizes the field in the capacitive eo region for a given rf drive powe  the remainder of work analyzes all of the rf configurations shown in fig  the proposed modulator can in principle be implemented in any material platform that allows 4 integration of the optical and the rf resonator  especially attractive are high-index-contrast waveguide platforms with effcient phase shifter mechanisms, with several metal layers available for building integrated inductor  with tight confinement of the optical and rf fields in the high-indexcontrast active cavities and rf resonant enhancement, such platforms are promising candidates for implementing compact and effcient modulators based on the proposed concep  in particular, we are interested in realization of the proposed concept in advanced cmos processes, such as zero-change monolithic electronic-photonic 45 nm cmos demonstrations of high-speed microring modulators on an advanced digital cmos processwhich has been recently geared toward millimeter-wave and 5g applicationsand the availability of high-q inductors in the process design kit pave a direct path to realization of the proposed modulato ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1_2", "Section": "1.2 Comparison to prior art", "Text": "over the past decades mz modulators have been extensively used in mwp systems, both in discrete component and photonic integrated circuit for  broadband mz modulators with up to 40 ghz bandwidth have been demonstrated in silicon modulation beyond 100 ghz has been achieved with mz modulators based on lithium niobate, electro-optic polymer, and plasmonic waveguides mz modulators, employing traveling-wave electrodes, overcome bandwidth limitations due to the rc time constant and provide impedance matching with the input feed lin  nevertheless, the bandwidth of these devices does have limits imposed by imperfect velocity matching between the rf and optical waves which has stronger impact on the bandwidth of longer device  therefore, there is a tradeoffbetween effcient modulation and large bandwidth, unless near-perfect velocity matching is achieve  microring modulatorsshown in fi  2, take advantage of optical field enhancement in the cavity through multiple round trip propagation when implemented in high-index-contrast materials, these devices feature small size and tight spatial confinement of optical and rf electric field  the small size of the device eliminates the need for traveling-wave electrodes and permits a large rc-time-limited bandwidt  in silicon photonics, microring modulators have been extensively used for low-energy data modulation and have enabled energy-effcient photonic interconnects modulation frequencies up to 40 ghz have been achieved similar to mz modulators, regular microring modulators suffer from a tradeoffbetween the modulation speed and effciency - in this case that is inherent to singly-resonant device  the design of regular microring modulators for rf applications has been studied in modulation at rf carrier frequencies extending far beyond resonance linewidth of the modulator has been demonstrated by using different resonant modes of millimeter-scale whispering-gallery mode disk or ring resonator  we refer to such modulators as fsr-coupled modulator  this approach eliminates the tradeoffbetween the modulation speed and effciency, and has been developed for lithium niobate whispering-gallery mode disk modulators which exhibit very high quality factor these modulators have found application as rf-optical mixers in photonic receivers that provide high sensitivity and large dynamic range the modulator studied in this work incorporates the best features of the regular microring modulator and the fsr-coupled modulator shown in fig  in particular, similar to the fsr-coupled device of fi  2, to decouple the speed from the cavity photon lifetim  this eliminates the speed-effciency tradeoffinherent in singly-resonant devices such as the regular microring modulator additionally, the frequency separation between the resonant modes is set by the strength of the coupling between the two cavities, as opposed to fsr-coupled devices where the resonance frequency separation is determined by the resonator radiu  additionally, the small capacitance of each active cavity permits large rc-time-limited bandwidth, which is discussed in detail in se  we previously proposed coupled-cavity modulators similar to fi  1 for on-chip wavelength conversion and modulation of high-carrier-frequency rf bandlimited signals the use of electro-optic coupled resonators for quantum microwave-to-optical conversion has been studied in it should be noted that dual-ring modulators were also proposed prior to these worksbut these were designed for baseband data modulation and operated in a different regim  in the remainder of this paper we analyze in detail the different optical and electrical designs of the proposed triply-resonant modulator, illustrated in fig  we start offwith a qualitative description of operation of the both optical and the rf parts of the device in se  in se  1 and find their optimal designs that maximize the conversion effcienc  in se  4 we explore the different rf circuits shown in fig  1 and derive formulae for the gain in conversion effciency produced by the resonant circuits of fig  1 and compared to the non-resonant case of fi  finally, se  5 summarizes and discusses the results of this wor ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Principle of operation", "Text": "before turning to the mathematical analysis of the proposed modulator, in this section we provide a qualitative description of its optical and rf constituents and their different configurations as shown in fi ", "Subsections": [{"Section_Num": "2_1", "Section": "2.1 Problem formulation", "Text": "our goal is to develop a modulator which effciently converts rf signals into the optical domai  optimizing the modulator for the most effcient conversion into the optical sideband alone is relevant when the sideband is detected without the carrier, such as in direct-detection receivers or photonics-assisted microwave radiometers another application is coherent communications systems, where the modulated signal is detected via the homodyne or heterodyne technique by interfering the sideband with a local oscillator", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2_2", "Section": "2.2 Optical design", "Text": "the two proposed optical modulator designs - the basic and the generalized coupled-cavity designs are shown in detail in fig  the basic version of fi  the coupling strength between the symmetric supermode and the input/output waveguide is characterized by the external energy amplitude decay rate re,s since both supermodes are coupled to the same bus waveguide through the bottom cavity, the external energy decay rates of the supermodes are equal to each other and are equal to half of the energy amplitude coupling rate of the bottom cavity to the bus waveguide,   therefore, the energy gets redistributed between the symmetric and the antisymmetric supermodes at each time ste  the modulated light couples back into the bus waveguide and leaves through the output por  it is essential that the resonance frequencies of the supermodes stay the same during modulation, which happens since the combined optical path length of the two microresonators does not change when their resonance frequencies are modulated in push-pull fashio  this allows the laser pump and the optical sideband to stay on resonance with the symmetric and antisymmetric modes throughout the modulation proces  the idea underlying the proposed modulator concept is that the modulation effciency depends on the resonant enhancement of each of the interacting wave  this is confirmed by analytic derivation of the conversion effciency in the appendix a, which shows that the effciency contains a product of the lorentzian lineshape of the symmetric resonance at the frequency of the pump laser and the lorentzian lineshape of the antisymmetric resonance at the frequency of the sideband, see e  for the moment, we only consider the effciency limitation due to photon lifetime in the optical part of the modulator; the rf frequency response of the circuits is considered late  the above has several implications for the modulator performanc  second, for the best modulation effciency, the laser pump should always be aligned to the frequency of the symmetric resonance this means that the rf bandwidth of the modulator does not depend on the linewidth of the symmetric supermode, and is equal to the linewidth of the antisymmetric supermode in case the rf bandwidth needs to be increased, the antisymmetric supermode linewidth needs to be broadene  in the basic design this can be achieved by increasing the coupling strength between the bus waveguide and the rin  however, this is accompanied by an unwanted reduction in q-factor of the symmetric supermode, which reduces the pump enhancement and degrades the effciency of the modulato  this brings us to the generalized configuration shown in fi  the independent control of the q-factors of the symmetric and the antisymmetric modes is achieved with a novel interferometrically coupled input/output bus waveguide configuration, where the symmetric supermode is coupled only to the input waveguide and the antisymmetric supermode is coupled only to the output waveguide in this case, the external energy decay rates of the two supermodes can also be set independentl  the interferometric coupling in the generalized coupled-cavity modulator of fi  2 works in the following wa  the light from the symmetric supermode can couple back to the input waveguide, but not to the output waveguide because of destructive interference of the waves coupled from each of the two rings into the output waveguid  as a result, there is no net coupling of light from the symmetric mode to the output waveguid  the situation is reversed for the antisymmetric supermode, which has fields in the two rings oscillating out of phase with respect to each othe  the waves coupled from the antisymmetric supermode into the input waveguide interfere destructively and the waves coupled into the output waveguide interfere constructively, so that the antisymmetric supermode is coupled only to the output but not to the input waveguid  the electric field configurations of the symmetric and antisymmetric supermodes are illustrated in fi 3 rf configurations different ways in which the rf drive can be applied to the active cavities of the proposed modulator are illustrated in fig  in the simplest, non-resonant scenario, shown in fi  1, the transmission lines are directly connected to the terminals of the capacitive electro-optic region of the active cavitie  from the rf perspective, the active cavity is a capacitor cm connected in series with a resistor r  the capacitor acts as a phase shifter that tunes the resonance frequency of the optical cavity in response to the rf signal, either by means of electrical charge accumulated on the capacitor plates or by the electric field between them changing refractive index of an electrooptic materia  the resistor accounts for the parasitic series resistance between the capacitor plates and the terminals of the active cavitie  in the non-resonant configuration of fi  this can be considered a consequence of the frequency of the rf signal not matching the frequency of the rf resonance, which in the absence of a series inductance can be viewed as being infinit  this is illustrated in the bottom plot in fi  8 the rf resonance frequency can be shifted to a finite frequency by connecting an inductor in series with each active cavit  this resonant configuration is shown in fi  by appropriately choosing the inductance l1, the frequency of the rf resonator formed by the inductor l1 and the capacitor cm can be matched to the carrier frequency of the rf drive,   rf resonance boosts the voltage on the capacitors of the active cavities, improving the effciency of the modulatio  the resonance removes the reactive load from the transmission line, however, perfect load matching is still not guaranteed due to the parasitic resistance r  therefore, the dip in reflection function, plotted in fi  critical coupling between the rf feed line and the resonator can be achieved by introducing an impedance transforming circuit between them, as shown in the resonant-matched configuration in fi  the figure shows an l-match impedance down-converter, which consists of capacitor c2 and inductor l2, and converts the higher resistance of the load to the lower characteristic impedance of the transmission line at critical coupling, the dip in the reflection function reaches zero, as shown in fi  1 and field is maximized on the load capacitor,   the eo region of the modulator cavitie  a detailed analysis of the different rf schemes is carried out in se  4, where the parasitic resistances of the inductors are taken into accoun ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Analysis of the optical design", "Text": " this section gives a detailed analysis of the performance of the optical part of the modulator, specifically the basic and generalized configurations shown in fi  the derivation is provided in the appendix   these parameters are illustrated in fi  resonance sym  the discussion below examines the maximum conversion effciency that can be achieved with the two proposed modulator configuration  first, we consider a single-frequency rf signal, and find the parameters which maximize the conversion effciency at this frequenc  we then find the rf bandwidth obtained in this modulato  if an rf bandwidth different from the obtained bandwidth is desired, the parameters of the modulator can be changed to ensure the required bandwidth; this is done in the last part of this section which optimizes the modulator parameters for maximum conversion effciency while ensuring the required rf bandwidt  by inspection of e  this is not surprising, since under this condition pump and optical sideband fields are maximally enhanced in the resonator  from e  the frequency splitting between the resonances must match the rf frequency, in agreement with the discussion in the preceding section  the next step is to find the optimum coupling strength  in the appendix   for a very weak modulating signal when the resonance frequency swing is much smaller than the intrinsic resonance linewidth,   if the modulation is not weak, the above formula can be viewed as a modified critical coupling condition which takes into account the modulation-induced coupling between the supermodes, which acts as an additional source of loss for the supermode  note that according to e both of the two proposed 10 configurations of fig  2 have the same supermode coupling coeffcients, and show identical performanc  fi  4 plots the conversion effciency versus normalized resonance frequency swing,   for the blue line, the modulator parameters are optimized at each point along the x-axi  this is the weak modulation regime where e  this means that the effciency of the modulator is higher if there is a higher probability of a pump photon being converted to a sideband photon, compared to the probability of losing the photon to different loss mechanisms in the cavitie  when the modulation is very strong,   the dashed lines in fi  this is in stark contrast to the modulation effciency of the regular microring resonator modulators, which drops as the signal frequency increase  a thorough analysis of the regular microring modulators for optimal rf-to-optical conversion is given in as an example, in fi  4 the small-signal conversion effciencies of the proposed and the regular microring modulators are plotted versus the rf signal frequency normalized by the cavity resonance intrinsic linewidth 2r  in the case of the proposed modulator it is done by adjusting the coupling strength between cavities; in the case of the microring modulator, by adjusting the cavity resonance linewidth through an appropriate choice of ring-bus coupling strengt  as expected, the effciency of the proposed modulator stays constant, while the effciency of the microring modulator drops as the rf frequency increase  let us now find the rf bandwidth of the modulator optimized for the best performance at a single rf frequenc  strictly speaking, e  is applicable to harmonic rf input onl  however, for weak modulating signal that does not deplete the laser pump, formula can be applied to each spectral rf component independently and used to study the frequency response of the modulato  the maximum values of each response are indicated on the blue curve in fi  4 with a matching black marke  note that the rf bandwidth is simply equal to the optical bandwidth of the antisymmetric resonance, which is 4 times the critical coupling rate given by e  small-signal conversion effciencies of the proposed and the regular microring modulators versus rf carrier frequency, where the modulators are optimized for maximum effciency at each point along xaxi  the analysis above found a modulator design which provides the maximum conversion effciency at a given rf carrier frequency and determined the resulting rf bandwidth for this desig  this approach works well for narrowband rf signals, however, many applications require rf bandwidths wider than provided by the above desig  expression is only valid when the external coupling rates are given by in the above expression, the second term can usually be neglected in comparison to the first term except when the modulation is stron  widening of the bandwidth comes at the expense of reduced conversion effciency due to an increase of the denominator in e  in the basic design, re,a can be increased by increasing the coupling between the microresonator and the bus waveguid  in contrast, in the generalized architecture of fi  thus, broadband modulation can be realized with significantly higher effciency in the generalized than in the basic design, in principl  this is straightforward for the basic architectur  the external coupling rates are set to be equal in e  are substituted into e  for the generalized architecture the process is slightly more involve  one of the decay rates is found from e substituted into e  and the optimum value of the second decay rate is found among the extrema of e  finally, the above expressions for coupling rates are substituted into e  in the limit when the modulation is weak and the required photon-lifetime-limited bandwidth is much larger than the intrinsic linewidtheq  in fig  5 and the external decay rates, given by eq  fi  andrespectivel  the peak conversion effciency of both designs drops as the target bandwidth increases, however, the drop is significantly slower in the generalized than in the basic design the improvement the generalized design provides over the basic design is shown in fi  it is negligible when the target rf bandwidth is close to the intrinsic resonance linewidth, which means that the generalized design gives little advantage over the simpler basic design if the target rf bandwidth is low, or the microrings are loss  the advantage of the generalized design increases as the target rf bandwidth increases relative to the intrinsic linewidth of the resonator  for instance, when the required rf bandwidth is about 40 times larger than the intrinsic linewidth, the generalized design provides 10 db higher conversion effciency than the basic design", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Analysis of the RF design", "Text": "in the previous section, the analysis was focused on determining the optimum optical design of the proposed triply resonant modulato  in this section we turn our attention to the rf design, and find the gain in conversion effciency that the resonant and resonant-matched circuits in fig  1 and produce compared to the non-resonant circuit in fi  1, for a given input rf powe  the rf designs are replicated on the left side of fi  6, with corresponding equivalent circuits shown on the right sid  the rf signal is brought to the modulator by a transmission line with characteristic impedance z  the voltage amplitude across the capacitor of the active cavity vcm is equal to the product of vrf and the voltage enhancement frequency response of the rf circuit, vcm = |h| vrf according to e  therefore, the gain in conversion effciency produced by the resonant and resonant-matched circuits is equal to the magnitude squared of the voltage enhancement produced by these circuits relative to the non-resonant desig  next, the voltage enhancement frequency responses of the different circuit configurations are foun  the non-resonant circuit configuration is shown in fi  here the two active cavities of the modulator are directly connected to the terminals of the transmission lines that deliver the differential rf signal from a signal source to the modulato  the equivalent circuit of one of the branches is shown on the right of fi  6, where the active cavity is represented by the series connection of the capacitor cm and the parasitic resistance r  the factor of two in the numerator indicates that the low-frequency components reflect entirely due to the large impedance of the capacitor, doubling the voltage on the loa  note that rc-timelimited bandwidth depends not only on the parasitic resistance rm, but on the impedance of the transmission line as wel  the capacitance of state-of-the-art silicon microring modulators is on the order of femtofarads or several tens of femtofarads, and the resistance is between tens and hundreds of ohms as an example, the blue line in fi  in the resonant configuration of fi  6, series rlc circuits are formed by the active cavities and integrated inductor  the equivalent circuit is shown on the right of fi  this means that the voltage across the capacitor is qtot rf times larger at resonance than at low frequencie  the magnitude of e  is plotted in fi  as the resonance frequency approaches the rc-time-limited bandwidth, the maximum voltage gain, shown by the dotted line in fi diminishes, which happens due to the reduction of total quality factor of the rlc resonato  7 for several values of ql and the same values of cm, rm and zo as in the previous example  for instance, at 50 ghz a resonant circuit with an inductor that has q-factor ql=10 provides 10 db gai  even larger gains are possible for smaller capacitances and resistances of the active cavitie  6 and of the resonant circuit of fi  values of cm=5 ff, rm=100 ohm, zo=50 ohm are assumed for the plots in and the additional gain in conversion effciency the resonant-matched circuit provides compared to resonant design versus normalized parasitic resistance of the active cavitie  it is worth noting that the rf resonance quality factor qtot rfas well as the optical photon-lifetime determine the overall rf bandwidth of the modulato  one can show that, in the resonant circuit of fi  if the characteristic impedance differs from the load resistance, an impedance matching circuit can be implemented with lumped elementsas is done in the resonant-matched design in fi  if the load resistance is lower than zo an impedance up-converter, shown on the bottom right in fi  6 can be use  here, for the sake of simplicity, we assumed the inductors have infinite quality factor  figure 7 shows the dependence of g21 on the rm/zo rati  the gain that the resonant-matched design provides compared to the resonant design is about   since the matching circuit adds additional complexity to the system, its implementation is justified only when there is a large impedance mismatch between the load and the transmission line", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Discussion and summary", "Text": "in this work we proposed and studied a triply-resonant modulator architecture for high-carrierfrequency rf modulatio  the modulator was studied by analyzing the designs of its optical and rf parts separately, with eq  and describing the performance of the optical and the electrical constituent  by combining these equations, the full electro-optic response of the modulator was foun  on the optical side, it was shown that the performance of the proposed modulator is intrinsically insensitive to rf carrier frequency this is in contrast to regular resonant modulators, where the photon lifetime degrades the performance as rf carrier frequency goes u  for the proposed modulator, the photon lifetime is decoupled from the rf carrier frequency, and determines the rf bandwidth around the carrier frequenc  two optical designs - the basic design shown in fi  1 and the generalized design shown in fi  if the spectral width of the rf signal is lower than or on the order of the intrinsic linewidth, the basic optical design shown in fi  1 works well, and the more complex generalized design brings no performance advantage  however, the generalized design is preferable for rf signals with spectrum significantly wider than the intrinsic linewidth of the cavit  by proper engineering of loaded q-factors of the supermode resonances in the generalized design, the conversion effciency of wide-spectrum rf signals can be significantly increased in comparison to the basic desig  on the rf side, it was shown that the effciency of the modulator can be enhanced by connecting inductors in series with the capacitors of the active cavities to form lc resonators with resonance frequency equal to the rf carrier frequenc  additionally, an impedance matching scheme was proposed for situations when there is large impedance mismatch between the rf feed line and the modulato  in the impedance-matched design, input rf power is directed to maximally build up energy in the active region capacitance of the modulator, maximizing its effcienc  a useful figure of merit can be found from the weak signal peak conversion effciency formula combining eq  the last term in e  with the increasing importance of high bandwidth rf signal processing, and the complex photonic and electronic-photonic integrated circuit platforms that are emergingmodulators such as those proposed may find a natural place in mwp integrated circuit  in addition to experimental validation, work remains to address the linearity and power handling of these design  triple-optical-cavity systems for dual sideband generatio  the analysis below is general and applicable to both the basic and the generalized architectures of fig  we start with the system of two coupled cavities, which are not coupled to external waveguides, and which have their resonance frequencies modulated in push-pull mod  energy amplitude decay rate ro accounts for the intrinsic losses in the microresonators, and is assumed equal in both her  the derivations are easily modified to remove this assumptio  by solving e  the two supermodes are referred to as symmetric and antisymmetric because according to e the fields in the two rings are in phase for one supermode and out of phase for the othe  according to e  the prior analysis considered the isolated system of coupled cavities which are not coupled to external waveguid  in the proposed modulator the only optical input is the laser pump s+1, therefore s+2 is always assumed to be zer  the fields at the output of the modulator have amplitude s-1 for the residual pump and amplitude s-2 for the generated optical sideban  in this work, we consider the basic and the generalized designs of fig  2 for physically realizing the described coupling of the supermodes to the input and the output port  in the basic designthe input pump light couples to the symmetric mode and the light in the optical sideband is extracted from the antisymmetric supermode via the same bus waveguide, which is coupled to one of the cavitie  as mentioned in sec  in the generalized designthe pump is coupled into the symmetric supermode and and the generated sideband is coupled out of the antisymmetric supermode through separate input and output waveguide  each waveguide is coupled to both cavities with coupling strength described by cavity decay rates re,in and re,ou  as explained in sec  for numerical simulations of the cmt equations' evolution, they may be retaine  using the convention introduced in se  3 and fi  substituting the expressions for s+1, b1, and b2 into e  the conversion effciency of the modulator, defined as the ratio of the output power at optical sideband to the input laser power according to e  21 it is important to note that for weak modulation,   this confirms the idea that for effcient sideband conversion, each of the interacting optical waves need to be resonant in the devic 1: list of the commonly used symbols and their descriptio  ro intrinsic decay rate of the cavity and supermode energy amplitudes due to losse  re coupling rate from the bottom cavity to the bus waveguide in the basic desig  re,in coupling rate from the cavities to the input waveguide in the generalized desig  re,out coupling rate from the cavities to the output waveguide in the generalized desig  re,s coupling rate from the symmetric supermode to the input/output waveguid  re,a coupling rate from the antisymmetric supermode to the input/output waveguid  zo characteristic impedance of the transmission line delivering the rf signa  cm capacitance of the electro-optic region of the active cavitie  rm parasitic resistance of the active cavitie  l1 inductance of the monolithically integrated inductor  rl1 parasitic resistance of the inductor l  ql quality factor of the inductor  qtot rf total quality factor of the rf resonato  acknowledgements this work was supported in part by ball aerospace and technologies cor  and by nsf award #170159  we thank todd pett of ball aerospace for his input and discussion ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190 toront edu abstract triangular, overlapping mel-scaled filters are the current standard input for acoustic models that exploit their input's time-frequency geometry, because they provide a psychoacoustically motivated time-frequency geometry for a speech signa  f-bank coefficients are provably robust to small deformations in the scal  in this paper, we explore two ways in which filter banks can be adjusted for the purposes of speech recognitio  second, by rearranging the order of operations in computing filter bank features, features can be integrated over smaller time scales while simultaneously providing better frequency resolutio  we make all feature implementations available online through open-source repositorie  initial experimentation with a modern end-to-end cnn phone recognizer yielded no significant improvements to phone error rate due to either modificatio  the result, and its ramifications with respect to learned filter banks, is discusse ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "time-frequency analyses of speech have long been the dominant feature representation for speech recognitio  there have been many different transformations that attempt to localize an event in time and frequency, such as wavelets and wigner-ville distribution  a powerful and persistent form of analysis is the short-time fourier transform it and its derivatives treat a speech signal as a series of windowed stationary processe  f-banks post-process the stft by isolating and integrating over different bands of the power spectru  the stationary assumption of the stft combined with cleverly spaced band-pass filters makes for a very robust time-frequency representation of the speech signa  as neural architectures become more sophisticated, a higher resolution may in fact become more desirabl  we can incrementally improve the f-bank formula by optimizing the time-frequency trade-off while respecting the conventions of time-frequency features namely, an audio signal produces a sequence of frames uniformly sampled in tim  there are many alternatives to the triangular filter that may have desirable properties for as  two such filters are the gabor and gammaton  the former has a provably optimal time-bandwidth productwhereas the latter more closely resembles the stimulus response of the human auditory system both are much more efficient in terms of time and frequency resolution, but because because the stft pipeline integrates over a uniform window in time, much of that resolution is sacrifice  1the uncertainty principle, as applied to digital signal processing, states that one cannot have an arbitrarily fine resolution in both time and frequency filters with short temporal support will have wide bandwidths, and very narrowband filters will have longer temporal suppor  a second potential improvement is to swap the order of integration in the stft pipelin  together, these two improvements would make a filter bank with better time and frequency resolution, though they could also be employed separatel  this paper's contributions are twofold: first, we present the aforementioned modifications to speech features in detai  these modifications, alongside more traditional speech features, are made available in the accompanying open-source python package2as well as through the open-source matlab repository covarep second, we separately evaluate these adaptations in the framework of an end-to-end speech recognition tas  constructing a cnn from the architecture described in for the timit phone recognition task, we decode the phone sequence internally using connectionist temporal classification we discuss the position of our results as it relates to current trend of learning filter banks from the raw wavefor  2 mel-scaled log filter banks to illustrate what a coefficient of an f-bank captures in time and frequency, we adapt an argument from the hk are triangular windows with vertices derived by linearly sampling the mel scale the mel scale, based on psychoacoustic experimentation, is roughly linear with respect to frequency below 1000hz and logarithmic abov  clearly, the frequencies captured by a coefficient of the f-bank are limited to the nonzero region of c h  because the bandwidth of hk scales with its central frequency, f-banks are robust to time warping in contrast, acoustic models that operate directly on the normalized power spectrum are sensitive to time-warpin  multiplying the signal with a window in time blurs the filtered frequency response  this functions similarly to a rolling average over b f and effectively increases the bandwidth of all h  while not particularly detrimental to wideband filters, the bandwidth of the window can be similar or greater than that of the narrowband filters, which can drastically reduce their discriminative powe  the hk are also entirely real in the fourier domai  in effect, signal transients smoothly transition between overlapping frame  this makes the integration difficult to interpret temporall ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Mel-scaled log filter banks", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Filter types", "Text": "triangular or square-root filters are not the only types of filters that can be scaled in the mel-domai  the first modification to the spectro-temporal recipe that we explore is to the type of filter employe  we experiment with two additional types of filter: the gabor filter and the gammatone filte  2https://githu com/sdrobert/pydrobert-speech 3https://githu 1 gabor filters gabor filters have been explored in a variety of context  2-d gabor filters are often applied to spectrograms for asr as a collection of spectral-temporal features their 2-d construction allows the bank to capture meaningful geometric structures, such as formant movemen  likewise, 2-d gabor bases have been learned as convolutional layers the present paper focuses on the design and evaluation of spectrogram-like features, rather than high-level spectro-temporal features that sit atop a spectrogra  recently, trained end-to-end cnns for phone recognition with weights initialized to a gabor filter ban  gabor filters are simply gaussian windows with a complex carrie  neighbouring filters' frequency supports intersect at their -3db bandwidth  gabor banks are calculated the same way as in equationexcluding the point-wise square roo  gabor filters have a provably optimal time-frequency trade-off their regions of effective support in both time and frequency are bounded above by a gaussian windo  one downside to the gabor filter is its symmetric frequency respons  log-linear scales of speech perception, such as the mel scale, are decidedly asymmetri  the gammatone filter helps mitigate this trade-of ", "Subsections": [{"Section_Num": "3_2", "Section": "3.2 Gammatone", "Text": "gammatone filters were derived in their skewed frequency response lend themselves nicely to existing models of speech perception gammatones have been employed in asr directly or used as a starting point in learned feature representations the gammatone does not have an optimal time-frequency trade-off like the gabor filte  it is a first-order approximation of the gammachirp filter, which is time-scale optimal incorporating the gammachirp into the standard time-frequency pipeline is difficult, however, because the gammachirp is not a linear-time invariant syste ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Short integration", "Text": " increasing the width of the window will decrease the magnitude of the spectral leakag  however, a wider window will capture more of the signal in time, decreasing its temporal resolutio  by taking inspiration from scattering transformswe can modify f-bank computations to better capture low-frequency informatio  the key observation is that the low-pass window has been shifted out of the nonlinear regio  furthermore, though a scattering path is a contractive operation with a normed space as its image, the modulus is nonlinea  the nonlinearity makes it difficult to patch together a cohesive 2-d geometry along higher-order path  thus, scattering transforms are most useful in classifying stationary processes instead of using a cascade of filters to capture high-frequency information, we can simply repeat the process in equationbut with a shorter windo  all energy within the modulus originated from the bandwidth of interest hence, the sif-bank produces a more accurate representation of the frequency domai  since the filter bank need only be invariant to small transformations between frames, we can choose t to be proportional to the frame shif  frequencies above said limit will be subject to aliasin  the size of the window t can be chosen to enforce this limi  one may adjust t until the zero-crossing or -3db bandwidth of the main lobe of the window's frequency response matches 1/ hert  decreasing the size of both the frame shift interval and window size can increase the temporal resolution of a shortintegration filter coefficient when the bandwidth of the window is much smaller than the bandwidth of the modulus in equation a downside of short-integration is that decreasing the window size will not increase temporal resolution when the bandwidth of the modulus is smaller than that of the window already, as is the case for narrowband filters in the ban  for the low-frequency narrowband filters in a psychoacoustic filter bank, windowing has little effect, as the temporal support of these filters already far exceeds that of the frame shif  thus, the coefficients corresponding to a narrowband filter in a sif-bank have poorer temporal resolution than those in an f-ban  this apparent downside is specious: the poor temporal resolution of narrowband filters is an inescapable property of the uncertainty principle, not the short-integration proces  any improved temporal resolution in stft-based filter bank features is paid for with a less faithful frequency representation, discussed in section   it is worth noting that, due to the poor temporal resolution of narrowband filters, their coefficients will be more highly correlated across time than wideband filter  this makes the sampling along those bands highly redundant when the frame shift is smal  redundant representations are not necessarily a bad thing for classification; sequential frames of f-banks are overlapping in time, for exampl  deltas and double deltas are merely convolutions of f-bank coefficients, which will clearly correlate the frame  previously, f-banks were shown to be more effective than cepstral coefficients for deep learning precisely because the coefficients of the former are more strongly correlated equation generalizes to arbitrary choices of filter bank  thus, we can combine the gabor filters and gammatone filters with short-integration to generate short-integration gabor banks and short-integration gammatone banks the theoretical benefit of gabor filters is their increased temporal resolution over f-banks, taking far less time to decay to near zero, whilst gammatone filters are more faithful to human auditory perceptio  lastly, we address the computational efficiency of the short-integration filter ban  stft-based computations have identical computational complexity with short-integration   the short-integration implementations in are fft-based, performing the overlap-save method of convolutio  because the fft is not preceded by windowing, shortintegration requires much larger ffts to avoid the effects of circular convolutio  this, and the required inverse fft, increases computational tim  fortunately, a corresponding decrease in computational time comes from performing fewer ffts in tota  traditional filter banks require an fft for each frame of coefficients, since each frame windows the signal anew, whereas short integration can leverage coefficients from a single fft-ifft in multiple frame  hence, overall computational times between the types of computation are quite comparable", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Experiment", "Text": "in order to explore the efficacy of filter types and methods of computation in speech recognition, we tested them as drop-in replacements for f-banks in a deep end-to-end recognizer designed for f-bank  the timit phone recognition task allows for fast experimental comparison and reduces the impact of language modeling on experimental result ", "Subsections": [{"Section_Num": "5_1", "Section": "5.1 Data", "Text": "we used kaldi's timit recipe to partition the audio dat  timit's core test set is comprised of 192 utterances of 24 unique speaker  a 50 speaker set of 400 utterances is peeled from timit's complete test set for early stoppin  462 speakers and 3969 utterances comprise the training se  dialect sentences were remove  the full set of 61 phone labels were used for training and decoding, but collapsed to the standard 39-phone set when calculating phone error ratewhich includes the silence phon ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5_2", "Section": "5.2 Model", "Text": "the acoustic model, developed with keras and tensorflowmirrors the one described in the model comprises of mostly convolutional layers with maxout activations connectionist temporal classification acts as the loss function for the networ  ctc embeds the forward-backward algorithm into backpropagation to output phone labels directly, curtailing the need for external sequence modeling maxout activations take the per-unit maximum of the output of at least two weight matrices that have received the same inpu  this doubles the number of trainable weights in memor  after discussion with the first author ofwe halved the weights listed therein to fit the  3 million parameter point listed with a size-2 maxout functio  the first four convolutional layers have 64 feature maps; the remainder have 12  above the convolutional layers sit 3 time-distributed fully connected layer  each layer has 512 time-distributed hidden unit  a final time-distributed weight matrix constructs the activation matrix over the 61 phone labels for ct ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5_3", "Section": "5.3 Training and decoding", "Text": "in the first stage of training, optimization is performed with adam at a learning rate of 10-  dropout of probability  3 is applied after the activation function of each laye  the stage ends when a model's validation loss has not improved for 50 epoch  afterwards, optimization continues using stochastic gradient descent at a learning rate of 10-8 and an l2 weight regularization penalty of 10-  the early stopping regime is the same as in the first stag  weights are saved after each epoch, and the weight set with the lowest validation loss from the second stage is used to decod  initially, we tuned the beam width used in decoding on the development se  however, we quickly found that larger beam widths were almost always preferable, so we fixed the width to 10  note that development of the cnn was always performed using f-banks; no architectural or optimization decisions were influenced by the experimental filter ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5_4", "Section": "5.4 Features", "Text": "the model is trained on four feature sets of identical shap  f-bank is our implementation of the standard log melscaled triangular filter bank, g-bank refers to the log mel-scaled gabor filter bank proposed in section   for the standard stft pipeline features40 log filters plus one energy coefficient are calculated every 10ms over a frame of 25m  the short-integration filters' window size was chosen to be 20ms,   filters are spaced uniformly on the mel-scale between 20hz and 8000h  deltas and double deltas are concatenated to the end of each frame vector, totalling 123 dimension  pre-emphasis, dithering, and compression were enabled at their standard kaldi value  an additional baseline, kaldfb, was included to test kaldi's built-in f-bank implementation as a sanity chec  5 filter test per kaldifb 1 75 table 1: mean and standard deviation phone error rate on the timit test se ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5_5", "Section": "5.5 Evaluation", "Text": "to evaluate the performance of the filters and computations, we performed a hybrid non-parametric statistical analysi  the friedman test is approprate for repeated measures of ranked data with more than two levels when the distribution of the dependent variable is not assumed to be gaussia  for the filter bank with the lowest mean per, we performed a wilcoxon signed-rank test between pers derived from regular computations versus those derived from short-integration computatio  10 trials were performed with different seeds for each combination of filter and computation, for a total of 70 trained model  those trials were removed from analysis, leaving 9 trials eac ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "6", "Section": "6 Results and discussion", "Text": " table 1 shows that the f-bank condition lead to the lowest mean pe  a wilcoxon signed-rank test found no significant difference between distributions of per across f-bank and sif-bank features therefore we cannot conclude that one feature representation lead to better average pers overal  switching from kaldi's f-banks to any of our filter banks improved per by approximately   furthermore, the size of the variance between runs is large enough to discount any gains or losse  the average per across runs on the test set was 1  this is higher than the percentage reported in 1  one trial with sif-banks yielded a per of 1  experimentation excluding dithering tended to have lower overall pers of around 1  dithering inserts random noise into the signal, which is intended to prevent systems from over-fitting on the signa  it is therefore possible to reduce the overall per of the systems by removing dithering, but this is not recommende  one must be careful about what is concluded from these result  they are consistent with the null hypothesis, which suggests that swapping features in end-to-end phone recognition has little effect on per, we conjecture that this is indeed the cas  given the recent interest and success of learned filter bank representations, notably that ofone might extend this perspective and claim that exploring fixed filter banks is outdated and irrelevan  the difficulty comparing this work on fixed filter banks and previous work on learned filter banks is that the latter class of paper tends to present a single error rate when discussing results, whereas we have presented a proper statistical analysis of the results over repeated trial  it is unclear whether those individual numbers are representative of a trend or a lucky setting of parameter  for example, as mentioned, we observed the per of 1 89% from sif-bank  had we only presented the best results, short-integration would have beaten out the best result from we are not concluding here that sif-banks are better than learned filter banks4 or even traditional f-bank  the statistical analysis does not support that, nor does it seem likel  but neither can we come to understand more than what is merely possible from individual error rates", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "7", "Section": "7 Conclusions", "Text": "we presented two distinct ways of modifying traditional f-bank features in order to produce a higher-resolution timefrequency representation of a speech signa  the second is to window over the filter response only after taking its powe  this leverages the low-pass characteristic of the modulus to minimize information loss after windowin  experimentation with a modern end-toend cnn architecture with ctc yielded no significant effect of filters or the modification to computation on pe  all filter bank and computation implementations are available as an open-source python package as well as through the open-source matlab repository covarep all of our results are predicated on swapping out stft features for short-integration features without any adjustments to the model architectur  end-to-end neural asr backpropagates through the decoding proces  even at the level of phone recognition, end-to-end models are responsible for modeling an entire sequenc  contrast this with an acoustic model in a hybrid dnn-hmm, which is only responsible for discriminatively classifying each frame of feature  in the latter case, decisions are a function of only the features within a small context window, whereas an end-to-end system uses the entire utterance contex  it is likely that an end-to-end system might spend more resources building a probability distribution over sequences of phones than it does focusing on any interval of speech features, nullifying the differences between filter types and resolutio  in our experience, there is considerable variability in the outcome of the training process of end-to-end system  more experimentation with traditional, hybrid architectures could prove illuminating in this respec  another interesting question is whether a neural architecture exists that is capable of processing shorter frame interval  as was mentioned in section 4, frame shift and window size can drastically affect the resolution in time of the feature representatio  where the support in time of the square root filters in a mel-scaled f-bank range between about 90-400ms, mel-scaled complex gabor filters with roughly the same bandwidths in frequency range between about 3-24ms in support in tim  gabor filters are much more capable of representing information at much smaller time scale  unfortunately, existing dnns already suffer from vanishing and exploding gradients when dealing with long time series data the end-to-end cnn experimented on here relies on delta and double-delta features to transmit long-term dependency informatio  increasing the total number of frames in an utterance would make it more difficult to transmit this dat  also, averaging over shorter time intervals means less spectral smoothing, which may prove difficult for architectures that are not noise robus  such a feature set may nonetheless be a more user-friendly and robust alternative to backpropagating directly through the speech signal, without requiring the additional parameter ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "8", "Section": "8 Acknowledgements", "Text": "this research was funded by a canada graduate scholarship and a strategic project grant from the natural sciences and engineering research council of canad ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "a fully relativistic treatment of confined hydrogen-like atoms   noon august 13, 2019 abstract the dirac equation is used to provide a relativistic calculation of the binding energy of a hydrogen-like atom confined within a penetrable spherical barrie  we take the potential to be coulombic within the barrier and constant outside the barrie  binding energies are derived for the ground state of hydrogen for various barrier heights and confining radi ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "the effect of confinement on the energy levels of an atom has been studied by multiple authors, probably first by michels and deboerand then followed by sommerfeld and welker who computed the confinement radius at which the binding energy becomes zer  these and more recent authors provided non-relativistic treatments of the problem based on the schrodinger equatio  see in particular ref  3-7, the last of which provides additional source  the intent of this paper is to derive a mathematical model for confined relativistic hydrogen-like atoms, which can then be utilized to investigate the effects of confinement on the relativistic energy levels of hydrogen-like atom  the fields of nano-structures and semi-conductor quantum dots have stimulated renewed interest in the problem we consider as a consequence of the need to take account of the effect on atoms from confining boundarie  because confinement of the atom can cause the energy of its electrons to become relativistic, a relativistic treatment of the problem seems require  however, no previous relativistic treatment of the problem described above has been ascertaine ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Theory", "Text": " the solution of e  introduction of this potential into eq  differentiating e  solving e  for f and using it in e  in e  above, ii and ki are the modified bessel functions of the first and second kind of order i respectivel  here, for solutions of e  multiplying e  ii increases exponentially as x increases while ki decreases exponentially as x increase  from e  refers to the subscript   adding e  to e  combining e  above with equation e  and substituting into e  after division of e  by e  use of eq  and expresses the right hand side of e  meanwhile, the left hand side of e  can be evaluated by the use of eq  e ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Results", "Text": "whereas the derived model can be used to compute the effects of confinement on the relativistic energy levels of any hydrogen-like atom, the data presented will be restricted to the hydrogen atom solutions for the binding energy values of the confined atom, which can be calculated by solving e requires a numerical method that avoids taking derivative  this is because the unknown e exists in the first parameter of the hypergeometric functions, and derivatives with respect to their parameters necessitate further approximatio  this in turn would produce less accurate results for energ  a more complete table can be found in re  what will be employed to help solve for   to check the accuracy of the numerical analysis, brent's method will also be utilized, which similarly does not warrant differentiatio  in the case of an unconfined hydrogen atom, the requirement that the solution of the dirac equation in a coulomb field converges makes it necessary to cut off the hypergeometric serie  however, it will be shown later in this section that we still retrieve the usual states of the hydrogen ato  as the radius of confinement increases, all energy curves approach the value -1 6ev associated with the 1s1/2 state in the free ato  figure 1 also shows that as the height of the barrier increases, so does the binding energy the relationship between the barrier height and the binding energy can be seen more visibly in figure 2, where curves are plotted for four different fixed confining radi  the essentially identical results given by both methods serve as a check on the precision of the values displayed in figures 1 and   figure 2: binding energies for the 1s1/2 state of relativistic hydrogen as a function of the barrier height at multiple fixed confining radi  7 when the barrier wall is very close to the nucleus, the electron can only be in its ground stat  in particular, figure   graphs the binding energies derived from additional solutions of e  it is noteworthy that these values are determined only by k and the size of the confining volume restricting the electron to specific state  shows a set of solutions of e", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Conclusion", "Text": "using the dirac equation, a mathematical model for the confinement of relativistic hydrogenlike atoms in a spherical penetrable barrier was derive  binding energies for the ground state of relativistic hydrogen were calculated as a function of the confining radius at various barrier height  the binding energies grew with a decrease in the confining radius r and/or a growth in the barrier height, which is a direct consequence of the uncertainty principl  furthermore, the confined ground state atom's 8 binding energy asymptotically approached the -1 6ev binding energy of the free ground state atom as the confining volume gre ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Acknowledgment", "Text": "i would like to thank professor robert deck and professor jacques amar for their advice and encouragement throughout this stud ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": " even in the presence of a light particle mediating self-interactions, we find that the finite-size effect may dominate the velocity dependenc  we present an explicit example in the context of a qcd-like theory and discuss possible ways to differentiate puffy dark matter from the usual light-mediator scenario  particularly relevant for this are low-threshold direct detection experiments and indirect signatures associated with the internal structure of dark matte  introductio  the dark matter nature is one of the most important open questions of our centur  until now, we have only observed dm via its gravitational effects, with data supporting the hypothesis that dm is collisionless at large scales nonetheless, that requires the self-scattering cross section to decrease with the dm velocity, because the particles residing in larger dm halos move faste  this paradigm is known as selfinteracting dark matter and has attracted a lot of attention from astronomers and particle physicists in the last two decade  one reason for this is the apparent mass deficit in the inner regions of small-scale halos with respect to the predictions of collisionless d  for a review see another reason for the continued interest in sidm is that it gives clues about specific properties of dm, which can be used to search for i  for instance, large and velocitydependent cross sections might hint at a long-range force, which in turn suggests the presence of a light mediato  in fact, since such a particle is a rather generic feature of several well-motivated dm models, velocity-dependent sidm is often associated with a light mediato  as is shown in fi  solid, dashed and dotted lines correspond to the dipole, tophat and gaussian distributions fer becomes larger than r-1 dm, the internal structure of the particle is probe  this is indeed the desired velocity dependenc  we will refer to this scenario as puffy d  beside the self-scattering effects, the fact that dm has a finite size leads to a very rich phenomenology, as has been explored for several concrete dm candidates i  scattering of finite-size dm particle  let us first consider the scattering of two finite-size objects, which -for simplicity- will be modeled as a collection of point-like constituents that coherently scatter by means of a spin-independent yukawa interactio  the corresarxiv:190  we will also assume that the contribution of the binding force to the scattering rate is negligibl  this is the case   if such a force leads to a momentum-suppressed scattering amplitud  an illustrative example is the electron scattering off finite-size object  in this case, e  gives the well-known rutherford scattering formula, which can be used to infer the shape of finite-size object  the latter is the dipole distributiongenerally expected from wave-function solutions to various potential wells we apply now e  to non-relativistic d  assuming that the dm particle is spherical,   is assumed to be constan  fi  1 illustrates this for the three representative distributions as listed in table   together with e  ii  dm scattering in astrophysical halo  because of the form factor, for low velocities we expect isotropic scattering, whereas for larger velocities forward scattering is more probabl  fi  best-fit curves to data for the dipoletophat and the gaussian distributions in table   the inset shows the 95%   and the dm size together with the corresponding parameter sets plotted in the main figur  see the appendix for detail  in fact, fi  2 shows that there is a one-to-one correspondence between the latter and the self-scattering of finite-size d  the dm relative velocity in astrophysical halos typically follows a maxwell-boltzmann distribution truncated at the corresponding escape velocity, vma  this method was applied to five clusters fromseven lowsurface-brightness spiral galaxies in and six dwarf galaxies of the hi nearby galaxy survey sample fi  3 shows these results respectively in green, blue and re  while cosmological simulations show this semi-analytical method works for isolated halosrecent studies suggest that tidal stripping may further modify the density profile of satellite halos such effects are not included here, because the galaxies shown in fi  3 are in the fiel  postulating a dm finite size much larger than the range of the yukawa force,   the corresponding bestfit of e  to the data above is shown in fi  as expected from the aforementioned remarks, there is almost no dependence on details of the form factors even though they correspond to substantially different density distribution  the figure also shows that, in order to have the right velocity dependence, the dm size needs to be hundreds of times larger than the compton wavelengt  this explains the name puffy d  this shows that the mediator can still be substantially lighter than puffy d  consequently, puffy dm must lie at the gev scale or belo  i  a model of puffy d  here we only sketch a possible realization of puffy dm while details will be discussed elsewher  they respectively have charges +2/3 and -1/3 under a dark ud gauge group with nc =   there are no dark weak interaction  see text for detail  as a result, this model is a realization of puffy d  direct detection signatures are closely related to the dm finite siz  its first part is the form factor associated with the ud charge and the second one parametrizes the dependence on the mediator mas  fd = 0 because the dm particle is neutral under ud the latter is given by e  in the view of this, we estimate current direct-detection limits by implementing such a recoil spectrum in ddcalc the results are shown in fi  a salient aspect of this dm setup is that energy recoils are momentum-suppressed albeit the enhancement due to the large charge radiu  this is in sharp contrast to the direct detection of point-like sidm by means of light mediator  likewise, the internal structure of puffy dm allows for up-scattering processes, giving rise to a wealth of indirect search signatures if dm de-excites ejecting sm particle  while this is much greater than the typical galactic dm kinetic energy, dm might be excited by inelastic selfscatterings in massive clusters of galaxies or by collisions with high-energy cosmic rays the former case may lead to radio and x-ray signals or dm dissipative cooling ; while the latter might trigger novel signals in direct-detection and neutrino experiments as in   a detailed investigation is beyond the scope of this lette  before concluding, we would like to emphasize that puffy dm does not necessarily require qcd-like dynamic  indeed, puffy dm can be realized in other theories of extended object  the study of puffy dm in the form of defects is an ongoing projec  conclusion  we have shown that if dm is an extended object with a size hundreds of times larger than its compton wavelength, the corresponding self-interaction cross section varies with velocity in a way that is largely independent of its internal structur  a qcd-like theory where dm is a dark nucleon has been used to illustrate our results, which are nevertheless general and can be applied to a broader range of theorie  for this reason, we believe puffy dm opens up a new avenue for sidm model-buildin  is supported by the erc starting grant newave thanks the alexander von humboldt foundation for support while this work was complete  was supported by the nsf grant phy-1638509, by the  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Acknowledgments", "Section": " Acknowledgments", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Appendix:", "Section": " Appendix: The transfer cross section", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "References", "Section": " References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190  to conduct our analysis, we used the mathematical model of search behavior, comprising the sociophysics approac  the seasonal topics selected were s valentine's day, halloween and new year countdow  we also picked up the event like christmas and hallowee  we analyzed the influence of blogs and twitter on search behavior and found a deviation of interest in terms of timin  we also analyzed japanese seasonal event of eating eho-maki in february 3 and eels at the day of the ox in midsumme ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "some people believe that twitter and blogs are enthusiastically posted, but most people are supposed to search on the interne  therefore, analysis of the search behavior of people of society is very important in grasping social movement  influences people's search behavio  the subjects of the analysis are seasonally limited dates, and many people participate in the even  in many european countries, people return to their homelands for christmas and enjoy a long christmas brea  in japan, therefore, it is not until the new years vacation of the following week that people return to their hometown  thus, in japan, christmas is very similar to hallowee  1 in japan, fe 3 is the traditional day to eat eho-mak  eho-maki are thick sushi rolls shown in fi 1 which is believed to bring good fortune if eaten while facing the years eho like fi  not all, but many japanese people have eho-maki at this da  figure 1: eho-maki in japa  figure 2: how to eat eho-maki in japa  on the day of the ox in midsummer japanese have a custom to eat eel which started in the edo period, 18th centur  eel is a popular food for japanese people, and it is expensive, so eating eel on the day of the ox in midsummer once a year is a big concer  on the day of the ox in midsummer japanese have a custom to eat eel which started in the edo perio  the day of the ox, which is named after one of the twelve animals of the chinese zodia  according to one legend, long before the scientific reasons were established, in the 1700s wellknown scholar gennai hiraga came up with the custom as part of a marketing ploy to boost limp summer sales when the owner of a struggling eel store asked the wise man for some business advic  in fi 3, we show the typical cooking of eel in japan for the day of the ox in midsumme  thus, this study examines the peaks and falls in interest in these time-limited events, focusing on the medium used to perform searches and on what cohort of the population perform those searche  the mathematical model of search behavior is used for the analysis", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Theory", "Text": "in the theory of search behaviorthe interest and concern on a certain topic can be calculated using a mathematical model of differential equation  here, we introduce i as the interest or concern on a certain topi  we construct a mathematical model based on the mathematical model for the hit phenomenon within a society presented as a stochastic process of interactions of human dynamics in the sense of many body theory in physics advertisements act as external forces; communications with friends are a form of direct communication and its effect is considered as interaction with the intention of friend  the rumor effect is considered as the interaction among three persons and a form of indirect communication as describe  in the model, we use only the time distribution of advertisement budget as an input, and word-ofmouth represented by posts on social network systems is the observed data for comparison with the calculated result  the parameters in the model are adjusted by the comparison with the calculated and observed social media posting dat  word-of-mouth represented by posts on social network systems like blog or twitter is used as observed data which can be compared with the calculated results of the mode  the unit of time is a da  here, it is assumed that the height of interest i of people attenuates 3 exponentiall  although it is known that this is known to occur in movies and the likeattention such as events and anniversaries is known to attenuate by a power functio  in the case of social interest, we attenuate the intermediate between the exponential function and the power functionbut here we simply adopt exponential deca  we consider the above equation for every consumers in the societ  using the mean field approximation, we obtain the following equation as equation for averaged intention in the societ  the derivation of the equation is explained in detail in re using this equation, our calculations for the japanese motion picture market have agreed very well with the actual residue distribution in time the advertisement and publicity effects are obtained from the dataset of m data and the wom represented by posts on social network systems are observed using the system of hottolin  we found that the indirect communication effect is very significant for huge hit movie ", "Subsections": [{"Section_Num": "2_1", "Section": "2.1 Extension to include Twitter and Blog", "Text": "in the new mathematical model for search behavior, we use daily blog and twitter postings as the external forc  therefore, we extend the above mathematical model for hit phenomena to include the effects of twitter and blog as external field as follow  i + pi2 correspond to the direct and indirect communications in the previous mathematical model for hit phenomen  in the model of the present paper, these terms correspond to the direct and indirect effect of searching the topic by other peopl  on the real calculation, we use advertisement time data on tv from m data c  lt  and the internet news site data, daily twitter posting data and daily blog posting data on the certain topic from hottolink c lt  the daily search data comes from google trend as the reference of our calculatio  we define here r-factor to check the correctness of the adjustment of parameter  thus, we use a random number to search for the parameter set that minimizes   this random number technique is similar to the metropolis method, which we have used previousl  we use this r-factor as a guide to obtain the best parameters for each calculation in this pape  we employ the monte carlo method like metropolis method to fine the minimum of   this is very similar for finding the minimum of total energy in the first principle calculatio  it is well-known that there are several ways to find the minimum condition like the steepest descent, the equation of motion method and the conjugate gradient metho  even in the actual calculation of the first principle calculation or the density functional theory, we should be careful of the danger of local minimum trappin  in this paper, the way we employ is just do the calculation using the several initial value in the metropolis-like method to avoid the local minimum trappin  to check the accuracy of the parameters adjusting, we use the r-factor valu  for every calculation which we show in this paper, the r-factor is below  ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Data", "Text": "for the investigation of this article, we should use google trends data for the target data of our calculatio  the data of google trends can be obtained on the google trends pag  the daily posting number to twitter and blog are obtained from the service kuchikomi kakaricyo of hottolink c lt  the mass media advertisement data can be obtained from m data c lt  via the kuchikomi kakaricyo servic ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Result", "Text": " 5 the calculation results of all coeffcients for christmas are shown in the fi  for christmas, the calculation results of all coeffcients for the first month before christmas and one month after christmas are shown in the fi  in particular, here we examine the difference whether search behavior is affected by blog or influenced by twitte  shown in the fi 5 are cblog and ctwitter values before and after christma  looking at the results, twitter's influence is big before christmas and blog influence is big after christma 7 and fi  for new year's countdown, we analyze the event 1week before and 1 week after the event in order to avoid the effect of the christmas before de  from these, it is possible to read the common tendency that twitter influences strongly before the event, and after the event the effect of blog is stron  next, we introduce the calculation results of eho-maki and eel of the midsummer's day eating foods decided on a fixed date in fi  as you can see from the results, the influence of blog and twitter on people's search behavior is opposite to the previous example  in the case of this foods, it is from blog that is influenced before the event and it is influenced by twitter after the even  the qualitative behavior is same for the two event food  figure 5: result of calulation of ct witter and cblog before and after christma  figure 7: result of calulation of ct witter and cblog before and after new year countdow ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Discussion", "Text": " these two groups are thought to depend on surprises or happening for the event and its preparatio  at christmas in japan, there are not many things to consider beforehand, such as what kind of surprise there are lovers to book a christmas dinner wit  at japanese valentine's day, women collect information in advance, whether women make their own chocolate for her lover or they purchase high-end chocolate at some famous brand sho  meanwhile, there are few kinds of eho-maki to eat as an event, and there is no element to look into in advance as there are also decided how to ea  also, there is no surprise when eating eho-mak  as for the eel to eat on the midsummer day of the ox, as shown in the fi 3, the method of cooking has been decided traditionall  it is impossible to make home made, so japanese people have to eat eel at a restauran  figure 9: result of calulation of ct witter and cblog before and after the ehomak  in advance is the only restaurant to ea  besides, there is not much difference in eel's cuisine for each restauran  in this way, events that need to check information suffciently in advance are affected by twitte  therefore, if search actions that are strongly influenced by twitter are observed beforehand, those who come to the event are collecting informatio  for those people, event-related marketing will be effectiv  in this way, the method of this research can be applied to marketin ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "6", "Section": "6 Conclusion", "Text": "posting on blog and twitter is an act performed by some people in societ  on the other hand, search behavior is used by most people who use the interne  therefore, analysis of search behavior on the internet is very important in social analysis in that it can also target people without voic  in this research, we have analyzed such search behavior on the internet using mathematical model of search behavio  as an object, we observed preliminary excitement and post cool down at the event to be held in a short tim  according to the analysis, it turned out that these are divided into events requiring preparatory preparations for surprises, and events not being prepare  twitter has a strong 8 figure 10: result of calulation of ct witter and cblog before and after the day of the ox in midsumme  influence on events requiring advance preparations for surprise  in other events the effect of twitter is after the inciden  this research result is expected to be applicable to marketin  acknowlegement the authors are grateful for helpful discussion to yasuko kawahata of gunma university, japa ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190 00077v1 1 jan 2019 derivations and reflection positivity on the quantum cylinder slawomir klimek and matt mcbride abstrac  we describe the general structure of unbounded derivations in the quantum cylinde  we prove a noncommutative analog of reflection positivity for laplace-type operators in a noncommutative cylinder following the ideas of jaffe and ritter proof of reflection positivity for laplace operators on manifolds equipped with a reflectio ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1. Introduction", "Text": "part of this work is a continuation of the program started in kmrsw1 and kmr of studying unbounded derivations in quantum domains, their implementations, and possible spectral triples associated to the  another part of this work was inspired by a glimm and jaffe note gj on reflection positivity for the laplace operator in r  additionally, we were influenced by the jaffe and ritter paper jrwhich considered reflection positivity for laplace operators on manifolds equipped with a reflectio  this inequality is a key step in proving the reflection positivity axiom of osterwalderschrader for the free field, see gjbook reflection positivity has been generalized in many directions, of particular interest for this note is already mentioned manifold generalization in jr a natural question is then if such ideas can be extended to noncommutative geometry to include examples of laplace-type operators on noncommutative manifold  one of the simplest possibilities, studied in detail in this paper, is a quantum cylinder which classically has a natural reflection through the middl  the noncommutative cylinder was constructed in kl3 and further studied in kmrs and kmr it has a natural rotational symmetry as well as a reflection, as will be shown later, and it also has an analog of the lebesgue measur  to define a class of interesting, reflection invariant, laplace-type operators in the corresponding hilbert space we use rotationally covariant unbounded derivations on the quantum cylinder that we studied in kmrsw1 and kmr with proper choices we indeed get a reflection positivity for such analogs of laplace operators with the proof following closely the ideas in jr every step was carefully motivated by the corresponding classical geometry concepts and their noncommutative version  the paper is organized as follow  in section 2 we review the quantum cylinder and discuss its geometric hilbert space constructed from an invariant weight playing the role of the lebesgue measur  we also discuss the reflection operator necessary for the reflection positivity for the laplace-type operators we conside  in section 3 we classify all unbounded derivations on the quantum cylinder that arise from the dense subalgebra considered in section   finally in section 4 we show how to implement those derivations which are invariant and covariant and create laplace-type operators from those implementation  moreover we prove the reflection positivity for such a class of laplace-type operator ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2. Quantum cylinder", "Text": " to introduce it we need some notatio  uek = ek+  also, we see that is a cartan pair r the constants are denoted by   the set of all such functions will be denoted by c+/0  in fact, we have the following useful identificatio  useful_gens proposition   proo  it is well defined on all of a because it preserves the relation hdef_ref the hilbert space h carries natural representations of   both maps have trivial kerne  the subspace is a natural domain for unbounded operators considered in later section  the symmetries of a can be implemented in h as follow  proo  the statements follow from direct calculation  nti_on_gens proposition   proo  the first part of the proposition follows from calculation  this completes the proo  pos_neg_su we have the following key propert  6 klimek and mcbride lus_h_minus proposition   proof", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3. Classification of Derivations in the Quantum cylinder", "Text": "the purpose of this section is to give a description of unbounded derivations in a defined on   one can think of derivations as noncommutative analogs of vector fields, and they will be used in the next section as building blocks in constructing quantum laplace-like operator  the key to understanding the structure of derivations in a is the following simple observatio  proo  it follows that d is in   to describe its kernel we need the following concept  the set of all such functions will be denoted by cin  var_der_rep proposition   derivations and reflection positivity on the quantum cylinder 7 proo  the proof is a slight extension of proposition  4 of kmrsw2 inner_coef proposition   proo  full details of an analogous result for a class of more complicated examples is given in theorem  10 of kmrsw2therefore we only give a brief outline her  proo  it was shown in kmrs that k is the commutator idea  thus belong to k for all n and a belong to   then proposition approx_inner_coef  3 implies that dn is approximately inne  we also get the following immediate corollar  class_der proposition   dfgdef_ref proo  since pol is the algebra of trigonometric polynomials, a derivation is completely determined by its action on the generator ei  this kind of lifting problem is often the most diffcult step in classification of derivations with respect to an ideal in the algebr  proo  this completes the proo  notice that we could not have simply defined df,g by the above formulas on generators as it is not immediately clear that they unambiguously extend to a derivation on all of   putting together all of these results, we get the following classification theorem, which is the first main result of this pape ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4. Reflection Positivity of Laplace-type Operators", "Text": "this section contains our main results on reflection positivity of laplace-type operators constructed using special kind of derivations described and classified abov  in order to construct interesting, geometrical laplace-type operators we want to implement such derivations as operators in the hilbert space h defined in in kmrsw1 and kmrthe invariant and covariant derivations were easily implementable because the representation of the algebra in the hilbert space considered in those papers had a cyclic vecto  in the present situation this is no longer the case, thus we need a new argument to show that the invariant 12 klimek and mcbride and covariant derivations can be implemented us unbounded operators on domain d defined in the following proposition gives a description of such invariant and covariant implementations and uses an approximate identity argumen  r_implement proposition   proo  both parts have almost identical proofs; we will concentrate below on the covariant cas  suppose d is an implementation of a covariant derivation as in the statement of the propositio  such a choice can be made since f has finite suppor  reflection positivity: invariant cas  we describe here the easier of two interesting reflection positivity result  we proved in proposition der_implement   when this happens is the subject of the following propositio  proo  using proposition morph_anti_on_gens   this finishes the proof as, given dbeta, the sequence beta is determined up to additive constan  the following is the reflection positivity inequality in the invariant cas  proo  the proof of the theorem is computational and it is simplified by the fact that the implementation operator dbeta is diagona  moreover by proposition theta_h_plus_h_minus   reflection positivity: covariant cas  its implementations, according to proposition der_implement   the conditions for this to be true are described in the next statemen  proo  using proposition morph_anti_on_gens   proo  the implementation of the covariant derivation from now on will be denoted by dbet  the following is the second main result of this paper, reflection positivity for a class of quite non-trivial laplace-type operators coming from covariant derivation  _pos_covar theorem   let m2 be a positive numbe  proo  to contrast it with classical case, integration by parts/stokes theorem are replaced by more complicated discrete version  we will now concentrate on proving the above inequalit  the first goal is to express the left-hand side of it as boundary term  this is done in two step  to prove positivity we need the following observation that relates boundary terms of the type we encountered above with manifestly positive sums over bigger region  pos_bndy lemma   this completes the proof", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "References", "Section": "References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190 00078v5 27 jan 2020 birational superrigidity is not a locally closed property ziquan zhuang abstrac  as an application, we show that birational superrigidity is not a locally closed property in modul  we also prove that the alpha invariant function is constructible in some families of complete intersection ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1. Introduction", "Text": " in particular, such fano varieties are not rationa  this property identifies a very special class of fano varieties and an interesting question is whether the set of birationally superrigid fano varieties form a nice modul  moreover, such moduli satisfies the valuative criterion of separatedness by a recent result of stibitz and the author in addition, birational superrigidity is a constructible condition by so the next step is to check whether birational superrigidity is a locally closed property or no  unfortunately the answer is no in general, and one of the main purposes of this note is to construct such counterexample  the analogous openness question for birational rigidity also has a negative answer by the 3-dimensional counterexample in in both examples, birational rigidity is a locally closed property in the corresponding modul  our construction is based on the degeneration of hypersurfaces into double cover  we show that with suitable choices of fs and gs, this provides the counterexample we want in every suffciently large odd dimensio  notation as abov  let x belong to pn+  the study of the birational superrigidity of xs,t in the above example can be further divided into two part  we first prove the birational superrigidity of smooth double covers of fano index 1 by generalizing the multiplicity bound in the complete intersection case indeed we prove something stronger: theorem   we refer to for the definition of k-stability in the above statemen  in our cases, k-stability is a direct consequence of the birational superrigidity by a simple application of note that theorem  2 generalizes the results of by allowing more general weighted complete intersections and removing the generality assumptions next we handle singularities that may appear on x0 belong to a2).  to do so we resolve the singularities by weighted blowups and analyze the maximal singularities on the resolutio  this paper is organized as follow 1 and present a few further question  finally in the appendix, we prove that the alpha invariant function is constructible in some special case  birational superrigidity is not a locally closed property 3 notation and convention  we work over   unless otherwise specified, all varieties are assumed to be normal and divisors are understood as q-diviso  it is said to be movable if m is movable the notions of terminal, canonical, klt and log canonical singularities are defined in the sense of acknowledgemen  the author would like to thank his advisor j anos koll ar for constant support, encouragement and numerous inspiring conversation  shokurov for helpful discussions and for pointing out several useful references and asher auel, yuchen liu and ziwen zhu for helpful conversation ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2. Higher codimensional alpha invariants", "Text": "in this section, we study properties of higher codimensional alpha invariants and construct examples to show that they are not lower semi-continuous in genera  this is of independent interest and the results are not used until the end of the note, so readers who are mainly interested in the proof of our main theorems may feel free to skip this sectio  let be a klt pair and l an ample line bundle on   it is also not hard to see that the infimum in can be replaced by taking the limi  4 ziquan zhuang proo  the argument is essentially the same asand we reproduce the proof for reader's convenienc  where j denotes the multiplier ideal of the pai  proo  the result then follows from proposition   before we prove this proposition, let us explain why it leads to the failure of lower semi-continuity for higher codimensional alpha invariant  proof of proposition   by lemma   proo  the argument is similar to those of ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3. Weighted complete intersection", "Text": "in this section we prove theorem   proo  proo  we prove by induction on   proo 2 applied to x  the main point of lemma   our choice of n above is probably far from optimalbut it is suffcient for our nee  proof of theorem   the argument is almost identical to the proof ofso we only give a sketc  by lemma 3", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4. Hypersurface with ordinary singularities", "Text": "in this section we prove theorem   the proof is a bit lengthy, so we divide it into several step  we first treat the rigidity of   then is canonical along   proo  we start with some multiplicity estimat  we also need to show that has canonical singularities away from v thus has canonical singularities away from v the rest of the proof is similar to those in by and lemma   proo  this follows directly from and the proof of 10 ziquan zhuang proof of theorem   suppose that is not canonical at some point x belong to x with multiplicity m  byhas canonical singularities outside points of multiplicity at most n -  by lemma   it follows that the only possible maximal singularity of x is the ordinary blowup of a point of multiplicity n -  by standard argumentthis proves parts and of theorem   proo  from the above proof of lemma   hence we may assume that m0 = n -  for this we use the criterion and need to analyze the singularities of a few more auxiliary pair  then is lc over the smooth locus of   proo  we first show that is lc outside a set of dimension at most   by and lemma   let d be an effective divisor on   then is lc at   proo  let m be a suffciently divisible integer such that md and mm are both integra  in other words, is lc at   assume that is lc away from a finite number of points, then is l  proo  by and lemma   proof of theorem   it then follows from that x is k-stabl  let x belong to x be a point of multiplicity m  it remains to consider the case m0 =   we claim that in a neighbourhood of e, is lc outside a finite number of poin  by lemma   in other words, ) is kl  it remain to treat the case when d is supported on d0 by lemma 4", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5. Counterexample", "Text": "in this section we complete the proof of theorem   proof of theorem   by and theorem   it then follows from the proof of theorem   it remains to show that the pair is also canonical at   the rest of the argument is similar to theorem   as in the proof of theorem   the same proof as in theorem   the proof is now complet  first, we expect some weaker variant of birational rigidity to be an open propert  to make this precise we need a definitio  note that in our main example and in the 3-dimensional exampleevery member of the family is birationally rigid and in particular soli  in the example ofit is also proved that a general member of the family is solid since 14 ziquan zhuang there is at most one other mori fiber space that's birational to i  these motivate the following question", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Appendix", "Section": "Appendix A. On a conjecture of Tian", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "References", "Section": "References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190  the temporal discretization can be based on either the backward euler method or the convexsplitting metho  we show that the fully discrete scheme admits a unique solution, and we establish optimal convergence rates for all variables in the l2 norm for arbitrary polynomial order  in terms of the globally coupled degrees of freedom, the scalar variables are superconvergen  another theoretical contribution of this work is a novel hdg sobolev inequality that is useful for hdg error analysis of nonlinear problem  numerical results are reported to confirm the theoretical convergence rates", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": " email: cglwdm@uest ed cn department of mathematics and statistics, missouri university of science and technolog  email: handaoz@ms  email: singlerj@ms  email: ywzhangf@ude  the cahn-hilliard equation is a fourth order, nonlinear parabolic equation which was originally proposed by cahn and hilliard as a phenomenological model for phase separation and coarsening in a binary allo  in a series of works infeng and karakashian design and analyze a dg method of interior penalty type based on the fourth order formulation of the cahn-hilliard equatio  optimal error estimates in various energy norms are established: see also kay et a  propose and analyze a different dg method that treats the cahn-hilliard equation as a system of second order equations allowing a relatively larger penalty ter  a fully adaptive version of the interior penalty dg method was recently constructed in for the cahn-hilliard equation with a source and optimal l2 error bound were derived; see also for solving the advective cahn-hilliard equatio  the local discontinuous galerkin method has also been proposed for the discretization of the cahnhilliard equation by writing it as a system of four first-order equation  dong and shu in analyzed an ldg scheme for general elliptic equations including the linearized cahn-hilliard equation and obtained optimal error estimate in l  recently, an ldg method has been employed for solving a number of cahn-hilliard fluid models, c  the dg method is however often criticized for the larger amount of degrees of freedom compared to the continuous galerkin method in the seminal work cockburn et a  propose a hybridizable discontinuous galerkin method for second order elliptic problem  in a nutshell, the hdg method maps the flux and solution into the numerical trace of the solution via a local solver, which are in turn connected by the continuity of fluxes across inter-element boundaries hence the 2 globally coupled degrees of freedom are those numerical traces, resulting in a significant reduction of the number of unknowns in traditional dg method  moreover, the hdg methods possess the same favorable properties as classical mixed method  in particular, hdg methods provide optimal convergence rates for both the gradient and the primal variables of the mixed formulatio  this property enables the construction of superconvergent solutions, contrary to other dg method  these advantages of the hdg methods have made hdg an attractive alternative for solving problems governed by pdes and pde control problems, see to the best of our knowledge, there does not exist an hdg work that achieves optimal convergence rates for all variables for a fourth order proble  the hdg framework with reduced stabilization and polynomials of mixed orders was first introduced by lehrenfeld in where it was alluded that the scheme could be a superconvergent method,   optimal convergence and hence superconvergence was then rigorously established for convection diffusion problemsnavier-stokes equationsand more recently for linear elasticity problems these convergence rates are further validated by numerical experiments in section   a particular theoretical contribution of this article is the estab3 lishment of a novel hdg sobolev inequality which is a useful tool in the numerical analysis of nonlinear problem ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 The HDG formulation", "Text": "to introduce the fully discrete hdg formulation for the cahn-hilliard equation, we first fix some notatio  let eh denote the set of all faces e of all simplexes k of the triangulation t  now we introduce the fully discrete hdg formulation of the cahnhilliard equation based on backward euler method and convex-splitting approac  we shall also make use of the standard l2 projection, denoted by pm, onto mh", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Preliminaries", "Text": " we shall also utilize the following version of the piecewise poincar efriedrichs inequalityc  let v be a piecewise h1 function with respect to the partition t  the following hdg poincar e inequality is then an immediate consequence of lemma  1, the cauchy-schwarz inequality and the triangle inequalit  now, we glean some basic properties of the operator   first, the definition of a in e  immediately implies lemmas   next, we show that the operator a satisfies the following boun  proo  then the bound follows from the cauchy-schwarz inequality and the inverse inequality this completes the proo  furthermore, we establish a crucial lemma that bounds the gradient of the scalar variable in terms of the flux variable and the reduced stabilizatio  proo  the desired inequality now follows by combining the last two inequalitie  this completes the proo  finally, we show that the operator a satisfies a version of the discrete lbb conditio  proo  we only give the details of the proof of the inequalitysince the argument for is simila  then follows immediatel  this completes the proo  we now introduce the hdg inversion of the laplace operator equipped with homogeneous neumann boundary conditio 8 is well define  proo  then definition   this is only possible if uh =   the proof is complet  by definition   this concludes our proo  in addition, by the definition  6 one can easily establish the following relatio  for the error analysis of the nonlinear equation we need to establish the discrete hdg sobolev inequalities for which we will make use of the so-called oswald interpolation operator now we are ready to prove the hdg sobolev inequalitie  proo  we only give the proof of the inequalitysince is a direct consequence of the inequality and the poincar e-friedrichs inequality in lemma   the desired inequality now follows from the inequalities and this completes the proo  the combination of the above theorem and the triangle inequality gives the following hdg sobolev inequalit  proo  the hdg sobolev inequality now follows from the inequality the second inequality follows from the first inequality and the hdg poincar e inequality this completes the proo ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Well-posedness of the HDG formulation", "Text": "in this section we establish the well-posedness of the hdg methodthat is, existence and uniqueness of solutions as well as the energy stability of the solution  the results differ slightly between the fully implicit discretization and the convex-spliting method : the cs time marching enjoys unconditionally unique solvability and stability while there is a time-step constraint in the fi scheme for uniqueness and stabilit  for convenience, we will focus on the analysis of one method and point out the difference of the other", "Subsections": [{"Section_Num": "4_1", "Section": "4.1 Existence and uniqueness", "Text": " the hdg scheme admits at least one solutio  proo  thanks to lemma   by lemmas   by lemma   we proceed to show that is the solution to the hdg scheme noting that eq  next, we prove that they are true for the w1, w2 belong to w  this completes the proo  proo  givenwe let and be two solutions of hence all terms on the left hand side of are nonnegativ  for the case of the backward euler method,   in light of e  one then obtains uniqueness of the solution for the backward euler schem  this completes the proo ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4_2", "Section": "4.2 Energy stability", "Text": " we first recall some useful identitie  let a,b be two real number  the first energy identity makes use of the negative norm and takes the following for  letbe the solution of proo  this completes the proo  next, we give the second energy identity which involves the l2 norm of pn   letbe the solution of the energy identity now follows from applications of the identities in lemma   there is a negative term,   in theorem  6, we bound this term and derive the stability resul  let c be the product of the constants in and lemma   proo  the energy bounds now follow from the energy law above, the energy identity ii and lemma  4 where all terms are associated with the positive sig  we note that in theorem   hence, we need a refined analysis for this ter  the assumption is the same as in theorem   proo  taking = in e  next, by the hdg sobolev inequality in corollary   21 we then use theorem   from the hdg sobolev embedding inequality corollary  6 in the derivation of the last ste  this finishes the proo  for this, we introduce the discrete laplacian operato  let un h be the solution of for uh = un h in e by the uniqueness of the solution, in light of e  one readily obtains the estimate in light of the stability boun  this completes the proo  proo  by the uniqueness of solutions to the elliptic projectionin view of remark   collecting the above estimates, one concludes the proo  using lemma  9 immediately gives the following resul  let un h be the solution of", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Error analysis", "Text": "in this section, we provide a convergence analysis of the fully implicit hdg method for the cahn-hilliard equatio  the convex-splitting scheme can be similarly treate  first, we give our main result  then, we define an hdg elliptic projection as inwhich is a crucial step to prove the main resul  in the end, we provide rigorous error estimation for our fully implicit hdg metho  throughout, we assume the data and the solution of are smooth enoug  the generic constant c may depend on the data of the problem but is independent of h and may change from line to line", "Subsections": [{"Section_Num": "5_1", "Section": "5.1 The main result", "Text": "we can now state our main result for the hdg metho  let and be the solutions of andrespectivel  assume the solution attains the maximum regularity for the best approximation results in to the best of our knowledge, is the only work for fourth order problems using an hdg method with polynomial degree k for all variable  they obtained an optimal convergence rate for the solution and suboptimal convergence rates for the other variable  in contrast, the hdg method proposed in this work deals with a nonlinear fourth order problem and achieves optimal convergence rates for all variable  moreover, from the view point of degrees of freedom, we obtain the superconvergent rate for the solutio ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "6", "Section": "6 Numerical results", "Text": "we consider two examples on unit square domains in r  in the first example we have an explicit solution of the system ; in the second example an explicit form for the exact solution is not know 3177e-05 order  3185e-05 order  0629e-06 order  0697e-06 order  2935e-07 order  2940e-07 order  2358e-08 order  2090e-08 order 2", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "7", "Section": "7 Appendix", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Agent-based", "Section": "Agent-based simulations of China inbound tourism network", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "machine learning materials physics: integrable deep neural networks enable scale bridging by learning free energy functions   teicherta,   natarajanc,   van der venc,   although free energy data is not generally found directly, its derivatives can be observed or calculate  in this work, we present an integrable deep neural network that can be trained to derivative data obtained from atomic scale models and statistical mechanics, then analytically integrated to recover an accurate representation of the free energ  the idnn is demonstrated by training to the chemical potential data of a binary alloy with b2 orderin  the resulting dnn representation of the free energy is used in a mesoscopic, phase field simulation and found to predict the appropriate formation of antiphase boundaries in the materia  in contrast, a b-spline representation of the same data failed to resolve the physics of the system with suffcient fidelity to resolve the antiphase boundarie  since the fine scale physics harbors complexity that emerges through the free energy in coarser-grained descriptions, the idnn represents a framework for scale bridging in materials systems", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "an accurate description of the free energy plays a critical role in many physics-based models of materials system  the euler-lagrange equations of stationary problems are obtained by requiring that the first variational derivative of the free energy functional vanis  in the example of elasticity, this leads to the equilibrium equation satisfied by the displacement fieldedu preprint submitted to elsevier may 17, 2019 arxiv:190 00081v3 16 may 2019 equations can also require first variations of the free energy as input  for example, the variational derivatives of the free energy with respect to composition and order parameters define the chemical potential fields used in phase field dynamics a related result is that the first derivative of the free energy density function with respect to appropriate strain measures gives the conjugate stres  second derivatives with respect to the temperature yield the heat capacity, those with respect to strains define elasticities high-fidelity evaluations of free energy density functions are attainable by a combination of atomic scale models and statistical mechanic  implementing these computations, however, can be challenging for a number of reason  free energy data is often computed at individual points rather than as an analytic function directl  as such, it becomes necessary to use a fitting process to create a faithful mathematical model of the free energ  additionally, realistic data for the free energy can contain regions with rapid fluctuations, along with other regions with very gradual slopes finally, because of the importance of the derivatives of the free energy, it is desirable that the fitting function be smoot  all of these considerations pose challenges to the fitting techniqu  machine learning methods are readily applicable to this problem, and we specifically consider deep neural networks because the activation functions used in dnns are generally global functions with one local feature, dnns are capable of capturing local phenomena without negatively affecting longer range attributes of the dat  one potential disadvantage of dnns is that they are not, in general, analytically integrabl  this is a particular challenge in the case of fitting free energy data, because the free energy is often not directly measured or computed from atomic models and statistical mechanic  instead, the derivatives of the free energy are first observed or computed, then integrated to find the free energy of the system such an approach is of particular importance in cases where the chemical po2 tential representation must be integrated with respect to chemical variables to obtain the free energy density, whose derivatives with respect to strain then yield the stress for elasticity in order to preserve as much information about the derivatives as possible, it is ideal to train directly to the derivative data itself, as opposed to a numerically integrated data se  this requires an alternate form of dnn that can be trained to derivative data, then analytically integrated to represent the free energy itsel  we present such an approach in this wor  in this first presentation of our proposed framework, we consider the problem of chemistry, postponing coupled problems for later communication  to demonstrate the method of training a dnn to derivative data of the free energy, we consider a simple binary substitutional alloy with b2 ordering on a bcc parent structure the resulting free energy, as a function of composition and an order parameter, is then used in the cahn-hilliard and allen-cahn phase field equation  in section 2, we present the form of an integrable deep neural network section 3 describes the b2 material system and the method of calculating the chemical potential data via a combination of atomic scale and statistical mechanics computation  in this context, the idnn training to the chemical potential data and the resulting free energy dnn are shown in section   we describe the phase field formulation in section  2 and present the computational results obtained using the dnn representation of the free energ  section 4 describes a process for fitting the chemical potential data using b-splines and compares the resulting fit and phase field simulation with those of the dn  concluding discussions are presented in section  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Integrable Deep Neural Network", "Text": " in this instance, the output is a scala  loss function is generally the mean square error for regression problem  ndata is implied in the definition of the ms  as argued in the introduction, it can be desirable to train a function with data describing the derivative  while it is possible to train a standard dnn directly to the derivative data, there are at least two drawback  the first is that the standard dnn cannot, in general, be analytically integrated to recover the originating functio  furthermore, in the multidimensional case with multiple sets of partial derivative data, the trained standard dnns representing the partial derivatives would not necessarily be themselves the derivatives of the same functio  this inconsistency poses a problem even in situations where mathematical representations of only the partial derivatives are needed and not the integral itsel  as such, it will be referred to in this work as an idnn the antiderivative of the idnn is simply a standard dnn with the weights and biases of the trained idn  we emphasize that the form of the idnn in e  thus, once the weights and biases of an idnn have been trained usingthe analytically integrated dnn is constructed by simply using the idnn's weights and biases in a standard dnn given by and ", "Subsections": [{"Section_Num": "2_1", "Section": "2.1 Enforcing symmetries", "Text": "neural networks have the property of being uniform approximations of continuous functions over compact domains however, underlying symmetries of the domain are not guaranteed to be reproduced exactly by the dn  given the symmetry of the domain ofthe function is also symmetric under the same operation,   any artificial neural network that approximates f must reproduce this symmetry exactl  this can be enforced on the dnn by first transforming the inputs to a set of symmetric functions these functions map all symmetrically equivalent points in the space on to the same value, while also differentiating symmetrically distinguishable point  a dnn that approximates any function in this class can be guaranteed to obey the required symmetry by using as the input function  in general, a set of invariant inputs to the neural network may be defined by generating symmetry invariant polynomial functions with algorithms described by thomas and van der ven we will denote these functions as   bridging atomic to continuum scales via an idnn representation of the free energy density the phase field model is an outgrowth of the allen-cahn theory that was originally developed to describe the motion of anti-phase boundaries in ordered compound  many binary alloys form a disordered solid solution at elevated temperature that transforms to an ordered compound at low temperatur  a wellknown example is the order-disorder transition in brass, where an equiatomic alloy of cu and zn forms a disordered mixture over the sites of a body centered cubic lattice at high temperature, but then orders into a cscl-type structure at low temperature  the low temperature cuzn phase can be viewed as an ordered arrangement of cu and zn over the sites of the bcc lattice, as illustrated in figure   this ordering on bcc is referred to as b  because of the translation symmetry of the bcc lattice, there are two ways to form distinct b2 orderings on the same parent bcc lattic  alternatively, cu could occupy the corners, while zn occupies the body centered site  the two orderings are identical, differing only by a rigid translation in spac  an alloy that orders upon cooling may adopt the 6 first variant in some regions of the solid and the second variant in other region  when the two variants impinge, they are separated by an anti-phase boundar  order parameters can be used to distinguish regions where the constituents of the alloy are ordered from regions where they are disordere  the order parameters should also be able to distinguish between the different translational variants of the ordered phas  order parameters that satisfy these conditions for the b2 ordering on bcc are well known in a binary a-b alloyeach sublattice concentration xi is defined as the fraction of crystal sites belonging to sublattice site i that are occupied by b atom  this becomes clear when considering an alloy containing an equal number of a and b components a binary alloy can be modeled as a lattice model that tracks the configurational degrees of freedom associated with all possible ways of arranging a and b atoms over m sites of a crysta  second order phase transitions are estimated from the divergence of the heat capacit  are determined by the chemistry of the alloy and can be parameterized by training to a database of first-principles electronic structure calculation  the nearest neighbor pair interaction was chosen such that the b2 ordering is a ground state the finite temperature thermodynamics associated with the order-disorder phase transition can be calculated with monte carlo simulations applied to the cluster expansion hamiltonia  figure 2 shows the temperature versus composition phase diagram of the model allo  the b2 stability domain is separated from the disordered solid-solution domain by a second order phase transition conventional grand-canonical monte-carlo techniques restrict the calculated 8 free energy to only the thermodynamically stable region  the cluster expansions, and statistical mechanics calculations were performed with the casm code the measured ensemble averages were then used to calculate free energy derivative  the idnn was implemented as a custom estimator using the tensorflow libraryand was defined by two hidden layers with 10 units per laye  the idnn was trained for 500 epochs using the adagradoptimizer, with learning rates of  5 applied at different stages of trainin  a batch size of 10 was used, with 105,061 points in the training set and 35,021 points used for cross-validatio  the resulting learning curve is plotted in figure 4, showing the decrease of both the training and cross-validation mean square errors as training progresse  since the idnn has a more complex form than the standard dnn, it is reasonable to expect that it may require more training to achieve comparable error  the cross-validation error is nearly indistinguishable from the training erro  dnns were trained to the same chemical potential data, with the same symmetry conditions impose  all twenty neural networks consisted of two hidden layers of ten neurons, each with different initial values for the weights and biase  they were trained for 20 epochs with a learning rate of   the resulting learning curves appear in figure   after 20 epochs, the average mse for the standard dnns was higher than the average mse for the idnn  thus, the added complexity of the idnn does not inhibit the trainin  figure 6 shows the original chemical potential data compared with the associated idnn that was trained to the corresponding chemical potential dat  it also shows two views of the free energy surface as represented by the analytically integrated dn  given that the wells exist at the same composition, the material will not separate into multiple phases, but instead form anti-phase domain  this reflects the expected physics of the system, described at the beginning of section   phase field computation to demonstrate the use of the dnn representation of the free energy in computations, the analytically integrated free energy dnn was used in phase field computation  the phase field model was based on the coupled cahnhilliard and allen-cahn equationssolved using isogeometric analysis the simulation was performed using the mechanochemiga code1, which is based on the petiga and petsc libraries, and run on the xsede comet hpc cluster 1code available at githu  each neural network was trained for 20 epochs, with different initial values for the weights and biase  the cross-validation error is nearly indistinguishable from the training erro  the added complexity of the idnn does not inhibit trainin  it models the overall composition of the system through c, while conserving mas  bottom row: the analytically integrated free energy density dn  the two equations are coupled through the chemical potentials being derived from the same free energ  periodic boundary conditions were applie  phase field results we considered a two-dimensional domain, discretized by a 200*200 element mes 01, representing a material that had just been quenched from a higher temperatur  periodic boundary conditions were applie  as shown in figure 7, the initially disordered domain gradually forms regions of the two ordered state  the antiphase boundary has completely formed within 40 time step  the chemical potentials are represented as idnns, integrated to yield the free energ mn -  then we can rewrite e  equations and are applied, with datasets in the space corresponding to the space for b-spline surface  selection of knots while the method presented in the previous section can be used to optimize the values of the control points for given knot vectors, the locations of the knots also can be optimized to minimize the erro  our approach was performed in two step  first, we divided the data into training data and validation data higher numbers of knots were not used due to the resulting oscillatory behavior of the fit, particularly in regions of missing dat  knots placed at chebyshev nodes also resulted in inaccurate fluctuations where data was missing when more than one interior knot was use  in comparing the resulting cross-validation error, it was found that thirteen uniformly distributed interior knots gave the lowest error with  25 * 10-  in the second step, we used matlab's genetic algorithm optimization routine to attempt to improve the b-spline fit using thirteen interior knots per knot vector appropriate inequality constraints were applied to maintain monotonically increasing knot vector  the algorithm terminated after 50 generations, with a mse of  903 * 10-  since this was not an improvement over the error with uniformly spaced knots, the b-spline fit with thirteen uniformly spaced interior knots per knot vector was taken as the best fit using b-spline  b-spline results, and comparison with dnn the resulting b-spline representations of the chemical potentials and the free energy density are plotted in figure 1  16 figure 10: top row: plots of the chemical potential b-spline surfaces with data in gre  bottom row: free energy density b-spline surfac 1*10-4 for compariso  while this condition holds true for the dnn representation, it does not hold for the bspline representatio  it is possible, however, that other methods for knot selection or additional constraints might produce a better fit with b-spline  the b-spline representation is more computationally effcient than the dn 6 order parameter, -  the phase field code evaluates the free energy and all first and second derivatives at each quadrature poin  using the de boor algorithm for b-spline evaluationthe flop count for each function evaluation is about 170  significantly, the flop count for the b-spline evaluation does not depend on the size of the knot vecto  however, an improved evaluation algorithm could potentially reduce the flop coun  the use of accelerators could also speed up the function evaluations, without reducing the total flop coun  while the flop count for the free energy evaluation of the dnn is more than  5 times that of the b-spline, this affects only the computation time for the assembly of the residual vector and tangent matri  the wall time for the matrix-vector solution will be equivalent for the two methods, assuming both fitted functions are similar enough to give comparable condition number  for the dnn and b-spline fits presented here, the average computation time per time step over the first 5 time steps using b-splines was 2 7 s using the dn 1 times that of the computation using the b-spline to represent the free energ  for larger problems, the matrix size increases and solver time comes to dominate the average computation tim  in this limit the wall times will converg ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Bridging atomic to continuum scales via an IDNN representation of the free energy density", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Comparison with B-spline surface fit", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Conclusions", "Text": "this communication adds to our nascent, but growing body of work in machine learning and artificial intelligence targeting higher fidelity models of ma18 terials physics we have explored machine learning as an approach to bridging scales, by focusing on the representation of complexity emerging from fine scale physic  at the core of this work is the idea of an analytically integrable deep neural network to represent such function  the idnn is of particular use in the context of mathematically representing the free energy density of a material, where only the derivative data is originally known and the trained function must be integrated to recover the free energ  it is highly relevant to multidimensional systems, where multiple sets of partial derivative data must be trained against simultaneously under the constraint that all trained functions are the partial derivatives of one common functio  using as a prototypical case a binary alloy, an idnn was trained to two sets of chemical potential data, which were found using first principles calculation  symmetry with respect to the order parameter was embedded in the idn  we anticipate an expansion of these approaches to embedding fundamental aspects of the physics into machine learning models as this field develop  phase field simulations with the analytically integrated dnn representing the free energy recovered the proper physics of the system, showing the creation and subsequent coarsening of antiphase domains in the materia  this example demonstrates the ability of the idnn to capture the relevant physics of a material syste  we note that in earlier work, we have demonstrated that b-spline representations themselves are superior to the more traditional redlich-kister polynomials at resolving rapidly varying thermodynamic functions it is natural to expect that high-dimensional chemical potentials and free energy densities will present greater challenge ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "6", "Section": "6 Acknowledgements", "Text": " this work has also been 19 supported in part by national science foundation dmref grant #1729166, integrated framework for design of alloy-oxide structure  simulations in this work were performed using the extreme science and engineering discovery environment comet at the san diego supercomputer center through allocations tg-mss160003 and tg-dmr18007  xsede is supported by national science foundation grant number aci-154856  computing resources also were provided in part by the nsf via grant 1531752 mri: acquisition of conflux, a novel platform for data-driven computational physics", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190  we reformulate this in terms of stability of filters with respect to a given weight function, and then provide a combinatorial condition which is necessary and suffcient for this filter stability property to hol  examples are given to show that this new condition allows for easier and unified proofs of some results inand furthermore allows us to verify the amnm property in situations not covered by the results of that pape  secondary 43a2 ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": " banach algebras with such a stability property are said to be amnm or f-stable hence, one can define the notion of amnm for the pair directly without relying on the banach-algebraic concept indeed, the present paper has been written so that no banach algebra theory is require 1 below, and so it was clear that the earlier paper left significant room for improvemen  for example, is the weighted semilattice in example 3", "Subsections": [{"Section_Num": "1_2", "Section": "1.2 New work", "Text": "in this paper we sharpen some of the techniques introduced inand put them in a more systematic framework, to obtain a necessary and suffcient criterion for a weighted semilattice to be amn  this was already noted in but we take the opportunity to give some extra details and make some minor correction  these and other preliminary results are given in section 2, which also has a brief discussion of characters on semilattices and the corresponding filter  the discretization procedure mentioned above allows us to change perspective from character stability to filter stability our criterion is then an intrinsic characterization of the filter stability property, in terms of a combinatorial property of the pair that we call propagatio  the precise statement is given in theorem   we highlight two easy applications of theorem  1 has propagation at all level  therefore, by theorem   details are given in section  5, which also explains why the results of are insuffcient her  hence we can apply theorem  7 once again, to deduce that is amn  this was already observed inbut theorem  7 provides a unified framework to see why this result hold  given the result stated above inand given that there is a weighted semilattice which is not amnm is not amn ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1_3", "Section": "1.3 Future work", "Text": "we would like to extend the construction used to prove theorem   this runs into some serious technical obstacle  we intend to pursue this in greater depth in forthcoming wor ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Background and preliminaries", "Text": " the zero function on a given set will usually be denoted by 0, the domain being clear from contex  both the semigroup definition and the order-theoretic definition fit well with the intuition from the next exampl  we now turn to character  it turns out that b s is a sub-semilattice of 2  we will adopt this convention, which fits ; the remarks above show that the concepts of semicharacter and character coincide for semilattice  we shall reserve semicharacter for general commutative semigroup  this perspective was already exploited inand will be pursued more systematically in this pape  we therefore review some of the details for the reader's convenienc  the set of all filters in s will be denoted by fil ", "Subsections": [{"Section_Num": "2_3", "Section": "2.3 AMNM for weighted semilattices", "Text": "in this section, we set up the notion of amnm for weighted semilattices in a way that does not require definitions or results from the theory of banach algebra  we also take the opportunity to make precise some arguments from that were unclear or incomplete as stated there: see lemma   we start in a more general settin  let s be a semigrou  we shall often refer to the pair as a weighted semigrou  the following lemma is not needed for the eventual application to semilattices, but we include it for sake of completeness and for possible use in future wor  let s be a semigrou  this follows from a technique known in the banach-algebraic setting to make this paper more self-contained we provide a full proof, adapted from parts of the proof in proof the following terminology is modelled on the corresponding terminology for banach algebras, as found in or let be a weighted semigrou  we now specialize to the setting where s is a semilattic  another special feature is that we can improve on lemma   the following lemma plugs a small gap in the statement/proof of proo  let s belong to   arguing similarly to the proof of lemma   by the remarks following definition   6 the following result is essentially the same asphrased in different languag  however, the proof there was left to the reader, and as mentioned above there was a missing step glossed ove  we take this opportunity to provide a slightly weaker but more precise statement, with a complete proo  let be a weighted semilattic  proo  the equivalence of and is routin  finally, suppose hold  thus hold  3 characterizing stability of filters   the precise statement of our characterization is given as theorem  7, and we shall build up to it in stage  then is amn 7 that is amn ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Characterizing stability of filters", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Breadth and propagation", "Text": "the following is a small variation on the previous exampl  the new notion of propagation gives a very natural viewpoint on this construction, with the key details demonstrated in the following exampl  let s be a semilattic  our terminology is not entirely standard; the same property is referred to in as meet irredundan  12 the breadth of a semilattice sheds some light on its structure, and is related to more familiar order-theoretic concepts such as height and widt  diverse behaviour occurs even among semilattices of breadth   for instance, the following example shows that every infinite k-ary rooted tree is a semilattice with breadth 2 that contains infinite chains and infinite antichain  an infinite k-ary rooted tree is an infinite rooted tree in which every vertex has k childre  suppose s is a semilattice with finite breadt  with the tools of the previous section, we can give an alternative approach to this resul  as observed in the remarks before definition   now we can apply theorem   motivated by this result, it is natural to ask the following question, which was raised implicitly in questio  let s be a semilattice with infinite breadt  this is the final main result of our pape  if has infinite breadth, then there is a log-weight on s which fails 1-propagatio  13 the proof of theorem  8 takes up the rest of this sectio  then there exist b1, proo  proo  we construct both sequences together by induction on  en-1 and f1,fn-1 with the desired propertie  now let b1,xn} is incompressible keeping the notation of proposition   proo  then does not have 1-propagatio  proo  now let zn = join belong to fi  by the properties listed in proposition   thus zn belong to w  on the other hand, by lemma   the authors thank the organizers of these meetings for invitations to attend and for pleasant environments to discuss researc  the first author acknowledges financial support to attend the latter meeting, in the form of a travel grant from the faculty of science and technology at lancaster universit  15 the third author acknowledges the financial supports of a fast start marsden grant and of victoria university of wellington to attend both meeting  further work was done during the second author's visit to lancaster university in november 2014, which was supported by a scheme 2 grant from the london mathematical society she thanks the department of mathematics and statistics at lancaster for their hospitalit  the first author thanks   horv ath and   lauststen for discussions in recent years concerning amnm phenomena, and for their encouragement to write up the results presented her  the second author also acknowledges support from national science foundation grant dms-1902301 during the preparation of this articl ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190 00084v1 1 jan 2019 arc-transitive graphs of valency twice a prime admit a semiregular automorphism michael giudici and gabriel verret abstrac  we prove that every finite arc-transitive graph of valency twice a prime admits a nontrivial semiregular automorphism, that is, a non-identity automorphism whose cycles all have the same lengt  this is a special case of the polycirculant conjecture, which states that all finite vertex-transitive digraphs admit such automorphism ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1. Introduction", "Text": "all graphs in this paper are finit  this question has attracted considerable interest and a generalisation of the affrmative answer is now referred to as the polycirculant conjecture one line of investigation of this question has been according to the valency of the graph or digrap  every vertex-transitive graph of valency at most four admits such an automorphismand so does every vertex-transitive digraph of outvalency at most three every arc-transitive graph of prime valency has a nontrivial semiregular automorphism and so does every arc-transitive graph of valency 8 partial results were also obtained for arc-transitive graphs of valency a product of two primes we continue this theme by proving the following theore  arc-transitive graphs of valency twice a prime admit a nontrivial semiregular automorphis ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2. Preliminaries", "Text": " we will need the following well-known result  proo  see for example a transitive group of degree a power of a prime p contains a semiregular element of order   proo  in a transitive group of degree a power of a prime p, every sylow p-subgroup is transitiv  a non-trivial element of the center of this subgroup must be semiregula  giudici and   verret", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3. Arc-transitive graphs of prime valency", "Text": "in the most diffcult part of our proof, the arc-transitive graph of valency twice a prime will have a normal quotient with prime valenc  we will thus need a lot of information about arc-transitive graphs of prime valency, which we collect in this sectio  this fact will be used at the end of section   proo  hence g/p acts faithfully, and quasiprimitively or biquasiprimitively on   we may thus assume that p is a 2-grou  if g/p contains a semiregular element of odd prime order, then lemma  2 implies that so does   we may assume that this is not the cas  similarly, we may assume that |v| is not a power of   | is a power of   let us first recall the notion of coset graph  observe that the action of g on the set of vertices by right multiplication induces an arc-transitive group of automorphisms such that h is the stabiliser of a verte  moreover, every arc-transitive graph can be constructed in this way the graphs in and of theorem  1 have a triangl  proo  note that g acts 2-transitively on the set of right cosets of ng with the stabiliser of any two points being isomorphic to c/  repeat this procedure until no more vertex outside s have this propert  proo  by lemma   it follows that, running the process described in definition  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4. Proof of Theorem ??", "Text": " it follows from lemma   giudici and   let k be the kernel of the action of g on n-orbit  it follows from lemma   we may thus assume that we are in case of corollary   note that p is a 2-grou  let u be the other neighbour of w in vp thus the graph induced between adjacent p-orbits is a union of c4'  we say that z is the buddy of x with respect to yp moreover, each vertex has the same number of buddie  thus it remains to consider the case where v has p buddie  we first prove the followin  similarly, cx must be the buddy of c with respect to bp these are distinct, which is a contradictio  combining the claim with corollary   on the other hand, m is an irreducible -module over gf of dimension at least tw  since g/p is nonabelian simple or has a nonabelian simple group as an index two subgroup, this implies that m is a faithful irreducible -module over g  by lemma   by corollary   as above, m is a faithful irreducible -module over gf of dimension at least tw ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "References", "Section": "References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Observation", "Section": "Observation and study of the decay J/'", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190  closed ordinal ramsey numbers are a topological variant of the classical ramsey number ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1. introduction", "Text": " should such an ordinal exist, let rcl denote the least such ordina  call rcl the closed ordinal ramsey number of   for a history of partition relations and rado's arrow notation see see also and the author's", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2. preliminaries", "Text": " to denote ordinal  2010 mathematics subject classificatio  primary 03e0  secondary 03e1  key words and phrase  partition calculus, countable ordinal  the leaves are the points of cb rank   in line with the standard order on ordinals, it is preferable to visualize the root as being on top and the leaves on the botto  then, the n-th level corresponds to the points of cb rank   see figure 1 for a visualizatio  let g = be a grap  we only consider graphs where the edge relation is symmetri 2 and section 3 of in particular, see definitions   inthe filter fn was defined smaller, hence a canonical colouring there is more restrictiv  in this paper, we will not need that extra strengt  the following theorem allows us, for our purposes, to assume that every arbitrary colouring we encounter is canonica ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3. Upper bound", "Text": " proo  clai  proof of clai  assume the contrar  by ramsey's theorem and triangle-freeness, y has an infinite independent subse  fix some k", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4. Lower bound", "Text": " denote ln = t=  proo  so cb = n -  so cb cannot take any value, a contradictio  proo  we prove by induction on   since x is bounded, cb is finit  clai  proof of claim", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5. Acknowledgments", "Text": "the research was conducted at ben-gurion university of the nege  the research was partially supported by isf grant n  181/16 and 1365/1 ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "References", "Section": "References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190  weinberg abstract analysis of sliding window detection detection processes requires careful consideration of the cell under test, which is an amplitude squared measurement of the signal plus clutter in the complex domai  some authors have suggested that there is sufficient merit in the approximation of the cell under test by a distributional model similar to that assumed for the clutter distributio  under such an assumption, the development of neyman-pearson detectors is facilitate  the purpose of the current paper is to demonstrate, in a modern maritime surveillance radar context, that such an approximation is invalid", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "I", "Section": "I Introduction", "Text": "sliding window detectors assume the existence of a series of clutter measurements, from which a single measurement of the clutter level is take  this is then compared with a cell under testwhich is being tested for the presence of a target in clutte  the measurement of clutter level is then normalised in such a way that the probability of false alarm can be stabilised adaptivel  in a maritime surveillance radar context, one could be examining returns from the sea surfac  clutter measurements are returns from the sea surface in the absence of a targe  the analysis of these detection processes began with the pioneering work in ; a modern account can be found in the problem of interest, in the current paper, is analysis of the cut in the presence of a targe  some recent studies have suggested that the cut could be approximated by the same distribution used for the clutter, but with different distributional parameter  this is certainly the case in the context of in addition to this, an approximation is applied for the cut, relative to the additivity associated with rayleigh variate  the difference is that the pareto parameters are modified to account for the presence of a targe  this paper will show that this is a somewhat invalid assumption in the context of modern maritime surveillance rada ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "II", "Section": "II Problem Specification", "Text": "the cut consists of a single complex clutter measurement added to a complex target mode  for the puposes of simplicity, it is assumed that the target model is bivariate gaussia  this signal is denoted s througou  the complex clutter return is assumed to result from a compound gaussian process with inverse gamma texture, which yields the pareto type ii clutter model in the intensity realm the approach to be examined is the assumption that there are cases where the distribution of z is also pareto distributed, but with modified shape and scale parameter ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "III", "Section": "III An Approximation for the CUT", "Text": " examination of the integrand in suggests it is necessary to examine approximations for its third ter  then the term under consideration can be written t = e-g  hence g is an increasing functio  since an approximation is required for g it is worth examining the construction of a taylor series for i  in order to achieve a pareto approximation it is necessary to apply a linearlisation to   1this corrects a typesetting error inwhere a redundant 2 appears in the expressio  this result appears to suggests that there are cases where the signal plus clutter distribution can be approximated by a pareto mode  the very nature of the clutter being modelled by a pareto distribution is that it is extremely difficult to detect the presence of a low powered target in spiky x-band clutter hence, from a practical perspective, the conditions under which the approximate pareto distribution is produced for are not really achievable or usefu ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "IV", "Section": "IV Conclusion", "Text": "it was shown that the distribution of signal plus clutter could be approximated by a pareto distribution, under the condition that the target model has very small power relative to the clutter leve ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "References", "Section": "References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "multi-echelon supply chain inventory planning using simulation-optimization with data resampling anshul agarwal* abstract modeling and optimization of multi-echelon supply chain systems is challenging as it requires a holistic approach that exploits synergies and interactions between echelons while accurately accounting for variability observed by these system  we develop a simulationoptimization framework that minimizes average inventory while maintaining desired average beta service level at stocking location  we use a discrete-event simulation framework to accurately capture system interaction  instead of a parametric estimation approach, the demand and the lead time variability are quantified by bootstrap sampling the historical data, thus preserving the true nature of the variabilit  we compare three different open source simulationoptimization libraries - the derivative free methods from scip optimize, a bayesian optimization algorithm scikit-optimize, and a radial basis function based black-box optimizer rbfop  the experiments demonstrate practical applicability of our approac  the optimization results demonstrate a preference for a centralized inventory planning scheme that help with risk poolin  moreover, with no order placement cost, the optimal solution tends to order more frequently in order to lower inventor ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "agarwal@unite 00090v1 1 jan 2019 placement of inventory between echelons new shipments are first stored at a central facility, which then acts as an internal supplier to other stocking locations as the product travels through multiple stages before reaching the final custome  in this work we address the challenging problem of optimizing inventory in such complex system  inventory optimization of a multi-echelon system can be approached by applying single-echelon optimization techniques to every echelo  however, such an approach fails to achieve true network inventory optimizatio  disregarding the network view of inventory usage and applying stand-alone replenishment strategies to one echelon without considering impact on other echelons can lead to redundant safety stock or customer service failure despite adequate inventory a multi-echelon setting can lead to sophisticated complex interactions between the inventory levels of upstream and downstream stocking location  inventory unavailability at the upstream node can increase lead time for downstream facilitie  centralized upstream inventory can help downstream units to lower safety stoc  upstream nodes serving customers as well as multiple downstream facilities may need to prioritize between multiple order  large but sporadic orders from downstream units can make it challenging for the upstream facilities to follow consistent inventory management policies, and thus it can lead to higher average inventory over tim  on the other hand, upstream facilities can choose a lower service level target as long as customer service levels from the downstream nodes are being me  every echelon can manage inventory with a custom policy instead of the standard reorder point or base stock approache  modeling and optimization of such systems is therefore challenging as it requires a holistic approach that exploits such synergies and interactions between echelons several articles address inventory optimization for multi-echelon systems based on the earliest model of sherbrookeganeshan and hopp et a  developed two-echelon model  glasserman and tayur developed a multi-period model, but assumed a base-stock policy with 1 day review period and constant order placemen  simpsongraves et a inderfurth et a and you et a  developed models for a base-stock policy using a guaranteed service time approach in which each stage quotes a guaranteed service tim  lee and billington and ettl et a  developed an algebraic stochastic service time approach that 2 characterizes random lead time delay induced downstream due to stockouts at upstream unit  inventory optimization literature assumes a pre-defined probability distributioninfinite series, or simplified custom algebraic expressions to quantify demand and lead time variability observed in supply chain  such assumptions can simplify or inaccurately represent the true nature of variability observe  we argue that this may not adequately capture the complex multi-echelon interactions described above and, as a results, can underestimate optimal inventor  therefore, in this work we follow a simulation-optimization framework to address this problem as it does not suffer from such assumptions however, we do assume no time correlation in the historical dat  this can be easily relaxed by using a time-series model to forecast demand and lead tim  this allows us to capture sophisticated system interactions as well as any custom rule  any custom inventory policy can be incorporate optimizea bayesian optimization algorithm scikit-optimizeand a radial basis function based black-box optimizer rbfopt there is no published work on this compariso  the article is organized as follow  the next section describes the problem addressed in this work and the example networ  further in section 3 we describe the inventory optimization algorithm with simulation and optimization module  we also illustrate a python implementation of the algorithm in section   in section 5 we present optimization results of the example network and a comparison of the open source solver  section 6 concludes the article", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Problem Description and Assumptions", "Text": "figure 1 illustrates the multi-echelon supply chain network used in this work to demonstrate the simulation-optimization algorith  the network comprises both elements of a multi-echelon system where a stocking facility can either purely serve customer demand or purely replenishes another facility, or does a combination of bot  it gets its replenishment from the first facilit  the purpose of inventory optimization is to minimize total system cost  in this work we assume no order placement cost, and mainly focus on minimizing average onhand inventory for all stocking locations while ensuring that a minimum customer service level target is me  however, the framework presented is generic and any kinds of costs or parameters are straight-forward to incorporat  we use beta service level in this work we choose to have a service level target for only customer serving facilitie  the time gap between placing orders and receiving replenishment is characterized by an overall lead tim  any unfulfilled customer order can either be back ordered or assumed to be lost sales; we study both case  note that the facilities do not share their inventory levels or any other information with each 4 othe  each of them operate independentl  the optimization algorithm, being cognizant of the entire system, optimizes all echelons simultaneousl ", "Subsections": [{"Section_Num": "2_1", "Section": "2.1 Key assumptions", "Text": " when the inventory position falls below the reorder point, an order of amount of is placed by the facilit  we optimize both base stock and reorder point levels in this wor  if a stocking unit does not have enough on hand inventory, it waits until there is sufficient inventory to replenish the entire order volum  such a wait adds to the downstream unit's lead tim  instead of assuming a pre-defined probability distribution, we follow a data-driven distribution to quantify variabilit  we bootstrap samples from the historical data during simulation iteration  this inherently assumes that there is no time correlation in the historical demand and lead time, as well as the future will be similar to the histor ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Inventory Optimization Algorithm", "Text": "we develop a simulation optimization framework to minimize inventory while ensuring a desired fill rat  in this framework, the simulation module acts as a black-box for the optimization modul  the simulation process can accurately capture system dynamic  because of the blackbox nature, the optimization model has visibility of the system dynamics only through the inputs to and outputs from the simulation mode  the optimization model invokes simulation module multiple times for function evaluation  we ensure we obtain a feasible and a good quality solution by running the algorithm for several iteration  this indeed results in a computationally slower framewor  it can be easily tailored to the problem need  the next two sections describe the simulation and optimization algorithms in detai ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Algorithm Implementation", "Text": " in this work we utilize and compare three open source libraries: the derivative free methods from scip optimizea bayesian optimization algorithm scikit-optimizeand a radial basis function based black-box optimizer rbfopt below we provide a brief note on the methodologies used by these solver  scip optimize this is a set of optimization algorithms provided by the scipy python packag  it provides various algorithms for both constrained and unconstrained optimizatio  quasi-newton strategies sr1 and bfgs are available to approximate hessian updat  several algorithms require gradient or jacobian information to be provide  in this work we use the derivative free algorithm  scikit-optimize or skopt this is a non gradient-based optimization algorithm that minimizes expensive and noisy black-box function  it is built on numpy, scipy, and scikit-learn python package  it essentially utilizes a bayesian optimization approac  here the unknown objective is considered as a random function for which a prior distribution is defined using a gaussian proces  function evaluations are treated as data and used to update the prior to form the posterior distribution for the objective functio  this posterior is used to maximize a simple utility functio  scikit-optimize also provides capabilities to, instead of a gaussian process, use random forest and xgboost algorithms to approximate the noisy function  rbfopt this library implements the radial basis function method originally proposed by gutmann with a few extension  here they build and iteratively refine a surrogate model of the unknown objective functio  first, for an initial set of chosen sample points that satisfy the bounds and integrality constraints, a surrogate interpolation function is built using a combination of radial basis functions and polynomial  the accuracy of the surrogate model is assessed and model selection is performed automatically in the algorithm using a cross validation schem  next step 13 is to choose a trade-off between exploration and exploitation based on gutmann's idea of bumpines  exploration implies trying to improve the surrogate model in unknown parts of the domain, whereas exploitation implies trying to find the best objective function value based on the current surrogate mode  the next point in the search space, to which the algorithm moves, is determined based on this trade-of  gutmann's algorithm provides strategies to evaluate this trade-off based on least bumpy points, and solves a local search optimization problem to determine the next iterat ", "Subsections": [{"Section_Num": "4_2", "Section": "4.2 Implementation Code", "Text": "an open-source python-based implementation of our inventory optimization algorithm is available here we implement the simulation framework using the open-source simpy platform both back order and lost sales simulations are implemented separatel  a user needs to pre-specify which simulation option is used by the optimization routine  for optimization, we provide separate implementation for all three black-box optimization librarie  we compare the results obtained with the three solvers in the next sectio ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Results and Solver Comparison", "Text": "we demonstrate the inventory optimization algorithm and compare the open source optimization libraries on the example supply chain network shown in figure   as described before, all facilities follow a combination of the reorder point and base stock policie  when the inventory position falls below reorder point, an order of amount = is placed by the facilit  we optimize both base stock and reorder point level  the initial guess is feasible,  e, the facilities satisfy their respective service level target with this initial gues  the base lead time is the minimum replenishment lead time from the corresponding supplier facilit  the true lead time experienced is the base lead time + a random variable componen  this random variable component is obtained from the historical dat  during the simulationoptimization, we bootstrap from the historical data to get this variable componen  similarly, the customer demand for each facility is also bootstrapped from the historical data during simulation  the historical data assumed and used for this example is not correlated with tim  this is not impractica  for the objective function we use a penalty of 106 for not meeting the service level targe  we perform 20 simulation replications and take an average across replications for the average on-hand inventory and service level penalty for the objective functio  we assume the initial inventory to be 90% of the base stock level for all facilitie  also, each simulation is run for a 360 days time perio  both simulation and optimization are performed on a mac os with a   table 2 shows the black box algorithm used from each python package and their parameter setting optimize: here we run the nelder mead algorithm for 100 cycle  in each cycle the algorithm runs for 50 iteration  after 50 iterations another cycle starts with the current best solution as the initial guess for this new cycl  the cycle then runs for another set of 50 iteration  this sequence continues until we either exceed the maximum time limit of 1 day or complete all 100 cycle  we run the algorithm for 1000 cycles, each cycle explores a different start state with 20 iteration  we varied the random start state from 0 to 100  a kappa of 50 was chosen to further enhance the exploration capability of the algorith  we chose a random seed of 707 in order to be able to reproduce the result  note that the cpu time is based on 20 simulation replications per optimization iteratio  for more replications the cpu time will increas  also, the optimal base stock and reorder points pertain to the example data used in the case stud  we include them in the table in order to perform a 16 table 3: optimal results and solver comparison when the unmet demand is back ordered scip  it was able to achieve the best reduction in the objective function and the lowest inventory level  moreover, it was able to achieve results in fewest iterations and within the best cpu time of less than two hours among the three solver optimize with the nelder mead algorithm doesn't seem to perform wel  the final optimal rop and base stock are quite high and not very different from the initial gues optimize, it took four times more iterations and 40% more cpu tim  it is computationally most expensive com17 table 4: optimal results and solver comparison when the unmet demand is considered as lost sales scip  because the algorithm relies more on the random starting state, while it can produce better results, the exploration approach with multiple random start states is computationally more intensiv  tables 3 and 4 illustrate the best results obtained among various tests with different random start states for the case study in this wor  for a different problem, the statistics could be differen  for instance, while rbfopt appears to be a clear winner when compared to scikitoptimize for the back order case, the final optimal objective for both are quite close for the lost sales cas  therefore, it is not definitive that scikit-optimize is better or worse than rbfop  however, in all our tests, rbfopt's results were always superior compared to the other  the optimal rop values are almost comparable across all three solver  both scikit-optimize and rbfopt are able to achieve lower inventory by attaining lower base stock values when compared to scip optimiz  we observe this trend in our case studies with both scip optimize and scikit-optimiz  however, rbfopt in its results show the opposite behavio  also, while scip optimize and scikitoptimize were faster in the lost sales case compared to back order, rbfopt took longer to complete iteration  this indicates potential for getting a more improved solution with rbfopt for the lost sales case if it is run longer for more iteration  this behavior is more apparent in the lost sales cas  we infer that optimal systems demonstrate a preference for a centralized inventory planning and positioning scheme that help with risk poolin  this is the influence of assuming no order placement cost for our case study as the system now prefers to place frequent orders with no additional penalt  thus, it is important to make appropriate assumptions to model a system as it significantly impacts the optimal solution and the behavior", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "6", "Section": "6 Conclusions", "Text": "we approach the problem of optimizing inventory in multi-echelon supply chain systems by developing a simulation-optimization framewor  the simulation capability, by design, can model sophisticated system interactions as well as any custom rule  moreover, a black-box optimization algorithm wrapped around the simulation allows determining optimal system planning decisions while ensuring system dynamics are accurately capture  in order to preserve the true nature of the variability experienced by the system, we quantify demand and lead time variability by bootstrapping historical dat  in order to demonstrate practical applicability of the framework, we test it on a 3-echelon example networ  average inventory is minimized while ensuring that the desired average beta service level at stocking locations is achieve  we assume no order placement cos  because there is no single proven simulation optimization platform, we compare three open source solver packages: scip optimize, scikit-optimize, and rbfop  thus, it is important to make appropriate assumptions to model a system as it significantly impacts the optimal solution and the modeled behavio ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": " inspired by biophysical studies, we present python code for handling clusters on a 2d periodic lattic  properties of individual clusters, such as their area, can be obtained with a few function call  our code invokes an unsupervised machine learning method called hierarchical clustering, which is simultaneously effective for the present problem and simple enough for non-experts to grasp qualitativel  moreover, our code transparently merges clusters neighboring each other across periodic boundaries using breadth-first searchan algorithm well-documented in computer science pedagog  in order to extract insights from such sim*corresponding autho  e-mail address: everestl@us edu preprint submitted to computer physics communications january 15, 2019 arxiv:190 00091v2 11 jan 2019 ulations, we need robust methods for identifying clusters of simulated objects on the lattic  solution method: hierarchical clustering first identifies all potential cluster  then, breadth-first search connects together clusters that neighbor each other across periodic boundarie ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "there has long been an interplay between physics and subsets of machine learning, before the latter became widely known outside technical field  for instance, clustering methods has been utilized to speed up force calculations in n-body cosmological simulations in general, clustering methods expose natural classes within data while making as few assumptions as possibl  these methods are well-documented by pedagogical publications and can be readily adopted by physicist  this paper will apply hierarchical clustering to discrete lattice systems in biophysic  making certain assumptions such as fast diffusion within each lattice sitewe can simulate the system's time evolution using gillespie's algorithm; see for a recent revie  given the importance of membrane protein organization, it is vital to develop techniques for identifying and describing molecular clusters on a simulation lattic  hierarchical clustering is one possible choic  to our knowledge, this technique has first been utilized in membrane biophysics by shomar et alwho in turn were inspired by a nanoscopy experiment the matlab clustering code in was published as part of its supplementary material  unfortunately, that code does not model diffusion, and therefore does not correspond to any particular boundary condition  2 here we present an improved implementation which may be useful to biophysicists and practitioners of other fields alik  first and foremost, assuming a diffusive system, our code readily connects together lattice sites that touch across periodic boundarie  given the prevalence of periodic boundary conditions, we believe this is an important featur  second, our code is written in python, which unlike matlab is non-proprietary and free-to-us  this paper is structured as follow  section 2 gives an overview of the hierarchical clustering metho  section 3 describes the 2d lattices to which our code is applied, and how periodic boundaries are handle  a summary and conclusion follow in section   all code can be found online at https: //githu com/openerror/physicslatticeclusterin ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Hierarchical Clustering \u2014 Theory and Example", "Text": " overview of algorithm there are many implementations of hierarchical clusteringsome of which are parallelized starting from single observations below we will give a qualitative overview of the procedure, assuming serial computations, followed by a toy exampl  assume that we have n observations in a d dimensional spac  with a dissimilarity measure defined, the algorithm iteratively merges the two observations or clusters that are the most simila  to make concrete the dissimilarity d between clusters of observations, various schemes or linkages have been develope  belo ", "Subsections": [{"Section_Num": "2_2", "Section": "2.2 An Example", "Text": " assume that sites neighboring each other belong to the same cluster, and that only non-diagonal neighbors are considere  with the stated criteria, we expect to obtain 4 cluster  we can visualize the merger of observations and clusters through a treelike dendrogra  the leaves nodes at the bottom of the dendrogram represent the original n observation  as we move up the graph, we see the leaves merging into new nodes, which may undergo further merger  the height in the dendrogram, at which a pair of nodes combine, indicate the dissimilarity between the node  the dendrogram in figure 1b is consistent with the hierarchical clustering describe  the 3-member cluster produced is then merged with the sites at the top left and right corners successivel  in order to obtain clusters at the desired scale, we cut the dendrogram horizontall  any mergers performed above the cut are ignored, while those that occur at one level below are retaine ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2_3", "Section": "2.3 Sample Code and Required Libraries", "Text": "for the code that produced figure 1, please see the jupyter notebook submitted with this publicatio  all functionality presented so far is implemented in the function detectclusters, located within the file clusterin p  please see the docstring under the function declaration for detail  b) dendrogram tree demonstrating mergers of observations and clusters throughout the procedur  leaf nodes indicate single observations, which are merged at nodes higher in the graph; the vertical height at which nodes are located represent the dissimilarity between the pair of merged clusters/observation  all required python libraries are listed in requirement txt; the hierarchical clustering routines in particular are shipped with the scip  given an existing python installation, these libraries can be installed quickly using the console comman  pip install -r requirement txt or through whichever package manager preferre ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Handling Periodic Boundaries of the 2D Lattice", "Text": "when simulating large systems, periodic boundaries are commonly adopted to make computations more tractabl  how then can we take into account periodic boundaries in hierarchical clustering? while it would be costly to modify an existing implementation directly, we can achieve our goal by processing the simulation lattice and the clustering output togethe  the logic is as follows:   identify clusters that are touching the boundary, and determine the coordinates where the touching occur  for the clusters identified, check if they neighbor each other across the periodic boundarie  if they do, consider the touching clusters as on  the above logic is implemented in the functions extractclustercoordinates, located within clusterin py and groupin py respectivel  the former function takes the output of detectclusters to generate the final outpu  the function groupin  for identical input data, scipy's hierarchical clustering implementation would give identical cluster id ", "Subsections": [{"Section_Num": "3_1", "Section": "3.1 The Use of Breadth-First Search To Merge Clusters", "Text": "one non-trivial task that our code has to accomplish is to identify clusters that are touching transitivel  how then can we robustly identify all three of them as one single cluster? to accomplish the task above we have utilized breadth-first searcha standard technique in computer scienc  given a graph comprised of nodes and edges, starting from a given node bfs would find all other nodes reachable by traversing an edge  the problem of merging transitively-neighboring clusters can be translated into a graph-search problem: clusters become nodes that connect to their direct neighbors through edges", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Conclusions", "Text": "to sum up, this paper has solved a common problem in lattice simulations, using only a combination of standard techniques from machine learning and computer scienc  we hope that our work would inspire further imports of information-technological techniques into physic  although this paper has only worked on 2d lattices, with a few modifications the same code can be applied to a rectangular 3d lattic  however, for more complicated geometries substantial edits would be necessar  in particular, we may need a new measure for the dissimilarity between observations, in addition to alternative methods for handling boundary points or even non-periodic boundary condition  non-trivial geometries arise in attempts to accurately describe molecular diffusion on cellular membranesand present additional needs for appropriate analytical technique ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Acknowledgements", "Text": "this work was supported by nsf award number dmr-1554716 and the usc center for high-performance computin  declarations of interests: none", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190 esole@northeaster harvar edu abstract: we study the geography of crepant resolutions of e7-model  an e7-model is a weierstrass model corresponding to the output of step 9 of tate's algorithm characterizing the kodaira fiber of type iii*over the generic point of a smooth prime diviso  the dual graph of the kodaira fiber of type iii*is the affne dynkin diagram of type e e  a weierstrass model of type e7 is conjectured to have eight distinct crepant resolutions whose flop diagram is a dynkin diagram of type e  we construct explicitly four of these eight crepant resolutions forming a sub-diagram of type d  we explain how the flops between these four crepant resolutions can be understood using the flops between the crepant resolutions of two well-chosen suspended pinch points", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "1 defining the e7-model ", "Subsections": [{"Section_Num": "1_2", "Section": "1.2 E7 facts and conjectures", "Text": ".  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1_3", "Section": "1.3 Summary of results", "Text": ".  ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Preliminaries", "Text": "", "Subsections": [{"Section_Num": "2_1", "Section": "2.1 G-models and Coulomb phases", "Text": ". 2 geography of minimal models: decomposition of the relative movable cone into relative nef-cone 1 e7 root system and dynkin diagram2 root system of types e7 and e8, and representation 56 of e 3 chamber structure of the hyperplane i ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 The hyperplane arrangement I(E7, 56) ", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Minimal models Y4, Y5, Y6, and Y8", "Text": "1 overview of the sequence of blowups defining the resolutions ", "Subsections": [{"Section_Num": "4_2", "Section": "4.2 The geometry of Y4", "Text": ".  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4_3", "Section": "4.3 The geometry of Y5", "Text": ".  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4_4", "Section": "4.4 The geometry of Y6", "Text": ".  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4_5", "Section": "4.5 The geometry of Y8", "Text": ". 6 spp flops1 prepotential and coulomb phases2 counting charged hypermultiplets in 5d using triple intersection numbers2 counting hypermultiplets in 6d using anomaly cancellation conditions in m-theory and ftheory compactifications, such geometries produce e7 gauge theorieswith matter transforming in the adjoint and the fundamental representation of e7 of dimension 5  we will denote that representation as 56 in the rest of the pape  the intersection polynomial is not invariant under flops and has to be computed in each of the crepant resolution  the last few years have seen a deep improvement of our understanding of the geography of crepant resolutions of weierstrass model  crepant resolutions of a singular weierstrass model are relative minimal models over the weierstrass model and are connected to each other by a finite sequence of flop  one significant incomplete problem at the boundary between birational geometry and string geometry is the explicit construction of all the crepant resolutions of a given weierstrass model coming from tate's algorithm the geography of these crepant resolutions is the study of the network of flops connecting the  semi-simple cases and cases with non-trivial mordell-weil groups have also been investigated recently there are still some important omission  with the exception of infinite series, e5=spin, e6 and e7 are the two crucial cases left for which the details of the crepant resolutions defining each chamber are still a myster  we would like to start a detailed exploration of the minimal models of the e7-mode  in this context, minimal models are crepant resolutions over the weierstrass mode  we would like to explicitly construct each minimal model over the weierstrass model as a projective variety defined by a crepant resolution of the weierstrass model of an e7-model and study the geography of these different crepant resolution  an e7 weierstrass model is conjectured to have eight distinct crepant resolutions whose flop diagram is a dynkin diagram of type e  however, to this day, these crepant resolutions have not been identifie  we will construct four of the eight conjectured minimal models of an e7-model and show that their flops define a dynkin diagram of type d  in our construction, the birational geometry of the suspended pinch point will be used to model the flops of the minimal 1we attach a graph to the collection of minimal models of a weierstrass model such that the nodes of the graph are in bijection with the minimal models and two nodes are connected by an edge when the two corresponding minimal models are connected by a flo  such a graph is called the graph of flops of the minimal model  if the elliptic fibration is a calabi-yau threefold, the graph of flops corresponds to the incidence graph of the distinct chambers of the coulomb branch of the five-dimensional gauge theory obtained by a compactification of m-theory on the elliptic fibratio  2 models we discus  the name e7-model stems from the fact that the dual graph of a kodaira fiber of type iii*is the affne dynkin diagram of type e e  let b be a smooth variety of dimension two or higher over the complex number  the projective bundle x0 is the ambient space for a weierstrass model throughout this paper, we work over the complex numbers   the locus of points of b over which the fiber is singular is v the fibers of a smooth weierstrass model are all irreducible of type i0i1and ii reducible fibers appear only after resolving the singularities of a singular weierstrass model up to codimension-tw  such an elliptic fibration is called an e7-mode  the collision of singularities iii*+i1 is not in the list of miranda as the two fibers have distinct j-invarian 2 e7 facts and conjectures in this section, we recall some facts and conjectures about the e7-model that are relevant to appreciate the questions addressed in this pape  topological invariant  the hodge numbers and the euler characteristic of a crepant resolution are invariant under flops and can therefore be computed in any crepant resolutio  the euler characteristic over a base of arbitrary dimension of an e7-model has been computed in the hodge numbers and the euler characteristic of an e7-model in the special case of an elliptically fibered calabi-yau threefold that is obtained by a crepant resolution of a weierstrass model are independent of the choice of crepant resolution and are given in the characteristic numbers of an e7-model are computed in see also singular fibers of fibral divisors and the minuscule representation 5  thus, the matter representation associated with an e7-model is the direct sum of the adjoint and the fundamental representation of e  non-higgsable cluste  this is famously illustrated by the non-higgsable cluster corresponding to the local calabi-yau threefold defined over the quasi-projective surface given by the total space of the line bundle op  see loss of flatnes  when the base of the fibration is at least of dimension three, a crepant resolution of the e7-model does not give a flat fibration over the base as there are codimensionthree points over which the fiber contains a full quadric surface as discussed in the partial toric resolutions of the geography of crepant resolution  the authors of conjectured that there are eight crepant resolutions of the weierstrass model of an e7-model and the graph of their flops is a dynkin diagram of type e  this is based on studying the hyperplane arrangement defined by the weights of the representation 56 inside the dual fundamental weyl chamber of e  the crepant resolutions were not constructed explicitl  see also the fiber structur  this has not been verified geometrically in more than one chambe  we give four distinct crepant resolutions of the weierstrass model of the e7-mode  we show that the graph of the flops between these four crepant resolutions is a d4-dynkin subdiagram of the expected e8 flop-diagram as illustrated in figure   we show that the d4-dynkin diagram of flops can be understood by the flops of two suspended pinch points as illustrated on figure   for each resolution, we identify the new curves that appear over v they are extreme rays of the resolution and their geometric weights are always in the minuscule representation 5  all other fibers are always one dimensional over any point of the bas  we compute the triple intersection numbers of the fibral divisors and in this way give a geometric derivation of the chern-simons levels and of the superpotential of an e7-model in the four chambers studied in this pape  we compute the number of hypermultiplets in an m-theory compactification on a calabi-yau threefold that is an e7-model by comparing the triple intersection numbers with the oneloop corrected superpotential of a five-dimensional supergravity theory with lie group e7 and matter transforming in the adjoint and the fundamental representation 56 the factor of 1/2 indicates that each intersection point contributes one half-hypermultiplet, which is possible because the representation 56 is pseudo-rea  for convenience, we collect our results in the following page 1: adjacency graph of the chambers of the hyperplane arrangement   each chamber is simplicia  the colored chambers forming a subgraph of type d4 are those corresponding to the nef-cone of the crepant resolutions constructed in this pape  together, they define a sign vector for the hyperplane arrangemen  these weights satisfy a partial order whose hasse diagram is a dynkin diagram of type e7 as illustrated in figure   these fibers should be compared with the affne dynkin diagram of type e e8 in figure   the degenerated fiber is a kodaira fiber of type ii*with a node contracted to a poin  in each chamber, the node that is contracted is differen  these fibers are computed directly from explicit crepant resolutions of singularitie 2 the irreducible components of the singular fibers over codimension-two points of the base determine a finite set of weights belonging to the representation   in the language of five-dimensional supersymmetric gauge theories with eight supercharges, each distinct crepant resolution of the weierstrass model corresponds to a different chamber of the coulomb branch of the theor  string dualities suggest that the graph of flops between distinct minimal models is isomorphic to the adjacency graph of the chambers of the hyperplane arrangement i whose hyperplanes are the kernels of the weights of the representation r restricted to the dual fundamental weyl chamber of   this picture requires that the elliptic fibration is a calabi-yau threefold, but we expect that the hyperplane arrangement is valid in a larger setup than the calabi-yau threefold case and even relevant to study varieties more general than elliptic fibration  for example, it can be extended to the case of q-factorial terminal singularities that are partial resolutions of threefolds with cdv singularitie 2 geography of minimal models: decomposition of the relative movable cone into relative nef-cone  the description of the coulomb branch of a five-dimensional gauge theory with gauge algebra g and a representation r in terms of a hyperplane arrangement i is strikingly similar to the point of view of birational geometers who rely on several cones to describe the birational geometry of projective varieties in the same birational clas  in particular, we refer to section 1  the dual graph of a kodaira fiber is always an affne dynkin diagram of type ad  10 its decomposition theorem into relative nef-cone  then f contracts all fibral divisors not touching the zero section of the fibratio  in the case of an elliptic surface, that is enough to see that the weierstrass model is the canonical model of y each divisor di is a fibration over s and we denote its generic fiber by c  the curves ci generate an open convex cone in n  this cone corresponds to the dual weyl chamber of   geometrically, we expect d to be the cone of relative movable divisors mo  a pseudo-isomorphism is a birational map that is an isomorphism in codimension-on  a small q-factorial modification is a pseudo-isomorphism between q-factorial projective varietie  any nef-divisor is movable and the partition theorem implies that any movable divisor becomes nef after a finite number of flop  for any two q-factorial varieties y1 and y2 related by a birational map that is an isomorphism in codimension-one, the cones n1 and n1 can be canonically identified by pushforward and pullbac  the closure of the ample cone is the nef-con  since the pullback of an ample divisor is movable, the identification of n1 and n1 also embeds the ample cone of x2 onto a subcone of the movable cone of x  the hyperplane arrangement i describes the decomposition of the closed relative movable cone into relative nef-cones corresponding to each individual distinct crepant resolutio  we then study the chamber structure of the hyperplane arrangement   the coxeter number of the lie algebra of type e7 is 1  the determinant of the cartan matrix of the lie algebra of type e7 is   hence, the fundamental group of the root system of type e7 and the quotient of the weight lattice modulo the root lattice are both isomorphic to z/2  in bourbaki's tables, our simple roots are denotedrespectivel  removing the black node gives the dynkin diagram of type e  the numbers inside the nodes are the multiplicities of the kodaira fiber of type iii* and the dynkin labels of the highest roo  thus, the coxeter number of e7 is 1  removing the black node gives the dynkin diagram of type e  the number in the nodes are the multiplicities of the kodaira fiber of type ii*. 2 root system of types e7 and e8, and representation 56 of e  we now give a quick description of the root system of type e7 and the weight system 56 of e7 in terms of the root system of type e  we follow borcherds's lecture notes on lie groups the smallest non-trivial representation of e7 is of dimension 56 and often called the fundamental representatio  the representation 56 of e7 is minuscule, self-dual, and pseudo-rea  the roots of e8 form the unique eight dimensional even unimodular lattic  the roots of e8 are the vertices of the gosset polytope the roots of e7 are the vertices of the polytope 23  the weights of 56 are the vertices of a delaunay polytope 321 also called the hesse polytope by conway and sloan  the weyl group w preserves the scalar product between root  all the roots of e8 can be organized by their scalar products with respect to the chosen root s of e  each of 56+/- form a weight system of the irreducible representation 56 of e  using the scalar product of e8, the weight system 56+ is composed of the closest roots to   the root system e7 defined by = 0 and the two representations 56 are on parallel hyperplane  consider the root s = of e  in the basis of fundamental weights, the weights of the representation 56 are listed with the corresponding hasse diagram is given in figure  3: hasse diagram for the weights of the representation 56 of e  the white nodes are those corresponding to the weights used to define the sign vector that characterizes the chambers of the hyperplane arrangement   in a given chamber of i, each white node takes a specific sign ).  the red nodes correspond to weights in the positive conical hull of positive root 3 chamber structure of the hyperplane i the following theorem was given in without a formal proo  we give a proof here using the language of sign vectors in the spirit of the hyperplane arrangement i has eight chamber  each chamber is simplicia  the adjacency graph of the chambers is isomorphic to the dynkin diagram of type e  proo  since the minuscule representation of e7 is self-dual, it is enough to consider only half of its weights to study the hyperplane arrangement   the weights that do not intersect the interior of the dual fundamental weyl chamber are such that all their coeffcients have the same sign when expressed in the basis of simple positive root  in 3each weight of the representation 56 has norm square 3/2 and has scalar product +/-1/2 with any other weight of 5  thus, writing the sign vector as in figure   the negative sign flows as the arrows of figure   there are exactly eight possibilities satisfying these two rule  they are listed in table 1 and we check explicitly by solving inequalities that they all occu  their adjacency graph is a dynkin diagram of type e8 illustrated in figure  1 where we label the chambers by the corresponding simple roots of e  the partial order of weights provide the hasse diagram presented above and corresponding to a decorated dynkin diagram of type e  the linear forms defined by these weights give the sign vector for the hyperplane arrangement   we also study the flops between these distinct crepant resolution  we can tell them apart by identifying the extreme rays of de crepant resolution over the weierstrass mode  to each extreme ray c, we associate a unique weight of the representation 56 of e  the identification is given by computing minus the intersection of the curve c with the fibral divisors di not touching the section of the elliptic fibratio  we give two distinct resolutions for y4 and y5 to ease the description of the flop  we will give two trees of blowups defining respectively crepant resolutions of the triples and the flops within each of these triples will be modeled by the flops of a suspended pinch poin  the graph of flops of a suspended pinch point is a dynkin diagram of type a3 and gluing two such dynkin diagram along two consecutive nodes gives a d4-dynkin diagra  for us, the two consecutive nodes will correspond to y4 and y5 and this explains why we give two distinct resolutions for these two minimal model  conventions for blowup  each crepant resolution is an embedded resolution defined by a sequence of blowups with smooth center  we abuse notations and call the proper transforms of the variables involved in the blowup by the same nam  in particular, after the blowup, cannot vanish simultaneously as they are projective coordinates of a pn-  we work with embedded resolution  conventions for cartan divisors and fiber 7 and d0 is the divisor touching the zero section of the elliptic fibratio  by definition, each dm is a fibration of a rational curve over the divisor s in the bas  we denote the generic curve of dm by c  we recall that the weight of a vertical curve c is by definition the vector of minus its intersection number with the fibral divisors: they are not independent, as the linear combination with coeffcients gives zer  4these coeffcients are the multiplicities of the node of the fiber iii*and also correspond to the dynkin labels of the highest root of e7 with one given for the extra nod 2 the geometry of y4 we study the minimal model y4 using the resolution discussed in equation y4 is then the proper transform of the weierstrass model of equation after the blowups leading to x+ 7 in equation the relative projective coordinates are: the divisor for the special fiber is se1e2e2 3e4e2 5e2 6e4   all the fibral divisors are p1-bundles with the exception of d3 and d  one might think that the curve c7 degenerates at a =   they are extreme 20 ray  we also note that the fiber c6 over v jumps in dimension and becomes a quadric surfac  thus the resolution of the e7-fiber does not give a flat elliptic fibration when the base is a threefold and a and b can vanish simultaneously on   no other fiber component jumps in dimensio 3 the geometry of y5 we study the minimal model y5 using the resolution discussed in equation y5 is the proper transform of the weierstrass model of equation after the blowups leading to x7 in equation the relative projective coordinates due to the successive blowups are the naming is chosen to reflect the intersection of the original curves rather than our choice of blowupcij is the intersection of di and dj).  the curve c47 intersects transversally the curve c3 and the curve c46 intersects transversally the curve c  after removing the first componentthese vectors become weights in the representation 56 of e  the weyl orbit of each of these weights produces the full representation 56 of e7 since it is a minuscule representatio  as in remark  4 the geometry of y6 the minimal model y6 is discussed using the resolution in the projective coordinates are these are weights of the representation 56 of e 5 the geometry of y8 we study the minimal model y8 using the resolution discussed in equation y5 is the proper transform of the weierstrass model of equation after the blowups leading to x7 in equation where the relative projective coordinates are6 spp flops binomial hypersurfaces are simple algebraic varietie  they can be instrumental in our understanding of flops between minimal models over a weierstrass mode  the flop diagram of this binomial variety matches those of the spin-model and also defines the hexagon of flops of the su model the whitney umbrella is not a normal surface as it has singularities in codimension-on  the whitney umbrella plays an important role in the geometry of weak coupling limits the suspended pinch point appears in other areas of string geometry the suspended pinch point is a threefold defined as the double cover of c3 branched along a whitney umbrell  its defining equation in c4 is u2 = x2 -y2  the variety is normal and has three distinct crepant resolutions whose graph of flops is a dynkin diagram of type a  the three crepant resolutions of the suspended pinch points are presented algebraically in figure   we also give a toric description of all the crepant partial resolutions in figure   the algebraic crepant resolutions given in figure  5 should be compared to the right tails of the trees of resolutions presented in equations and this cone has a unique crepant resolution obtained, for example, by blowing up such a quadric cone has two crepant resolutions connected by an atiyah flop and obtained by blowing up andrespectivel  consider the blowup centered at the diagram of flops of the crepant resolutions of this orbifold is also a d4-dynkin diagram as seen in figure   the suspended pinch point is at the bottom ro  the three crepant resolutions are on the top ro  the four partial resolutions are in the middle ro  the external varieties of the middle row have the singularities of a cylindrical quadric cone k/ while the others have singularities of a quadric threefold with a double point k/. 7: flops between the four crepant resolutions of the singularity k/.  this binomial variety is the double cover of a3 branched along the three coordinate axe  this singular variety is a z2 * z2 orbifold of c 8: some of the singular binomial varieties encountered in this pape  consider the proper transform y of the weierstrass equation after the four blowups defining x4 in equation such a singularity admits three crepant resolutions described by the tail of the tree of blowups in and mimic the blowups in figure   the same story hold for the flops between the minimal models y8, y5, and y4 by considering the proper transform of the weierstrass model after the first five blowups defined in the tree of equation thus, we again have an equation of the form e4y2 = e3  we recognize the form of the binomial variety of a suspended pinch poin  the last three blowups in each branch of the tree presented in equation mimic the resolutions of the suspended pinch points as given in section   in five-dimensional spacetime, a massless tensor multiplet is dual to a massless vecto  in what follows, we assume that all tensors are massless and are dualized to vector  the kinetic terms of all the vector multiplets and the graviphoton as well as the chern-simons terms are determined by a real function of the scalar fields called the prepotential  9: the triples and both form an a3 dynkin diagram where the nodes are the varieties and two nodes are connected if they are related by a flop as shown on the left and middle picture  for each of the triples, the flops mirror those of a suspended pinch poin  to a product of abelian factor  this implies that the charge of a hypermultiplet is simply a weight of the representation under which it transforms in the presence of hypermultiplets charged under a representation r of the gauge group, the coulomb phase of the theory is characterized by a one-loop quantum correction to the superpotential derived by integrating out massive hypermultiplet  the full quantum superpotential f is protected from further corrections by supersymmetry and is a piecewise cubic polynomial of the scalar field  the metric of the scalar fields of the vector multiplets is the matrix of second derivatives of f and it is differentiable in open regions that define distinct coulomb branches separated by walls on which some of the massive hypermultiplets become massless and should be added to the low energy description of the theor  the intrilligator-morrison-seiberg prepotential is the quantum contribution to the prepotential of a five-dimensional gauge theory after integrating out all massive field  we denote by ri the representations under which the hypermultiplets transfor  the weights are in the dual space of the cartan subalgebr  this choice makes it possible to remove the absolute values in the sum over the root  if none of these hyperplanes intersect the interior of the dual weyl chamber of g, we can safely remove the absolute values in the sum over the weight  30 otherwise, we have hyperplanes partitioning the dual fundamental weyl chamber into subchamber  two such subchambers are adjacent when they differ by the sign of a unique linear for  within each of these subchambers, the prepotential is a cubic polynomia  but as we go from one subchamber to an adjacent one, we have to go through one of the walls defined by the weight  the transition from one chamber to an adjacent chamber is a phase transitio 1 prepotential and coulomb phases it is immediate to compute the prepotential for a gauge theory with gauge group e7 coupled to na hypermultiplets tranforming in the adjoint representation and nf hypermultiplets transforming in the fundamental representation 5  these are classified in table   imposing these signs together with the condition of being inside the open dual weyl chamber defines each chambe  we illustrate the process for chamber   the signs defining chamber 5 are2 counting charged hypermultiplets in 5d using triple intersection numbers we can compute the number of charged hypermultiplets by comparing the triple intersection numbers with 6fim  we illustrate the process in chamber   we can apply the same technique with the same result in the other chambers as the number of multiplets does not change between different phases of the coulomb branc  in the best cases, anomaly conditions can even completely fix the matter content of a given six-dimensional theor  this was already brilliantly illustrated by seiberg just after the first string revolution in a paper in which the absence of anomalies was used to derive the number of matter multiplets in a six-dimensional superconformal field theory such theories have chiral spinors and chiral tensors and have potential gravitational, gauged, and mixed local anomalie  these local anomalies can be cancelled by the green-schwarz mechanis  f-theory compactified on a calabi-yau threefold gives a supersymmetric six-dimensional gauged supergravity for which the green-schwarz mechanism was studied by sadovsee also in what follows, r represents the riemann curvature of spacetime and tr rn are computed with respect to the fundamental representation of s  the gauge group is g and charged hypermultiplets transform in representations ri of the gauge grou  we can distinguish two types of hypermultiplets: those that are neutral and those that transform under some representation ri of the gauge grou  in our conventions, hypermultiplets that are charged under a zero weight of the representation are considered neutra  we take it to be the fundamental representatio  these group theoretical coeffcients are listed in the anomalies are canceled by the green-schwarz mechanism when i8 factorizes the first equation gives us the number of tensor multiplets nt the second equation is the vanishing of the coeffcient of tr r4 and will be checked at the en  the third equation is automatically satisfied since e7 has no independent quartic casimir see hypermultiplet  the number is a half-integer when s has odd self-intersectio  if s is a rational curve, we see that requesting n56 to be non-negative forces the self-intersection number s2 to be greater or equal to -  a rational curve with self-intersection -n is called a -n-curv  is supported in part by the national science foundation grant dms-1701635 elliptic fibrations and string theor  is supported by the national science foundation through a graduate research fellowship under grant dge-1144152 and by the hertz foundation through a harold and ruth newman fellowshi  a relevant definitions definition   definition   under mild assumptions, the discriminant locus is a divisor in the bas  the prime components of the pre-image of prime components of the discriminant locus are called fibral divisor  definition   a prime weil divisor d is said to be q-cartier when some integral multiple of d is a cartier diviso  a variety y is said to be q-gorenstein if its canonical class is q-cartie  definition   definition  5 resolutions of singularities).  in this paper we only consider strong resolutions and simply call them resolution  remark   a crepant resolution of y is automatically a nonsingular minimal model over y distinct crepant resolutions are connected by a chain of flops if we assume the existence and the termination of flop  kawamata has shown that the seminal results of birkar-cascini-haconmckernan together with the boundedness of length of extremal rays imply that different minimal models are connected by a sequence of flop  a variety can be q-factorial in the projective category but fail to be q-factorial in the analytic categor  an important consequence of the following lemma is that terminal q-factorial singularities are obstructions to the existence of a crepant resolutio  lemma  ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5  Application to N=1 five-dimensional theories", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "6", "Section": "6 Application to N=(1,0) six-dimensional theories", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "A", "Section": "A Relevant Definitions", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "The", "Section": "The dynamics of bimeron skyrmions in easy-plane magnets induced by a spin supercurrent", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190  we propose a dirac operator on the noncommutative torus, which is consistent with the ikkt model, based on noncommutative geometr  next, we consider zero-mode equations of the dirac operator with magnetic fluxe  we find that zero-mode solutions have the chirality and the generation structures similar to the commutative cas  moreover, we compute yukawa couplings of chiral matter field wased jp contents", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 IKKT model", "Text": "", "Subsections": [{"Section_Num": "2_2", "Section": "2.2 NC torus in the IKKT model", "Text": ". 1 twisted bundle on the torus ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Toroidal compactifications of 10D SYM theory", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Dirac operator on the NC torus and the IKKT model", "Text": "1 differential operators on the nc torus ", "Subsections": [{"Section_Num": "4_2", "Section": "4.2 Dirac operator on the NC torus based on the IKKT model", "Text": ".  ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Zero-mode analysis on the magnetized NC torus", "Text": "", "Subsections": [{"Section_Num": "5_1", "Section": "5.1 Twisted bundle on the NC torus", "Text": ". 2 zero-modes of the dirac operator / dphys ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5_3", "Section": "5.3 Eigenvalues of the Laplacian", "Text": ".  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5_4", "Section": "5.4 Normalizations and Yukawa couplings", "Text": ".  ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "6", "Section": "6 Conclusions and discussions", "Text": "25 a effective action of the ikkt model superstring theory is only defined perturbatively and has infinite degenerate vacu  therefore, it is said that superstring theory has no predictions for our world, and we need a more fundamental theor  matrix models are proposed as a non-perturbative formulation of superstring theor  in this paper, we focus on ishibashi-kawai-kitazawa-tsuchiya model this model is derived from the matrix regularization of the green-schwarz action in the schild gauge or large-n reduced model of ten-dimensional super yang-mills theor  in the ikkt model, matter fields and degree of freedom of spacetime are considered to be embedded in matrice  several attempts have been made to show i  in re the authors considered an intersecting flat d-branes system based on the ikkt mode  to analyze the chirality and the generation structures of the system, they used the analogy of the harmonic oscillator in quantum mechanic  they showed the existence of a chiral zero-mode which are coming from a string connecting two different d-brane g, two d-branes are orthogona  therefore, they could not realize multiple chiral zero-mode  in addition, in more general configurations, we cannot easily find the chiral zero-modes by using the same method because the complicated contribution comes from the mixing term between two d-brane  in re the authors considered intersecting fuzzy spheres as compact d-branes and realized two chiral zero-mode  to admit a fuzzy sphere as a classical solution, we must introduce a new ter  however, the origin of such a term is not clea  in either case, it is diffcult to obtain the number of chiral zero-modes that we expec  in re the authors challenged the realization of three generations by using numerical analysi  by considering the squashed fuzzy sphere in addition to the fuzzy spheres, they succeeded in realizing three generations numericall  on the other hand, the authors of re considered a fuzzy torus with a magnetic flux based on the finite-matrix formulation of gauge theories  they computed the index of the overlap dirac operator on the fuzzy torus by the monte carlo simulations and showed the numerical results that are consistent with the index 1in this paper, we use noncommutative to describe infinite dimensional representations or operator algebras and fuzzy to describe finite dimensional representations or approximation  in this formulation, the exact relationship with the ikkt model is not clear because we must introduce the special type of the dirac operator to obtain the non-trivial index of the dirac operato  moreover, it is still diffcult to compute physically important quantities like yukawa coupling  the purposes of this paper are to define a dirac operator that is consistent with the ikkt model, to analyze the chirality and the generation structures and to compute yukawa couplings of chiral matter field  to analyze concretely, we consider the noncommutative torus as a classical solution of the ikkt mode  it is known that the nc torus as an irrational rotation ring can be realized in the ikkt model in addition, we consider the analogy of the toroidal compactifications of 10d sym theor  in re the authors considered the toroidal compactifications of 10d sym theory with magnetic fluxe  a key concept is the twisted bundl  the twisted bundle can be interpreted as a compatibility condition between the periodic boundary conditions on the torus and the gauge transformation  this compatibility condition implies that the magnetic fluxes are quantized, then the zero-mode solution of the dirac equation can have the chirality and the degeneracie  the authors of re  identified these results as the chirality and the generation structures in the four-dimensional effective theory and computed the yukawa couplings by computing the overlap integral over the toru  from the viewpoint of phenomenology in superstring theory, this result is very importan  therefore, we consider the twisted bundle on the nc toru  in the following, we propose a dirac operator on the nc torus that is consistent with the ikkt model, and we solve the zero-mode equation of this dirac operator with magnetic fluxe  then, we compute the normalization factors of the zero-mode solutions and yukawa couplings of chiral matter field  the organization of this paper is as follow  in section 2, we briefly review the ikkt model and the realization of the nc torus in the ikkt mode  in section 3, we review the basic results of the toroidal compactifications of 10d sym theor  in section 4, we introduce differential operators on the nc torus based on noncommutative geometr  in addition, we propose a dirac operator that is consistent with the ikkt mode  in section 5, we solve the zero-mode equation by using the analogy of the fourier transformatio  in addition, we compute the normalization factors of the zero-mode solutions and the yukawa 3 couplings of chiral matter field  to compute, we define the trace that is consistent with the gauge transformations and the torus translation  section 6 contains conclusions and discussio  indices are contracted by the minkowski metri  on the other hand, we will see that this action admits an infinite dimensional representation like linear operators in the next subsectio  eq  if we identify xm as a 10d spacetime coordinate, e  in this sense, we often say that the degree of freedom of spacetime is embedded in matrice g, a mechanism of compactification  however, we can show that attractive forces act between the eigenvalues in the one-loop effective potential around this vacuu  therefore, they do not spread, and the correspondence with the original theory does not hol  in this case, we need more conditions on gauge groups we omit the identity matri  when each matrix has a finite size, e  is a contradictio  we should interpret e  as it is satisfied at large matrix siz  this is the correspondence between a function algebra and a matrix algebra in the matrix regularizatio  we can also interpret e  as a d-branes configuration as follow  it is an advantage of this model that a many-body system of d-branes can be realize  when we consider two sets of e  in general, e  is not a d-branes configuratio 2 nc torus in the ikkt model in this subsection, we briefly review the nc torus in the ikkt model based on ref  if the trace in the action has the cyclic property, this unitary transformation becomes a symmetry of the ikkt mode  the infinitesimal form of e  corresponds to e  the authors of re  showed that the nc torus can be realized by this unitary transformatio  we restrict the action to the subspace of belonging to the same gauge class before and after translations in the directions x4 and x  we can see that any finite dimensional representations do not satisfy the conditions from e  e  is the algebra of the nc torus in mathematic  we use hat to indicate that it is an operator we will define the trace which is consistent with the gauge transformations and the conditions and are satisfie  3 toroidal compactifications of 10d sym theory in this section, we review the basic results of toroidal compactifications of 10d sym theory based on ref  we use the same gauge choice in section   we interpret the x5-direction as wel  obviously, we should confirm only about x4-directio  actually, we can realize e  the above concept is called the twisted bundl  e  namely, the magnetic flux on the torus is quantize  f s is periodic with respect to the x5-directio  from the first condition of e  in other words, the sign of the magnetic flux determines the chirality of the zero-mode solutio  in addition, the summation with respect to q means that there are n-independent zero-mode solution  in re these results are interpreted as the chirality and the generation structures in the four-dimensional effective theor  therefore, each degenerated solution of e  each cs q is determined by the normalization condition,   from e  we can easily generalize the above discussion to non-abelian, bi-fundamental or higher dimensional toru  4 dirac operator on the nc torus and the ikkt model in this section, we introduce differential operators on the nc torus and construct them from the ikkt mode  therefore, derivations are completely defined by how they act on the two generator  we can confirm that the basic derivations commute each othe  first, we focus on noncommutative sym theory based on the ikkt model we refer to appendix a for the detail  ), and we omit the subscription b  13 we can confirm that the basic derivations satisfy e  in addition, e  and the jacobi identity assures the commutativity of the basic derivations as mentioned above, we focus on the first two extra dimensions that are the nc torus we can verify that eq  and are equivalen  therefore, the operator is a suitable dirac operator on the nc torus based on the ikkt mode  the partial derivatives are defined by e and the gauge field is introduced by e  this comes from e  4the subscription phys means that the dirac operator has the mass dimension   this mass dimension is the same with the usual dirac operator in qf  however, the original action does not have the field strength which is defined by the covariant derivatives therefore, if we want to consider fermions in a fundamental representation, we should realize as a part of the action 5 zero-mode analysis on the magnetized nc torus   the background gauge field is varied by the torus translations  we can realize e  as the gauge transformations,   this part does not affect e  therefore, in the following, we assume this part is the identity matrix and omi  next, we must consider the consistency condition corresponding to e  e  in the following, we show the zero-mode solutions in fundamental representation bifundamental representatio  is written by the operators, this equation has the same form with e  therefore, we expect that the zero-mode equation can be constructed from that of e  here, we consider the fourier transformation  the fourier transformation of the whole zero-mode solution of e  we expect that the whole solution of e  however, e  does not satisfy e  because the quantization conditions for the magnetic flux are differen  then, we can obtain the zero-mode solutions5,   we can confirm that the above expansions satisfy e  the normalization constant c will be computed in subsection   however, this form is diffcult to use when we compute the normalization constant and yukawa coupling  can be interpreted as the operator form of e  up to the normalization constan  from this observation, we can obtain the solutions of e  we can confirm that e  satisfies e  and the periodic boundary conditions if s12f12 >   the eigenvalue problem of the laplacian relates to that of the square of the dirac operato  therefore, we can see that e  is not only the zero-modes of the dirac operator but also the lightest mode of the laplacia  in addition, we can construct the eigenmodes of the laplacian corresponding componen  by considering the analogy of the harmonic oscillator,   however, the bosonic part of the effective action is the fourth order for the nc parameter if we ignore the order included in the definition of the partial derivative  since f2 = 0 corresponds to the case we should note the definition of the trace on the infinite dimensional spac  in addition, we should confirm that the trace is well-defined with respect to the compactification conditions therefore, the gauge symmetry of the action is still hel  similarly, we can verify that all traces defined by the appropriate completeness relation are equivalen  we should consider a sign assignment of the magnetic fluxe  the lightest mode bosons have the same matrix structure with the chiral zero-mode fermion  in the actionthe yukawa couplings are described as the product of three matrices,   if operators commute each other, these operators can be regarded as c-number  this allows us to use convenient formula  from e  the yukawa couplings differ from the commutative case by the overall factor only if we fix the generation numbers n23, n21 and n1  6 conclusions and discussions in this paper, we performed the analysis of the chirality and the generation structures on the magnetized nc torus based on the ikkt mode  in subsection  2, we proposed the suitable dirac operator on the nc torus by considering noncommutative geometr  in section 5, we analyzed the zero-mode solutions of the dirac operator we propose  we showed that zero-mode solutions have the chirality and the generations structure  in addition, we computed the yukawa couplings of chiral matter field  compared with the commutative case, the difference of the yukawa couplings is the overall factor onl  advantages of our method are we can consider geometric conditions such as periodic boundary conditions we can write down the analytic form of zero-mode solutions which can easily be compared with the commutative cas  this is important to observe nc effects from the ikkt mode  when we consider the microscopic world, we compute the physical quantities through functions on spacetime like wavefunction  on the other hand, in noncommutative geometry, we consider a function algebra on a certain space which has a nc produc  for example, the star-product is a nc product in the context of deformation quantizatio  in the relationship between analytical mechanics and quantum mechanics, this corresponds to the replacement of the coordinate of the phase space by operator  therefore, in the sense of noncommutative geometry, we can admit that the chirality and the generation structures of our zero-mode solutions have the physical meanings even though zero-mode solutions are written by the operator  from a phenomenological point of view, our results may not be new because the 7the normalization constants are slightly different between the commutative case and our cas  this is because the integral of 1 is normalized to obtain the area of the torus in the commutative cas  this normalization is natural from the viewpoint of the nc torus without magnetic fluxe  25 difference of the yukawa couplings is the overall factor onl  however, the important point of this paper is that the ikkt model can describe the string-motivated model including the nc effec  the ikkt model is considered as a non-perturbative formulation of superstring theor  therefore, we expect that the ikkt model should describe the results of string-motivated models known so fa  from this point of view, our results are importan  the toroidal orbifolds are typical models in string phenomenolog  in magnetized toroidal orbifolds, the generation structure differs from the toroidal compactifications without orbifold projection  in ref the nc toroidal orbifolds are considere  on the other hand, in general, the yukawa couplings in the toroidal compactifications are functions of the complex structure moduli and the wilson line  therefore, the values of the complex structure moduli and the wilson lines are important to compare with the observed values we can consider the complexification when we introduce the basic derivation  in our results, the magnetic flux played an important rol  however, we introduced the magnetic flux by han  we expect that the magnetic flux is also generated from the dynamics of the ikkt mode  recently, in ref the authors showed that the magnetic flux may come from the tachyon condensations induced from the dynamics of d-branes and non-bps d-brane  we expect that our results can be described by the full dynamics of the ikkt mode  our method, especially the ansatz of the zero-mode solutions which are similar to the fourier transformation, depends on the gauge selectio  therefore, we should confirm the gauge invariance of our result  is a quadratic equation with respect to the t  this situation is the same with a gauge theory on a nc space with the star-product formulatio  this is an open questio  if we cannot find, different gauge backgrounds may correspond to physically different theorie  in addition, the yukawa couplings are the same with the commutative case in this limi  however, from the discussion in re g, the effective metri  this is also an open question for u  acknowledgments the author would like to thank hiroyuki abe for helpful comment  27 appendix a effective action of the ikkt model we derive the effective action of the ikkt model by considering e  we refer to for the basic technique  in this appendix, we consider the whole spacetime agai  first, we consider the bosonic part of the action  for concreteness, we consider the u gauge grou  accordingly, the trace of the action is defined on the operator space and the gauge grou  next, we introduce the abelian magnetic flux on the extra-dimensional space  by substituting the backgrounde  sb,other contains irrelevant terms for our main discussion  in e we can see the laplacian on the extra-dimensional space, i", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": " our local steering procedure is based on a new notion of a convex probabilistically safe corridor that is constructed around a configuration using tangent hyperplanes of confidence ellipsoids of gaussian mixture models learned from prior collision histor  accordingly, we propose to expand a random motion planning graph towards a sample goal using its projection onto probabilistically safe corridors, which efficiently exploits the local geometry of configuration spaces for selecting proper steering direction and adapting steering stepsiz  we observe that the proposed local steering procedure generates effective steering motion around difficult regions of configuration spaces, such as narrow passages, while minimizing collision likelihoo  we evaluate the proposed steering method with randomized motion planners in a number of planning scenarios, both in simulation and on a physical 7dof robot arm, demonstrating the effectiveness of our safety guided local planner over the standard straight-line planne ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "I", "Section": "I Introduction", "Text": " e-mail: jinwookh@sea upen  e-mail: omu arslan@tuebinge mp  lee is at cornell tech, new york, ny 1004  e-mail: ddl46@cornel edu fi  1: probabilistically safe corridor in 3d space constructed around a sample configuration by using tangent hyperplanes of confidence ellipsoids of a learned gaussian mixture model of configuration space obstacle  local steering via probabilistically safe corridor in 2d space: an rrt is extended along the safe direction towards the projection of a sample goal onto the associated probabilistically safe corridorinstead of the standard straight-line extension towards the sample goa  however, its computation also requires a distance-to-collision measur  since the exact computation of distance-to-collision in complex high-dimensional configuration spaces is hardgaussian mixture learning and locally weighted regression are applied to construct approximate probabilistic models of collision and collision-free subspaces of configuration spaces for fast collision checking and biased sampling over free space and difficult regions of configuration space  in particular, simultaneous modeling of collision and free subspaces is shown to be critical for local planning around narrow passages in this paper, by combining the strengths of andwe introduce a new notion of probabilistically safe corridors for probabilistically safe guided local steering for sampling-based planning without requiring an explicit computation of distance-to-collisio  more precisely, we construct a probabilistically safe corridor around a configuration using tangent hyperplanes of confidence regions of learned gaussian mixtures that separate the input configuration from the confidence ellipsoids, as illustrated in fi  accordingly, we propose a probabilistically safe local steering primitive towards a sample goal configuration via its projection onto the probabilistically safe corridor, as shown in fi  since the proposed steering method exploits the local geometry of configuration spaces via learned gaussian mixture models and generates steering motion within probabilistically safe corridors, in our numerical simulation and experiments, we observe that it yields a better exploration of configuration spaces while minimizing collision likelihoo  using numerical simulations and real experiments, we demonstrate that the proposed probabilistically safe local steering approach can dramatically improve the performance of randomized motion planners around narrow passages and significantly outperforms the straight-line local planner in high dimensional configuration spaces by decreasing the number of collision ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "II", "Section": "II Related Work", "Text": "sampling-based planning approaches suffer from heavy computational time in complex environments since they typically require a considerable number of sample configurations and their collision check  exact safety certificates are also utilized for minimizing the computational cost of collision checks however, these methods are still not able to address the narrow passage problem of sampling-based motion plannin  in order to resolve the narrow passage problem, zhang and manocha present a steering approach that retracts sample configurations to become more likely to be connected to nearby nodes however, it requires a significant number of iterations to find a new collision-free configuration that is around the collision boundary, and also requires an appropriate distance-to-collision measur  in practice, since the exact distance-to-collision measurement in high dimensional configuration spaces is very hard, its applicability is also limited to low dimensional motion planning problem  local safe corridors - recently find significant applications in collision-free motion planning by using sequential composition of simple local planners such safe corridors are usually constructed based on a convex decomposition of the environment, which requires an explicit representation of the environmen  this construction is further extended to integrate local system dynamics and local workspace geometry in kinodynamic motion planning however, the original construction of sensory steering requires an explicit representation of configuration space obstacles or an explicit distance-to-collision metric, and so its direct application to high dimensional motion planning is limite  ii  safety-guided rrt via probabilistically safe corridors in this section, we first present a brief overview of how learning of gaussian mixtures1 can be used for approximate probabilistic modeling of configuration spaces, and then introduce a new notion of a probabilistically safe corridor around a configuration that identifies a safe neighborhood of the configuration with minimal collision ris    2safety guided steering via probabilistically safe corridors can be integrated with any motion planning algorithm as a local steering primitive, especially for uncertaintyaware belief-space planning, which we plan to explore in a future pape  fi  2: examples of learned gaussian mixture model  gaussian mixtures in the 3d workspace shown in fi  9, gaussian mixtures in the configuration space of a 2dof planar manipulato  it is also important to highlight that one can simply use gm and gm to estimate how likely a configuration is in collision, which is leveraged in for fast collision checking and biased samplin  in this paper, we apply the meanshift clustering method with a gaussian kernel for learning gaussian mixtures using collision information of sample configurations obtained during previous attempts of a randomized motion planner, which is a convenient way of learning from past experiences and exploiting the collision histor  the kernel bandwidth b can be set based on the desired level of spatial resolutio  with the bandwidth b, we initialize the clusters and then perform a single step em update to estimate cluster statistic  in fi  2, we present some examples of constructed probabilistic models of different configuration space and workspace by the suggested approac  fi  2 shows a probabilistic model to define the collision space from 3d point clouds obtained by a depth senso  fi  2 shows the generated probabilistic models using collision information of samples in the configuration space of a 2dof planar manipulato  such probabilistic representations of configuration spaces can be utilized for collision likelihood estimation, as a computationally efficient alternative to the exact distance-to-collision measurement although confidence regions of an arbitrary probability distribution cannot be expressed explicitly in terms of simple geometric shapes and so are needed to be computed numericallyconfidence regions of gaussian distributions have an analytical ellipsoidal for  3: gmm confidence region  super level sets of gaussians at the confidence levels corresponding to a shared probability leve  an example configuration space and the associated confidence ellipsoids of learned gmm distributions from collision samples ).  with this approach, we obtain confidence regions of gaussian mixture models that approximately represents configuration space obstacles, as illustrated in fi  proof: by definitionthe probabilistically safe corridor sco is constructed as an intersection of halfspaces and so is a convex polytop  thus, the result follow  hence, the result directly follows from and the fact that for any safe configuration p belong to rn cgm fi  4: local steering via probabilistically safe corridor  example tree extension using a probabilistically safe corridor in 2d space, probabilistically safe corridor in 3d spac  the probabilistically safe corridor sc is bounded by tangent hyperplanes of confidence regions of individual gaussians that strictly separates the point p from the gaussian confidence ellipsoid  fortunately, many gaussian mixture learning algorithms yield proper mixture models with minimal overla  guided steering via safe corridors we now describe a novel use of probabilistically safe corridors for guided local steering of sampling-based planning, in particular, rrt  then, a new configuration qnew is slightly extended from qnear towards qrand, say using the standard straight-line steerin  if qnew is collision-free, it is added to the tree as a new node, which is connected to the nearest nod  if qnew collides with an obstacle, then tree construction repeats with another qran  in this paper, we propose a new approach for tree expansion where qnew is adjusted to head towards collision-free space using probabilistically safe corridors sco, as shown in fi  hence, the tree is extended towards qproj instead of qrand, as shown in fi  one computational challenge of our guided steering approach is that it requires to recompute the metric projection of qrand onto sco for each new selection of qrand and so qnea  metric projection onto a convex polytope can be solved using any state-of-the-art quadratic optimization solve  for efficiency, we apply the active-set method for quadratic optimization, which is an iterative solver that ensures a feasible solution and a decrement on the objective function at each iteratio  this enables us to inherit some useful information from prior computation and stop its computation after some desired number of iteration  in order to reduce to computational cost, we keep qrand the same until a maximum number of iteration max iter is reache  this enables us to warm-start the active set method with the active constraints of the previous computatio  if active constraints at the optimal solution are given, then a quadratic optimization problem with inequality constraints can be converted into a quadratic problem with equality constraints, which requires significantly less computational time to solve the optimization proble  for example, previous active constraints could be still active for slightly changed qnear if the sample goal qrand is kept the sam  therefore, to increase computational efficiency, we always check first if the quadratic optimization is feasible with previously active hyperplane constraints of probabilistically safe corridor  if the random goal qrand satisfies the safety corridor constraints, then the tree is directly extended to the random goal, just like the standard straight-line extension metho  using forward kinematics, we define xrand to be the end-effector position of the random goal qrand and xnear to be the end-effector position of the nearest node qnear of qrand in tree t here, our objective is to steer the end-effector position xnear towards xrand via the projection xproj of xrand onto the sco along the safe corridor sco in 3d space, as shown in fi  in fi  5: examples of task-space steering of a robotic manipulato  for modeling the free space, which is used for biased sampling over the free space as described in this sampling method increases the likelihood of a new sample being collision-free, and so can increase the computational efficiency of planning as discussed belo ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "III", "Section": "III Safety-Guided RRT  via Probabilistically Safe Corridors", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "IV", "Section": "IV Results", "Text": "we evaluate sg-rrt in various environments using both a simulator and a real robo  we analyze the performance of sg-rrt by comparison with several existing rrt approache  in addition, we demonstrate sg-rrt on a real humanoid robot and provide results under real setting  all experiments are performed on a  7ghz pc, and all planners are implemented in matla    learning gaussian mixture models in all our experiments, we learn gaussian mixture models offline by using the samples generated during the standard rrt planning ) and by manually selecting the kernel bandwidth for the meanshift clustering so that the desired level of representation resolution is guarantee  gmm learning takes  97 seconds for 1,096 clusters from 19,456 collision samples for 7dof manipulator, and  64 seconds for 189 clusters from a 3d point cloud for task space plannin 01 for all case  in future work, we plan to consider online gmm learning for adaptive motion planning in dynamic environment  2dof planar manipulator for ease of visual presentation, we first consider motion planning of a 2dof planar manipulator whose first link is  4 units long and second link is   in fi  6, we compare the computational performance of several variants of rrt planners with and without our proposed safety guided steerin  7: safety-guided rrt planning performance with respect to the number of collision samples used for gmm learning fi  in overall, we observe that our safety guided steering increases computation performance significantly over the standard straightline steering by dramatically reducing the required number of planning iterations to find a path between any given start and goal pair, as shown in fi  because safety guided steering via probabilistically safe corridors minimizes collision risk by adaptively adjusting steering direction and stepsiz  finally, we find it useful to emphasize that the construction of and the projection onto a probabilistically safety corridor takes around   in fi  as expected, the performance of rrt planning with safety guided steering increases with the increasing size of training data as a result of increasing accuracy of the gaussian mixture mode  in fi  8, we present an application of our safety guided steering to the probabilistic roadmap planning of the 2dof planar manipulato  as seen in fi  8, our safety guided steering noticeably increases the connectivity of a prm as compared to the standard straight-line planne  here, two vertices of a prm is said to be connected if safety guided steering can joining them in at most 100 table i: gmm and prm computation times gmm construction time prm construction time nu  of sampling gmm total nu  of prm collision connected samples time time time vertices time checks prm 300  4377 18,361 no 1,000  3674 35,306 no 2,000  0856 55,517 no 4,000  0590 81,274 yes 6,000  2114 107,841 yes 8,000  6380 134,851 yes 10,000   finally, to briefly compare the computation cost of the learning phases of the gmm and prm methods, we provide in table i the average computation time for the gmm and prm constructions for the 2dof planar manipulator plannin  as expected, for the same number of samples, gmm learning is around two orders of magnitude faster then the prm construction because the connectivity test of prms is significantly computationally costly than the nearest neighbor search and the statistics computation of gm  7dof manipulator in 3d space in order to validate the performance of sg-rrt quantitatively in high dimensional space, we compare it with traditional approaches with a 7dof manipulator in 3d space using the webots simulator of the cyberbotics lt  compan  fi  9 shows the simulation scenario that is composed of seven sequential planning task  this scenario includes a difficult task, where the robot must remove its arm from the lower shelf and then insert it into the upper shel  the simulation trials are repeated 50 times for accurate evaluation, and we use the average execution time and the number of collision checks as the evaluation criteri  in addition, since we can apply gmmbased sampling as described in section iii-  note that we apply a bidirectional method in all approache  the gmm-rrt can be faster than the standard rrt, and the gmmsg-rrt is the fastest among all approache  the wssg-rrt and the gmmwssg-rrt are faster than the rrt and gmm-rr  this demonstrates that the end-effector of the manipulator is effectively guided by the safe corridor in the high dimensional space, and it can reduce the computational time and the number of collision checks compared to traditional approache  we also observe in fi  9 that sgrrt planning is faster and requires less collision checks in configuration spaces than in task spaces, because probabilistically safe corridors are geometrically more informative when constructed in configuration spaces than in task space  therefore, the tree extension with the safe corridor is significantly more efficient than the traditional method  physical robot experiments we demonstrate the performance of sg-rrt on a 7dof manipulator of an actual humanoid robot and an rgbd camera with the scenario shown in fi  the robot is positioned 35cm from the shelf on the tabl  figure 10 presents the comparison results of gmmsg-rrt and the standard rrt in terms of the execution time and the number of collision check  note that we apply a bidirectional method and give 10% goal biased sample  since the gmmsg-rrt adjusts a new node in the direction that avoids obstacles using probabilistically safe corridors and also utilizes biased sampling over collision-free space, the sample connectivity increases around narrow spaces, and tree expansion efficiently avoids obstacle  gmmsg-rrt is significantly efficient even when the robot needs to insert its arm onto the shel  on the other hand, the computational time and the number of collision checks for the standard rrt planner dramatically increases in such complicated tasks", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "V", "Section": "V Discussion", "Text": "in this paper, we present an effective local steering approach for sampling-based motion planning using probabilistically safe corridors of learned gaussian mixture models of configuration space  we construct a probabilistically safe corridor around a configuration using tangent hyperplanes of confidence ellipsoids of gaussian mixture models that are learned using collision history to approximate configuration space obstacle  accordingly, we propose a probabilistically safe local steering primitive that extends a random motion planning graph towards a sample goal using its projection onto the associated probabilistically safe corridor, which heuristically minimizes collision likelihoo  we observe that the proposed local steering approach improves the performance of sampling-based planning in challenging regions, especially narrow passages, by adjusting steering direction and stepsiz  in our simulations and experiments with a real robot manipulator, we demonstrate that our proposed safety guided local planner shows significant performance improvement over the standard straight-line planner for randomized motion planning of 2dof and 7dof manipulator  in a future paper, we plan to extend our work using online gmm learning for uncertainty-aware adaptive plannin ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "References", "Section": "References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190  the results are compared with the bethe-heitler formul  we indicate that both the topt-description and a soft version for the bremsstrahlung process predict a strong screening parameterdependent cross section, which is missed by previous bremsstrahlung theor -m; 1 -t; 95", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "when electrons scatter offelectric field of proton or nucleus, they can emit real photon  this is bremsstrahlung bremsstrahlung appears in nearly all branches of physic  bethe and heitler first gave a quantum-mechanical description of the bremsstrahlung emission at the coulomb potential of an infinite heavy atom the bethe-heitler formula is an elementary and important equation in quantum electromagnetic dynamics and astrophysic  the bremsstrahlung process of electron in the coulomb field is a second-order process, which involves the photon emission of electron and the coulomb scatterin  however, these two sub-processes have divergence  the former is infrared divergence, while the later origins from the long-range 1/r potentia  for a neutral atom, the nuclear coulomb potential is completely screened by the electron cloud, which reduces a significant contribution from scattering distance larger than the atomic radiu  therefore, the coloumb potential in the s-matrix element should be replaced by a phenomenological screening potentia  however, the complex correlations between the above mentioned two sub-processes hinder us to obtained an exact analytical solution for the integrated cross sectio  bethe and heitler take an extra model to introduce the screening radius r in the solution and the result predicts a slower ln r-dependent cross sectio  the bethe-heitler formula was broadly applied in astrophysic  recently, a puzzled difference of the energy spectra of electron/positron at gev-tev energy band in cosmic-ray raises our doubts to the validity of the bethe-heitler formula we found that the bremsstrahlung cross section at the soft photon limit contradicts with the predictions of the bethe-heitler formul  this method was successfully used to decompose a complex feynman diagram to a several simpler sub-processes in our previous works we consider the scattering of electron on a light nucleus, where the recoil effect is not negligible, since most targets in the interstellar medium are light nucle  besides, we will show that the contributions of interference terms between scattering and radiation sub-processes can be neglected due to the recoil effect, thus we can further decompose the proces  the new bremsstrahlung formula predicts a strong r2-dependent cross sectio  the result will inspire us to review the traditional electromagnetic shower theory at the extreme condition  the paper is organized as follow  in se  2 we detail the derivation of the bremsstrahlung formula using the top  then we compare the results with the bethe-heitler formula in se  a short summary is given in se  3 2 the bremsstrahlung cross section with screening potential the bethe-heitler formula assumes that the target atom is infinitely heav  for using the topt in the following derivation, we consider a more general case: electron scattering offa finite heavy ato  figure 2: the topt decomposition of fi  dashed lines indicate the time ordered of the proces  it seems that the topt decomposition complicates the calculation with increasing the propagators however, the backward component will be suppressed at higher energy and small emitted angl  therefore, the contributions of the backward propagator are negligibl  this not only reduces the number of diagrams, but also allows us to factorize the complex feynman graph due to the on-mass shell of the forward propagato  this is the theoretical basic of the equivalent photon approximatio  we use the processes in fi  3 to show this approximatio  we take the laboratory frame, where the target atom is at rest, but the incident electron has a high energ  this is an infinite momentum frame for the electro  note that the physical picture of the same process has different appearances in the different coordinate frames, even in the different infinite momentum frame  not disappea  thus, the contributions of fig  3b and 3c are not negligible due to the coherence between fi  2a and 2  now we consider the recoil effec  in this case, the contributions of the interferant processes in fig  inhibite  we discuss the process involving in fi  note that a q2-dependent term in e  is absent when the target is a spin-0 particle, however, it is does not change the following result  combining eq  10 we calculate the integrated bremsstrahlung cross section at a given initial energy through the angle-integra  the contribution of the second term on the right side is negligible comparing with that of the first ter  however, the third integral is not so luck  relates to a maximum impact parameter bmax for the scattering of electron in a central coulomb potentia  has two space scale  now we calculate the process involving in fi  corresponding to e  12 are used", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 The bremsstrahlung cross section with screening potential", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Comparing with the Bethe-Heitler formula", "Text": " thus, we can not distinguish whether the photon is emitted from the incoming or outgoing electron  in opposite to the bethe-heitler formula, e  presents two factorized sub-processes, because of the recoil effect suppresses the contributions of fig  3b and 3  unfortunately, e  does not present the screening effect since it used a pure coulomb potentia  as a screening radius r,  one can get the different r-dependent result  for further comparison, we refer e  to rewrite e  the double lines are the eikonal form of the electron propagator  figure 6: the factorized bremsstrahlung process for a soft photon versio  the bremsstrahlung cross section has its soft versio  two electron propagators in fi  but with a 1/e2 i -suppletio  we emphasize that e  we consider the integral of e  at the er limi  because of m2 e in ln is not introduced as a cut-offparameterthe theory itself does not have any restrictions on the value of q  it implies a negative cross sectio  according to e  the coeffcients of e  are smaller than that of e  besides, the contribution of the third term in e  is more negative due to the recoil effec  therefore, our formula is consistent with the soft version of bremsstrahlung, but both contradict with the betheheitler formul  according to the qed-results either e  or e  we think that the measurements of the 18 differential bremsstrahlung cross sections belong to this exampl  they detect the angular or energy distributions for the photon  we have suggested to measure the high energy electron spectra when they pass through the completely ionized and extremely thin atmosphere besides, the above mentioned uncertainties can be effectively canceled though the comparison with a normal radiation lengt  we should mention the classical bremsstrahlung theor  it is the radiation of accelerated charged particle during its collisions with atomic electric fiel  this is not surprising, since e  corresponds really to the nonrelativistic limit in e  therefore, the description of bremsstrahlung in the classical electrodynamics using at high energy is not suffcient", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Summary", "Text": "the bethe-heitler formula describes bremsstrahlung of high energy electrons in a pure coulomb potential, which may lead to an infinite total cross section since the coulomb scattering is a long-range interactio  a natural method is to use a screening potential to replace the coulomb potentia  however, the complex interference effect between scattering and radiation sub-processes makes a diffculty for us to get an analytical solution if considering the screening potentia  it brings the uncertainty in the bethe-heitler formul  for this sake, we re-derive the formula for the bremsstrahlung cross section of electron in the atomic field using the topt framewor  we prove that the recoil corrections of a finite mass atom at high energy may further decompose the bremsstrahlung cross section to two sub-processes at the equivalent photon approximatio  the improved bremsstrahlung formula contains the screening potential and predicts a strong r2-dependent bremsstrahlung cross sectio  the results remind us to review the traditional electromagnetic shower theory at the extreme condition  acknowledgments author thanks   ruan and   wang for useful discussion  this work is supported by the national natural science of china ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190 00104v1 1 jan 2019 on polyhedral formula for kirillov-reshetikhin modules chul-hee lee abstrac  we propose a method to prove a polyhedral branching formula for kirillovreshetikhin modules over a quantum affne algebr  when the underlying simple lie algebra is of exceptional type, such a formula remains mostly conjectura  we convert a polyhedral formula into an identity between two rational functions of a single variable with only simple poles at known location  it is then suffcient to check the equalities of the residues at those poles, which are explicitly computable quantitie  by following this strategy, we obtain a computer-assisted proof of a conjectural polyhedral formula in type f ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1. Introduction", "Text": "let g be a complex simple lie algebr  the kirillov-reshetikhin modules constitutes an important family of finite-dimensional irreducible representations of the quantum affne algebra u  an interesting problem is to understand how a kr module or their tensor product decomposes into irreducible uq-module  the fermionic formula by kirillov and reshetikhinproven through a series of worksgives an answer to this question by expressing the multiplicity of each irreducible summand as a certain combinatorial rul  however, it is diffcult to use in practice, and it is often desirable to have a more explicit and computationally cheaper way of decomposing a single kr modul  if g is of classical type, there is a well-known explicit formula called the domino removal rul  it is a polyhedral formula in the sense that the highest weight of an irreducible summand with non-zero multiplicity is characterized as a lattice point in a suitable bounded polyhedro  such a formula with multiplicity remains largely conjecturaland furthermore, even a conjectural formula has not been written completely in fact, the only known polyhedral formula with multiplicity is when g is of type g2 by chari and mour  since their method is rather specific to type g2, it seems diffcult to adapt it to other cases in genera  in this paper, we propose a method to prove a polyhedral formul  1 2 chul-hee lee are written in some exponential for  they are essentially the residues at the poles of the generating function of the characters of kr module  it turns out that it is possible to decompose a polyhedral formula into a finite list of identities involving these coeffcients; see our method seems quite appropriate for a computer-aided mechanical approac  as our main objective in mind is of exceptional type, such a mechanical approach could be justifie  after presenting the general strategy, we consider a special case when g is of type f  we obtain a computer-aided proof of the following polyhedral formula conjectured in : theorem   let g be of type f  here we have used the same convention for enumerating the nodes of the dynkin diagram as in this paper is organized as follow  in section 2 we review the necessary background for our approach such as the q-system, and linear recurrence relations satisfied by the characters of kr module  in section 3, we explain the steps for proving a polyhedral formula for kr module ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2. Background", "Text": "notatio  we will use the following notation throughout the pape  ", "Subsections": [{"Section_Num": "2_1", "Section": "2.1. Some properties of characters of KR modules", "Text": " nakajima and hernandez proved the q-characters of kr modules satisfy the t-system from which we obtain the q-system by ignoring the spectral paramete  note that shows that d has only simple root  we call the mukhin-young formula, which is originally conjectured in on polyhedral formula for kirillov-reshetikhin modules 5", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2_2", "Section": "2.2. fermionic formula", "Text": "the fermionic formula, proposed by kirillov and reshetikhinconcerns the decomposition of a tensor product of kirillov-reshetikhin modules into irreducible uq-module ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2_3", "Section": "2.3. polyhedral formula", "Text": " a polyhedral formula for the decomposition of kr modules will mean a formula of the form in the next section, we explain an approach for a proof of 6 chul-hee lee", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3. framework for proving polyhedral formula", "Text": " also make sure that the degree of the denominator of p is greater than that of its numerato  recall that we already know explicitly where the poles of q are located, which are always simpl  on polyhedral formula for kirillov-reshetikhin modules 7 suppose that we already have proved in a nutshell, it is possible to check both and algorithmicall  in the next section, we follow this strategy to prove a conjectural polyhedral formula in type f4, where we discuss some practical issues in our method in detai ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4. Proof of Theorem ??", "Text": "1 let g be a simple lie algebra of type f  the formula for a = 1 is given in and will be used late  the main goal of this section is to prove theorem   we often need to explicitly deal with w or its subgroup  one may refer to for an algorithm to find the weyl orbit of a weight or a minimal coset representative for a coset of a standard parabolic subgroup 8 chul-hee lee of a weyl grou  the accompanying mathematica notebook file for some computer calculations is available at https://githu com/chlee-0/kr-polyhedral-formul ", "Subsections": [{"Section_Num": "4_1", "Section": "4.1. Step ??", "Text": ".  let us find the poles of   they are uniquely determined by this form of decompositio  because these expressions are long but easy to find, we do not write them here; one can refer to the accompanying file for an explicit descriptio  on polyhedral formula for kirillov-reshetikhin modules 9 proo  we may use computers to verify this directl  below, we will explain how to reduce the amount of calculation to check while this reduction is not essential as long as we focus on type f4 whose weyl group is manageable in size, it might be useful for treating a similar vanishing sum over bigger groups in other type ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4_2", "Section": "4.2. Step ??", "Text": ".  it remains to carry out the second step of our strategy in section 3 to prove we first explain how to compute both sides, and then check their equalit  proo 2, the mukhin-young formul  by proposition   in fact, this identity is a special case of once we subtract every summand, the result becomes be zero, as we wante  on our desktop computer with a  50ghz cpu and 8gb of ram, it took about 1450 seconds to complete this calculatio ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "References", "Section": "References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Top-Assisted", "Section": "Top-Assisted Di-Higgs boson Production Motivated by Baryogenesis", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": " the construction of such methods heavily relies on the legendre expansion technique in conjunction with the symmetric conditions and simplifying assumptions for order condition  new families of symmetric integrators as illustrative examples are presente  for comparing the numerical behaviors of the presented methods, some numerical experiments are also reporte ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "numerical integration that preserves at least one of geometric properties of a given dynamical system has attracted much attention in these years it is evidenced that numerical methods with such a special purpose can not only perform a more accurate long-time integration than those traditional methods without any geometric-feature preservation, but also produce an improved qualitative behavior for more details, we refer the interested readers to and references therei  reversible systems and reversible maps are of interest in both aspects of theoretical study and numerical simulation for many differential equations email addresses: tangws@lse c a cnjjzhang06@outloo com preprint submitted to elsevier june 11, 2019 arxiv:190  it is known that a number of symmetric integrators automatically possess this property,   to be specific, we quote the following result from thanks to the property of reversibility preservation, symmetric integrators often have an excellent long-time numerical behavior than those non-symmetric integrators for reversible systems so far, a wide variety of effective symmetric integrators have been proposed in the context of geometric integration, the greatest interest has been given to the development of symplectic integrators for solving hamiltonian systems over the last decades besides, a numerical method which is energy-preserving and reversibility-preserving can also be of interest they can be viewed as the natural generalizations of numerical methods with finite stages it is shown in that by using continuous-stage methods many classical rk, prk and rkn methods of arbitrary order can be derived, without resort to solving the tedious nonlinear algebraic equations in terms of many unknown coeffcient  2 the construction of continuous-stage methods seems much easier than that of those traditional methods with finite stages, as the associated butcher coeffcients are continuous or smooth functions and hence they can be treated by using some analytical tools therefore, continuous-stage methods have granted us a new insight for numerical integration of differential equations and some subjects in this new area need to be investigate  since symmetric integrators possess important theoretical and real values in numerical ordinary differential equationswe are concerned with the development of new symmetric integrators for solving second-order ordinary differential equations the construction of such methods in this paper is on the basis of the notion of csrkn methods and heavily relies on the legendre polynomial expansion techniqu  furthermore, by using gaussian and lobatto quadrature formulas we show that new families of symmetric rkn-type schemes can be easily devise  moreover, by theorem  1, these methods are also reversibility-preserving and therefore very suitable for solving reversible system  this paper will be organized as follow  in section 2, we introduce the exact definition of csrkn methods for solving second-order odes and the corresponding order theory previously developed in will be briefly revisite  some numerical experiments are reported in section   at last, we give some concluding remarks in section 6 to end this pape ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Continuous-stage RKN method and its order theory", "Text": " for more details, see", "Subsections": [{"Section_Num": "2_1", "Section": "2.1 Continuous-stage RKN method", "Text": " compared with an s-stage rk method applied to the corresponding first-order system deduced fromthe rkn method is preferable since about half of the storage can be saved and the computational work can be reduced a lot as a counterpart of the classical rkn method, the csrkn method can be formally defined", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2_2", "Section": "2.2 Order theory for RKN-type method", "Text": " in view of theorem  3, we have the following result for analyzing the order of the rkn method with tableau", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Conditions for the symmetry of csRKN methods", "Text": "now let us introduce the definition of symmetric methods and then show the conditions for a csrkn method to be symmetri  symmetry implies that the original method and the adjoint method give identical numerical result  an attractive property of symmetric integrators is that they possess an even order proo  firstly, let us establish the adjoint metho  therefore, we have get the adjoint method defined by and in the following we present a preferable result for ease of devising symmetric csrkn method  proo  by putting theorem  3 together, we can devise symmetric integrators of arbitrarily high orde  as symmetric methods possess an even order, it is suffcient to consider those order conditions for odd orders, so we can increase two orders per ste  we present the the following result without a proof then the corresponding csrkn method is symmetric and of order 2 at leas ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Symmetric RKN method", "Text": "in this section, we show that symmetric rkn methods can be easily derived from symmetric csrkn methods by using quadrature formula  proo  by theorem  3 presented insuch methods are also symplectic and thus suitable for solving general second-order hamiltonian system  the resulting symmetric rkn methods are shown in table   2this can be easily checked by the classical order conditions that listed in we point out that: the left family of rkn methods in table  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Numerical experiments", "Text": "in this section, we perform some numerical results for comparing the numerical behaviors of the presented method 5, where the initial values are taken the same as that given in global errors of the numerical solutions by the above four methods with six small step sizes are shown in fi 1 with log-log scales, which verifies the order of all the method  from fi  it is observed that the energy error keeps bounded for the rkn-diagsymp metho  besides, it seems that the non-symplectic rkn-a method gives a better behavio  however, when we integrate the system on a much longer time intervalit gives a worse result compared with the rkn-diagsymp method from these numerical tests we may conclude that symplectic-structure preservation is more essential than the reversibility preservation of the reversible hamiltonian systems in long-term numerical simulatio  nevertheless, for general reversible non-hamiltonian systems, symmetric methods are also preferabl ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "6", "Section": "6 Concluding remarks", "Text": " the crucial technique based on legendre polynomial expansion combining with the symmetric conditions and order conditions is fully utilize  the reference line has slope 4 in every subplot 16, integration interval16, integration interval families of symmetric integrators are derived in use of gaussiantype and lobatto-type quadrature formula  it is worth observing that other quadrature formulas can also be considered for devising symmetric integrators and more free parameters can be led into the formalism of the butcher coeffcient  acknowledgements the first author was supported by the national natural science foundation of chinachina scholarship council and scientific research fund of hunan provincial education department the second author was supported by the foundation of nsfc and phd scientific research foundation of east china jiaotong universit ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190  based on the similarity of motif profiles, we further propose to estimate the similarity coefficients between different time series and classify these time series with high accurac  we further apply the motif analysis method to the ucr time series classification archive and provide evidence of good classification ability for some data set  our analysis shows that the proposed triadic time series motif analysis performs better than the classic dynamic time wrapping method in classifying time series for certain data sets investigated in this work", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "quantifying the similarity of time series has always been a very useful primitives for time series analysis, with applications to many fields the key point of measuring similarity is to define a suitable and effective distance between two time series the widely adopted definitions of distance include the euclidean distance and correlation measures for most time series analysis problems, the dynamic time warping provides a highly competitive distance metric to get the best performance of dtw, we need to regulate its unique parameter to optimize the dynamic time warping's window width the complexity of the dtw method is relatively high, so many researchers provide some improved methods to have better performance moreover, practitioners generalize the dtw to some multi-dimensional time series classification experiments similar subsequences in time series can be defined as time series motifs, which characterize the temporal properties and dynamics of the corresponding long time series it is useful for exploratory data mining and often used as inputs for classification of time series, clustering, segmentation time series motif analysis has been widely used in diverse fields gomes and batista presented a sax-based motif discovery method to classify the urban sound wang et a  proposed a method to automatically detect repeating segments in music and two time series data sets email address: wxzhou@ecus ed cn preprint submitted to elsevier january 3, 2019 k-motifs in time series data and their methods play an important role in several time series data mining tasks by using motif discover  lots of researchers have used time series motifs analysis for applications in many different domains triadic time series motifs are inspired by the network motifs in visibility graph and horizontal visibility graphs mapping from time series the six triadic time series motifs are similar in some features with sequential hvg motifs and ordinal patterns the permutation entropy based on ordinal patterns is a natural complexity measure and useful in the presence of dynamical or observational nois  similarly, the triadic time series motif analysis can also mine the dynamical characteristics of time series from complex syste  xie et a  used the triadic time series motif analysis to uncover the different dynamics in the heartbeat rates of healthy subjects, congestive heart failure subjects, and atrial fibrillation subjects and identify the bullish and bearish markets from the price fluctuations of financial market  it is of great significance to be able to discover the characteristics of time series from different types of chaotic map  we also apply the triadic time series motif analysis to classify the time series in 128 data sets from ucr time series classification archive", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Triadic time series motifs", "Text": "triadic time series motifs are determined by the relative magnitude and ordinal order of three data points that are randomly chosen from the time series xie et a  when two data points are identical, we treat it as if the latter data point is larger than the former on  figure 1: illustrative example showing the six types of triadic motifs in time serie  the time series motifs are different from the conventional motifs of horizontal visibility graphs considering the triadic hvg motif, there are only two admissible motifs in undirected hvgs, one being a chain and the other being a triangl  as shown in fi  time series motifs consider not only the visibility between data points, as hvg motifs, but also the order and relative magnitudes of the point  hence, time series motifs explore finer structures of hvg motifs", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Triadic time series motif analysis of chaotic maps", "Text": " chaotic maps we perform triadic time series motif analysis numerically for different time series in continuous and discrete dynamic system  the logistic map is a representative example of how complex, chaotic behaviour can arise from very simple nonlinear dynamical equatio  mathematically, the logistic map is written as xn+1 = rx  the specific equations for these four types of dynamic systems are given below the hyperchaotic folded-tower map is written as uf8f1 uf8f4 uf8f4 uf8f4 uf8f4 uf8f2 uf8f4 uf8f4 uf8f4 uf8f4 uf8f3 xn+1 = axn - 78zn + by ", "Subsections": [{"Section_Num": "3_2", "Section": "3.2 Occurrence frequency distributions of triadic motifs", "Text": "we first generate time series by using the logistic map with control parameter   the parameter r ranges in the interval of   by definition, motifs m5 and m6 cannot appea  this analytical result is verified by the numerical simulations, as shown in fi 5, the time series is slightly more complicate  as shown in fi 6, the result of the iterative run will switch between the period type and the chaotic typ  2 has a big differenc  in fi  3, the length of time series is 512 and there are big difference in the occurrence frequency of motif m3 and motif m4 between the four types of discrete chaotic time serie  it can be imagined that the longer the time series is, the larger the difference of the occurrence frequency of the individual motifs will be, and the easier it is to distinguish the the four types of discrete chaotic time serie  in fi  2, we can use a single indicator f3 to distinguish the five types of logistic time serie  the length of time series is 51  classification of time series the triadic motif analysis is applied to the classification of time series to investigate the effectiveness of the similarity measure of time serie  in order to classify different time series, we need to extract the features of time serie  the triadic time series motifs are used as the features of time series, and then the time series are classified based on the motif occurrence frequency distribution  therefore, we consider the influence of time series lengths on the accuracy of classificatio  we compare two classical methods for measuring the similarity of time series: one is the simple euclidean distance method, and the other is the dynamic time warping method we select the nearest neighbor method to classify the time series based on the three similarity measure 8 and 4 and from the four chaotic map  for each type of time series, we generate 100 time series as the training set and 100 time series as the test se  the three colors in fi  the green dot indicates the accuracy beta of classification of the 1nn method based on the dt  the ordinate represents the discriminant correctness rate based on the training set and the test se  the abscissa represents the time series length   in general, the dtw-based discriminant accuracy is the best and the motif profile method performs slightly worse, especially when the time series length is less than 20  the euclidean distance method is the wors  usually, the longer is the time series, the more information is extracted by the method  this is mainly because that the euclidean distance calculation is simpl  the time series contains 512 data point  for each type of logistic time series, we generates 100 time series training sets and 100 test set  the ordinate represents the average classification accuracy rat  the abscissa represents the length of time serie  the three colors correspond to three similarity measurements: the motif distribution, the dtw and euclidean distanc  the relationship between the data deletion rate e and the average classification accuracy rate for the five types of logistic time serie  the relationship between the data deletion rate e and the average classification accuracy rate for the four chaotic map  conducive to depicting the similarity between time serie  the euclidean distance method has a relatively good effect when the time series length is less than 5  in order to analyze the accuracy of classification in the case of data loss, we perform the same analysis on the time series after data deletio  the length of the original time series is l = 51  we randomly delete a proportion of the data from each time serie  we then classify the remaining data and calculate the classification accuracy rate  fi  for the logistic maps, the motif profile method is more robust to data deletion than the dtw metho  in contrast, for the chaotic time series, the dtw method outperforms the motif profile metho  not surprisingly, each method has its own advantages and disadvantage  different methods usually have different performances when they are applied to different time serie  triadic time series motif analysis of the ucr time series classification archive to test the effectiveness of this method on similarity measures of time series, we use this method to classify real time serie  the data source is from the ucr time series classification archive the ucr time series classification archive contains 128 data sets, each of which is divided into a training set and a test se  the dataset website also presents some results about classification accuracy of three method  the first method uses the nearest neighbor method to classify based on the euclidean distanc  the second uses the nearest neighbor method to classify based on the dtw metho  the correct rate is represented by beta, where the highest is 100% and the lowest correct rate is 1  the third method is based on the improvement of dt  we apply our method to the 128 data sets and compared the results with those obtained from the first and second method  each radar chart corresponds to a data se  each solid line in the radar map represents the average motif occurrence profile of a class of time series in the data se  fi  5 shows the radar charts of the motif occurrence profiles averaged within different classes of time series in six representative data set  each radar chart corresponds to a data se  each solid line in the radar map represents the average of the motif occurrence profile f of one category of time series in the data se  the time series belonging to the same class in the training set and the test set are included in the averaging proces  it can be seen that the six radar charts are very different, implying that the motif profile method can classify different data sets effectivel  for each data set, the difference between the profile lines in the corresponding radar chart represents the difference between different time serie  the classification will be more accurate if the difference is large  the three radar charts on the top panel of fi  5 have many profile lines that are not sufficiently separated, which indicates that it would be hard to distinguish those categorie  in contrast, each of the three radar charts on the bottom panel of fi  indeed, the classification accuracy is low for the former data sets and high for the later data sets each data point in the figure corresponds to a data se  euclidean distance, and dtw base  we use the triadic motif occurrence profile as the characteristic time series feature to classify the 128 data set  the classification accuracy is shown in fi  the 11 data sets is shown in the lower right triangle of fi  fi  6 also compares the classification accuracy of the motif profile method and the euclidean distance metho  in general, the dtw method does a very good job in the measurement of time series similarit  our method is superior to the dtw method for some data set ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Triadic time series motif analysis of the UCR Time Series Classification Archive", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Conclusions", "Text": "it is of great significance to be able to discover the characteristics of time series from a unique perspective through novel method  here, we studied the characteristics of time series through triadic time series motif  we defined six different network moti  the simulation analysis finds that the distributions of the motif occurrence frequencies corresponding to logistic maps and chaotic time series all have their own characteristic  the motif occurrence profiles can quantify the time series characteristics in different dynamical systems and show comparative classification power as the dtw metho  we apply the motif analysis to the ucr data set  the advantage of the euclidean distance method is that the calculation is simple and fas  the dtw method performs best, but in some data sets, the performance is not as good as the motif profile metho  our method has better accuracy than the dtw method for 11 data set  the starting point of our method is completely different from the euclidean distance method and the dtw metho  this study is based on the complex networks, and mines the features in the time serie  it is expected to be effectively improved in future research and provide a more effective method for measuring the similarity of time serie  indeed, there are many methods for extracting motifs from time serie  different motif extraction methods can describe 8 different time series feature  in order to improve the practicality of our method, we will develop different motif recognition methods to measure time series similarit  acknowledgements this work was supported by national natural science foundation of china and fundamental research funds for the central universities", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": " abstract in this paper, we focus on the node-based epidemic modeling for networks, introduce the propagation medium and propose a node-based susceptible-infected-recoveredsusceptible epidemic model with infective medi  theoretical investigations show that the endemic equilibrium is globally asymptotically stabl  numerical examples of three typical network structures also verify the theoretical result  finally, we discuss the impact of the epidemic spreading rate of media as well as the effective recovered rate on the network average infected stat ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "ed cn 1 arxiv:190  investigations on these models have important significance in public-health domain, especially in infectious disease epidemiology, by providing a number of interesting and unexpected behavior  the theoretical studies of epidemic spreading models in complex networks rely mostly on the mean-field theory approaches, especially on degree-based mean-field theory which was the first theoretical approach presented for the analysis of general dynamical processes on complex networks therefore, the epidemic spreading model based on dbmf theory depends in general on the statistical topological properties of the underlying networks instead of the whole network structure, resulting into the loss of detailed features of network topologies such that it is diffcult to deeply understand the effect of network structures on the disease propagatio  to the best of our knowledge, in 2009, mieghem et a  firstly proposed the continuous-time node-based sis epidemic spreading model for understanding the influence of network characteristics on epidemic spreadin  youssef and scoglio established a new individual-based sir model with the whole description of network structure  very recently, yang et a  suggested a node-based susceptible-latent-exploding-susceptible model, and in the same year they presented a heterogeneous node-based sirs model where each node has the different infected and recovered rates the above models assume that disease transmission takes place between individuals in network  however, diseases are propagated not only by the contact between individuals in the same population, but also by the contact between individuals and infective medi  for this case, shi et a  established a new sis epidemic model with an infective medium, which describes epidemics transmitted by infective media on various complex network  by differentiating the infective medium from individuals, yang et a  proposed a modified sis mode  wang et a  presented a modified sis with an infective vector by incorporating some infectious disease  it is noteworthy that these existing models with infective media are degree-based instead 2 of node-base  the motivation of this paper is to build a node-based sirs epidemic model with infective media on various complex networks by integrating the node-based approach and the infective medium, and investigate the stability of the equilibrium as well as the influence of network structures, the infective medium and the effective recovered rate on the network infected steady stat  the rest of this paper is organized as follow  some definitions and lemmas are introduced in se  in se  3, a node-based sirs epidemic network model with infective media is built and then its equilibrium is give  the global asymptotical stability analysis with respect to the equilibrium is performed in se  in se  5, numerical simulations of three typical network topologies are provided for further verifying the theoretical result  the correlation between the infected percents of nodes and its degree, as well as the impact of some critical parameters on network average infected percents, are studied theoretically and numericall  finally, some conclusions and discussions are given in se ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Preliminaries", "Text": "first, some requisite definitions and lemmas are given as follow  definition 1 : a matrix is metzler if its all off-diagonal entries are non-negativ  obviously, the diagonally stable matrix is hurwitz stable, and the opposite is also true for metzler matrice  lemma 1 : a hurwitz and metzler matrix is diagonally stabl  then, x14 a d2 d1 d3 x15 is diagonally stabl  parameters description si the percents that node i is susceptible at time   ii the percents that node i is infected at time   ri the percents that node i is recovered at time   sm the percents that media is susceptible at time   im the percents that media is susceptible at time   betam the probability that a susceptible node is infected by an infective medi  beta the probability that a susceptible node is infected by an infected neighbo ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Model formulation", "Text": " the sirs network part, the media par  the performance examined in appendix c shows that model is able to well forecast the epidemic dynamics of model built by means of markov chain techniqu  furthermore, the dynamical behaviors of approximation models is more easily studied by applying the stability theory and method, and thus the similar approximation model is directly built and studied in a large number of related literature  the phenomenon can be understood by the fact that the endemic disease remains safely under cover in each individual in some local area  although the equilibrium is given in the implicit form, it can be calculated out by the numerical iterative metho  the infected medium terms not only increase the dimension of sirs models, but more importantly make stability analysis more complicated, especially in the part of global attractivit ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Stability analysis with respect to equilibria", "Text": " next, we show that all the eigenvalues of b have negative real par  here we consider an undirected and connected graph, so the adjacent matrix a is an irreducible one, indicating that k2 is also an irreducible matri  that is to say, d is a hurwitz and metzler matrix", "Subsections": [{"Section_Num": "4_2", "Section": "4.2 Global attractivity", "Text": "to proof the global attractivity, it needs to determine the positively invariant set to explore the asymptotic behavior of solutions of e  10 both functions are continuous and exist right-hand derivatives along solutions of e let y is the solution of e  by the way, without the medium propagation,  ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Numerical simulations", "Text": " each type of network is with 100 nodes, respectivel  in fact, the steady state of each node is closely related with its degree according to the formula of equilibri  please refer to appendix b for the detailed derivatio  for the nw small-world network, the node with large degree has large infected equilibrium state, as shown in fi  however, the correlation is a little weaker for the ba scale-free network which is a heterogeneous one, please see fi ", "Subsections": [{"Section_Num": "5_2", "Section": "5.2 Impact of system parameters", "Text": " according to the implicit differentiation theorem, it is easy to obtain the following theoretical results through the formula of equilibri  furthermore, we numerically verify the theoretical implications for three typical network topologies, respectivel  fi  it is shown from fi  a possible reason is that the small-world network has the properties of the short average path length and small average degre ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "6", "Section": "6 Conclusions and discussions", "Text": " the curves from bottom to top in each subplot correspond to the value of beta =  85, respectivel  16 only an equilibrium yet not virus-free one that is always globally asymptotically stable through the stability analysi  without the medium propagation, the model has a virusfree equilibrium which is globally asymptotically stable when the maximum eigenvalue of topological matrices is less than the effective recovered rate /bet  three typical networks,   numerical simulations also show that the sparse network has less infected percent  finally, theoretical and numerical studies on the influence of the effective recovered rate and medium propagation rate on network average infected percents imply that network average infected percents go up with the increase of the medium propagation rate numerical investigations further show that the medium propagation rate does nothing with network average infected percents for homogenous networks, and the infected percents decease exponentially with the increase of the effective recovered rat  conflicts of interest the authors declare that they have no conflicts of interest regarding the publication of this pape  we can assert that the equilibrium is one and only if h is monotonic and exist a unique fixed poin  proof: existenc  hence, the fixed point is uniqu  this completes proo  has two opposite sign root  the relation between exact markov model and approximation model please refers to the reference next, we select three typical networks with 100 nodes, and use the gillespite algorithm 20 to simulate the solution of the markov model where model parameters and initial conditions are the same as those in se  furthermore, the estimation for small-world and scale-free networks is better than that for the fully connected network  on the whole, the performance of this new model is good for describing the real markov mode  next, we prove that j is invertible and all the elements of -1 are negativ  on the other hand, it follows from e  it follows that all the eigenvalues of matrix -j are negative, and -j is metzler and irreducibl  according to the result inall the elements of matrix -1 are negativ  the proof of theorem 3 is complete ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "a jp, fujiwara@kitasato- a jp and ozaki@tokai- jp 11 april 2019 abstrac  it is shown that the morse index changes at a bifurcation point and all solutions bifurcating are approximated by variational functions responsible for the change of the morse inde  thus, to our numerical studies, change of the morse index is not only necessary but also suffcient condition for bifurcation for these choreographie  further we observed that the change of the morse index is equal to the number of bifurcated solutions regarding solutions with congruent orbits as the same solutio  phy  a: mat  theo ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": " chenciner and montgomery gave a mathematical proof of its existence for a = 1 by variational metho  the detailed initial conditions for three bodies are found in arxiv:190  sbano and southall proved that there exist at least two n-body choreographic solutions for suffciently large period, and there exists no solution for small perio  here the morse index is a number of independent variational functions giving negative second variation of action functiona  on the other hand gal an et al showed that the h solution bifurcated from figure-eight choreography by changing the masses of three bodie  they also found many different periodic orbits on figure-eight there are several researches on the morse index for periodic solution of threebody proble  barutello et al calculated the morse index mathematically for the lagrangian circular solution, and hu and sun for elliptic lagrangian solutions, to discuss the linear stabilit  in this paper, we show a relationship between the morse index of the figureeight choreographies and periodic solutions bifurcating for a system of three identical bodies interacting through a homogeneous potential or through lj-type potential in section 2, we show the morse index changes at a bifurcation point and solutions bifurcating are approximated by variational functions responsible for change of the morse inde  in section  9966 where the morse index change  in section  3424 is show  these bifurcations at a =   in section   in section  2, we show the rest three points yield choreographic solutions less symmetric than figure-eigh  we can say yes, they exist under lj-type potentia  section 5 is a summary and discussion  our numerical results in this paper were calculated by mathematica 1 ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Morse index and bifurcation", "Text": " the subscript of six component vector is assumed to be in the range between 1 and  ", "Subsections": [{"Section_Num": "2_2", "Section": "2.2 Bifurcation and eigenvalue problem", "Text": " morse index and bifurcation for figure-eight choreographies 5 - ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2_3", "Section": "2.3 Morse index and bifurcation", "Text": " thus then are derived by we confirmed that the inequalities and corresponding to the picture of bifurcation as shown in figure 1 hold in our numerical calculation ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Morse index and bifurcation for homogeneous system", "Text": " bifurcation at a =  9966 at a =  78343 in   thus we denote orbits with this symmetry by dx  black orbit is symmetric itself but two gray orbits collectivel  consequently all bifurcating solutions from a =  9966 will be searched within dxy symmetr  conditions for q to be dxy, derived in appendix   for given period t, the three conditions determine the three parameters morse index and bifurcation for figure-eight choreographies 8 -  dxy solutions bifurcated from a =  02, are shown with parameters for initial condition and the index  9966 very close to a =  345 close to a =   bifurcation at a =  3424 as shown in table   in this section we investigate this point in similar manner as in section  345 are shown as a close point to a =  3424 because of numerical convenienc  we denote former by dx and latter d  here the orbits of dx can have non zero total angular momentum l since sum of signed area of the three orbits can be non zero whereas it is zero for solutions d2, dxy and the figure-eight choreography because of two fold symmetry at origi  conditions for q to be dx, derived in appendix   for given period t, four conditions determine four parameters dx and d2 solutions at a =   conditions for q to be d2, derived in appendix   for given period t, three conditions determine three parameters knowing the existence of dx, we recently re-found dx and d2 using the above equations with newton's method: six dx and six d2 solutions bifurcate in the right side of bifurcation poin  then the orbits of six dx solutions are all congruent, in direct isometry or mirror inversion, and the orbits of six d2 solutions are s  morse index and bifurcation for figure-eight choreographies 11 table  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Morse index and bifurcation for LJ system", "Text": " the bifurcations are the both sides and the number of incongruent bifurcating solutions nb's are both tw 878 is plotte  thus there exist two bifurcated solutions for t > 1 *10-9 - *10-9 - 878 yielding dxy solution 836 yielding dxy solution  both solutions are bifurcated from left side of bifurcation point but for t > 1 878 one from right side and the other from left sid  the bifurcations are one side in the right side and the number of incongruent bifurcating solutions nb's are both tw ", "Subsections": [{"Section_Num": "4_2", "Section": "4.2 Choreographic bifurcation", "Text": "615 and t = 1  we denote this orbit by c  conditions for q to be cx, derived morse index and bifurcation for figure-eight choreographies 13 1 111 yielding dx and d2 solution  dx solution and d2 for t = 2 *10-6 - 861 yielding dx and d2 solution  dx solution and d2 for t = 2  for given period t, four conditions determine four parameters we denote this orbit by c  conditions for q to be c2, derived in appendix   for given period t, three conditions determine three parameters an index morse index and bifurcation for figure-eight choreographies 14 -  we denote this orbit by c  conditions for q to be cy, derived in appendix   for given period t, the six conditions determine six parameters these couples of congruent orbits bifurcate in the right side of the bifurcation point and nb =  ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Summary and discussions", "Text": " the bifurcations are confirmed numerically by newton's metho  if the number of parameters is three as in the dxy, d2 and c2 cases, the parameters of the solution are represented graphically like the parameter for the dxy solution and the figure-eight choreography are observed as crossing points of two curves in figure 1  newton's method sometimes does not converge unless initial parameters are good enoug  we introduced another graphically assisted method: draw one condition as a function of one parameter by newton's method in one lower dimensio 5 corresponds to figure-eight choreography and two other zeros dx solutions in figure 1  map to search dxy solutio 69571 for a = 1 homogeneous potentia  crossing points of two curves show the parameters for the dxy solutions and for the figure-eight choreograph 5 corresponds to figureeight choreography and two other zeros bifurcating solution d  we assume the euler characteristics conserves at the both sides of bifurcation point, and hold  restrictions of bifurcation by conservation of the euler characteristics and in table 3, these restrictions on bifurcation are tabulate  indeed, symmetry of q is useful for numerical calculatio  acknowledgments we thank kazuyuki yagasaki for his valuable comment on variated orbits at symposium on celestial mechanics and n-body dynamics this work was supported by jsps grant-in-aid for scientific research 17k05146 and 17k05588 appendix   the subscript and index for body is assumed to be in the range 1 and   a configuration that a body b is at origin is called as euler configuratio  appendix   appendix   appendix   morse index and bifurcation for figure-eight choreographies 19 appendix   appendix   appendix  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Appendix", "Section": "Appendix A Conditions for solutions", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "an active learning framework for efficient robust policy search sai kiran narayanaswami* the university of texas at austin nskiran@c utexa edu nandan sudarsanam robert bosch centre for data science and ai indian institute of technology madras nandan@iit a in balaraman ravindran robert bosch centre for data science and ai indian institute of technology madras ravi@cs iit a in abstract robust policy search is the problem of learning policies that do not degrade in performance when subject to unseen environment model parameter  it is particularly relevant for transferring policies learned in a simulation environment to the real worl  several existing approaches involve sampling large batches of trajectories which reflect the differences in various possible environments, and then selecting some subset of these to learn robust policies, such as the ones that result in the worst performanc  we propose an active learning based framework, effacts, to selectively choose model parameters for this purpose so as to collect only as much data as necessary to select such a subse  we apply this framework using linear bandits, and experimentally validate the gains in sample efficiency and the performance of our approach on standard continuous control task  we also present a multi-task learning perspective to the problem of robust policy search, and draw connections from our proposed framework to existing work on multi-task learning", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": "recent advances in deep reinforcement learning algorithms have achieved remarkable performance on continuous control tasks traditionally, these algorithms are used to learn policies to perform a given task in simulatio  indeed, the prospect of being able to deploy policies learned in simulation on real-world systems such as physical robots is one of the major drivers for research in reinforcement learnin  one class of approaches towards this goal that has gained traction is to learn from multiple simulated domains that approximate the real target domai  these usually correspond to an ensemble of environment models with various parameters such as the mass of a part of a robot or the coefficient of friction between the robot's foot and the groun  given such an ensemble, the problem of robust policy search is to learn policies that perform well across this ensembl  one prominent group of approaches in this class involves sampling model parameters from the ensemble and collecting batches *work performed when author was at the robert bosch centre for data science and ai, iit madra  0preprint: this is the authors' own version of their work that is published in the 5th joint international conference on data science and management of data doi:1 1145/349370 3493712 of trajectories simulated using these parameterswhich are then used for training a policy, typically by a model-free rl algorith  these approaches differ mainly in the way in which they choose subsets of these trajectories to focus on for policy learnin  in this work, we demonstrate a novel way to improve their sample complexity while maintaining the performance and robustness of the learned policy through the use of active learning for intelligently selecting model parameters for which to sample trajectories for learnin  active learning is used to directly acquire some desired subset of the trajectorieswhile collecting as little additional data as possibl  in contrast, existing methods sample parameters directly from the ensemble, and possibly discard large portions of the collected trajectories the structure of the framework and the use of active learning for trajectory sampling results in some connections between robust policy search and multi-task learnin  we discuss the relation between the two problems, as well as the differences in their solution approache ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Related Work", "Text": "learn controllers with a specific functional form using trajectories sampled for parameters drawn from an ensemble, and optimize for the average case performanc  propose epopt, which learns arxiv:190 00117v2 21 nov 2021 narayanaswami e  a  a neural network policy using a model-free drl algorithm, but on simulated domains sampled from an ensemble of model  an adversarial approach to training is taken that involves selectively exposing to the model-free learner only data from those sampled models on which the learner exhibits the least performanc  epopt optimizes the conditional value at risk, which has also been used for learning robust option  even though this is a more sophisticated approach than the former and is demonstrated to have greater performance and robustness, the number of trajectories collected is still very larg  propose an approach that optimizes the average case performance, but additionally performs explicit system identification, and the estimated model parameters are fed to a nn policy as additional context information alongside the original observation  also uses system identification on data from the real world to decide the parameters on which to trai  also perform system identification, but operate in a belief space over the model parameter  again, the data requirements are quite large, both for policy learning as well as system identificatio  a recent work that learns from an ensemble of models isbut the ensemble here consists of learned dnn models of the dynamics for use in model based rl, rather than being induced by changing physical properties of the environmen  a similar ensemble generated by perturbing an already learned model is used for planning through in this work also does not deal with model uncertainties with physical meanin  approaches related to learning from an ensemble of models have also been studied under dynamics randomization and domain randomization although uses only an appropriate subset of models to train on, none of the above approaches consider ways to sample trajectories only as necessar  our proposed framework employs active learning to decide with data from only a few model parameters the models for which the agent requires more trainin  active sampling approaches have also been explored for task selection in multi-task learning bya viewpoint we discuss in more detail in section  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Background", "Text": "1 rl on an ensemble of models we work with the same setting described in where the model ensemble is represented as a family of parametrized mdps on a fixed state and action spac e, different parameters induce different dynamics and reward  we note here that we say parameter even if it is a vector rather than a real numbe  this translates to being able to perform well on some unknown target domain, and also potentially handle variations not accounted for in   however, this objective could be close to the maximum even if there are sharp drops in performance in some regions of  ", "Subsections": [{"Section_Num": "3_3", "Section": "3.3 Linear Stochastic Bandits", "Text": "here, we provide a quick overview of linear stochastic bandits since they play an important role as solutions to the active learning problem in section   the lsb problem is one of finding the optimal arm from a given set of arms x similar to the standard multi-armed bandit problem, but with the average reward from each arm being an unknown linear function of the features associated with that ar  although it may seem restrictive to assume a linear dependence on the arms, more expressiveness can be achieved by using a feature transformer, in a manner similar to the practice for linear regressio  there have been several approaches to solving the lsb problem under various objective  one group of works are based on the principles of the upper confidence bound algorithm for mab problem  the other popular class of approaches is based on thompson samplin  an active learning framework for efficient robust policy search use active learner to learn about policy performance by collecting trajectories as necessary sample parameters according to given objective based on learned performance profile use active learner to learn about policy performance by collecting trajectories as necessary perform policy update using trajectories collected at each of these parameter  figure 1: main loop of the effacts framewor  although robust policy search is expected to require more data than standard rl, improvements to its sample efficiency are still necessary in order for it to be viable in complex real-world system  to motivate our developments to improve on the sample efficiency, we start with the observation that there is some functional dependence of the performance of a given policy on the model parameter corresponding to a task under consideratio  existing approaches all disregard this dependence when evaluating their objective, leading to increased sample complexit  the increase in sample complexity is more severe when using the cvar objective such as indue to having to discard most of the trajectories collected in order to estimate the cva  here, we wish to devise a strategy to utilize the information from the aforementioned functional dependence effectively so as to minimize such wastag 1 active learning and the effacts framework active learning is a paradigm where the agent chooses data to learn from based on its previous experience it has been used to speedup learning tasks, especially in situations with limited dat  an active learner not only needs to work with as few samples as possible, it also needs to account for the uncertainty in whatever data it has collecte  thus, quite clearly, the problem of efficiently performing such sampling is connected to active learnin  the case of active learning that is of interest to us is when the agent is allowed to sample output for arbitrary points in the input space we now outline our active learning framework for learning robust policie  to do this, an active learner sequentially picks some parameters and trajectories are sampled for each of them by setting the environment to these parameters and running the current polic  parameter selection: the second phase uses this assessment to decide which parameters from p trajectories need to be collected fo  this selection is guided by some given objective that enforces robustnes  based on this performance profile, it returns a set p of parameters at which the policy needs to be traine  the policy update is then performed using trajectories collected for each parameter in p, and this is done for a given number of iteration  any particular instantiation of effacts is defined by the choice of active learning algorithm and also the scheme used to select which parts of p to sample from,  e by specifying the learnperf and selectparams subroutine 2 applying effacts we analyze one such instantiation based on the cvar objective for parameter selectio  the use of bandit algorithms for active learning is well studied in both multi-armed bandit as well as linear stochastic bandit setting  the parameter narayanaswami e  a  spaces involved in robust rl are invariably continuou  lsbs and gaussian process regression are two well-known approaches that can perform active regression on continuous space  its arms are simply a large enough collection of parameters spread across   feedback is given to the bandit in an adversarial manner, being proportional to the negative of the return obtained on that sampled trajector  this causes it to seek out regions with low performance, and in the process learn about the performance across   this behaviour is especially useful for identifying the worst-case parameters that are used in optimizing the cvar objectiv  we call this algorithm effacts-c-b and its learnperf and selectparams subroutines described in algorithm  3 sample efficiency due to the fact that effacts-c-b strategically chooses parameters at which to collect trajectories, we expect it to be able to maintain robustness and performance while collecting fewer samples than existing approache  we later show that efficiency gains of this order are indeed attainable in practic  5 connections to multi-task learning the problem of robust policy search on an ensemble of models can also be viewed as a form of transfer learning from simulated domains to an unseen real domain further, the process of learning from an ensemble of models can be viewed as a multi-task learning problem with the set of tasks corresponding to the set of parameters that constitute the source domain distributio  learning a robust policy corresponds to maintaining performance across this entire set of tasks, as is usually the goal in mtl setting  mtl, which is closely related to transfer learning, has been studied in the drl context in a number of recent works however, these works consider only discrete and finite task sets, whereas model parameters form a continuu  has employed a bandit based active sampling approach similar to what we have described here to intelligently sample tasks to train on for each iteratio  the feedback to the bandit is also given in an adversarial manne  however, we note several differences when it comes to a parameterized mtl settin  the first is the functional dependence of the task performance to an underlying parameter as discussed earlie  in the discrete mtl settings usually studiedthere is no such visible dependency that can be modele  this means that any algorithm that is adapted to the parameterized setting need to be reworked to utilize such dependencies as effacts doe  the task selection procedure in effacts differs from the one in in that the performance is sampled for several tasks in between iterations of policy trainin  additionally, one single task is not chosen in the end, rather the active learner is used to inform the selection of a group of tasks as necessary for the given objectiv ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 Active Learning for Efficient Trajectory Sampling", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Connections to Multi-Task Learning", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "6", "Section": "6 Experiments", "Text": "as epopt uses the cvar objective, it is a very suitable baseline against which to compare effacts-c-b for demonstrating the benefits of introducing the active learne  we conduct experiments to answer the following questions, which we will reference as rq1, rq2 et 1 implementation details and hyperparameters the experiments are performed on the standard hopper and halfcheetah continuous control tasks available in openai gymsimulated with the mujoco physics simulator the values given here specify the means and standard deviations for normal distributions truncated at the low and high points mentioned her  this is equivalent to the probability density of a normal distribution with parameters being zeroed outside the intervaland being normalized so that it integrates to  1 for the hopper and half-cheetah task  the bands indicate the confidence intervals of the performance measured across 5 runs of the entire training procedur  mass, friction with the ground, foot joint damping and joint inertia  we also use the same statistics for the source distributions of the parameters which are described in table   we emphasize that because we are using the same environments in and also the same policy parameterization, results reported there can be used directly to compare against effacts-c-  we use the hyperparameter settings for trpo suggested in openai baselines on which our implementation is also base  these are shown in table   one difference from epopt's implementation is that we use generalized advantage estimation instead of subtracting a baseline from the value functio  for value function estimation narayanaswami e  a  hyperparameters median %tile avg %tile st  de 1, which are measured every fifth iteration from the 100th to the 150th iteration  hyperparameter value timesteps per batch 1024 max kl   policies are parameterized with nns and have two hidden layers with 64 units each, and use tanh as the activation functio ", "Subsections": [{"Section_Num": "6_2", "Section": "6.2 Bandit Algorithm", "Text": "for all our experiments, we implement the bandit learner using thompson sampling due to its simplicity, following the version described in the hyperparameters introduced by this are as in table 4 the arms of the bandit are model parameters taken uniformly across the domain and converted to feature value  in order to allow for some degree of expressiveness for the fit, we apply polynomial transformations of some particular degree to the model parameter  the features input to the bandit are 4th degree polynomial terms generated from the model parameter  this amounts to 5 terms in the 1-d case and 15 in the 2-d cas  these arm representations are then standardized before being used by the bandi  the negative returns given as feedback to the bandit are scaled by a factor of 10- 3 performance and robustness in this section, we perform the following experiment to evaluate effacts-c-b for the objectives of rq  in this experiment, only one environment parameter is varied, creating a 1-d model ensemble on which to evaluate the algorith  the torso mass is varied in both the hopper and the half-cheetah domains keeping the rest of the parameters fixed at their mean value  the performance of the effacts-c-b learned policy is then tested across this range, and the results are shown in figure   we use trust region policy optimization for batch policy optimization and run it for 150 iteration  for this part, we use 4th degree polynomial transformation  this setting samples the least number of trajectories per iteration, 30, which is just one eighth of the 240 drawn in epop  although there is one region where it is unstable, it is still able to maintain its performance everywhere else, thus attaining the same level of robustnes as epop  further, the performance achieved is almost as good as, and possibly better than epop  the other two settings which perform almost as well in both tasks collect 45 each, which amounts to an even larger reduction of 8 5% while still retaining performance and robustnes  in this experiment, we vary the friction with the ground in addition to the torso mass, thus creating a two dimensional ensemble of parameter  here, we run trpo for 200 iterations, and again use 4th degree polynomial transformation  figure 3 shows the results obtaine  full performance is maintained over almost all of the parameter space in both domains, again being comparable to or better than in1 for one run or effacts-c-  collected trajectories is obtained even in a higher dimensional model ensembl  this is also despite the added challenge of an increased number of parameters for the bandit to fit", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "6_5", "Section": "6.5 Visualizing the Bandit Active Learner", "Text": "in figure 4, we present the outcome from one particular iteration of trainin  the true performance profile is estimated by collecting 100 trajectories to calculate the mean return at each parameter we see that the bandit takes exploratory actions that don't lead to the worst return  however, it quickly moves towards the region with low returns, and the final fit is close to the true mean performanc 1 percentile according to the source distributio  thus, we cannot reuse these to perform learning with the cvar objectiv  we also note that a perfectly learned performance profile is not necessary to sample from the worst trajectorie  as long as the fit is reasonably accurate in that region, the output trajectories will be of good qualit  in practice, we expect lsb algorithms to be capable of doing this as they tend to focus on these region  individual trajectories are shown as dots or crosses, with the position indicating the parameter value and the return obtaine  of trajectorie  for this, we first evaluate the average return for a batch of samples from p by collecting a large number of trajectories at each paramete  we note that these trajectories are solely for the purpose of analysis and are not used to perform any learnin  then, the percentile of the trajectory deemed to have the greatest return among those chosen based on the bandit learner is computed using the returns in this batch by using a nearest-neighbor approximatio  this is done across several iterations during the training, and the median percentiles along with other statistics are reported in table 2 for the hopper tas  particularly, we investigate the use of a modified version of the thompson sampling algorithm above that is suited for nonstationary scenario  this is done in order to reuse performance history from previous iterations to estimate the bandit's parameters with the aim of achieving a further reduction in sample complexit  we compare this version with the original setup in figure   we see that the setting with the smallest number of trajectories narayanaswami e  a 25 was used, with the other hyperparameters remaining unchange ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "6_8", "Section": "6.8 Other Remarks", "Text": "we note that we do not perform pre-training as in epopt where the entire batch of trajectories is used for policy learning for some iterations at the beginning this has been reported to be necessary, possibly due to problems with initial exploration if using the cvar objective from the beginnin  effacts-c-b on the other hand works without any such ste  this is because algorithms like trpo have been known to require at least that much data per iteratio  7 conclusions and further possibilities we developed the effacts framework for using active learning to make an informed selection of model parameters based on agent performance, which can subsequently be used to judiciously generate trajectories for robust r  with an illustration of this framework based on linear bandits and the cvar objective, we have both demonstrated its applicability for robust policy search as well as established its effectiveness in reducing sample complexity by way of empirical evaluations on standard continuous control domain  we also discussed our work in the context of multi-task learning along with the similarities and differences between these setting  our work opens up requirements for active learning algorithms that can work well with even lesser data than we need her  methods like gaussian process regression are known to be efficient, but not in high dimensional space  another possibility for robust policy search itself is to develop objectives that can speedup learning as well as make use of the features of effacts to maintain sample efficienc  with our interpretation of robust policy search as a parameterized version of multi-task learning, a natural next step would be to adapt developments in the usual discrete mtl setting to robust policy searc  it would also be worthwhile to similarly investigate the applicability of meta learning, as it would prove useful for both dealing with large disparities between the source domains and the real world, as well as coping with unmodeled dynamics", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "7", "Section": "7 Conclusions and Further Possibilities", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "8", "Section": "8 Acknowledgments", "Text": "the authors thank the robert bosch center for data science and ai, iit madras for funding this work and providing the requisite computing resource  we also thank aravind rajeswaran for valuable pointers and suggestions, and openai for their excellent codebase of deep rl algorithm ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "References", "Section": "References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1. Introduction", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2. \u00c9nonc\u00e9 pr\u00e9cis du r\u00e9sultat", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3. D\u00e9monstrations du \u00e9nonc\u00e9", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "R\u00e9f\u00e9rences", "Section": "R\u00e9f\u00e9rences", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": " digital object identifier 1 1109/acces  sait in particular, convolution neural networks have demonstrated their effectiveness in image detection and recognition application  however, they require intensive cpu operations and memory bandwidth that make general cpus fail to achieve desired performance level  more precisely, fpgas have been recently adopted for accelerating the implementation of deep learning networks due to their ability to maximize parallelism as well as due to their energy efficienc  in this paper, we review recent existing techniques for accelerating deep learning networks on fpga  we highlight the key features employed by the various techniques for improving the acceleration performanc  in addition, we provide recommendations for enhancing the utilization of fpgas for cnns acceleratio  the techniques investigated in this paper represent the recent trends in fpga-based accelerators of deep learning network  thus, this review is expected to direct the future advances on efficient hardware accelerators and to be useful for deep learning researcher ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "I", "Section": "I Introduction", "Text": " the field of dl emerged in 2006 after a long pause in the area of neural networks research a key aspect in dl is that the networks and/or their weights are not designed by human being  instead, they are learned from data using a general purpose learning procedurein dl, each layer is designed to detect features at different level 00121v1 1 jan 2019 shawahna et a : fpga-based accelerators of deep learning networks for learning and classification: a review level the output of the first layer becomes input to the second layer which produces higher level features, for example semi-circles, and squares the next layer assembles the output of the previous layer to parts of familiar objects, and a subsequent layer detects the object  as we go through more layers, the network yields an activation map that represents more and more complex feature  the deeper you go into the network, the filters begin to be more responsive to a larger region of the pixel spac  higher level layers amplify aspects of the received inputs that are important for discrimination and suppress irrelevant variation    this has been primarily possible due to the excellent ability of deep learning in discovering intricate structures in high-dimensional dat  emergence of deep learning networks convolutional neural networks are considered as one of the most influential innovations in the field of computer vision the success of deep learning networks grew to prominence in 2012 when krizhevsky et a  utilized cnns to win the annual olympics of computer vision, imagenet large-scale vision recognition challenge imagenet is a standard benchmark dataset used to evaluate the performance figure   imagenet competition results of object detection and image classification algorithm  it consists of millions of different images distributed over tens of thousands of object classe  cnns have achieved even better accuracy in classification and various computer vision task  the classification accuracy in ilsvrc improved to 8  fi  1 shows the accuracy loss for the winners of imagenet competitions before and after the emergence of deep learning algorithm  thereafter, large host companies started using cnns at the core of their service  however, the classic use-case of cnns is for image and speech processing a typical cnn is a multi-layered feed-forward ann with a pipeline-like architectur  specifically, each layer performs a well-known computation on the outputs of the previous layer to generate the inputs for the next laye  images, audio files, and recorded videos are examples of the input data to be classified using cnn  on the other hand, the network weights are the data generated from training the cnn on a dataset containing similar inputs to the one being teste  hardware acceleration of deep learning networks to provide more accurate results as well as real-time object recognition, for example in applications such as robots and auto-piloted cars, the size of the convolution neural network needs to be increased by adding more neural network layers however, evolving more and new type of nn layers results in more complex cnn structures as well as high depth cnn model : fpga-based accelerators of deep learning networks for learning and classification: a review a computational challenge for general purpose processors in practice, cnns are trained off-line using the backpropagation process then, the off-line trained cnns are used to perform recognition tasks using the feed-forward process therefore, the speed of feed-forward process is what matter  gpus are the most widely used hardware accelerators for improving both training and classification processes in cnns this is due to their high memory bandwidth and throughput as they are highly efficient in floating-point matrix-based operations -.  however, gpu accelerators consume a large amount of powe  therefore, their use in cnn-based applications implemented as a cloud service on large servers or in battery operated devices becomes a challeng  furthermore, gpus gain their performance from their ability to process a large image batch in paralle  for some applications like a video stream, input images should be processed frame by frame as the latency of the result of each frame is critical to the application's performanc  for some tracking algorithms, the result of one frame affects the process of the next frame nurvitadhi et a  recently evaluated emerging dnn algorithms on latest generations of gpus and fpgas the experimental results show that current trends in deep neural networks favor fpga platforms as they offer higher power efficiency fpga and asic hardware accelerators have relatively limited memory, i/o bandwidths, and computing resources compared with gpu-based accelerator  however, they can achieve at least moderate performance with lower power consumption the throughput of asic design can be improved by customizing memory hierarchy and assigning dedicated resources as an alternative, fpga-based accelerators are currently in use to provide high throughput at a reasonable price with low power consumption and reconfigurabilityconvolutional neural networks have a very useful property, that is, each feature map neuron shares its weights with all other neurons the authors inproved that the highest energy expense results from accessing the off-chip dram memory for data movement rather than computatio  in other words, the energy cost of the increased memory accesses and data movement due to the large number of cnn operations often exceeds the energy cost of computationthus, cnn accelerators need to carefully consider this to achieve efficient architecture in terms of time and powe  in this paper, we review the current status of using fpgas as accelerators for implementing deep learning network  we highlight the implementation challenges and design directions used to tackle those challenge  we also provide future recommendations to maximize the performance of fpgas as accelerators for deep learning networks and simplify their us  the remainder of the paper is organized as follow  section ii provides background information about cnns, their key operations, and some well-known deep learning network  in addition, it introduces the basic structure of fpgas and highlights their features enabling them to accelerate computationally intensive application  it also discusses the implementation challenges of deep learning networks on fpgas and how these challenges can be overcom  section iii reviews existing cnns compression techniques and presents the current status of accelerating deep learning networks using asic-based and fpga-based accelerator  section iv describes the use of metaheuristics in the design and optimization of cnns implementatio  section v summarizes existing design approaches for accelerating deep learning networks and provides recommendations for future directions that will simplify the use of fpga-based accelerators and enhance their performanc  finally, section vi concludes the pape ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "II", "Section": "II Background and Terminology", "Text": "this section gives an overview of the key operations and terminology used in convolutional neural networks and provides examples of well-known deep learning network  in addition, it illustrates the basic structure of field programmable gate arrays and how deep learning methods can benefit from the capabilities of fpga  the last subsection highlights the challenges of implementing deep learning networks on fpga    the filter is an array of numbers called weights or parameter  these weights are computed during the training phas : fpga-based accelerators of deep learning networks for learning and classification: a review single numbe  the inputs and outputs of the conv layer are a series of fm array  the sliding of the filter window and performing the operation is known by the verb convolving, hence the noun convolutioneach outputinput fm pair has a set of weights equal to the kernel size and each output fm is computed based on the sum of the convolution operations performed on all input fm  note that different conv layers in the same cnn model vary considerably in their size  loop unrolling maximizes the parallelism of conv macs computation which requires a special consideration of processing elements and register arrays architectur  fi  2 illustrates the loop unrolling of conv loops level  2) activation functions activation function in neural networks is similar to action potential in animal cells such as neuron  a neuron is said to fire if it emits an action potentia  another popular activation function is fpxq tanhpxq the above standard sigmoid and tanh non-linear functions require long training time than standard sigmoid and tanh function  it detects high frequency features with a large respons  if we normalize around the local neighborhood of the excited neuron, it becomes even more sensitive as compared to its neighbor  at the same time, it will dampen the responses that are uniformly large in any given local neighborhoo  if all the values are large, then normalizing those values will diminish all of the  so, basically it performs some kind of inhibition and boosts the neurons with relatively larger activation  normalization can be done within the same feature or across neighboring features by a factor that depends on the neighboring neuron  expressions to compute the response normalized activity can be found inpooling layers are periodically inserted in between successive convolutional layer  they operate independently on every depth slice of the input and resize it spatially using the max operatio  alexnet cnn architecture addition to the popular max pooling, the pooling units in some cnns are also used to perform other functions, such as avg and min operations the last fully connected layer is the classification layer and it holds the output such as the class scores examples of deep learning networks we list in this subsection some of the well-known deep learning network  each convolutional layer performs the activation function using rel  the architecture of alexnet cnn is shown in fi  the exact number of conv layers in each group depends on the version of the vgg, visual geometry group, mode  table 1 shows the number of conv and fc layers for the most commonly used vgg model  unlike alexnet and vgg models where the layers are connected in sequence, the interconnections in resnet layers are in the form of a directed acyclic graph resnet50 and resnet-152 are widely used, especially for image classificatio  field programmable gate arrays fpgas are off-the-shelf programmable devices that provide a flexible platform for implementing custom hardware functionality at a low development cos  fi  4 shows a basic structure of an fpg  fpgas are widely considered as accelerators for computationally-intensive applications as they enable models with highly flexible fine-grained parallelism and astable   cnn layers for vgg model  fpga basic structure sociative operations such as broadcast and collective response fpgas have the advantage of maximizing performance per watt of power consumption, reducing costs for large scale operations this makes them an excellent choice as accelerators for battery operated devices and in cloud services on large server  fpgas have recently been widely used for deep learning acceleration given the flexibility in implementing architectures with large degree of parallelism resulting in high execution speeds the adoption of software-level programming models such as the open computing language standardin fpga tools made them more attractive to use for deep learningin addition, the feed-forward nature of deep learning algorithms makes fpgas offer a clear advantage as they can create customized hardware circuits that are deeply pipelined and inherently multithreaded fpgas also have the capability of partial dynamic configuration, which allows part of the fpga to be reconfigured while the rest is being use  this could be of potential benefit to deep learning methods where the next layer could be reconfigured while the current layer is being use  for example, alexnet cnn has over 60 million model parameters which need 250mb of memory for storing the weights based on 32-bit floating-point representation as well as requires around  5 billion operations for each input image this large amount of storage required is not supported by existing commercial fpgas and hence the weights have to be stored on external memory and transferred to the fpga during computatio  without careful implementation of deep learning networks and maximizing resource sharing, the implementation may not fit on fpgas due to limited logic resource  the problem exacerbates with more complex models such as vgg cnn model which have 16 layer  for example, the vgg-16 cnn model has 138 million weights and needs over 30 gops although the current trends in implementing cnns is going toward compressing the entire cnn model with dramatically reducing data bit-widthit is expected that future cnn models will get more complex with larger number of layers as the amount of training data continues to grow and the problems to be solved get more comple  in addition, different layers in cnns have different characteristics which result in different parallelism and memory access requirement  different layers in a cnn network exhibit vastly different amounts of intra-output and inter-output parallelism intra-output parallelism parallelizes the computation of a single output image since it is the sum of n input-kernel convolution  however, inter-output parallelism is based on computing multiple output fms in paralle  furthermore, convolutional layers are computational-centric while fully connected layers are memory centric however, the number of operations in fully connected layers are in the order of  21 gops, while the number of weights are on the order of   thus, the developed cnn accelerator must be designed carefully to meet the varying requirements of different layers and needs to be flexible to maximize the performance for each cnn laye  as technology advances, fpgas continue to grow in size and capabilitie  it is crucial to have some mechanisms for addressing the requirements for efficient implementations of deep learning network  addressing hardware resource limitations requires reuse of computational resources, and storing of partial results in internal memorie  data transfer and computational resource usage are significantly impacted by the ordering of operations and selection of parallelism in the implementation of cnns on fpga  careful scheduling of operations can result in significant reduction in external memory access and internal buffer size  external memory bandwidth requirements can be also decreased by using reduced precision for representing the weights with minimal impact on solution quality, which also results in a better energy efficienc  in addition, the number of external memory accesses can be reduced by utilizing on-chip memory and exploiting data reus  furthermore, the large number of weights in the fully connected layer can be reduced, based on utilizing singular value decomposition with a small impact on accurac : fpga-based accelerators of deep learning networks for learning and classification: a review review various design approaches used to cope with those challenges for implementing deep learning network  ii  acceleration of deep learning networks: current status in this section, we will start by covering convolutional neural networks compression techniques as they have a significant impact on the implementation complexity of cnn  cnns compression techniques target the minimization of the number of operations and the memory footprint with minimal impact on accurac  then, we discuss hardware acceleration techniques for deep learning algorithms and cnns based on both application specific integrated circuit and field programmable gate array implementation    cnns compression in this subsection, we review techniques that target the compression of cnns which results in significantly reducing their implementation complexity with minimal impact on accurac  denton et a  proposed a technique to reduce the memory footprint for the network weights in object recognition system  they used singular value decomposition and filter clustering methods for this purpos  the results for convolutional model of 15 layers in show that the proposed technique speeds up the operations in convolutional layers by a factor of 2, compared to cpu eigen3-based library implementation in another work, han et a  employed network pruning techniques - to reduce the over-fitting and complexity of neural network model  in a subsequent work, han et a  proposed a deep compression technique for more reduction of the storage requirements of cnns through the enforcement of weights sharin  deep compression basically consists of pruning, trained weights quantization, and huffman coding pipeline stage  asic-based accelerators in this subsection, we present some recent work in the area of hardware-based accelerators an asic-based hardware accelerator referred to as diannao was designed for large-scale convolutional neural networks and deep neural network  diannao accelerates neural networks by minimizing memory transfers, which opened a new paradigm for hardware accelerator  since the weights are repeatedly used in the computations of convolution layers, frequent memory access can significantly degrade the overall performanc  therefore, the authors exploited the locality properties of neural network layers to design custom storage structures that take advantages of these propertie  in addition, they employed dedicated buffers and tiling techniques to reduce the overall external memory traffic through increasing data localit  chen et a  also observed that using short fixedpoint representation of feature maps and weights can also significantly reduce computation resources and memory footprin  they found that the area and power of a 32bit multiplier can be reduced by a factor of   the experimental results demonstrated that diannao has an average performance of 452 gops with power consumption of 485 m  the results depicted that using 16-bit arithmetic units instead of 32-bit ones introduced only  26% accuracy loss on mnist dataset on the other hand, the scalability and efficiency of diannao accelerator are severely limited by the bandwidth constraints of the memory syste  in a related research work, chen et a proposed dadiannao multi-chip supercomputer which offers sufficient memory capacity suitable for on-chip storage of all weights in cnn  this system is mainly important for today's large-scale deployments of sophisticated industry and consumers service  dadiannao uses 16-bit fixed-point numbers in the inference process like diannao, but it is implemented using 28nm technolog  the results show that dadiannao outperforms the performance of a single gpu architecture by up to 65 01% accuracy error rate on mnist dataset for a 64-chip syste  another member of the diannao family, called pudiannaohas been designed using tsmc 65nm process to support multiple techniques and scenarios of machine learning pudiannao accelerates different ml techniques through extracting their critical locality properties and computational primitives with the use of on-chip storage as well as 7 novel functional unit  experimental results show that pudiannao is   processing element architecture in; flexflow, 2d-mapping diannao architectures have not been optimized to be used for embedded application  to improve the scalability and energy efficiency of diannao design discussed inshidiannao accelerator was proposed shidiannao is designed especially for real-time object recognition applications such as selfdriving cars, smartphones, and security using 65nm cmos technolog  the proposed accelerator directly connects with a cmos/ccd sensor in the image processing chi  in addition, all the weights of cnn layers are stored in sram on-chip memory, as the target here is small cnn model  shidiannao is embedded inside the processing chip to eliminate off-chip dram memory accesses and minimize data movements between the sram holding the cnn model and the individual processing elements from the senso  shidiannao has a power consumption of 32  moreover, shidiannao has   therefore, they cannot be efficiently adapted to different application demands in addition, asic designs have a long development cycle and lack flexibility for handling varying dl network design  similar approaches to the diannao family of techniques are presented in with similar limitation  isaac and prime have explored in-memory processing to design an acceleration architecture for neural network  the proposed isaac architecture has achieved better improvements of 1  lu et a  reviewed current accelerators that exploit the intrinsic parallelism and observed a mismatch between the parallel types supported by the computing engine and the dominant parallel types that appear in cnn workload  due to limitations of dataflow of each of the above three architectures, most existing accelerators support only one specific parallelis  systolic architectures exploit synapse parallelism2d-mapping architectures exploit neuron parallelismand tiling architectures exploit feature map parallelism with three components that can be either left serial, or parallelized, we get 23 possible combination  to address the above problem, and support all possible processing styles, lu et a  proposed a flexible dataflow architecture, called flexflow, with minimal control  flexflow supports all types of data paths in each type of parallelism in different layers efficientl  pes are arranged in rows where each row can complete one convolution and serve one output neuro  the adders in each pe row are connected to form the adder tre  fi  5 illustrates the proposed pe in flexflow and that in 2d-mapping architectur  by eliminating dependency between adjacent pes, the proposed convolutional unit supports the comprehensive mfmnms parallelism  to cater to different types of parallelisms, they also proposed a hierarchical dataflow with high data routability and low control overhea  they also presented a method to determine parallelization type and degree for each conv laye  comparison was made with three typical architectures using six practical workloads, including alexnet and vg  they also examined the scalability of flexflow in terms of resource utilization, power, and area with growing scales of computing engin  in terms of performance, flexflow demonstrated over 420 gops performance with 1 ghz working frequenc  it also outperformed others in terms of data reusability and power efficienc  fpga-based accelerators in this subsection, we will review recent techniques employing fpgas for the acceleration of deep learning network  for each reviewed technique, we will highlight the key features utilized to maximize performance and throughput in the acceleration proces  fpga implementations of cnns appeared in the mid1990's when cloutier et a  designed the virtual image processor on altera epf81500 fpg  vip is a single-instruction stream multiple-data streams multiprocessor architecture with a 2d torus connection topology of processing elements vip improves the performance through the use of low-accuracy arithmetic to avoid implementing full-fledged multiplier  fortunately, recent digital signal processing -oriented fpgas include large numbers of multiply-and-accumulate units which allow for extremely fast and low power cnn implementation  thereafter, fpga implementations of deep learning networks have mainly focused on accelerating the computational engine through optimizing conv layer operation  several studies in the literature - have reported fpga-based implementations of convolution operatio  farabet et a  presented an fpga implementation of cnn that uses one dedicated hardware convolver and a softprocessor for data processing and controlling, respectivel  the proposed implementation is referred to as convolutional network processor cnp exploits the parallelism of conv layers to accelerate the computational engine of cnns while fully utilizing the large number of dsps, the mac hardware units on fpg  the proposed architecture consists of virtex4 sx35 fpga platform and external memor  the authors designed a dedicated hardware interface with the external memory to allow 8 simultaneous read/write accesses transparentl  in addition, they used first in first figure   out buffers between the fpga and the external memory chip in both directions to guarantee the steadiness of dataflo  the vector arithmetic and logic unit in cnp implements 2d conv, pooling, and non-linear activation function operations of convolutional network  the implementation of 2d conv with kernel of size 3 is shown in fi  it can be seen that the proposed convolutional module accomplishes k2 mac operations simultaneously in each clock cycl  cnp represents fms and weights using 16-bit fixed-point forma  the proposed accelerator has been implemented for a face detection system with lenet-5 architecture in addition, cnp consumed less than 15 watts of powe  sankaradas et a  proposed a massively parallel coprocessor to accelerate cnns using virtex5 lx330t fpga platfor  the proposed accelerator mainly focused on optimizing computation engine by employing the parallelism within convolution kernel and fm  each vpe consists of multiplier-accumulator and programmable register units to hold kernel weights and fm dat  to hold the massive intermediate data of cnns, the authors employed a dedicated off-chip memory with a large bandwidth on the coprocessor car  moreover, the proposed accelerator uses a low precision data representation feature with memory packing to further improve the memory bandwidth as well as the throughpu  20-bit and 16-bit fixed-point representations were utilized for kernel weights and fms, respectivel  maple processing core architecture the authors examined their architecture on cnn with 4 conv layers and without any fully connected layer for a face recognition applicatio 2 ghz amd opteron processor with less than 11 watts of power dissipatio  however, the proposed accelerator cannot be used to accelerate full cnns as it uses few conv layers without any fc laye  a full cnn model consists of both conv layers and fc layer  thus, an efficient cnn accelerator for real-life applications is needed to consider bot  similar approaches to the work of sankardas et a  are presented into accelerate support vector machines maple is a programmable fpga prototype system presented to accelerate both learning and classification tasks in applications with unstructured large amount of dat  the authors analyzed five workload domains to help in designing mapl  they found that their computations can be structured as parallel streams of vector or matrix operation  thus, they architected maple as a 2d grid of vector processing elements as shown in fi  in this way, matrix multiplication is accomplished by streaming the multiplicand matrix rows through the pes where each pe performs a mac operatio  the pes are organized in clusters, where each group is served by a separate memory bank of the banked off-chip memories, which create independent streams for processormemory computatio  moreover, maple uses on-chip smart memory blocks to process the large intermediate data on-the-fly using inmemory processin  fi  8 shows the architecture of the smart memory bloc  to illustrate the idea of on-the-fly in-memory processing, lets consider finding the maximum k element  the filter compares the input data with the figure   maple smart memory block threshold value if the input value is greater than val, it updates the list by replacing val at address addr with the input valu  then, the scanner searches for the new minimum value in the list and updates the threshold val and addr accordingl  it should be mentioned here that the employment of in-memory processing reduced the off-chip memory traffic by   maple prototype has been implemented on virtex5 sx240t platform running at 125mh 3 ghz nvidia tesla c870 gpu implementatio  chakradhar et a  proposed a dynamically configurable cnn architecture on fpg  the coprocessor is designed such that it automatically configures the software and the hardware elements to fully exploit the parallelism at the workload leve  dc-cnn is responsible for executing cnn applications and its architecture is shown in fi  to determine the best feasible combination for each layer, the system analyzes the workload using integer factorization techniques because it is considered fast for small numbersdynamic programming is also used to quickly prune infeasible combination 35 ghz nvidia's gpu implementatio  the results show that dccnn achieved   it is worth mentioning that dc-cnn is the first architecture that achieves a performance suitable for real-time processing for video streaming as it processes up to 30 frames per secon  in addition, dccnn is more energy-efficient than the gpu implementation as it consumes 14 watts, while more than 150 watts are consumed by the gp  on the other hand, the authors modeled their architecture on a cnn with 3 conv layers only without any fc layer which makes it unsuitable for today's other real-life application  a second-generation of cnp architecture has been proposed in by designing a stream processor syste  the proposed design replaces the dedicated hardware convolver in cnp with multiple parallel vector processing units, named as alus, laid out in a 2d gri  each alu is composed of four local routers, one global router, and a streaming operato  the local routers are used to stream data to/from the neighbor  streaming data to and from global data line is done through the global route  the streaming operators in the alu are fully pipelined to produce a result per clock cycle as described in with the use of q 8 coding to represent fms and weight  the proposed system also uses a multi-port direct memory access streaming engine to allow individual streams of data to operate seamlessly within processing block  the results show that the proposed stream processor system can run small cnns at up to 30 fps while consuming about 15 watt  an improved version of cnp architectures given inwas presented in and referred to as neuflo  particularly, neuflow has replaced the 2d grid of alus with a 2d grid of processing tiles each pt consists of local operators and a routing multiplexer the top three pts have been implemented to perform mac operatio  general-purpose operations, such as dividing and squaring, figure 1  the architecture of neuflow have been implemented at the middle three pt  therefore, the middle row of neuflow can be used for normalizatio  finally, neuflow's bottom pts row implements non-linear operation  moreover, each operator employed input and output fifos to stall its pipeline when require  on the other hand, pt's mux is used to connect its local operators with the neighboring pt's streaming operators and off-chip memory instead of the used local routers and global router discussed in neuflow uses a dataflow compiler, named luaflow, to translate a high-level flow-graph representation of cnns in torch5 into hdl scripts with different levels of parallelis  in addition, luaflow produces a binary code configuration file and holds it in the embedded control uni  thereafter, the control unit configures the 2d grid of pts and the dma ports through run-time configuration buse  a smart memory module has been designed to support multiple asynchronous accesses of off-chip memory through its reconfigurable port  peemen et a  utilized the flexible off-chip memory hierarchy method to design a configurable memory-centric accelerator template for a variety of models of cnn  this accelerator exploits data reuse in complex access patterns to reduce off-chip memory communication, which minimizes the bandwidth requirement  the memory-centric accelerator maximizes the efficiency of on-chip memories for better data locality using loop transformation and block ram -based multibank on-chip buffers at the same time, it minimizes the size of fpga on-chip memories to optimize energy and area usage, which are key requirements for embedded platform  the memory-centric accelerator uses a simd cluster of mac pes with flexible reuse buffers to accelerate the conv laye  the acceleration template has been implemented on virtex6 fpga : fpga-based accelerators of deep learning networks for learning and classification: a review cessor has been utilized to configure and communicate with the accelerator via fifo-based fast simplex link the proposed accelerator has been analyzed for a cnn vision task of size   neural network next is a real-time systemon-chip computing system for deep learning networks on mobile device  the architecture of nn-x consists of a host processor, a co-processor, and external memor  the co-processor accelerates the learning networks by parallelizing their operations throughout arrays of configurable processing elements referred to as collection  each collection contains one convolution engine, one pooling module, and one non-linear operato  the conv engine accelerates the conv operation by fully pipelining the incoming data with the use of cache memorie  the collections are able to communicate with one another using the collection route component to achieve cascaded pipelining, which results in reducing accesses to external memor  the data transfer between the collections and the external memory is accomplished throughout the co-processor full-duplex memory router, which provides independent data stream  eight collections have been employed to achieve large parallelis  zhang et a  proposed a roofline-based model to accelerate convolutional neural networks on fpga  the roofline model is an intuitive visual performance model used to relate the attainable performance to the peak performance that can be provided by the hardware platform and the off-chip memory traffic the focus in their work is primarily on accelerating the convolutional layers as it consumes more than 90% of the computational time during the prediction process in doing so, the authors optimized both the computation operations and the memory access operations in convolutional layer  they considered a cnn application composed of five convolutional layers that won the imagenet competition in 2012 the proposed accelerator uses polyhedral-based data dependence analysis to fully utilize all fpga computational resources through loop unrolling, loop pipelining, and loop tile size enumeratio  note that loop unrolling maximizes the parallel computation of conv mac operation  on the other hand, local memory promotion and loop transformation are used to reduce redundant communication operations and to maximize the data sharing/reuse, respectivel  subsequently, the roofline performance model is used to identify the optimal design from all possible solutions in the design spac  zhang et a  accelerator architectur  for the output fms and input fms, respectivel  however, designing a cnn accelerator with different unrolling factors to each convolutional layer is challengin  therefore, the proposed architecture enumerates all possible valid designs to find uniform cross-layer unrolling factor  thereafter, the hardware accelerator is implemented based on the crosslayer optimal unrolling factor  the proposed accelerator composed of a computational engine and memory sub-system is depicted in fi  the computation engine is designed as tm duplicated treeshaped poly structures with tn inputs from the input fms, tn inputs from the weights, and one input from the bia  to overlap data transfer with computation, on-chip buffers are operated in a ping-pong manne  in addition, two independent channels are implemented for load and offload operations to increase the bandwidth utilizatio  moreover, microblaze processor is used to send configuration parameters and commands for the accelerator over axi4lite bu  the cnn accelerator communicates with external data transfer engines through fifo interfaces, where the data transfer engines are used to access ddr3 dram memory through axi4 bu  the accelerator is designed using vivado 201 4 high level synthesis tool and implemented on xilinx vc707 fpga board clocked at 100 mh  the experimental results depict that the proposed implementation achieves a peak performance of 6 62 gflops as well as a 1  in addition to this, the results show that the proposed fpga architecture is 2  the proposed implementation has some limitations such as designing the accelerator with new cross-layer unrolling factors for different architectures of cnn  fur12 volume 4, 2018 shawahna et a : fpga-based accelerators of deep learning networks for learning and classification: a review figure 1  top-level archeticture of microsoft cnn accelerator thermore, using the cnn accelerator with uniform unrolling factors might be sub-optimal for some conv layers, which affects the overall performanc  a year later, ovtcharov et a  at microsoft research utilized catapult hardware infrastructure, a dual-socket xeon server equipped with stratix-v gsmd5 fpga, to design a specialized hardware for accelerating the forward propagation of deep cnns in a power-constrained data cente  the top-level architecture of the proposed cnn accelerator is shown in fi  multi-banked input buffer and kernel weight buffer are used to provide an efficient buffering scheme of fms and weights, respectivel  to minimize the off-chip memory traffic, a specialized network on-chip has been designed to re-distribute the output fms on the multi-banked input buffer instead of transferring them to the external memor  the 3d convolution operations and other cnn operations are independently performed using spatially distributed scalable vectors of pe  the controller engine is responsible for streaming and data delivery of multi-banked input buffer and kernel weight buffer data to each of the pe vector  in addition, it supports configuring multiple cnn layers at run-tim  qiu et a  proposed an fpga design to accelerate cnns for a large-scale image classification challenge on embedded system  the focus was on accelerating both conv and fc layers, since they are considered as the most computational-centric and the most memory-centric operations in cnns, respectivel  the proposed accelerator reduces the resource consumption using specific design of convolver hardware modul  in addition, the authors applied singular value decomposition to the weight matrix of fc layer in order to reduce memory footprint at this layer to further reduce memory footprint and bandwidth requirement of cnn, they proposed a dynamicprecision data quantization flow componen  this component is responsible for finding the optimal fractional length for weights in each layer as well as the optimal fractional length for fms in adjacent layers, while achieving minimal accuracy los  then, it converts the floating-point numbers representing weights and fms into fixed-point number  in addition, the authors proposed a data arrangement scheme that maximizes the burst length of each transaction to the external memory to accelerate conv and fc layers, as well as to avoid unnecessary access latenc  note that maximizing the dram burst length raises up the effective dram bandwidththe proposed architecture consists of a processing system and programmable logic cnn computations are performed through special design of processing element modules in fpg  the convolver complex is designed as a classical line bufferas shown in fi  14, to achieve convolution operations as well as to compute fc layer multiplication of matrix-vecto  the pooling layer implemented in the maxpooling module is used to output the maximum value in the input data stream with a window of size   the activation function of cnn is applied to the input data stream using the non-linearity modul  the adder tree accumulates the partial sums generated from the convolver  finally, data shift and bias shift modules are responsible for accomplishing dynamic quantizatio  the proposed embedded fpga platform has been implemented using vgg-16-svd network with 16-bit fixedfigure 1  processing element module of qiu et a  embedded accelerator architectur : fpga-based accelerators of deep learning networks for learning and classification: a review figure 1  convolver complex design of qiu et a  embedded accelerator architectur  point numbers on zynq xc7z045 platfor  the results demonstrate that applying svd technique reduces memory footprint of fc layer by 8 04% while introducing an accuracy loss of only   finally, the overall performance of the proposed accelerator reported is 13 66% and a total power consumption of   deepburning is an fpga-based neural network design automation too  it allows for building learning accelerators for specific nn with optimized performance and custom design parameters configuration using a preconstructed register transfer level module librar  the rtl library holds the hardware descriptive scripts for nn reconfigurable components as well as their configuration script  in addition, it contains other rtl building blocks for logical and arithmetic operations such as the connection box and approximate lookup table in order to design an optimized hardware, deepburning compresses the passed nn model to the greatest extent using temporal and spatial folding which helps also in satisfying the resource constraints and minimizing the required hardware module  in addition, the deepburning compiler investigates the accelerator on-chip memory size and throughput to properly tile and partition the nn weights and feature data layout  moreover, deepburning uses the address flow component to automatically fetch and store off-chip memory and on-chip memory dat  the authors compared the performance of deepburning with that inconsidering alexnet cnn model, as they both operate at 100 mh  they considered a high budget resources constrained deepburning on zynq-7045 devic  the results show that deepburning is   an opencl-based optimization framework to accelerate large-scale convolutional neural network models was proposed by suda et a  subsequently, they analytically and empirically modeled the execution time for each layer as a function of the above mentioned variable  then, genetic algorithm was used to explore the design space for finding the optimal combination of the key design variables considering the resources constraint  the authors implemented the scalable conv block in a similar fashion to that in as a matrix multiplication by flattening and on-the-fly rearrangement of the feature dat  the opencl software has been utilized in their work due to its parallel programming model as well as its ability to integrate the compiled rtl design with external memory interfacing ipswhich uses memory coalescing technique with complex load and store unit  in addition, it has optimized matrix multiplication and cpufpga communication librariesthe framework is used on both vgg-16 and alexnet cnn models which are implemented on p395-d8 and de5-net fpga boards with fixed-point operations according to their precision stud  they compared the proposed implementation with  3 ghz core i5-4590 cpu implementation that uses caffe tool with atlas optimized library for matrix/vector operation  the results show that the opencl optimized framework on p395d8 achieved   zhang et a analyzed the transformation of conv and fc layers to regular matrix multiplication presented in prior work to address this problem and improve the bandwidth utilization, they designed a uniformed matrix multiplication kernel that uses either input-major mapping or weight-major mapping techniques while computing fc laye  in imm, the designed kernel batches a group of different input fms together, and then performs the matrix multiplicatio  imm technique improves the data reuse of fc weight : fpga-based accelerators of deep learning networks for learning and classification: a review is much larger than the input fm matri  in particular, it loads input fm matrix to a weight buffer and loads weight matrix to input fm buffe  subsequently, a regular matrix multiplication is performed on these matrice  as a result, wmm may allow for a higher data reuse than imm, especially for input fms that can be reused multiple times considering the limited hardware resource  for the above, the roofline model was applied to identify the optimal mapping technique under different batch sizes and data precision  the results demonstrate that wmm is better than imm in term of data reuse and bandwidth utilization, especially in small batch sizes which is required for real-time inferenc  hence, the same matrix multiplication kernel is utilized for the computation of both conv and fc layers, but with the use of imm in conv layer and wmm in fc laye  with an easy-to-use developed tool, caffeine aids in automatically choosing the best hardware parameters, using the model files from caffe and fpga device specifications obtained from the use  caffeine fpga engine uses a highlevel synthesis -based systolic-like architecture to implement matrix multiplication kerne  it allows changing parameters such as number of pes, precision, and fm siz  caffeine further maximizes the fpga computing capability by optimizing multi-level data parallelism discussed in and pipeline parallelism using polyhedral-based optimization framework given in caffeine framework also handles the weights and biases reorganization in off-chip dram to maximize the underlying memory bandwidth utilizatio  in addition, the double-buffering technique is employed to prefetch the next data tile for each p  caffeine has been evaluated by implementing alexnet and vgg16 cnns on ultrascale ku060 and on virtex7 690t considering different precision  the vgg-16 implementation with 16bit fixed-point on ultrascale ku060 and virtex7 690t provided 4  a special case of dataflow, referred to as synchronous dataflowis a paradigm of computation that allows for representing a computing system as a streaming proble  in this way, sdf model can represent the hardware implementation of cnns using linear algebra and directed sdf graph each node of sdfg represents a hardware building block that can immediately start its computation as soon as the data are available through its input arc  such representation of cnn model offers a fast design space exploratio  venieris and bouganis employed sdf model to optimize the mapping of cnns onto fpgas based on hl  in particular, the proposed fpgaconvnet framework in takes as input a high-level script programmed by dl expert describing the cnn model, along with specifications figure 1  sdf graph partitioning of the targeted fpga platfor  thereafter, it parses the input script through a developed domain-specific language processor to model the cnn in the form of a directed acyclic graph where each node corresponds to a cnn laye  then, the dag-based cnn is transformed into an sdfg representation and modeled as a topology matri  the topology matrix contains the number of incoming parallel streams, the width of each data stream, and the production or consumption rates at each nod  in addition, the dsl processor extracts information about the platformspecific resource constraint  unlike other attempts, instead of exploring the design space for the optimal parameters of loop unrolling and tiling, fpgaconvnet explores the design space of the topology matrix components while considering the resource constraint  the graph partitioning splits the original sdfg into subgraphs and each subgraph is then mapped to a distinct bitstream as shown in fi  note that the proposed multibitstream architecture might have multiple conv layer processorsas in the provided exampl  this away, on-chip ram is used for intermediate results and data reuse within the subgraph, while accesss of off-chip memory is minimized and limited for input and output streams of the subgrap  however, this scheme adds reconfiguration penalty due to the need for reconfiguring the fpga when the data flows between adjacent subgraph  to amortize this overhead, several input data streams are processed in a pipelined manne  thereafter, each bitstream architecture is optimized using coarse-grained folding and fine-grained foldin  the fine-grain folding controls the unrolling and pipelining of the dot-product operations inside conv and average pooling unit : fpga-based accelerators of deep learning networks for learning and classification: a review in  therefore, fpgaconvnet employed simulated annealing to find the optimal partitioning points and folding factor  finally, fpgaconvnet uses optimal components to derive the configuration of pes and buffers, and generates a synthesizable vivado hls hardware desig  fpgaconvnet framework has been evaluated by mapping lenet-5 and scene labelling small cnn models with q 8 fixed-point representation onto a zynq-7000 xc7z020 fpga platform working at 100 mh  in mapping lenet-5, fpgaconvnet achieves up to   compared to tegra k1 gpu implementation of scene labelling cnn, fpgaconvnet surpasses tegra k1's power efficiency by   ma et a  proposed a python-based modularized rtl compiler to accelerate cnns by employing loop unrolling optimizationfor conv layer operation  a detailed review article of this work has been recently published and referred to as alamo the proposed compiler integrates both the rtl finer level optimization and the flexibility of hls to generate efficient verilog parameterized rtl scripts for asic or fpga platform under the available number of parallel computing resources ).  if nm is greater than the number of input fmsthe proposed compiler fully unrolls loop3 while it partially unrolls loop-4 to exploit the data reuse of shared features among nm{nif output fm  otherwise, it partially unrolls loop-3 which results in nif{nm repeated sliding of kernel windo  on the other hand, loop-2 is serially computed after loop-1 to minimize the number of partial sum  the overall modules of the proposed cnn accelerator are shown in fi  the controller is responsible for directing and ensuring in-order computation of cnn modules for each laye  the data routers oversee the selection of data read and data write of two adjacent modules as well as the assignment of buffer outputs to shared or pool multipliers of the multiplier ban  the feature buffers hold the fms using on-chip ram  the weight buffers are used to ensure the availability of conv and fc layers' weights before their computation as well as to overlap the transfer of fc layer weights with its computatio  the conv module consists of control logic, groups of adder trees, and relu component  the control logic component parametrizes the loop unrolling factors based on the configuration of each layer the conv module contains nm{nif adders to sum nif parallel multiplier results and accumulate the  moreover, the adder trees can be shared by layers with identical nif to be as one single modul  the relu component checks the input pixel sign bit to either output zero or the data pixel itsel  the pool module contains accumulators or comparators to perform average or maximum operation, respectivel  the norm module maintains the required components to perform the operations of local response normalization such as square, non-linearand multiplication operfigure 1  alamo overall acceleration modules ation  finally, the fc module shares the multiplier bank module with the conv module to perform the matrixvector multiplication alamo architecture permits the output pixels to be only stored in the feature buffers, which makes alamo suitable for cnns with only small intermediate data volume  the proposed rtl compiler has been tested by accelerating two cnn models; alexnet and nin the generated parameterized rtl scripts for alexnet and nin are synthesized using altera quartus synthesis tool and implemented on de5-net fpga boar  the experimental results for alexnet model are compared with the results for openclbased design as both use the same fpga board with similar hardware resources for alexne  alamo achieved   moreover, the overall throughput of nin model is   this is because nin has more conv layers and many of them have the same ni  liu et a  task-level parallelism involves executing multiple image prediction tasks simultaneousl  layer-level parallelism exploits pipelining across layers to enable parallel execution of all layers with different image  loop-level parallelism utilizes loop unrolling in performing convolutions and this can be achieved either through intra-output or inter-output parallelis  fi  17 shows the parallel framework exploiting these four levels of parallelis  the authors have used 16-bit fixed-point format for representing pixels in input feature maps and output feature map  however, they have used 32 bits for intermediate results which get truncated to 16 bit  in addition, they have used 8 bits for representing kernels and weight : fpga-based accelerators of deep learning networks for learning and classification: a review figure 1  parallel framework exploiting four levels of parallelism the proposed technique has been evaluated by implementing three cnn accelerators on the vc709 board for lenet, alexnet, and vgg-  it has achieved a throughput of 42 6 gops, and 47  in addition, the performance has been compared with matconvnet tool running the cnn models on intel core i7-4790k cpu and nvidia gtx-770 gpu however, the accelerators achieved higher power efficiency than the gpu implementations in all three networks with 2  fp-dnn is an end-to-end framework that automatically generates optimized fpga-based implementations of deep neural networks using an rtl-hls hybrid librar  the model mapper extracts the topological structure and layers configurations of dnn model from the tensorflow descriptions and generates an execution graph for the target mode  the execution graph shows layer-bylayer operations and read/write data transaction  fp-dnn compiler allocates off-chip dram data buffers to store intermediate data, weights, and model parameters and configuration  the model mapper maximizes the storage resource reuse through minimizing the number of required physical buffer  specifically, it formulates the data reuse problem as a graph coloring problemand then the left-edge algorithm is applied to generate kernel configuration and kernel schedul  on the other hand, the hardware generator uses the kernel configuration and the execution graph to generate the fpga hardware codes by instantiating the corresponding optimized templates from an expandable rtl-hls hybrid librar  each template is comprised of verilog-based computational engine and opencl-based control logics engin  the architecture of the proposed fpga-based accelerator consists of matrix multiplication and data arranger module  matrix multiplication module is a hand-written verilog code that is designed and optimized based on the hardware constraints of altera stratix-v gsmd5 fpg  it applies tiling and ping-pong double buffers techniques to improve the throughpu  on the other hand, data arranger is an opencl-based module that is responsible for mapping the computational part of a layer to matrix multiplication as well as performing data communication with off-chip memory and matrix multiplication modul  mapping dnns computational operations to matrix multiplication has been widely applied in prior studies, fp-dnn maps fc layer to matrix multiplication by batching input vectors togethe  before model deployment, fms and weights are rearranged in dram using the channel-major scheme to optimize the communication between the accelerator and off-chip dra  on the other hand, both floating-point and fixed-point representations have been supported for implementation, and they can be adjusted by the use  the proposed rtl-hls hybrid framework has been evaluated by accelerating vgg-19, lstm-lmresnet152 dnns on stratix-v gsmd5 fpg  note that this is the first work that implements resnet-152 on fpg  conducted a set of experiments to estimate the trade-off between the network size and precision using the roofline mode  they found that binarized neural networks require 2 to 11 times more operations and parameters than an 8-bit fixed-point cnn to achieve a comparable accuracy on mnist datase  finn generates a synthesizable c++ network description of a flexible heterogeneous streaming architectur  the architecture consists of pipelined compute engines that communicate via on-chip data stream  each bnn layer has been implemented using dedicated compute engines with 1-bit values for weights and fms; +1 and -1 are used to represent a set bit and unset bit, respectivel  in particular, the accumulation of a binary dotproduct has been implemented as a counter of set bits the popcount-accumulate reduces the number of required look-up tables and flip-flops by a half, compared to the implementation of signedaccumulatio  such an implementation of batchnorm-activation operations requires much smaller number of luts, without the need for dsps and ffs, compared to regular implementation of batchnorm-activatio  the accelerator architecture is composed of building blocks from the finn hardware librar  the matrix-vectorthreshold unit is the core computational building block as matrix-vector operations followed by thresholding form the majority of bnn operation  bnn weight matrix is distributed across the pes and stored locally in onchip memor  subsequently, the input images are streamed through the mvtu and multiplied with the weight matri  then, it figure 1  the architecture of mvtu pe compares the number of set bits to a threshold and produces a 1-bit output value as previously discusse  umuroglu et a  implemented the conv layer using a sliding window unit and an mvtu, where convolutional operation is transformed to matrix-multiplication of image matrix and filter matri  swu generates the image matrix to mvtu by moving the sliding window over the input fms, while the filter matrix is generated by packing the weights from the convolution filters as shown in fi  folding of mvm decides partitioning of the matrix across pe  every row of matrix tile is mapped to a distinct pe and every column of pe buffer is mapped to a distinct simd lan  the folding factors of bnn layers have been determined such that every bnn layer takes nearly the same number of cycle  to evaluate finn, the authors implemented cnv topology on xilinx zynq-7000 board at 200 mhz to accelerate bnns inference on cifar-10 cnv contains figure 1 : fpga-based accelerators of deep learning networks for learning and classification: a review figure 2  its topology is inspired by vgg-16 and binarynet although cnv accepts images with 24bits/pixel as an input and produces a 10-element vector of 16-bit values, 2-bits are used for representing intermediate results while 1-bit is used for representing conv and fc weight  experimental results demonstrated that the proposed design provides high performance while incurring low energy consumption finn outperforms the design by ovtcharov et a  by over 1  inloop optimization techniqueshave been employed in fpga to design a customized cnn accelerator through speeding up conv layer operation  please refer to fi  2 for more details on conv loops levels and their parameter  on the other hand, loop interchange technique has a great impact on the times of memory access as well as the number of partial sums since it determines the order of computing conv loop  subsequently, matlab scripts are used to randomly sample a subset of the solution space to find the optimal design configuration  this is due to the large solution space, more than   on the other hand, loop-1 and loop-3 are serially computed to prevent the movement of the partial sums between the mac units and consume them asap since both loop-1 and loop-3 need to be finished in order to obtain one final output pixe  more importantly, the order of loops computation has been found to be as follow  loop1 is computed first, then comes loop-3, and finally loop-2 and loop-4 are computed in any orde  finally, a customized convolution accelerator module with efficient dataflow has been designed based on the previous results and used for all vgg-16 conv layer  the conv accelerator consists of 3,136 independent mac units and 14 input pixel buffer  fi  the input pixels are shifted after fetching them out of the input pixel buffer  subsequently, they can be reused among the input register array  then, the input pixels are fed into the associated mac unit : fpga-based accelerators of deep learning networks for learning and classification: a review conv loop  on the other hand, dual weight buffers have been used to increase the throughput of fc layer through overlapping the inner-product computation with off-chip communicatio  the acceleration system has been written as parametrized verilog script  the experimental results show that the proposed accelerator has a throughput of 64 25 gops, which is more than   venieris and bouganis further extended fpgaconvnet framework to allow for optimizing either throughput or latency depending on the size of the workloa  for large workloads, weights reloading transformation has been introduced to efficiently design latency-critical cnns on fpg  upon the execution of a new subgraph, the subgraph's weights are read into the on-chip memory and the multiplexers are configured to form the appropriate datapat  fi  21 demonstrates how weights reloading is applie  the authors have mentioned that the required time for transferring subgraph's weights is much smaller than the average time for full fpga reconfiguration, 27 5 mb of weights for a vgg-16 layer on zynq xc7z04  in the situation discussed above, due to limited on-chip memory capacity, it might not be possible to load all weights required for a single conv laye  to handle this, the authors introduced an input fms folding factor with each conv laye  a conv layer is partitioned into fini subgraphs in which each subgraph executes a fraction of conv i to produce a fraction of the output fm  the proposed latency-driven methodology has been evaluated by implementing alexnet and vgg-16 with 16bit fixed-point precision for both on zynq xc7z045 at 125 figure 2  weights reloading overall dla architecture mh  the experimental results showed   lavin and gray demonstrated that cnn algorithms with small filters can be efficiently derived using winograd algorithm and fast fourier transform algorithm due to their advantages in improving resource efficiency and reducing arithmetic complexit  winograd computation involves a mix of element-wise and general-purpose matrix multiplication, where some of the matrices need to be transforme  in another work, zhang et a  implemented fft algorithm for cnn on fpga platfor  aydonat et a  presented a deep learning architecture based on openc  their proposed architecture reduces the external memory bandwidth requirements by an order-of-magnitude for both the convolutional and fully connected layer  this is achieved by caching all intermediate feature maps on-chip in stream buffer  for fully connected layers, image batching is used where a batch of images are processed together through the fully connected layer  the approach utilizes the winograd transformation to reduce the multiply-accumulate operations, which could reduce the number of needed operations by about 50%.  in addition, it uses half-precision floating-point operations with shared exponents, which significantly reduces the needed computational resource  the overall dla architecture is shown in fi  caches are used for storing filter weight : fpga-based accelerators of deep learning networks for learning and classification: a review layer are prefetched onto the caches while filter weights are loaded from the caches for a particular convolution laye  stream buffers store feature data and stream it to pe  each stream buffer is double-buffered similar to filter cache  images are loaded from the ddr and are stored in stream buffers before the first convolution layer starts executio  during a convolution layer execution, while feature data for a convolution layer is being streamed into the pes, the outputs of convolutions are simultaneously stored in the buffer  the streambuffer unit applies the winograd transformations to features, and streams the transformed features to the first pe which are forwarded through all the pes via the daisy-chained input connections between the  the relu unit receives the outputs of the pes via daisy-chained output connection  then, the normalization unit receives the outputs of the relu unit and applies the normalization formula across the feature map  the pooling unit receives the outputs of the normalization unit and computes the maximum value in a windo  the output of the pooling unit is stored back in the stream buffer for further processing, if more convolution layers are to follo  otherwise, the outputs of the pooling unit are stored in external memor  for the fully connected layers, features data are stored on pes caches while filter weights are stored in stream buffer  for the first fully connected layer, features data are read back from external memory and loaded onto the pe cache  the relu output is sent directly to ddr, without applying normalization or poolin  the sequencer generates the control signals to control the operation of the various blocks in dla according to the topology of the executed cn  executing a different cnn requires just changing the sequencer configuratio  the dla has been evaluated by implementing alexnet cnn on intel's arria 10 dev kit which contains a a10-1150 device using a 96 batch size for the fully connected layer  it achieved a performance of 1020 images/  in addition, it achieved   unlike dla architecture where a 1d winograd algorithm was employed to reduce arithmetic complexity, lu et a  implemented a novel fpga architecture with a two-dimensional winograd algorithm to accelerate convolutional computation of cnn  the overall architecture consists of line buffer structure and winograd pe engine, as shown in fi  transfer and computatio  thereafter, the input lines are rotated in a circular fashion to make the next n input lines read  a vector of pes is employed to achieve parallelism through unrolling loop 4 and loop 3 similar to that in to implement fc layer, the proposed accelerator uses the input line buffers to hold fc weights while input neurons are stored on the filter buffer  then, winograd pe engine is reused to implement fc operation but with bypassing the transformation stage  moreover, a batch of input fms are assembled and processed together in order to improve the memory bandwidt  an analytical model has been proposed for a fast design space exploration of optimal design parameters constrained by fpga configuration with a 16-bit fixed-point representation for both fm data and filte  the proposed accelerator has been evaluated by implementing alexnet and vgg-16 on xilinx zcu102 fpg  alexnet conv layers have 3 different filter  the design parameters are found to be equal to and for alexnet and vgg-16, respectivel  the experimental results demonstrated that the proposed winograd-based cnn accelerator has an average performance of 85 6 watts for bot  the proposed accelerator has also been evaluated on xilinx zc706 platform where the design parameters are found to be as and for alexnet and vgg-16, respectivel  the experimental results demonstrated that winograd-based cnn accelerator has an average performance of 20 6 watts for bot : fpga-based accelerators of deep learning networks for learning and classification: a review figure 2  compute unit with a 2d bram-to-pe interconnection the latest cudnn   zhang et a  presented an opencl-based architecture for accelerating cnns on fpg  they also proposed an analytical performance model to identify the bottleneck in opencl-based acceleration of vgg-19 ccn model on modern fpga platforms such as altera arria 10 gx 115  based on roofline mode analysis, it is shown that the bandwidth requirement of vgg-19 workload is higher than what is provided by the fpga boar  thus, they identified on-chip memory bandwidth as the key performance bottlenec  in addition, they observed that exploited data-level parallelism in the existing altera opencl library leads to wasteful replication of on-chip memory this is due to connecting each pe with a dedicated bram por  therefore, a verilog-based accelerator kernel has been designed and warped to an opencl ip in order to optimally balance on-chip memory bandwidth with workload computational throughput and off-chip memory accesse  the compute subsystem is organized hierarchically into compute units and pe  at pe level, the authors have figure 2  line buffer design designed a 2d multi-cast interconnection between brams and pes to improve the efficiency of on-chip bram usage by sharing the data of one bram port with several pes as shown in fi  the 2d dispatcher divides the work items into work groups each of size as shown in fi  thereafter, it adaptively schedules the work items within each work group to the cus starting with the lowest dimension to balance the memory bandwidth with capacit  the 2d dispatcher is also responsible for host/device memory data transfer  in addition, the authors have limited the maximum fan-out for registers to 100 in order to guarantee a higher frequenc  the conv layer has been implemented as a matrix multiplication by flattening and rearranging the data using line bufferas shown in fi  26, in a similar fashion to that in the line buffer converts continuous address stream from external memory into a stream conducive for conv operation to substantially reduce the bandwidth requirement of off-chip memor  to implement fc layer, the proposed accelerator uses one column of pes in the c  however, shen et a  noted that using a single globally-optimized clp design for the computation of conv layers of radically different configurations and dimensions leads to suboptimal performance and insufficient utilization of fpga resource  fi  27a demonstrates the use of a single clp to iteratively process l1l2, and l3 conv layers where the dimensions of the hardware and the layers are represented by the size and shape of the boxe : fpga-based accelerators of deep learning networks for learning and classification: a review figure 2  operation of conv layer processors on cnn with three conv layers than the dimension of the used cl  note that processing a conv layer with a dimension bigger than the dimension of clp, such as l3, requires the repeated use of clp to process different portions of the laye  they found that one quarter of dsp slices of squeezenet's clp remain unuse  even more worse utilization has been observed for alexne  the optimal single clp has not utilized, on average, more than one quarter of the arithmetic unit resource  on the other hand, they also noted that using one clp for each stage of conv layer in a fashion similar to that in is not efficient due to three reason  first, it reduces the on-chip bram buffer size of each clp which minimizes overall data localit  second, such one-to-one mapping of conv layers and clps requires orchestrating many off-chip memory accesses which incurs latency and bandwidth overhead  third, the overall control overhead scales with the number of clps which leaves insufficient resources for the computation of cn  to address the above inefficiencies, shen et a  proposed a multi-clp accelerator system for cnns where the available pfga hardware resources are partitioned across multiple smaller clp  each clp is tailored with a dimension that closely matches the dimensions of a subset of conv layer  thereafter, these specialized clps are used to concurrently operate on a batch of images to achieve a higher overall throughput, as shown in fi  27b, where the same hardware in fi  27a is partitioned into two parallel clps; clp1 and clp  shen et a  developed an optimization search algorithm that uses dynamic programming to find optimal design  for given configurations of cnn model and resource constraints of the targeted fpga platformit derives the optimal number of clps as well as the optimal mapping between conv layers and clps that maximize the performanc  the assignment of cnn layers to clps is static, where each cnn layer is mapped and bounded to a particular cl  subsequently, cnn layers are pipelined to their clp, as shown in fi  27b, where l1 and l3 are pipelined to clp1 while l2 is repeatedly processed on clp2 with very little idle hardware which improves the performance compared to single clp approac  moreover, the optimization algorithm also finds the optimal partition of on-chip bram resources of each clp that minimizes the overall off-chip memory accesse  note that the optimal dimension of each clp is found based on the work in subsequently, c++ templates are parameterized to design clps and to form a complete implementation of cn  a standard axi crossbar is used to interconnect the independent clp  the ping-pong double-buffering technique is also used for input fms, output fms, and weights to allow for transferring data while computation is in progres  the experimental results of implementing alexnet with a single precision floating-point using multiclp accelerator on virtex7 485t and 690t fpgas at 100 mhz demonstrate   for the more recent squeezenet network, the proposed multi-clp accelerator results in speedup of   wei et a  presented a systolic architecture for automatically implementing a given cnn on fpga based on opencl description, maximizing clock frequency and resource utilizatio  the proposed systolic architecture is shown in fi  each pe shifts the data of the weights and inputs horizontally and vertically to the neighboring pes in each cycl  the 2d structure of pes is designed to match the fpga 2d layout structure to reduce routing complexity and achieve timing constraint : fpga-based accelerators of deep learning networks for learning and classification: a review figure 2  systolic array architecture for cnn the technique first finds a feasible mapping for the given cnn to the systolic array to guarantee that proper data is available at specific locations in the pe array at every cycl  finally, the data reuse strategy is determined by choosing proper tiling size  the proposed technique has been evaluated using alexnet and vgg16 on intel's arria 10 gt 1150 boar  the technique has explored the use of both 32-bit floatingpoint and fixed-point using 8-bits for weights and 16-bits for dat  evaluation results show that, for the vgg16 cnn, the technique achieves up to 1,171 gops on intel's arria 10 device with a clock frequency of 23 85 mhz and -bit fixed-point representatio  in another recent research work, ma et a  generalized the previously proposed accelerator in to efficiently accelerate resnet-50 and resnet-152 on arria 10 gx 1150 fpg  in addition, local control logic and registers have been used with each primitive to control their computation order and to hold their configurations, respectivel  by doing so, resnets primitives can be efficiently reused for different parameters of each laye  therefore, a similar architecture and dataflow to that shown in fi  20 has been used for conv but with the use of two sets of register arrays; with shifting between the registersand without shifting between the registers in conv primitive with set-2, the input pixels are fed from the input pixel buffers into the corresponding registers without shifting, and then to mac unit  the skipped input pixels in configuration are not stored to the input pixel buffer  on the other hand, the configuration of the kernel and stride sizes is retained as the case while transferring repeated input pixels into the input pixel buffers and rearranging their storage pattern  the conv primitive also takes care of zero-paddings for different size configuration  the loop unrolling and tiling techniques in have also been employed to accelerate conv primitive with a uniform mapping of pes to all resnets conv layer  however, designing of efficient cnn modules is not enough, as the memory accesses and data movements between these modules must also be minimize  therefore, the authors have designed a layer-by-layer computation flo  the global control logic is responsible for governing the sequential operations of primitives and their dataflow through predefined and preloaded layered-based execution flowchart, as shown in fi  in addition, it has been modeled to reconfigure resnet primitives according to the parameters of each layer during runtim  for instance, it maps a particular number of pes to conv layer based on loop unrolling parameters as well as it controls the selection of register array type based on conv parameter  on the other hand, a custom dma manager has been designed to control the operations of dm  note that the dma is responsible for transferring the input fm pixels, weights, and output fm pixels between off-chip memory and on-chip buffer  unlike alamo architecture where the output pixels are only stored in on-chip buffers, this work as well as the work discussed in store the output pixels in off-chip memory with the use of loop tiling technique in order to have a flexible architecture that can process large-scale cnn  the dual weight buffers technique has not been used in this work due to the current trend in cnns where either the size of fc weights has been significantly reduced or the fc layers are completely removed such as in ni  the experimental results demonstrated that the achieved throughput for resnet-50 and resnet-152 are 28  finally, the authors mentioned that higher throughput can be achieved figure 2  execution flowchart of resnets layers: fpga-based accelerators of deep learning networks for learning and classification: a review figure 3  dlau accelerator architecture using batch computing wang et a  proposed a scalable design on fpga for accelerating deep learning algorithm  in order to provide a scalable architecture and support various deep learning applications, the proposed architecture utilizes the tiling technique in which the large-scale input data is partitioned into small subset  the size of the tile is configured to leverage the trade-off between the hardware cost and the speedu  moreover, the authors explored hot spots profiling to determine the computational parts that need to be accelerated to improve the performanc  the experimental results illustrated that matrix multiplication and activation functions are the key operations in deep learning algorithms as they consume about 9  thus, the proposed accelerator is responsible for speeding up both matrix multiplication and activation function computation  the embedded processor utilizes the jtag-uart to communicate with the acceleration unit the dlau unit accesses the ddr3 memory to read the tiled input data and to write the results back through the dma module during the executio  the dlau utilizes three fully pipelined processing units to improve the throughput, while minimizing the memory transfer operation  these units are tiled matrix multiplication unitpartial sum accumulation unitand activation function acceleration unit tmmu is responsible for multiplication and generating the partial sum  to optimize the performance, tmmu is structured as a pipelined binary adder tre  on the other hand, psau is responsible for accumulating the partial sums generated from tmm  finally, afau implements the sigmoid function using piecewise linear interpolation to speedup the computation with negligible accuracy los  since the processing units in dlau might have inconsistent throughput rates, each unit has input fifo buffer and output fifo buffer to prevent data los  the authors implemented the proposed architecture on xilinx zynq zedboard with arm cortex-a9 processors clocked at 667 mh  the experimental results demonstrated that the speedup of the dlau accelerator is up to 3  in addition, the results depict that the proposed architecture is quite energy-efficient as the total power consumption was only 234 m  in doing so, a user-friendly interface and an rtl-level compiler have been proposed to automatically generate customized fpga design  the authors have developed an expandable optimized rtl-based library containing the most commonly used cnn operation  these operations have been coded in verilog and designed based on the quantitative analysis and optimization strategies discussed in the compiler generates a dag-based structure for the used cnn model and then compiles it with rtl modules in the librar  the proposed compiler allows the user to input the high-level information of the used cnn model as well as the design variables with the resource constrains of the targeted fpga platfor  such utility facilitates the exploration of the best trade-off between the resource usage and the performanc  unlike the architecture in where individual conv module is assigned to each conv layer, the scalable rtl computing module proposed in this work is reused by all cnn layers of the same type for different cnns as shown in fi  note that it is not necessary to have all these modules in the architectur  for instance, the rtl compiler figure 3  overall architecture and dataflow: fpga-based accelerators of deep learning networks for learning and classification: a review figure 3  conv reconfigurable computing module will not compile or synthesize eltwise and combined batch normalization with scale modules for vgg-16 model which greatly saves the hardware resource  on the other hand, the authors categorized cnn layers into key layers and affiliated layers they have also defined layer combos, where each combo is composed of a key layer and several affiliated layer  layer combos are sequentially executed according to their order in the da  moreover, the layer combo is also divided into several sequential tile  the computation of each combo tile starts by reading its input pixels from off-chip dram and ends by writing back its output pixels to off-chip dra  the authors have also employed special storage pattern of both input pixels and weights on off-chip memory before the acceleration process to maximize data reuse and minimize of data communicatio  the architecture of conv module is designed based on the acceleration strategies inbut with a different organization of mac units as shown in fi  moreover, such organization enables to handle varying sizes configurations through generating different variants of conv register arrays during the compilatio 75 gops, and 27 67 gops, respectivel 13 gops, and 71  recently, the programmable solutions group at intel has developed an fpga software-programmable and run-time reconfigurable overlay for deep learning inference the developed overlay is referred to as the deep learning accelerator for the hardware side of intel's dla, the team has partitioned the configurable parameters into runtime and compile-time parameter  the run-time parameters allow for easy and quick use of different neural network frameworks, while the compile-time parameters provide a tunable architecture for performanc  intel's dla uses a lightweight very long instruction word network, an 8-bit unidirectional ring network, to support the control and reprogramming logi  the reprogramming of intel's dla overlay allows for consecutive runs of multiple nns in a single application run without the need for reconfiguring and recompiling the fpg  fi  33 shows that a 1d array of pes is used to perform convolution, multiplication, or any other matrix operation  each pe contains a double-buffered filter cache allowing for pre-loading of next filters while computin  the stream buffer employed the double-buffering mechanism as well to store the inputs and the intermediate data on-chi  to have flexible nn architecture, intel's dla employs an xbar interconnect that connects all the core functions require  thus, deep learning functions can be easily added to the overlay through the xbar by picking them from a suite figure 3  intel's dla: neural network inference accelerator: fpga-based accelerators of deep learning networks for learning and classification: a review of pre-optimized functions of the select frameworks that intel's dla use  the width adaptation module has been used to control the throughput of the functio  the authors mention that vectorization depends on the layers' dimensions of the considered framewor  however, they did not provide a systematic way for finding the optimal balance for the number of used pes and the size of the cache  for efficient use of resources, intel's dla maps avg pooling and fc primitives to convolutions in order to avoid having underutilized dedicated auxiliary function  in the slicing pass, the graph compiler breaks down the architecture into subgraph in such a way that they fit within the computing and storage resources of the overla  a single conv layer followed by a pooling layer is an example of cnn subgrap  the graph compiler optimizes the external memory spill-points by group slicing techniqu  the group slicing allows several sequential convolutions, for instance, of a single slice to be computed before moving onto the next slice while using the whole stream buffe  during the allocation pass, the graph compiler optimizes the use of a custom developed filter caches and stream buffer by managing the read and write from the stream buffer for each slic  moreover, it assigns an external memory address when the stream buffer is not big enough to hold the slice dat  finally, intel's dla compiler schedules the execution of subgraphs using cost-based priority queu  the authors utilized the software-programmable and run-time reconfigurable overlay to optimize the software and hardware implementation of googlenet and resnet cnn  the team pointed that multi-fpga deployment might be used to further improve the throughput of intel's dl  kaiyuan et a  proposed a complete design flow, referred to as angel-eye, for mapping cnns onto fpg  it adopts the approach of using a flexible hardware architecture and maps different cnns onto it by changing the softwar  the proposed design flow from cnn model to hardware acceleration is shown in fi  due to the large dynamic range of data across different layers, the best radix point is found for each layer for a given bit widt  they demonstrated that their strategy can simplify state-of-the-art cnns to 8-bit fixed-point format with negligible accuracy los  fi  35 and fi  36 show the overall architecture of angel-eye and the structure of a single pe, respectivel  the overall architecture is divided into four main compofigure 3  structure of a single pe the pe array implements the convolution operations in cnn and supports kernel level parallelism, and input and output channel parallelism  the use of on-chip buffers allows data i/o and convolution calculation to be done in paralle  the controller is responsible for receiving the instructions and issuing them to the other component  the compiler maps the cnn descriptor to the set of instructions that will be executed by the hardwar  it follows basic scheduling rules to fully utilize data localization in cnn and reduce data i/  the block partition step partitions the calculation of one layer to fit each block into the hardwar  the memory mapping step allocates memory for communication between the host cpu and the cnn accelerato  based on the block partition result, on-chip memory is allocated for the input and output feature map blocks and for the convolution kernels and bias value  the dependency check step checks data dependency among instructions and sets appropriate instruction flags to maximize parallelism between convolution calculation and data i/  in anda special register array architecture has been designed to rearrange buffers data and direct them into pes for the purpose of implementing conv module that supports specific stride and zero-padding setting  although the designed conv module is not generalized for any size configurations, it is composed of complex wire routing and control logic as shown in fi  to have flexibility in directing the dataflow of conv pixels, ma figure 3  et a  replaced the register array architecture in with a data router as shown in fi  the data router is a scalable set of data bus from buffer to pe the buf2pe data bus consists of simple register arrays with fifos in between to form a line buffer similar to that in the register array uses the fifo to pass its input pixels to the adjacent register  each buf2pe data bus has different data movements within its register arrays to implement specific stride and kernel size setting  unlike the register array architecture in where the west zero-paddings are handled by changing the storage pattern within the input pixel buffer, the buf2pe handles such kind of paddings by shifting the connection between the register arrays and the input pixel buffers to simplify the data transferring from off-chip memory to on-chip buffer  however, there is still a need for adjusting the storage pattern within the input buffers in order to handle other zero-padding  the global control logic is responsible for selecting the suitable buf2pe data bus from the data router as well as the suitable storage pattern within the input buffers based on the size configuration of conv laye  the conv module has also been optimized by reducing the required number of parallel adders that add the partial sums with biases as well as the number of parallel multipliers and adders needed to perform bnorm operation by serializing the parallel outputs using multiplier  in addition, 16-bit fixed-point has been used to represent both weights and pixels, while dynamically adjusting the decimal point in different layers to fully utilize the existing data width the proposed compiler in has been used to configure the parameterized verilog scripts of the overall cnn acceleration syste  experimental results show throughput degradation on both intel arria 10 gx 1150 and intel stratix v gxa7 in comparison to the results in i  metaheuristics in the design of convolutional neural networks currently, convolutional neural network structures are designed based on human expertis  implementation and performance summary of fpga-based accelerator 2 ghz amd opteron  35 ghz c870 gpu  2 ghz intel xeon  9 ghz intel xeon 1 4 ghz intel xeon  3 ghz intel i5-4590   implementation and performance summary of fpga-based accelerator 0 ghz intel core i7-4790k cpu 1 6 ghz intel xeon 8-core e5-2650v2 1 2% n/a n/a 2,46 4% n/a n/a 9 8% 1,790 n/a 4 4pa1q n/a 2 07 n/a n/a resnet-152 2 24pb1q n/a vgg-16pbq 3 8pb1q n/a vgg-16pbq 3 9pb2q n/a resnet-152pdq 2  recent research has demonstrated that a large number of weights in fully connected layers could be eliminated with minimal impact on accurac  in addition, although the suggested cnn structures by experts perform well for various applications, the question arises whether the suggested structures could be optimized for performance with minimal impact on accurac  since the designed cnn has a significant impact on the complexity of its implementation, we review in this section some approaches attempting to optimize the design of cnns using metaheuristic  np-hard combinatorial optimization problems appear in the design of cnn  some examples of areas include design of cnn structures, selection of weights and bias values to improve accuracy, and determination of optimal values of variables to reduce run-tim  below, we briefly touch upon some existing literature in these area    cnn structure optimization in the design of cnns, the number of possible network structures increases exponentially with the number of layer  xie and yuille used genetic algorithm in learning deep network structures the objective was to find the best cnn structure that would minimize the error rat  the cost function was the cnn accurac  they proposed an elegant encoding of chromosome using a fixed length binary string to represent each network structur  a cnn string represents only the convolution layer  in each generation, using standard genetic operations new individuals are generated and weak ones eliminate  the quality of an individual was assessed by its recognition accuracy which is obtained via the time consuming operation of training the network, and evaluating it on a validation se  two small data sets were used to run the genetic implementation via which they demonstrated the discovery of new structure  cnn weights and bias values optimization an attempt to train cnns using metaheuristics is presented in the objective again was to improve accuracy and minimize the estimated erro  the algorihtms compute the values of weights and bias in the last laye  these values are used as the solution vector denoted by x which is to be optimize  the stopping criterion is when the iteration count is reached or when the cost function goes below a pre-specified valu  cnn design variables optimization suda et a  presented a systematic methodology for design space exploration with the objective of maximizing the throughput of an opencl-based fpga accelerator for a given cnn model fpga resource constraints such as on-chip memory, registers, computational resources and external memory bandwidth are considere  peak performance is achieved for both, for the convolution operations, and for the entire cnn networ  reasonable estimation techniques and analytical formulations are required to efficiently traverse the design space in search of efficient solution ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "III", "Section": "III Acceleration of Deep Learning Networks: Current Status", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "IV", "Section": "IV Metaheuristics in the Design of Convolutional Neural Networks", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "V", "Section": "V Summary and Recommendations", "Text": "in this section, we highlight the key features discussed in the acceleration of convolutional neural networks implemented on fpgas, and provide recommendations to enhance the effectiveness of employing fpgas in the acceleration of cnn  all reviewed techniques are centered around accelerating the convolution operation as it consumes around 90% of the computational tim  this is achieved by utilizing parallel multiply-accumulate operations bounded by resource limitation  in addition, careful design of data access patterns are targeted to minimize the memory bandwidth requirements utilizing internal memory structures and maximizing data reus  this is crucial in the acceleration process due to the large memory data that needs to be accessed including feature maps and weight  to minimize the memory footprint and to achieve effective utilization of resources, some techniques optimize the number of bits used to represent the feature maps and weights with minimal impact on accurac  this is combined with the optimized selection of the number of fraction bits used for each laye  other techniques optimize the number of used weights in the fully connected layers as they are memory-intensiv  coprocessors are also employed to automatically configure both the software and the hardware elements to fully exploit parallelism to optimize parallelization of convolution operations, several approaches have been attempte  work load analysis has been tried to determine computations that can be structured as parallel streams the roofline model based accelerator uses polyhedral-based data dependence analysis to find the optimal unrolling factor for every convolutional layerand to fully utilize all fpga computational resources through loop pipelinin  to optimize performance, tiled matrix multiplication is structured as a pipelined binary adder tree for performing multiplication and generating partial sums an optimization framework has been proposed by suda et a  who identified the key variables of the design and optimize them to maximize parallelis  this pipelines the operation of the multiple clps achieving layer-level parallelism which maximizes resource utilization and enhances performance in comparison to using a single cl  complex access patterns and data locality are used in deepburning tool for better data reus  inthe authors explored hot spots profiling to determine the computational parts that need to be accelerated to improve the performanc  acceleration is accomplished by reducing the memory bandwidth requirement  techniques proposed exploit data reuse to reduce off-chip memory communication  loop transformations have also been used by reducing tiling parameters to improve data locality, and to reduce redundant communication operations to maximize the data sharing/reus  in the catapult project, fpga boards were integrated into data center applications and achieved speedu  microsoft research's catapult utilized multi-banked input buffer and kernel weight buffer to provide an efficient buffering scheme of feature maps and weights, respectivel  to minimize the off-chip memory traffic, a specialized network on-chip was designed to redistribute the output feature maps on the multi-banked input buffer instead of transferring them to the external memory to further reduce memory footprint and bandwidth requirement, optimal fractional length for weights and feature maps in each layer are use  singular value decomposition has also been applied to the weight matrix of fc layer in order to reduce memory footprint at this layer tiling techniques have been proposed where large-scale input data is partitioned into small subsets or tiles whose size is configured to leverage the trade-off between the hardware cost and the speedup automation tools have been developed that automatically build neural networks with optimized performance they employ pre-constructed register transfer level module library that holds hardware and configuration script  deepburn32 volume 4, 2018 shawahna et a  acceleration is achieved by employing loop unrolling technique for conv layer operation  some of the reviewed techniques also help minimize the size of fpga on-chip memories to optimize energy and area usageto enhance utilization of fpgas in cnns acceleration and to maximize their effectiveness, we recommend the development of a framework that includes a user-friendly interface that allows the user to easily specify the cnn model to be accelerate  this includes specifying the cnn model parameters in terms of number of convolution layers and their sizes, and number of fully connected layers along with other intermediate operation  the specified cnn model weights will be read from a fil  in addition, the user should have the option of specifying the fpga platform that will be used for implementing the cnn accelerator and the maximum tolerable error, along with the selection of a library from a set of applications to be used for model optimization and evaluatio  the framework then should perform optimizations to find the minimum number of bits that need to be used for representing the weights and feature maps and the number of fraction bits to be used for each laye  in addition, optimization of fully connected layers is performed to minimize the memory requirement  all such optimizations are carried out bounded by the maximum error specified by the user for the specified application librar  the framework should be designed based on the development of a scalable hardware architecture that works for any given fpga platform and achieves higher speedup with the availability of higher resource  the tool will then automatically generate the cnn model that will fit on the given fpga platform and will allow the user to evaluate the performance based on the chosen application librar  this will allow the user to evaluate the performance gains while evaluating different fpga platforms with different resource  the tool should have the option to generate performance measures based on different performance metrics as selected by the user such as number of frames processed per second or number of operations performed per secon  furthermore, it is desired to have the option for the user to specify the desired performance for a given cnn model and have the tool perform necessary analysis and evaluation and recommend to the user candidate fpga platforms for achieving the desired performance level  this will require the development of reasonably accurate analytical models that will estimate the needed resources for achieving the desired performanc  the user can then choose the recommended fpga platform and perform complete evaluation to verify that the desired performance levels are me ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "VI", "Section": "VI Conclusion", "Text": " the paper begins with a brief overview of deep learning techniques highlighting their importance, key operations, and application  special emphasis is given on cnns as they have wide applications in the area of image detection and recognition and require both cpu and memory intensive operations that can be effectively accelerated utilizing fpga inherent ability to maximize parallelism of operation  while the paper briefly touches upon the acceleration techniques for deep learning algorithms and cnns from both software and hardware perspectives, the core of this article has been the review of recent techniques employed in the acceleration of cnns on fpga  the paper also presented the use of tools for generating register transfer level scripts that not only help in automating the design process, but also help in exploring the design space and suggesting efficient hardwar  in addition, a brief review of the use of non-deterministic heuristics in solving np-hard combinatorial optimization problems in the design and implementation of cnns has been presente  finally, the paper summarizes the key features employed by the various fpga-based cnn acceleration techniques and provided recommendations for enhancing the effectiveness of utilizing fpgas in cnns acceleratio  we also like to acknowledge d  bremberg and m  sumaiya hussain sadiq for their help in professional english editing of this manuscrip  optimization mechanisms employed for fpga-based acceleration of deep learning network  optimization mechanisms employed for fpga-based acceleration of deep learning network : fpga-based accelerators of deep learning networks for learning and classification: a review", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "REFERENCES", "Section": "REFERENCES", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "finitely dependent processes are finitary yinon spinka abstrac  we show that any finitely dependent invariant process on a transitive amenable graph is a finitary factor of an    proces  with an additional assumption on the geometry of the graph, namely that no two balls with different centers are identical, we further show that the    process may be taken to have entropy arbitrarily close to that of the finitely dependent proces  as an application, we give an affrmative answer to a question of holroyd", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1. Introduction", "Text": " the process x is said to be finitely dependent if its restrictions to sets which are suffciently separated are independen  in particular, the independent process y considered above must now be an    thus, block factors of    processes provide a recipe for constructing invariant finitely dependent processe  it was a long-standing open problem to determine whether block factors of    processes are the only finitely dependent processes on z, until finally an example was given by burton-goulet-meester of a 1-dependent process which is not a block factor of any    proces  recently, holroyd and liggett showed that proper colorings distinguish between block factors of    processes and finitely dependent processes - no proper coloring of z is a block factor of an    process, but finitely dependent proper colorings exis  thus, it is not true that every finitely dependent process is a block factor of an    proces   thus, a block factor is a finitary factor in which the required distance is not only finite, but is determistically bounded by some constan  the main contribution of this paper is to prove that every finitely dependent process is a date: january 22, 202   proces  this result holds on any amenable graph   when it is further assumed that no two balls in g with different centers are identical, it is also possible to control the entropy of the    process involved, and the result becomes that every finitely dependent process is a finitary factor of an    process with only slightly larger entrop  definitions and main resul  we say that x is finitely dependent if it is k-dependent for some finite   suppose now that s and t are countabl  let 0 belong to v be a distinguished verte  a coding is called finitary if r is almost surely finit   proces  with a minor additional constraint on the geometry of the graph g, we can further control the entropy of the    process used in the coding  let us make some remarks about how the two theorems compare to one anothe  in theorem   in theorem   finitely dependent processes are finitary 3 the conclusion to hol  thus, the same coding can be used for all component ", "Subsections": [{"Section_Num": "1_2", "Section": "1.2. Discussion", "Text": "finite dependence and finitary factors have applications in computer scienc   process provides reliability as it means that the machines can determine their own roles in a distributed manner by following a common protocol, while using local randomness and communicating with finitely many other machine  for more informatio  we give a brief account of these work  we are unaware of any works regarding finitary factors for finitely dependent processes on other graph  a result by smorodinsky shows that every stationary finitely dependent process on z is finitarily isomorphic to an    proces  this result is not for the full autormorphism group of the graphbut rather only for the group of translation  in this respect, theorem  2 strengthens this resultas it provides a finitary factor which is also reflection invariant whenever the finitely dependent process is suc  the proof in is based on the so-called marker-filler method of keane and smorodinsky unfortunately, only a brief sketch of the proof is provided in and the details seem to be missing our proof is based on a different approach; see section 2 for an outlin  the question of whether there exists a stationary finitely dependent process which is not a block factor of any    process was raised by ibragimov and linnik in 196  some progress on this question was made until it was finally resolved in 1993 by burton-goulet-meester who gave the first example of a stationary finitely dependent process which is not a block factor of an    proces  in fact, they showed such an example in which the finitely dependent process is a 1-dependent hidden-markov process with finite energ  some history about finitely dependent processes that cannot be written as block factors is given in holroyd and liggett constructed a stationary 1-dependent 4-coloring and a stationary 2dependent 3-coloring of z, neither of which is a block factor of an    process holroyd subsequently showed that the 1-dependent 4-coloring is a finitary factor of an    proces  regarding the analogous statement for the 2-dependent 3-coloring, holroyd writes in that one may attempt to apply our method to the 2-dependent 3-coloring, but we will see that it meets a fundamental obstacle in this cas  our result shows that either coloring is a finitary factor of an    processanswering affrmatively question in in fact, the two colorings are also reflection invariant, and hence the finitary factors may also be taken to commute with reflection  related aspects of these two colorings were studied in it was shown in that each of these colorings is a finitary factor of an    process our result shows that each of these colorings is a finitary factor of an    process, where the factor map commutes with all automorphisms of z and the    process has entropy only slightly larger than the colorin  more specifically, they constructed a stationary 1-dependent 4d-coloring of zd and a stationary finitely dependent 4-coloring of z  however, unlike the above one-dimensional colorings, these colorings are only translation-invariant and not automorphism-invarian  in fact, it is still unknown whether there exists a finitely dependent coloring of zd which is invariant under all automorphisms of z  in the same paperholroyd and liggett also investigated the existence of stationary finitely dependent processes on z which are supported on a given shift of finite typ  it was later shown that there exists such a process which is also a finitary factor of an    process a block factor is precisely a finitary factor with bounded coding radiu  given a finitary factor which is not a block factor, it is natural to wonder about the typical value of the coding radiu   process, was shown in to be a finitary factor of an    proces  this finitary factor was shown to have power-law tail on the coding radius, thus yielding a perhaps infinite expected coding radiu  holroyd-hutchcroft-levy showed that there exist finitely dependent colorings of z which are finitary factors of    processes with exponential tail on the coding radiu  indeed, they showed that such a k-dependent q-coloring exists when is eitheror on the other hand, it is believed that when is orno k-dependent q-coloring is a finitary factor of an    process with finite expected coding radiu  we mention that optimal tails for the coding radius of colorings of zd and shifts of finite type on z have been studied in our main theorem gives no information about the coding radius beyond its almost-sure finitenes  still, some information about the coding radius may be extracted from the proof given here in the particular case of the 1-dependent 4-coloring ofthis improves the power in the power-law bound shown in in particular, theorem    process", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1_3", "Section": "1.3. Acknowledgments", "Text": "i would like to thank omer angel, nishant chandgotia, tom meyerovitch and mathav murugan for useful discussion  i am especially grateful to nishant chandgotia for suggesting to extend the result from zd to transitive amenable graphs, and to omer angel for jointly proving lemma  2 with m  i would also like to thank the referees for useful comment  finitely dependent processes are finitary 5", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1_4", "Section": "1.4. Notation", "Text": " the full automorphism group of g is denoted by au  the graph distance in g is denoted by dis ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2. Outline of proof", "Text": "  the first of these three will simply be given by an      processe  in these cases, we are led to consider random orders with suitable propertie  already the first part above relies on the aforementioned cell proces  before introducing this process, it is convenient to observe that it suffces to prove theorem  2 for 1-dependent random field  we will obtain a cell process a as a finitary factor of an    process y cell with arbitrarily small entrop  we do not explain here how this is done and refer the reader to section 4 for more details and to figure 1 for an illustration of the constructio  let us be slightly more specific about the way in which we sample x on a cel  in each cell in a1, we distinguish a vertex by choosing the smallest element in the cell according to the order given by y or  we call these distinguished vertices level 1 agent  note that the level n agents are obtained as a finitary factor of controlling the entropy: it is clear from the last observation above that there is plenty of waste in the above construction the problem is that we do not know ahead of time which samples of which distributions we will need access t  the basic solution to this is to place an infinite sequence of random bits at each site,   of course, this idea alone still does not provide any control on the entropy of y bit  for this, we must place a finite number of random bits at each site, and somehow still be sure that we are able to construct the required sample  the idea is to associate to each possible distribution we might require, a simulation which outputs a sample of the distribution in question from an input of unbiased random bit  the simulation is fed independent unbiased bits one-by-one, until at some point it halts and outputs the sampl  such simulations may be done effciently: the expected number of input bits read by the simulation is bounded by the entropy of the target distribution, up to an additive universal constan  we shall use such simulations whenever we sample x on a cel  thus, it will be important that the cells in a1 are typically large with small boundar  to solve this, we must allow to transfer bits from one location to anothe  this aspect of our construction is inspired by the algorithms in one consequence of the above description is that the steps of the construction cannot be directly related to the levels of the cell proces  the way this is done is as follow  this has the advantage that it ensures that no two agents ever try to read bits from the same location simultaneousl   process y ord with arbitrarily small entrop  though it does not follow from the above definition of finitary, it will 8 yinon spinka turn out to be the case that determining whether some vertex is the successor of some other vertex is also a finitary property once such an order is at hand, the proof continues as outlined abov  organizatio  in section 3, we introduce some preliminarie  in section 4, we prove the existence of a finitary cell proces  finally, we end in section 7 with some remarks and open problem ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3. Preliminaries", "Text": "", "Subsections": [{"Section_Num": "3_1", "Section": "3.1. Entropy", "Text": " we note that if y is an   ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3_2", "Section": "3.2. The mass-transport principle", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3_3", "Section": "3.3. Simulating distributions from random bits", "Text": "we shall use a result about the simulation of a given distribution from unbiased random bit  the first property says that we can use s to simulate the desired distribution from random bit  the following theorem follows from a result of knuth and yao let z be a discrete random variabl  knuth and yao show that the above is in fact optimal in a strong sense a version of this theorem was proved in via a more concrete constructio  the results in also provide explicit exponential bounds on the probability that the simulation uses more than n bits, but we shall not need thi ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4. The cell process", "Text": " we may identify a cell process a with the n-valued random field vbelong to   in this section, we show that finitary cell processes with arbitrarily small entropy exist on any transitive amenable grap  there exists an    in the general case, certain technicalities arise which make the proof somewhat longe  the reader may therefore wish to have in mind the case of g = zd on a first readin  the main idea of the construction is to use the points of a low-density bernoulli process to construct voronoi cellswhich are then used as the cells of a1  10 yinon spinka a1 going from a1 to a2 a2 figure   constructing the cell proces  the cells of a1 are simply the voronoi cells of a bernoulli proces  to get from a1 to a2, we consider the voronoi cells of a lower-density bernoulli process, and merge cells of a1 which are entirely contained in any such voronoi cel  repeating this procedure produces the cell proces  the green shade depicts regions belonging to the cell proces  the dark green depicts cells of a2 which are not cells of a  proces  it will then only remain to show that an increases to   this is where amenability comes into pla  to ensure that an increases to v, we must be careful in how we define the voronoi cell  the lemma was obtained jointly with omer ange  proo  proof of proposition   we shall construct a cell process a as a finitary factor of the    we note that cu ) may be empty and need not be connecte   bernoulli proces  finitely dependent processes are finitary 13 we now turn to the construction of the cell process   the first level set in the cell process is simply taken to be the vertices in a modified voronoi cell of u1,   the proposition now follows from markov's inequalit  in particular, automorphism-invariant cell processes do not exist on such a tre  moreover, by lemma   the site percolation constructed in is a factor of an    process, though it is not finitar ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5. Random total orders", "Text": "  process with arbitrarily small entrop  let us explain these propertie  with this viewpoint, the notion of finitary factor easily applies to such relation  a total order which is a finitary factor of an    process with infinite entropy is easily obtained from the order induced by uniform random variables in assigned to each verte  it is easy to see that this order almost surely has the same order type as   a total order which is a finitary factor of an    process with finite entropy was constructed in for any quasi-transitive graph satisfying a geometric condition similar to the application in did not require the    process to have arbitrarily small entropy and so this was not stated there, though it easily follows from the proof there that this is possibl  since the proof is short, we give it her  the following is essentially a reformulation of for our situatio  let g be a transitive non-empty graph satisfying  proo   it then follows from the definition of the lexicographical order that the factor is finitar 1 and the cell processes constructed in the previous section, we are able to construct a total order satisfying all three properties described abov  then there exists an    proo  by lemma    by proposition  1, there exist a cell process a that is a finitary factor of finitely dependent processes are finitary 17 an    in fact, it is the union of these total orders on the cells of a1dn of an-1 that are contained in   the former is ordered by giving an order to the cells d1, as mentioned in the beginning of the section, this implies that the factor is also finitary", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "6", "Section": "6. The finitary coding", "Text": "in this section, we construct a finitary coding for finitely dependent processe  we present the details of the proof of theorem  1 may be proved in a similar manner recall from the proof outline in section 2 that we shall construct a finitary factor from an    18 yinon spinka", "Subsections": [{"Section_Num": "6_1", "Section": "6.1. Choosing the parameters", "Text": " we shall choose the    we let y bits be any    recall from section 2 that we may assume without loss of generality that x is 1-dependen  by proposition  1, there exists a cell process a and an    let y ord be an    the construction of the finitary codin  recall also that the cell process a is a finitary factor of y cel  before giving the main definitions of the construction, we first set up some auxiliary notation and definition  as explained in the proof outline, we use simulations to obtain samples of distributions from random bit  we first equip ourselves with simulations of all the possible distributions we may require throughout the construction of the finitary codin  precisely, we proceed as follow  recall the definition of a simulation from section   by theorem   with the above notation and definitions, we may now proceed to construct the finitary codin  this will therefore define 20 yinon spinka zt+1 as wel  to facilitate the inductive definition of ubelong to v, we require some more definition  we define these notions inductively on   we thus begin with level 1 agent  finally, we are ready to define ubelong to   as mentioned, these numbers are always zero for non-agents,   proo  the proof by induction on t is straightforward from the definitions", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "6_3", "Section": "6.3. Concluding Theorem 1.2", "Text": "to conclude the proof of theorem   the former is stated in the following proposition whose proof is postponed to section   the output z has the same distribution as   proof of theorem   thus, in light of proposition   this follows rather easily from the constructio  to see this, we explain how to determine the value of zt v in a finitary manne  since a is a finitary factor of y cell, this may be done in a finitary manne  next, we find the level n agent un associated to the cell a  let us suppose by induction that all steps of the construction up to time t-1 are finitar  thus, we first find all level 1 agents which are contained in an 22 yinon spinka we now proceed to the next level  we again begin by finding all level m agents which are contained in an", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "6_4", "Section": "6.4. The output has the correct distribution", "Text": "in this section, we prove proposition   the proof is split up into several step  let ubelong to v be a collection of    proo  indeed, the induction hypothesis will then yield the desired resul  by lemma    the next step towards proving proposition   for this, we first show that every vertex is eventually inactiv  every vertex is almost surely eventually inactiv  proo  let eu be the event that u is active at infinitely many times   that is, the expected number of bits read by each site is precisely the expected number of available bits per sit  it remains to show that this is impossibl  recall that un is the level n agent associated to the cell a  this relates the expected number of available bits per site to the length of the input words used by the simulation  we would like to reach a contradiction to the fact that there are many available bits and that the simulation is effcien  suppose that u is a level n agen  it therefore follows from lemma   proo  let u be the level n agent un associated to the cell a  by lemma   the following immediately implies proposition   finitely dependent processes are finitary 25 proposition   proo  we also condition on throughout the entire proof, without explicitly mentioning thi  in particular, any statement about distributions or independence should be understood as conditional on the desired equality in distribution zc d = xc follows immediately from and both parts require some type of independence, which we now establis  let ubelong to v be a collection of    by lemma   in particular, zc is also a function of ubelong to   we now show to this end, let c1, to complete the proof, it remains to show ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "7", "Section": "7. Remarks and open problems", "Text": " we have given the details of the proof of theorem   to obtain a proof of theorem  1 with the total order induced by an    process consisting of uniform random variable  using this order in the proof of lemma  2 yields a random total order with the same properties as in lemma   when x is finite-valued, the proof then goes through with no 26 yinon spinka further modification  it is instructive to note that a shorter and conceptually simpler proof exists when one does not need to worry about the entropy of the    proces  this is essentially what is described in constructing a finitary coding in section   since we may place an infinite sequence of bits at every vertex, it will never run out of available bit  in this way, there is no moving around of bits from one location to anothe  either way, a nice feature of this construction is that the coding radius depends only on the cell process a constructed in section   we elaborate on this in the next remar  our main theorems give no information about the coding radius beyond its almost-sure finitenes  however, some information about the coding radius may be extracted from the proof given her 1 may be enhanced to give a universal bound on the tail of the coding radius for any fixed graph and finite-dependence rang   indeed, the sequence is governed by the properties of the cell process we remark that if one would like to simultaneously control also the entropy of the    we do not know whether condition is necessary as stated in theorem   indeed, the balls of radius 2 around and coincid  it is clear that x is 2dependent and aut-invarian  we claim that x is not a aut-factor of any    simple modifications to the proofs of lemma    process with slightly larger entrop  we did not pursue this directio  open problem  one may wonder about the situation on non-amenable graphs such as a regular tree namely, is every automorphism-invariant finitely dependent process on a tree a finitary factor of an    process? in fact, even the more fundamental question of whether such a process is a factor of    is still open; see the same questions may be asked on any transitive non-amenable grap  does there exist a stationary finitely dependent process on z that cannot be expressed as a finitary factor of an    a finitary isomorphism is a finitary factor that is invertible and whose inverse is also finitar  somorodinky showed that every stationary finitely dependent process on z is finitarily isomorphic to an    proces  is this true in higher dimensions? namely, is every stationary finitely dependent process on zd finitarily isomorphic to an  i", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "References", "Section": "References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": " we prove the existence of invariant measures for different switching rate  we demonstrate the applicability of our results for three nonlinear models arising in application ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1 Introduction", "Text": " then we look at the piecewise-deterministic markov process generated by switching between the vector fields f and   this idea is motivated by several observation  here we just name a few: in parametric families of vector fields, bifurcations occur genericall  therefore, they are immediately relevant for the study of pdmps as wel  in addition, the interplay between random switching and bifurcation points is not studied well enough ye  from the perspective of pdmps, this setting provides natural examples to test and extend the general theory of invariant measure  stochastic bifurcation theory is a very active area, where still many questions remain ope  hence, studying a well-defined set of standard cases involving bifurcations and stochasticity is highly desirabl 00124v1 1 jan 2019 parameters in many models are usually only known via a possible distribution and not exactl  therefore, our work contributes to the uncertainty quantification for nonlinear systems arising in application  before describing our main results, we briefly review some of the background from pdmps and from nonlinear dynamics to provide a broader perspectiv  the study of randomly switched deterministic vector fields goes back at least to the works of goldstein and kac we flow along fj for another exponential time and switch agai  this yields a continuous and piecewise smooth trajectory in rd that is, however, not the trajectory of a markov proces  to obtain a markov process, one needs to supplement the switching process on rd with a second stochastic process that keeps track of the driving vector fiel  the resulting two-component process belongs to the class of piecewise deterministic markov processes pdmps were first introduced by davis in an even more general settin  for instance, pdmps may involve jumps not only on the collection of vector fields but also on rd the class of pdmps considered in this article is also known under the names of hybrid systems and random evolutionsaside from their uses in modeling, randomly switched vector fields have intriguing theoretical propertie  for example, switching between stable vector fields can result in an unstable situation, and vice vers  recently, examples of randomly switched vector fields were found that exhibit such a reversal of stability for almost all realizations of switching times another interesting phenomenon is the regularizing effect random switching can have on a dynamical syste  for example, random switching between two lorenz vector fields with just slightly different parameter values induces an invariant probability measure that is absolutely continuous with respect to lebesgue measure on r3, whereas the dynamics associated to each individual vector field concentrate on attractors of lebesgue measure zero another recent topic is the ergodic theory for randomly switched vector field  important contributions to the question whether a switching system on a noncompact state space admits an invariant probability measure were made in in this work we focus on the invariant probability measure aspect and relate it to bifurcation point  bifurcation theory has become one of the most widely used techniques to study nonlinear systems informally, the main idea is to study vector fields under parameter variation and to determine at which points the dynamics changes fundamentally,  e, to detect the points where the phase portraits of the vector fields are not topologically equivalent upon small parameter variatio  almost full classification results exist for bifurcations with relatively few parameters,  e, codimension one or tw  these results provide suitable unfoldings, which are basically partitions of parameter space into non-equivalent phase portraits recently, substantial interest has been focused on understanding the interplay between stochasticity and bifurcation  particularly interesting dynamics seems to appear for sdes in oscillatory situations recently numerical and semi-analytical work shows that interesting effects also occur for switched systems near bifurcations therefore, it is very natural that one should try to link pdmps with bifurcation theor  in this paper, we provide a full mathematical classification of the pdmps associated to switched near local bifurcations for codimension one bifurcation  we not only include the generic fold and hopf bifurcations but also study the frequently occurring one-parameter transcritical and pitchfork bifurcation  in addition, we prove finite-time blow-up results for certain parameter regime  in summary, our theorems provide building blocks, which can be employed in various pdmp  the paper is structured as follows: in section 2 we provide more technical background from local bifurcation theory and pdmp  in section 3 we focus on all cases where below and above the bifurcation point there are non-trivial trapping region  in these cases we characterize the occurring invariant probability measures completel  in section 4 we consider the cases with only one non-trivial trapping regio  we again study the invariant measures in full detail but now also finite-time blow-up can appea  in section 5, we indicate how our results can be used in three models arising from application ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2 Background", "Text": "we briefly recall the technical background needed from the two main areas we consider in this wor  hence, this section mainly serves as a reference and to fix the notatio  readers familiar with local bifurcation theory and pdmps can skip ahead to section  ", "Subsections": [{"Section_Num": "2_1", "Section": "2.1 Local Bifurcation Theory", "Text": " we also refer to rd as the phase space of the phase space together with the foliation by trajectories x is called phase portrai  in the hyperbolic case, the hartman-grobman theorem implies that the systems and are locally topologically equivalent,   in particular, a bifurcation just corresponds to the appearance of a topologically non-equivalent phase portrait under parameter variatio  this approach presumes p is changed infinitely slowly to bring the system to and across the bifurcation poin  one option is to consider the case when p is just switched across the bifurcation point this leads us naturally to consider piecewise deterministic markov processes as introduced in the next sectio ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2_2", "Section": "2.2 Piecewise Deterministic Markov Processes", "Text": "in this subsection we introduce a class of pdmps characterized by poissonian random switching between a finite number of deterministic vector field  let i be a finite index set, and let ibelong to i be a collection of vector fields on rd with some degree of smoothnes  to introduce the basic framework, we just assume that ibelong to i are in c1, but for some of the results stated below higher degrees of smoothness are require  to be able to associate flows to the vector fields, we assume in addition that ibelong to i are forward complete,   given a starting point x0 belong to rd and an initial vector field fi, the random dynamical system we consider follows the flow associated to x0 and fi for a random tim  again, the system flows along fj for a random time until another switch occurs, et  for more general distributions of switching times, one needs to adjoin a third component that keeps track of the time elapsed since the latest switc  it is possible to consider the situation where the rate of switching depends continuously on the location of the switching trajectory on rdfor simplicity we assume that the switching rates do not depend on the process   we can then give the following rigorous description of below, we collect some results on existence, uniqueness and absolute continuity for ipm of that have been established in the literatur  we now outline an existence result from that will be needed later o  by krylov-bogoliubov, admits at least one ip  we call a point x belong to rd   reachable from y belong to rd if there is a finite sequence of indices i1,in belong to i and a corresponding sequence of positive real numbers t1, the following statements hol  in particular, there is no ipm that assigns positive mass to m+ *   now, we review suffcient conditions for uniqueness and absolute continuity of the ip  let l denote the lie algebra generated by ibelong to i,   the interesting condition is then existence of an accessible poin  besides, we have the following regularity resul ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3 Two Nontrivial Trapping Regions", "Text": " we split our analysis into two cases, which can occur for our normal forms in different parameter regime  or, trajectories leave any bounded set except for a set of measure zero, which is going to consist of unstable equilibria in our cas  in this section, we cover the case when such a trapping region exists both below and above the bifurcation valu  the case when a trapping region exists only below or only above the bifurcation value is covered in section  ", "Subsections": [{"Section_Num": "3_1", "Section": "3.1 Supercritical Pitchfork Bifurcation", "Text": " the dynamics of is easy to analyz  we now analyze the normal form from the viewpoint of pdmps by switching the parameter   the latter is precisely the ipm of the continuous-time markov chain e on the state space   the following statements hol  here, c is a normalizing constan  proof of theorem   a completely analogous reasoning applies to the positive invariant set2 does not appl  by theorem   therefore, they satisfy the formula in theorem   since x-1 is not integrable in a neighborhood of 0, we arrive at a contradictio  as shown in the proof of theorem   by theorem 2", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3_2", "Section": "3.2 Supercritical Hopf Bifurcation", "Text": " assume that l1 <   we now analyze the normal form from the viewpoint of pdmps by switching the parameter p, again working in polar coordinate  the following statements hol  first we show that any point in s1 * is accessible from s1 * fix two points 11 belong to s1 * and belong to s1 * by theorem   but theorem 3", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3_3", "Section": "3.3 Transcritical Bifurcation", "Text": " the dynamics of works as follow  by theorem   proof of proposition   now, we come to the second statemen  we will specify the function g later in the proo 6 and the ergodic decomposition theorem imply that there is no ipm assigning positive mass to *   looking at proposition   the answer follows from theorem   proof of lemma   following the proof of theorem  1 and applying theorem   the markov property and the fact that any switching trajectory starting in and converging to 0 has to visit points in m+ imply that this result extends to starting points x belong to the first return time for state must then be infinite with positive probabilit  the claim made in part 2 of theorem  7 then follows from lemma 3", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4 One Nontrivial Trapping Region", "Text": " the dynamics of works as follow  as for the transcritical bifurcation, the vector fields f-1 and f1 are not forward complet  or one can stop the pdmp with constant switching rates once it reaches certain threshold  besides, we have the following result that is reminiscent of proposition   since f-1 and f1 are odd functions, we may restrict ourselves to the intervalwith the understanding that there are completely analogous statements about the proof is analogous to the ones of proposition  7, and we omit i ", "Subsections": [{"Section_Num": "4_2", "Section": "4.2 Subcritical Hopf Bifurcation", "Text": "consider the same setting as in section   here, we encounter the same issue as for the subcritical pitchfork bifurcatio ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4_3", "Section": "4.3 Fold Bifurcation", "Text": " x+ is locally stable, while x-is unstabl  proof of theorem   the rest is analogous to the proof of proposition 3", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "5", "Section": "5 Applications", "Text": "in this section, we provide several very brief examples of systems where the switching viewpoint near bifurcations via pdmps can yield insight into concrete dynamical systems arising in application  in particular, the normal form results can be used suffciently close to bifurcation points after a normal form transformatio  furthermore, they can also be used directly to form conjectures about the dynamics of the application ", "Subsections": [{"Section_Num": "5_1", "Section": "5.1 The Paradox of Enrichment", "Text": "the paradox of enrichment is a classical topic in ecolog  linearizing around shows that the coexistence equilibrium is locally asymptotically stable for p belong to therefore, enrichment may lead to a potential increase in extinction event  of course, it is important to mention that there is still a debate in the literature on the mechanisms and possible variations of the paradox of enrichment we do not provide here a full discussion of the various arguments made in favor or against the paradox but instead point towards the effect of randomness in the parameter   then theorem  3 suggests an interesting dichotomy of the ergodic ip e, that we switch frequently enough from the periodic stable regime above the bifurcation to the stationary stable regime below the bifurcatio ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5_2", "Section": "5.2 Relaxation Oscillations", "Text": " we can also view p as a random parameter for the dynamic  if we switch the dynamics randomly above and below the fold bifurcation, theorem   hence, if we have random switching across both folds, then it is possible to obtain the classical structure of a relaxation oscillations this confirms similar observations made already numerically for a similar class of randomly switched van der pol oscillators in", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "5_3", "Section": "5.3 Adaptive Swarming", "Text": "the next ode model we are going to discuss is motivated by the swarming motion of locusts in a ring-shaped arena an adaptive network model for this situation was proposed in the network model views locusts in clockwise-moving and anti-clockwise-moving nodes and keeps track of interactions between different locusts/nodes via the links of the networ  this state corresponds to an equal number of left and right moving node  the parameter a0 controls the rate at which new connections between left and right moving nodes for  this corresponds to a classical symmetry-breaking and above the supercritical pitchfork, the two majority states are locally asymptotically stabl  then theorem   the interpretation for swarming is that we effectively can allow for a certain percentage of disordered motion as long as the switching rate back into the ordered phase is large enough to get an effective ordered phas  furthermore, if we have the case of three ipm, then we are bound to observe not only a pure ordered state but intermittent phases of disordered motion as the non-trivial measures are supported also near the locally unstable state above the bifurcation poin ", "Subsections": [], "Groundtruth": ""}], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": " for the paradigmatic q-state potts model it means a transition from the disordered color-symmetric phase to an ordered phase in which one color dominate  existing mean field theories imply that ssb in the microcanonical statistical ensemble should be a continuous proces  here we study microcanonical ssb on the random-graph potts model, and discover that the entropy is a kinked function of energ  this kink leads to a discontinuous phase transition at certain energy density value, characterized by a jump in the density of the dominant color and a jump in the microcanonical temperatur  this discontinuous ssb in random graphs is confirmed by microcanonical monte carlo simulations, and it is also observed in bond-diluted finite-size lattice system  in statistical physics a theoretical paradigm for ssb is the potts model, a simple twobody interaction graphical system in which each vertex has q discrete color states the equilibrium ssb transition of the potts model in the canonical ensemble, where inverse temperature beta is the control parameter, has been extensively investigated to compensate for the extensive loss of entropy, such a discontinuous transition is always accompanied by a discontinuous decrease of the system's energy density u the c1-continuity of s can be easily verified for the mean field potts model on a complete graph for finite-dimensional lattices the phase separation mechanism will guarantee a c1-continuous entropy profile in the thermodynamic limi  this ssb transition is driven completely by entropy competitions between the microcanonical polarized phase and the disordered symmetric phase, and at umic the mp phase is hotter than the ds phas  these theoretical predictions for random graphs are verified by microcanonical monte carlo simulation  the discontinuous ssb transition is also observed in three- and higher-dimensional bond-diluted lattices, but only for system sizes not too large the phenomenon of kinked entropy may persist in other multiple-state spin glass systems or combinatorial optimization problems our work also adds new insight on the debate about ensemble inequivalence mean field theor  arxiv:190  we now review the bethe-peierls theory for this model for simplicity we describe the theoretical equations for random regular graphs, which are maximally random except that every vertex has exactly k attached edge  this theory is exact for tree graphs, and because random graphs are locally treelike and there is no intrinsic frustration in the edge interactions, we expect it to be exact for rr graphs as wel  because short loops are rather rare in the graph, the k nearest neighbors of i will now be far separated in the perturbed cavity graph and consequently their color states will be independen  this self-consistent expression is referred to as a beliefpropagation equation the mean energy density u is obtained from e  the bp equation always has a trivial fixed point q= 1/q which corresponds to the disordered symmetric phase with all the colors being equally abundant here betacp is the lowest inverse temperature at which the cp phase becomes possibl  microcanonical ss  this fixed point is usually neglected because its free energy is higher than those of the ds and cp phases ).  but we find that it reveals a discontinuous microcanonical phase transition between the ds phase and a new microcanonical polarized phase of the configuration spac  this surprising feature of u and s leads to the two-branched entropy profile shown in the upper-left inset of fi  these two entropy branches merge and stop at umax, which is the maximal achievable energy density of the mp phas  the entropy of the lower mp branch is slightly lower than that of the ds phase so this branch has no physical significanc 4 f beta cp ds mp -  the ds solution is stable at inverse temperature beta <betads = 147, and the ds-cp phase transition occurs at betac = 174 with the energy density u dropping from -  the maximal achievable mp energy density is umax = -  microcanonical ensembl  notice that at u slightly below umic the entropy density of the mp phase is higher than that of the ds phas  in other words, at umic the partially ordered mp phase is hotter than the disordered symmetric phase and has a lower free energy densit  the discontinuous microcanonical phase transition will also occur in an extended potts model with additional kinetic energies monte carlo simulation  there are many discussions on microcanonical mc methodsand here we employ the simple demon method to draw a set of independent configurations which are located slightly below a prescribed objective energy level e  this mc dynamics obeys detailed balance, so the sampled color configurations all have the same statistical weigh  the simulation results obtained on a large rr graph instance are shown in fi  we indeed observe a discontinuous transition at the predicted critical energy density umi  we also consider bond-diluted d-dimensional hypercubic lattices of side length l with periodic boundary conditions a discontinuous ssb transition is observed in the mc dynamics for such bonddiluted lattice systems at high dimensions ) and also at the physical dimension d=3 conclusio  such a discontinuous transition was also observed in bonddiluted finite-size lattice systems in the future we need to investigate the geometric property of the configurations in the mp phaseand possible latent structures prior to the microcanonical transitionand to study systematically the microcanonical ssb transition in finite-dimensional finite-size systems and the associated inequivalence between the microcanonical and the canonical ensembles the discovered property of kinked entropy may be a general feature of random graphical models with a canonical discontinuous phase transition, and it may have important computational consequences in optimization tasks the following funding supports are acknowledged: national natural science foundation of china grants n  11421063 and n  11747601; the chinese academy of sciences grant n  qyzdj-ssw-sys01  numerical simulations were carried out at the hpc cluster of itp-cas and also at the tianhe-2 platform of the national supercomputer center in guangzho  zalt  potts, some generalized order-disorder transformations, pro  cambridge phi  so  wu, the potts model, re  mo  phy  baxter, exactly solved models in statistical mechanics phy  a: mat  theo  hu and   deng, universal critical wrapping probabilities in the canonical ensemble, nuclear phy -  xiang, phase transitions of ferromagnetic potts model on the simple cubic lattice, chinese phy  let  lee and   lucas, simple model for multiple-choice collective decision making, phy  re -  tang, phase transitions of the q-state potts model on multiply-laced sierpinski gaskets, eu  phy  xiang, partial order and finitetemperature phase transitions in potts models on irregular lattices, phy  re  let  salas, and   sokal, finite-temperature phase transition in a class of four-state potts antiferromagnets, phy  re  let  gross,   zhang, microcanonical thermodynamics of first order phase transitions studied in the potts model, an  re  martin-mayor, microcanonical approach to the simulation of first-order phase transitions, phy  re  let  see supplementary information for additional theoretical and numerical results, some technical details on constructing a bond-diluted lattice system, and a qualitative discussion of the nucleation phenomenon in finitedimensional system  let  che  phy  watanabe, evaporationcondensation transition of the two-dimensional potts model in the microcanonical ensemble, phy  re - -  re  let  m ezard and   mukamel, statistical mechanics of systems with long range interactions, aip con  pro    ruffo, statistical mechanics and dynamics of solvable models with long-range interactions, phy  re  murata and   phy  so  jp  sta  phy  huang, statistical mechanics   virasoro, spin glass theory and beyond-  xiao and  -  phy  a: mat  theo -  zhou and   sta  phy  pearl, probabilistic reasoning in intelligent systems: networks of plausible inference creutz, microcanonical monte carlo simulation, phy  re  let -  phy  a: mat  ge  janke, first-order phase transitions in the real microcanonical ensemble, phy  re    seoane, spin glasses on the hypercube, phy  re -  zhou and   ma, communities of solutions in single solution clusters of a random k-satisfiability formula, phy  re -  zhou and   sta  mec  ex  2010, p10010 6 kinked entropy and discontinuous microcanonical spontaneous symmetry breaking hai-jun zhou supplementary information", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Acknowledgments", "Section": " Acknowledgments", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "References", "Section": " References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "S1:", "Section": " S1: Exact results on complete graphs", "Text": "consider a complete graph in which every vertex interacts with every other verte  the color-symmetric fixed-point solution of e  if the energy density u decreases from the maximum value, then color symmetry has to be broke  therefore the critical energy density for spontaneous symmetry breaking is simply umic = -1 2  the other fixed-point solutions of e  3: q-state potts model on a complete grap  there is a continuous microcanonical ssb transition at energy density umic =-1 2q, which is identical to the maximum energy density the inset shows the non-concave relationship between the entropy density s and the energy density   the entropy density s at the polarized fixed point of e  the parameter m should be set to an integer value which maximizes   therefore, in the ssb phase there is only one dominant color, and all the other colors are equally abundant in the syste  this non-concave property means that there is a discontinuous ssb phase transition in the canonical ensemble at certain critical value betac of the inverse temperatur  to summarize, for the complete-graph q-state potts modelthe ssb transition is always a discontinuous phase transition in the canonical ensemble but it is always a continuous phase transition in the microcanonical ensembl ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "S2:", "Section": " S2: The random-graph Potts model in the canonical ensemble", "Text": "in the canonical ensemble the inverse temperature beta of the environment is the control parameter, and the energy density u of the system is not fixe  we now briefly describe some of the results obtained by the bethe-peierls mean field theory and by canonical monte carlo simulation  for concreteness we consider regular random graphs of vertex degree k =4 and set the number of colors to q=6, as in the main tex  the free energy density of the cp phase becomes lower than that of the ds phase as beta exceeds the critical value betac =  174, see fi 167 to a much higher value  833 and u drops from -  because of the high free energy barrier between the ds and cp phases, there is a strong hysteresis effect in the canonical mc simulation dynamics in the vicinity of betac, see fig 5 u beta cp ds mp mc1 mc2 fi  the bp fixed points are the intersection points of the curve b and the dashed diagonal lin  depending on beta there might be one, two, or three fixed point  the vertical dashed lines mark the canonical phase transition point betac =  when beta belong to the bp equation also has an unstable fixed point, referred to as the microcanonical polarized one, whose free energy density is higher than those of the ds and cp fixed point  this fixed point therefore is physically irrelevant in the canonical ensemble, see fi ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "S3:", "Section": " S3: The entropy kink at umic and the microcanonical inverse temperature", "Text": " we redraw the theoretical data of fi  2 at the vicinity of the critical energy density umic = - 864 in fi  the kink of the 9   5: this figure is complementary to fi  2 of the main tex  the same theoretical data in the upper-left inset of fi  entropy density is now quite eviden  associated with the entropy kink at umic is the discontinuity of the microcanonical inverse temperature bet  figure 6 shows the good agreement between the theoretically predicted and actually measured microcanonical inverse temperatures for the rr graph of degree k =4 at q=  this figure also shows the measured microcanonical inverse temperatures at different energy densities u for two of the eight-dimensional bond-diluted lattice systems used in fi  solid line and dotted line are theoretical predictions for rr graphs of degree k =4, and the vertical dashed line marks the predicted microcanonical phase transition point umi  symbols are microcanonical monte carlo simulation results obtained on the graph instances of fi  10 instance, and the difference increases as u further increase  further research is needed to fully understand such difference ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "S4:", "Section": " S4: The Potts model on RR graphs of large degree K", "Text": "we investigate here the asymptotic property of the microcanonical ssb phase transition of the potts model as the degree k of the rr graphs approaches infinit  the different sets of theoretical curves are for different values of   this asymptotic scaling behavior is in agreement with numerical computations, see fi e, when the graph becomes completely connecte  it would also be interesting to know the limiting behavor of the potts model as both q and k approach infinit  the results in fi  7 and fi  the different sets of theoretical curves are for different values of  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "S5:", "Section": " S5: The Potts model with large Q values on RR graphs of fixed degree K", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "S6:", "Section": " S6: Potts model with kinetic energies", "Text": "the potts model discussed in the main text only considers the interaction energies between neighboring vertices in the grap  here we introduce local kinetic energies to the vertices to make the model more general suppose there is a particle of mass m on top of each vertex i and this particle can move in a small confined space so it has a kinetic energy p2 i 2m, where pi is the momentum of this particl 5 beta fi  9: the potts model with kinetic energies for regular random graphs the discontinuous phase transition occurs at utotal = 321, at which the value of color interaction energy density u drops from u=- 842 to u=- 440, and the microcanonical inverse temperature drops from beta = 290 to beta =  from e  for intermediate values of utotal, e  as demonstrated in fi  9 for regular graphsthere is a discontinuous phase transition when the total energy density is decreased to the critical value utotal = ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "S7:", "Section": " S7: Bond-diluted lattice systems and short-range interaction range l", "Text": "the vertices in a d-dimensional hypercubic lattice of side length l are located at positions where xd are integer value  the total number of vertices in the system is n = l  each vertex has 2d bonds linking itself to its nearest neighboring vertices in spac  the length of the shortest loops in such a hypercubic graph is equal to four and it does not increase with system size   to make it more diffcult for nucleation to occurwe dilute this hypercubic graph by deleting a large fraction of bonds and keeping only k bonds for each verte  a maximally random bond-diluted lattice graph is constructed according to the following procedure:   construct an initial d-dimensional bond-diluted hypercubic lattice graph in which every vertex has exactly k active bonds this chain of alternative active and inactive bonds is further extended until it visits a vertex that is already in the chain if the length of this sampled loop is odd, nothing is change  but if the length of this loop is even, then all the active bonds in this loop are deleted from the graph while all the originally inactive bonds of this loop are added to the graph this switching action keeps the active degree of every involved vertex unchange  repeat steps and a large number of times to make the bond-diluted lattice graph as random as possibl  this loop-switching algorithm is similar to the algorithm used in re  it is easy to prove that this algorithm is ergodic and it leads to a uniform distribution among all the valid k-regular lattice graph  because all the bonds in the original hypercubic lattice are between spatial nearest neighbors, the constructed bond-diluted graphs naturally contain only bonds between nearest neighbor  we also consider diluted lattice graphs with longer interaction ranges to explore the effect of interaction range to the microcanonical ssb transitio  for this purpose, we consider a lattice system in which each vertex i at position of the periodic hypercubic lattice has d -1 bonds to all the other vertices located in or at the surface of the hypercubic box of side length centered on vertex   this lattice graph is much more densely connected than the simplest hypercubic graph of degree 2d, since each vertex has d -1 attached edge  to make it sparse we only retain k edges for each vertex and delete all the other edge  such a maximally random diluted graph can be sampled by the same loop-switching algorithm as described abov  the microcanonical mc simulation results shown in fi  10 clearly demonstrate that the interaction range has a dramatic effect on the ssb transitio ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "S8:", "Section": " S8: Droplet nucleation and phase separation", "Text": "here we review some of the key ideas of the droplet nucleation theory and discuss when droplet formation will be severely suppresse  to be concrete, we consider the d-dimensional hypercubic lattice of side length l with periodic boundary condition  in the ds phase all the q colors are equally abundant, while in the cp phase one randomly picked color is favored over all the other color  we consider the case of this canonical ssb phase transition being discontinuou  let us denote the energy densities of these two phases at betac as uc ds and uc cp, respectivel  similarly, the entropy densities of these two phases at betac are denoted as sc ds and sc c  because the ds-cp phase transition is an equilibrium one, we have sc ds -sc cp uc ds -uc cp = betac now we consider the microcanonical ensemble of fixed energy density u and assume u takes an intermediate value between uc cp and uc d  10: the potts model on four graph instances of three-dimensional short-range interaction lattice system  symbols are microcanonical mc simulation result  the vertical dashed line marks the predicted microcanonical ssb phase transition point umic = -  the argument goes as follow  suppose the ds and cp phases coexist in the system and r is the relative size of the cp phas  a consequence of e  is that, when r becomes suffciently large the surface interaction energy between the cp and ds phases will be negligible in comparison with the volume interaction energy of the drople  because of e the first derivative of s at u=uc ds is equal to beta  consequently s is c1-continuous at uc d  the inverse temperature of the system keeps the value betac during this cp phase expansion proces  this partially ordered phase for such finite-size systems may contain a percolating cluster of connected vertices which are all in the dominant color state 16 the required minimum radius rth for droplet formation and phase separation might be greatly increased in a bond-diluted lattice system as compared to an intact lattice syste  consider a rooted tree in the bond-diluted hypercubic lattice and assume the path length between two leaf vertices is ltree since each internal vertex of this tree has k attached edges, the total number of vertices in the tree is approximately kltree/  to guarantee the absence of loops the total number of vertices in the tree must not exceed the allowed number of vertices in the regio ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "References", "Section": " References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "mad: mapping and debugging framework for implementing deep neural network onto a neuromorphic chip with crossbar array of synapses roshan gopalakrishnan institute for infocomm research astar singapore roshan@i2 a-sta ed sg ashish jith sreejith kumar school of electrical and electronic engineering nanyang technological university singapore ashishji001@ nt ed sg yansong chua institute for infocomm research astar singapore chuays@i2 a-sta ed  hence, an automated mapping of any deep neural network onto the neuromorphic chip with crossbar array of synapses and an efficient debugging framework is very essentia  here, mapping is defined as the deployment of a section of deep neural network layer onto a neuromorphic core and the generation of connection lists among population of neurons to specify the connectivity between various neuromorphic cores on the neuromorphic chi  debugging is the verification of computations performed on the neuromorphic chip during inferencin  together the framework becomes mapping and debugging framewor  mad framework is quite general in usage as it is a python wrapper which can be integrated with almost every simulator tools for neuromorphic chip  this paper illustrates the mad framework in detail, considering some optimizations while mapping onto a single neuromorphic cor  a classification task on mnist and cifar-10 datasets are considered for test case implementation of mad framework", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "I", "Section": "I Introduction", "Text": "edge computing is one of the recent developments in the field of artificial intelligenc  the amount of data being processed with the ever increasing inter-connectivity of devices and internet of things, ranging from sensors to autonomous vehicles, demand for high real time data processing at the edg  among these devices, neuromorphic chip this research is supported by programmatic grant n  a1687b0033 from the singapore governments research, innovation and enterprise 2020 plan a version of this paper is submitted to ijcnn 201  has proven to be the efficient or potential candidate in terms of computational power and latenc  neuromorphic chips are developed in digitalanalog or mixed signal integrated circuit design  usual design trend is that mostly the computation and memory section is done in analog domain whereas, the communication between cores are maintained in digital domai  the neuromorphic chip discussed in this paper is based on crossbar architecture of non volatile memory synapse  however, one of the main challenges is to efficiently map the neurons on to the neuromorphic chip with hardware constraints such as core size, number of cores and fan-in/fanout the existing neuromorphic chips have a mapping framework which is more hardware specifi  ibm's truenorth chip uses corelet language based on matlab, a programming language specific to their hardwar  within this matlab framework, a mapping technique is integrated as a minimization problem spinnaker and brainscales uses a simulator-independent language, pynn based on pytho  sequential mapping is used in spinnake  neural engineering framework is developed for neurogrid neutrams addresses an optimized mapping technique based on graph partition problem: kernighan-lin partitioning strategy for network on chip  even though, every neuromorphic chip simulator tools are addressing certain mapping techniques, optimized mapping onto a single neuromorphic core is often neglected and left unexplored by defaul  most of these mapping techniques are hidden within a neuromorphic hardware specific simulators, which mitigate the requirement of an algorithm developer to understand the details of a neuromorphic chi  but, for an optimized co-development of a neural network model for a specific neuromorphic chip, the knowledge of hardware constraints is a mus  over the years, convolutional neural networks evolved to arxiv:190 00128v1 1 jan 2019 become more deep and wide with respect to the evolution of different classication tasks   from simple mnist handwritten digit classification to much more complex imagenet image classificatio  for mnist classification task, as the neural network is small, the neurons can be mapped manually onto a neuromorphic cor  but, for large networks in the case of imagenet classification, it is near impossible to manually mention how the neurons in every layers are mapped to each core in a neuromorphic chi  hence, an automated procedure is necessary for identifying the neuron addresses with corresponding synaptic weights and input value  in this paper, aforementioned issues are mitigated with the help of mad framework and its optimization  python wrapper is also suitable as a debugging tool for verification of the inferencing of neural network architectures on the neuromorphic chi  thus together the framework is called as mapping and debugging framewor  this python wrapper is developed in connection with the simulator inwhere most of the techniques are quite similar to neutrams the paper is organized as follow  section ii briefly describe about the crossbar array of synapses and the spiking neuron in a neuromorphic chi  section iii illustrates the details of mad framewor  section iv shows the implementation of mnist and cifar-10 classification task on mad framewor  finally the paper is concluded with discussion in section  ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "II", "Section": "II Materials and Method", "Text": "  spiking neuron integrator comparator reset spikes input current fi  block diagram of a spiking neuro  biological neuron computes the signal received through multiple dendrites and transmits the output signal through axons to other neurons connected in the network fig 1 shows a block diagram representation of the biological neuro  neuron has mainly two blocks, an integrator and a comparato  the integrator sums up all the input currents and build up the membrane potentia  this membrane potential is being monitored by the comparator to cross certain threshol  if the membrane potential crosses the set threshold, neuron emits an output spike and then resets the membrane potential back to its initial valu  the communication between neurons in the biological network or in a spiking neural network is with the help of these output spike  crossbar array of synapses input axons output neurons word line memory device synapse fi  crossbar array of synapses in a neuromorphic cor  fi  2 shows a crossbar array of synapse  the crossbar structure is very suitable for performing matrix dot vector multiplication along each column in a crossbar architectur  bit line collects all the weighted current at each synaptic nodes and delivers to respective output neurons for integratio  the weighted current depends on the memory element used in the intersection of word line and bit line as synaps  the synaptic weights, which draws analogy to conductances, are represented in the form of blue dots at the cross point  from kirchoffs current law, the total current flowing into each neuron from respective bit lines is the sum of currents flowing through each intersection in every colum ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "III", "Section": "III MaD Framework", "Text": "this section illustrates the details of the construction of mad framewor  the complete usage of the framework is explained with a flowchart as shown in fi  a particular neural network is chosen for a classification or a detection tas  the parameters like filter size, strides and padding among each layers are fixe  the chosen network is trained using deep learning tool for obtaining the weight files to be given as input to the mapping functio  core utilization is defined as the number of axons and neurons utilized in a single neuromorphic cor  core utilization, as shown in the flowchart, is an output from another function which calculates the number of axons and number of neurons used for mapping a section of particular layer onto a single cor  core utilization is represented as the details of the mapping function, core utilization and padding techniques are given in the subsequent subsection  this section is ended by including optimizations to be considered while mappin  fix parameter values select a neural network for a classification/detection task padding? python wrapper train using deep learning framework weights yes no connectivity matrix in dictionary format virtual padding technique core utilization total number of utilized cores connections between cores for simulator fi  flowchart of python wrapper: the details of the python wrapper is shown with a flowchar  the input and output of the mapping function that is used in the python wrapper is illustrated in the flowchar  the core utilization and weight files are marked in different color to show that these inputs are the results from other function    mapping function the mapping function is the core of the python wrapper as shown in fi  fi  3 shows the input and output of the mapping functio  the input size is the size of the input datasets, for e  stride and padding depends on the layers of the convolutional neural networ  the detailed calculation of the core utilization is mentioned in subsection iii-  weight files are the weights obtained after training the chosen neural network using deep learning too  the output section in fi  3 shows the necessary outputs that is obtained from the mapping functio  there are mainly three outputs, a connectivity matrix for verifying the interconnectivity between the cores and within the core, to verify the cores utilized and an automated generation of connection list for simulato  the steps for mapping are as follows:   all the neurons are first named to follow a regular pattern e  l1-f1-n this implies layer:1, feature map:1, and neuron in row:1 and column:  prepare a connectivity list of population of neurons in a particular layer connected to the previous laye  choose a population of neurons from a particular layer, based on the core utilization, to be mapped on to a particular cor  repeat this process until entire neurons in every layers are completely mapped onto the cor  since the naming and connectivity list are fixed at the beginning, the neurons and axons will be automatically duplicated among the cores for mappin  core utilization 2x8 4x4 4x10 6x6 neuron_col neuron_row layer n layer n-1 fi  two layers of convolution layer to illustrate the optimization of core utilizatio  layer n-1 neurons are in green, whereas layer n neurons are in re  synaptic connections are shown for two neurons in layer   consider two layers of a convolutional neural network shown in fig   the neurons in layer n is marked as red and neurons in layer n-1 is marked as gree  first two neurons in layer n is connected to layer n-1 and the synaptic connections are shown with straight line  likewise, the synaptic connections can be imagined throughout the layer with respect to the kernel size and strides used for convolutio  while mapping these two layers in fi  notice the overlap of filter window when it strides across the laye  in fact these overlapped green neurons can be mapped onto the crossbar array connections without any duplicatio  duplicating the axons, while one to one mapping of neurons connected to axons onto a core, is not a good design with respect to core utilization as input needs to be duplicated into many axons and also mapping requires bigger core sizes and ends up utilizing many cores hence, the toeplitz matrix method is utilized for efficient mapping of these layers onto a neuromorphic core without input duplicatio  toeplitz method for convolution is illustrated in inorder to calculate the core utilization, the number of neurons and axons connected together has to be chosen which could be entirely mapped onto a single cor  the number of axons can be evaluated as an algorithmic condition in the mapping function as there are overlapping axons whereas neurons selection become bit straight forwar  the overlapping axons are defined as the axons which share connections with more than a single neuron, the term overlapping is because of the overlapping nature of the axons with the neighbourhood of the kernel filter with respect to strides e  2 is considering only a single feature map, this can be easily extended to multiple feature maps by multiplying with respective channel siz  here, rows and columns of neurons correspond to neuron row and neuron col in e  that means the core utilization is in the former case and in the later cas  the intuition from this example case is that the neurons to be selected for mapping onto the core is better to be in square shape than in rectangular shap  graphical illustration of the theore  proof: the graphical illustration of the theorem is shown in fi  padding simply adds extra zeros around the input activations in a convolution layer during convolution operatio  in fact such added zeros doesn't provide any computational significance as mathematically zeros are multiplied and adde  while mapping, these padded zeros are in fact physical neurons, but need not be participating in computatio  if these neurons are considered as physical neurons during mapping, then there will be a lot of wastage on axon usag  this will reduce the optimized utilization of cor  hence, as shown in fi  later, while mapping onto the core these virtually padded neurons are removed from the connectivity list, reducing the fan in connection of those particular neurons in the periphery of a layer connected to those padded neurons in the previous laye ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "IV", "Section": "IV Results", "Text": "this section mainly provides an instance of the utilization of neuromorphic chip using a classification task on mnist and cifar-10 datasets through the parameters, core utilization and number of cores utilize  all the accuracies mentioned in this section is iterated for ten times and then averaged it ou  two sets of experiments are done for that purpose, one is to choose a particular neural network architecture for classification task on mnist and cifar-10 datasets and keep that architecture constant among different core size  different core sizes chosen here areand second set of experiment is to change the neural network architecture for different core size  this will change the accuracy of neural network architectures for different core sizes, but the number of cores utilized will remain sam  the softmax classifier output layer is not shown in the table  the neural network architecture is kept constant while mapping onto other core sizes as wel  from table iii and iv, the results for mnist and cifar-10 classification accuracy is constant among all the core sizes as the architecture remains same, while the core utilization and number of cores utilized changes with core size  for this set of experiment, the architecture is changed slightly to fit onto the respective core size  the modification of the network is only done on the number of feature maps or channels in different layer  this modification will not really affect the mapping muc  but, rather better accuracies are obtained with same number of cores utilize  the strides and padding used between all the layers are exactly same as mentioned in the previous subsectio  from table v and vi, the results for mnist and cifar-10 classification accuracy is shown for different core sizes and can be seen that the accuracy improves with increase in core size  this is obvious that bigger network can be mapped on to neuromorphic chips with bigger core sizes, bigger the network, better the accurac  the core utilization varies with mapping but the number of cores utilized remains same with core sizes", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "V", "Section": "V Discussion and Conclusion", "Text": "random access memories are popular in terms of inmemory computatio  resistive random access memory core 10 fi  division of a convolutional neural network layer into different neuromorphic core  became more popular in the field of neuromorphic computing chips with the capability of doing both computation and memory at the same tim  these two terminal rram devices are very much compatible with the crossbar array of synapses architecture, which enhanced its acceptance in the field of neuromorphic chip  the mapping of different portions of a convolutional layer onto different cores is shown in the fi  different colors within the layer shows that those neurons are mapped onto particular cor  for example, neurons in yellow are mapped onto core 1 and neurons in brown are mapped onto core 10 et  the challenges in mapping onto a single neuromorphic core are mainly explained in the section for optimization  one of the major priority while mapping is to choose the shape of the neurons in a layer that the chosen neurons and its corresponding axons could map completely onto a neuromorphic core without splitting the matrix between core  another concern is to avoid the padded neurons while inferencing or mapping as these padded neurons during training is necessary to keep the size of the input activations but during inference these padded neurons become hardware overhea  in this paper, these two challenges are mitigated using simple techniques in the mapping functio  from the results, it can be seen that bigger the core size, easier to map a bigger network and better the accurac  similarly, if the accuracy is fixed, then the lesser number of cores are utilized in a neuromorphic chip with bigger core siz  this is infact better compared to usage of more number of cores in a neuromorphic chip with smaller core sizes because the communication between neuromorphic cores will consume more power than the computation  eventhough the neuromorphic chip with bigger core size is preferable, the bottleneck is the design possibility of such bigger crossbar array of synapses with the latest cmos technolog  number of cores in a neuromorphic chip depends on the core size and the available silicon area for the chi  hence, number of cores and core size become a neuromorphic hardware constraint other than the major hardware constraints like synaptic noise, precision of weight and output  this paper gives an overview of mapping in neuromorphic chip with respect to the utilization of number of core  the python wrapper for mad framework can output a visual representation of each core in a format easily verifiable by the users the verification of network activations and inferencing becomes quite simple as wel  the code for python wrapper can be shared upon reques  acknowledgment", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "References", "Section": "References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "wild globally hyperbolic maximal anti-de sitter structures andrea tamburelli abstrac  contents introduction 1", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Introduction", "Section": "Introduction", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "1", "Section": "1. Background material", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "2", "Section": "2. Construction of the maximal surface", "Text": " description of the boundary at infinity 13   parameterisation of wild anti-de sitter structures 23", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "3", "Section": "3. Description of the boundary at infinity", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "4", "Section": "4. Parameterisation of wild anti-de Sitter structures", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "References", "Section": "References", "Text": "", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "Abstract", "Section": "Abstract", "Text": "arxiv:190  this verifies the advantage of deep nets in realizing complex feature  this exhibits a limitation of deep nets in realizing simple feature ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "I", "Section": "I Introduction", "Text": "deep learning is recognized to be a state-of-the-art scheme in artificial intelligence and machine learning and has recently triggered enormous research activitie  deep neural networks is believed to be capable of discovering deep features of data which are important but are impossible to be found by shallow neural networks a direct consequence of these challenges is that users hesitate to utilize deep learning in learning tasks with high risk such as the clinical diagnosis and financial investment, since it is not clear whether deep nets perform essentially better than the scheme in han  the first step is to correspond specific real-world applications to some data feature  the second step is to connect these data features with a-priori information which can be mathematically reflected by specific properties of function  in particular, local similarity usually corresponds to piece-wise smooth functions ; rotation-invariance generally corresponds to radial functions and sparseness on the receptive field frequently corresponds to sparseness in the spacial domain the last step is to pursue the outperformance of deep nets in approximating or learning these application-related   guo and   shi are the co-first autho  the corresponding author is   lin function  an extreme case is that there exist deep nets with two hidden layers whose capacity measured by the pseudodimension is infinitethe large capacity of deep nets inevitably makes the deep nets learner sensitive to noise and requires a large amount of computations to find a good estimato  in a nutshell, previous studies on advantages of deep nets showed that deep nets are capable of realizing various application-related data features, but it requires additional capacity cost  the first purpose of our study is to figure out whether the large capacity of deep nets to realize data features is necessar  one is that the number of layers of deep nets to realize various data features is small, the order of which is at most the logarithm of the number of free parameter  the other is that the magnitude of free parameters is relatively small, which is at most a polynomial with respect to the number of free parameter  with these two findings, we adopt the well known covering numberto measure the capacity of deep nets with controllable number of layers and magnitude of weights and present a refined estimate of the covering number of deep net  in particular, we prove that the covering number of deep nets with controllable depth and magnitude of weights is similar as that of shallow nets with comparable free parameter  as is well known, advantages of deep nets in realizing some special features do not mean that deep nets are always better than shallow net  our second purpose is to demon2 strate the necessity of deepening networks in realizing some simple data feature  the rest of paper is organized as follow  in the next section, after reviewing some advantages of deep nets in approximation, we present a refined covering number estimate for deep net  in section iii, we give a lower bound for deep nets approximation to show the limitation for deep nets in realizing simple feature  in the last section, we draw a simple conclusion of this pape ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "II", "Section": "II Advantages of Deep Nets in Realizing Feature", "Text": "in this section, we study advantages of deep nets in approximating classes of functions with complex feature  after introducing some mathematical concepts associated with deep nets, we review some important results in approximation theory which show that deep nets can realize some applicationrelated features that cannot be approximated by shallow nets with comparable free parameter  then, we present a refined covering number estimate for deep nets to show that deepening networks in some special way does not enlarge the capacity of shallow net    deep nets with fixed structures great progress of deep learning is built on deepening neural networks with structure  deep nets with different structures have been proved to be universal,   for examples, deep convolutional neural networks corresponds to toeplitz-type weight matrices and deep fully-connected nets deep nets with tree structure fi  structures for deep nets deep nets with tree structures usually correspond extremely sparse weight matrices figure 1 shows two structures for deep net  a recent focus in deep nets approximation is to pursue the approximation ability of deep nets with fixed structure  in this paper, we are interested in deep nets with structure g, the weight matrix in deep nets with tree structure g, the toeplitz-type weight matrix in deep convolutional neural network  it should be mentioned that the boundedness assumption in is necessar  this implies that the capacity of deep nets with two hidden layers and finitely many free parameters is comparable with that of lp, showing its extremely large capacit  studying advantages of deep nets in approximating functions with different a-priori information is a classical topi  it can date back to 1994, when deduced the localized approximation property of deep nets which is far beyond the capability of shallow net  we refer to for a formal definition of localized approximatio  since the localized approximation is an important step-stone in approximating piecewise smooth functions and sparse functions in spacial domainsdeep nets perform much better than shallow nets in related applications such as image processing and computer vision the following proposition, which can be found inshows the localized approximation property of deep net  mathematically, rotationinvariant property corresponds to a radial function which is by definition a function whose value at each point depends only on the distance between that point and the origi  in the nice papers, shallow nets were proved to be incapable of embodying rotation-invariance feature  to show the power of depth in approximating radial functions, we present the definition of smooth radial function as follow  denote by lip a the set of all -lipschitz continuous functions defined on   numerous learning problems in computer vision, gene analysis and speech processing involve high dimensional dat  these data are often governed by many fewer variables, producing manifold-structure features in a high dimensional ambient spac  a large number of theoretical studies, have revealed that shallow nets are difficult to realize smooth and manifold-structure features simultaneousl  where c3 is a constant independent of d0, d1, the previous studies showed that, compared with shallow nets, deep nets equipped with fewer parameters are enough to approximate functions with complex features to the same accurac  in the following table i, we list some literature on studying the advantages of realizing data future  covering number estimates in the above subsection, we have reviewed some results on the advantages of deep nets in realizing data features", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "III", "Section": "III Necessity of the Depth", "Text": "previous studies showed that, to realize some complex data features, deep nets can improve the performance of shallow nets without additional capacity cost    to present the approximation result, we at first introduce the following definitio  denote by lip the set of all -smooth functions defined on i  approximating smooth functions is a classical topic in neural networks approximatio  it is well known that the approximation rate can be as fast as o for neural networks with n free parameter  similar results has been derived in with deep nets with two hidden layers and a sigmoidal activation functio  the paper showed that deepening the networks can overcome this bottleneck for shallow net  however, it should be mentioned from that for other activation functions except the relu activation functions, such a bottleneck does not exis  thus, the paper indeed conduct a nice analysis on the necessity of deepening relu net  however, their established results can not illustrate the necessity of dept  in the following theorem that will be proved in appendix c, we show that deep nets cannot be essentially better than shallow nets in realizing the smoothness featur  comparison between deep and shallow nets the approximation rate if one only considers the smoothness featur  in other words, the smoothness feature is not sufficient to judge whether the depth of neural networks is necessar  remarks and discussions limitations of the approximation capabilities of shallow nets were firstly studied in in terms of providing lower bounds of approximation of smooth functions in the minimax sens  in another two interesting papers, limitations of shallow nets were presented in terms of establishing lower bound of approximating functions with some variation restriction  theorem 2 goes further along this direction and presents a negative answe  this result verifies the common consensus that deep learning outperforms shallow learning in some difficult learning tasksbut not alway  moreover, our result also implies that whether deep nets can help to improve the performance of the existing learning schemes depends on what features for data we are explorin  we declare that theorem 2 only presents limitations of deep nets in realizing smooth feature ", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "IV", "Section": "IV Conclusion", "Text": "in this paper, we study the advantages and limitations of deep nets in realizing different data feature  we also exhibit that for some simple data features like the smoothness, deep nets performs essentially similar as shallow net  this finishes the proof of lemma   our second lemma aims at deriving covering number of some matrix and vector with fixed free parameter  this completes the first estimat  the second estimates can be derived by using the similar approac  with these, we completes the proof of lemma   d  this completes the proof of lemma   with the help of the above two lemmas, we are in a position to prove theorem   we then use lemma 3 to estimate the second part of the above ter  we postpone the proof of theorem 3 to the end of this sectio  to present the limitations of deep nets, theorem 3 implies that we only need to estimate their covering number  we highlight that theorem 3 is motivated byin which a relation between the so-called pseudo-dimensions and lower bounds of approximation is establishe  however, estimating pseudo-dimensions of classes of functions is not so easy, even for shallow nets the following lemma which was proved in establishes a relation between n and   the following lemma can be found in both assertions yield f belong to lip and proves lemma 6 the last lemma describes the geometry of fgd this completes the proof of lemma   by the help of the above four lemmas, we are in a position to prove theorem   due tothere are more than one pf satisfying this completes the proof of theorem   this completes the proof of theorem   acknowledgement the research was supported by the national natural science foundation of china lei shi is also supported by the program of shanghai subject chief scientist", "Subsections": [], "Groundtruth": ""}, {"Section_Num": "References", "Section": "References", "Text": "", "Subsections": [], "Groundtruth": ""}]