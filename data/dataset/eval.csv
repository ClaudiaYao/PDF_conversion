Text,Groundtruth
"probabilistically safe corridors to guide sampling-based motion planning jinwook huh, om ur arslan, and daniel d. lee abstract in this paper, we introduce a new probabilistically safe local steering primitive for sampling-based motion planning in complex high-dimensional conguration spaces. our local steering procedure is based on a new notion of a convex probabilistically safe corridor that is constructed around a conguration using tangent hyperplanes of condence ellipso- ids of gaussian mixture models learned from prior collision history. accordingly, we propose to expand a random motion planning graph towards a sample goal using its projection onto probabilistically safe corridors, which efciently exploits the local geometry of conguration spaces for selecting proper steering direction and adapting steering stepsize. we observe that the proposed local steering procedure generates effective steering motion around difcult regions of conguration spaces, such as narrow passages, while minimizing collision likelihood. we evaluate the proposed steering method with randomized motion planners in a number of planning scenarios, both in simulation and on a physical dof robot arm, demonstrating the effectiveness of our safety guided local planner over the standard straight-line planner.","The paper introduces a new probabilistically safe local steering primitive for sampling-based motion planning in complex high-dimensional configuration spaces. The procedure is based on constructing convex probabilistically safe corridors using tangent hyperplanes of confidence ellipsoids of Gaussian mixture models learned from prior collision history. By expanding a random motion planning graph towards a sample goal using projections onto these corridors, the method efficiently leverages local geometry to select proper steering direction and adapt steering step size. The proposed steering procedure is shown to generate effective motion around difficult regions like narrow passages while reducing collision likelihood. Evaluation in various planning scenarios, including simulation and physical robot arm experiments, demonstrates the efficacy of the safety-guided local planner over standard straight-line planning methods."
"due to its simplicity and exibility in handling a diver- se set of conguration spaces without requiring an expli- cit representation, sampling-based motion planning is the mainstream approach to global motion planning for high- dimensional, highly nonlinear robotic systems, such as robot manipulators . however, the performance of such ran- domized motion planners strongly depends on the choice of distance measure, sampling method, and local steering; and is known to degrade signicantly around complicated regions of conguration spaces, such as narrow passages , . this performance degrade is usually considered as an issue of sampling, because uniform sampling has a voronoi bias towards yet unexplored larger regions of conguration spaces; and accordingly many heuristic rejection sampling approaches and retraction methods are suggested to mitigate this issue, but retraction methods often require a distance- to-collision measure , . on the contrary, assuming that this performance decay is due to the lack of effective local steering, in a geometric local steering policy that can feel the local geometry of conguration spaces is proposed for efcient planning around narrow passages; these authors contributed equally to this work. j. huh is with the general robotics, automation, sensing, and perception laboratory, university of pennsylvania, philadelphia, pa e-mail: jinwookh@seas.upenn.edu om ur arslan is with the autonomous motion department at max planck institute for intelligent systems, t ubingen, germany. e-mail: omur.arslan@tuebingen.mpg.de daniel d. lee is at cornell tech, new york, ny e-mail: ddl@cornell.edu fig. : probabilistically safe corridor in d space constructed around a sample conguration by using tangent hyperplanes of condence ellipsoids of a learned gaussian mixture model of conguration space obstacles. local steering via probabilistically safe corridor in d space: an rrt is extended along the safe direction towards the projection of a sample goal onto the associated probabilistically safe corridor , instead of the standard straight-line extension (blue dotted line) towards the sample goal. however, its computation also requires a distance-to-collision measure. since the exact computation of distance-to-collision in complex high-dimensional conguration spaces is hard , gaussian mixture learning and locally weighted regression are applied to construct approximate proba- bilistic models of collision and collision-free subspaces of conguration spaces for fast collision checking and biased sampling over free space and difcult regions of congurati- on spaces. in particular, simultaneous modeling of collision and free subspaces is shown to be critical for local planning around narrow passages . in this paper, by combining the strengths of and , we introduce a new notion of probabilistically safe corridors for probabilistically safe guided local steering for sampling-based planning without requiring an explicit computation of distance-to-collision. more precisely, we construct a probabilistically safe cor- ridor around a conguration using tangent hyperplanes of condence regions of learned gaussian mixtures that sepa- rate the input conguration from the condence ellipsoids, as illustrated in fig. . accordingly, we propose a pro- babilistically safe local steering primitive towards a sample goal conguration via its projection onto the probabilistically safe corridor, as shown in fig. . since the proposed steering method exploits the local geometry of congura- tion spaces via learned gaussian mixture models and generates steering motion within probabilistically safe corridors, in our numerical simulation and experiments, we observe that it yields a better exploration of conguration spaces while minimizing collision likelihood. in summary, the main contributions of the paper include: i) a novel geometric approximation of conguration space obstacles by condence ellipsoids of learned gmms, arxiv:v jan ii) a new construction of probabilistically safe corridors using tangent hyperplanes of condence ellipsoids, iii) an effective probabilistically safe local steering primitive that can minimize collision likelihood. using numerical simulations and real experiments, we de- monstrate that the proposed probabilistically safe local stee- ring approach can dramatically improve the performance of randomized motion planners around narrow passages and signicantly outperforms the straight-line local planner in high dimensional conguration spaces by decreasing the number of collisions.","Sampling-based motion planning is a prevalent method for global motion planning in complex robotic systems due to its simplicity and flexibility. However, the performance of such planners can degrade in narrow passages due to sampling issues. A new approach proposes a geometric local steering policy that considers the local geometry of configuration spaces, resulting in more efficient planning. This method, called probabilistically safe local steering, utilizes learned Gaussian mixture models to construct safe corridors for steering without requiring explicit collision computations. By enhancing exploration while minimizing collision likelihood, this approach significantly improves the performance of motion planners in high-dimensional spaces, particularly around narrow passages. Key contributions include a novel geometric approximation of configuration space obstacles, construction of probabilistically safe corridors, and an effective local steering method.Numerical simulations and experiments demonstrate the effectiveness of this approach."
"sampling-based planning approaches suffer from heavy computational time in complex environments since they typically require a considerable number of sample congu- rations and their collision checks. therefore, several biased sampling methods , and rejection sampling methods are proposed to reduce the number of sample nodes and so to improve computational efciency. however, these approaches have many heuristic parameters and require explicit conguration space information, such as visibility or collision boundaries, which usually limits their application to low dimensional settings. another alternative approach to increase the computation efciency is to reduce the number of collision checks, using either lazy collision checking or fast probabilistic collision checks , . exact safety certicates are also utilized for minimizing the computational cost of collision checks . however, these methods are still not able to address the narrow passage problem of sampling-based motion planning. in order to resolve the narrow passage problem, zhang and manocha present a steering approach that retracts sample congurations to become more likely to be connected to nearby nodes . however, it requires a signicant number of iterations to nd a new collision-free conguration that is around the collision boundary, and also requires an ap- propriate distance-to-collision measure. in practice, since the exact distance-to-collision measurement in high dimensional conguration spaces is very hard, its applicability is also li- mited to low dimensional motion planning problems. moreo- ver, workspace topology is utilized in biasing conguration space exploration for planning around difcult regions , , but the topology of high-dimensional conguration space is signicantly different and more complex than the corresponding workspace topology. local safe corridors recently nd signicant applications in collision-free motion planning by using se- quential composition of simple local planners . such safe corridors are usually constructed based on a convex decomposition of the environment, which requires an ex- plicit representation of the environment. in , a sensory steering algorithm is proposed for sampling-based motion planning that increases the connectivity of randomized mo- tion planning graphs, especially around narrow passages, by exploiting local geometry of conguration spaces via convex local safe corridors. this construction is further extended to integrate local system dynamics and local workspace geometry in kinodynamic motion planning . however, the original construction of sensory steering requires an explicit representation of conguration space obstacles or an explicit distance-to-collision metric, and so its direct application to high dimensional motion planning is limited. in this paper, we enhance this sensory steering algorithm to adapt it to high dimensional settings, such as robotic manipulation, by dening probabilistically safe corridors that are constructed using a learned approximate probabilistic model of a conguration space. iii. safety-guided rrt via probabilistically safe corridors in this section, we rst present a brief overview of how learning of gaussian mixtures can be used for approximate probabilistic modeling of conguration spaces, and then introduce a new notion of a probabilistically safe corridor around a conguration that identies a safe neighborhood of the conguration with minimal collision risk. accordingly, we propose a practical extension of the standard rrt planner, called safety-guided rrt , where tree extension is guided to ensure safety constraints dened by probabilistically safe corridors. a. gaussian mixture modeling of conguration spaces let c denote the conguration space of a robotic system embedded in an n-dimensional euclidean space rn, and denote by f c and o c, respectively, the free subspace and the collision subspace of the congura- tion space c, which, by denition, satisfy f = c \ o. in general, an explicit representation of the free space f or the collision space o in terms of simple geometric shapes is known to be very hard to obtain, especially for high- dimensional complex systems such as robotic manipulators. hence, as in , we consider approximate probabilistic representations of the free space f and the collision space o in terms of gaussian mixtures models, respectively, denoted by gm and gm, that are constructed using collision and collision-free sample congurations as described below. here, a gaussian mixture distribution gm, consisting of k n mixture components, is parametrized by a list of mixture means := k, a list of positive-denite cova- riance matrices := k and a list of normalized mixture weights := k, satisfying pk k= k = , and its value at a point x rn is given by gm := k x k= in, although other probabilistic models can be used for approxi- mating f and o, we nd it convenient to use gaussian mixtures since their condence regions can be accurately and efciently approximated using condence regions of individual gaussians which have an ellipsoidal form. safety guided steering via probabilistically safe corridors can be integra- ted with any motion planning algorithm (e.g., probabilistic roadmapsprms) as a local steering primitive, especially for uncertainty- aware belief-space planning, which we plan to explore in a future paper. fig. : examples of learned gaussian mixture models. ellipsoids show the condence regions associated with the condence level of = . gaussian mixtures in the d workspace shown in fig. , gaussian mixtures in the conguration space of a dof planar manipulator. where n is the multivariate gaussian distribution with mean and covariance matrix , n:= det exp  t  . note that the numbers of mixtures, kf and ko, used for modeling the free space f and the collision space o can be different, especially the meanshift clustering algorithm used in this paper automatically determines the number of mixture components using sample congurations based on a geometric bandwidth parameter as described below. it is also important to highlight that one can simply use gm and gm to estimate how likely a conguration is in collision, which is leveraged in for fast collision checking and biased sampling. in addition to such demonstrated potential improvements, we shall show below that condence regions of these gaussian mixture models can be utilized for understanding the local geometry of the conguration space c and for increasing the quality of the local steering heuristic (which is the euclidean distance in our case) to better approximate the true geodesic metric of the conguration space c. ) learning gaussian mixtures: one can use a num- ber of expectation-maximization variant methods for gaussian mixture learning for modeling the free space f and the collision space o using collision and collision- free sample congurations in an ofine or online manner, as in our previous work . in this paper, we apply the meanshift clustering method with a gaussian kernel for learning gaussian mixtures using collision information of sample congurations obtained during previous attempts of a randomized motion planner, which is a convenient way of learning from past experiences and exploiting the collision history. in addition, this approach resolves the problem that general mixture modeling approaches have no explicit way of determining the required number of mixtures, because the meanshift clustering requires a kernel bandwidth b instead of the number of clusters k. the kernel bandwidth b can be set based on the desired level of spatial resolution. with the bandwidth b, we initialize the clusters and then perform a single step em update to estimate cluster statistics. we set the membership weight value as zi k = if the ith point in n samples is included in the kth cluster, and zi k = otherwise. then, the cluster statistics (mass mk, mean k, covariance matrix k, and weight k) for the kth cluster are given by mk = n x i= zi k, k = mk n x i= zi kxi, k = mk pk j= mj , k = mk n x i= zi kt, for k {, , k}. in fig. , we present some examples of constructed probabilistic models of different conguration space and workspace by the suggested approach. fig. shows a probabilistic model to dene the collision space from d point clouds obtained by a depth sensor. fig. shows the generated probabilistic models using collision information of samples in the conguration space of a dof planar manipulator. such probabilistic representations of conguration spaces can be utilized for collision likelihood estimation, as a computationally efcient alternative to the exact distance-to-collision measurement . ) condence regions of gaussian mixtures: while a gaussian mixture model gm of the free space f can be used to bias sampling over the free space, in addition to its use in fast collision checking , we propose a new novel use of condence regions of a gaussian mixture model gm of the collision space o for understanding the local geometry of the conguration space c, which is the main contribution of the present paper. denition : the condence region cp of a continuous probability distribution p : rn r associated with a condence level is dened to be the super level set lp := {x rn| p } of p, for some r, over which the cumulative mass distribution of p is , i.e, cp = lp such that z lp pdx = . hence, it is convenient to have lp denote the level function of p that returns the corresponding level of p dening the condence region cp, i.e., cp = lp). although condence regions of an arbitrary probability distribution cannot be expressed explicitly in terms of simple geometric shapes and so are needed to be computed numeri- cally , condence regions of gaussian distributions have an analytical ellipsoidal form. remark : for any condence level , the ellip- soidal condence region cn and the level function ln of the gaussian distribution n are, respectively, given by cn = n xrn t f n o , ln = det exp  f n  , where f n : r denotes the cumulative probability distribution of n distribution with n degrees of freedom. hence, for any r, the condence level of the super x gaussian mixture mixture component mixture component confidence region x gaussian mixture mixture component mixture component confidence region fig. : gmm condence regions. super level sets of individual gaussians at condence level k = . super level sets of gaussians at the condence levels corresponding to a shared probability level. an example conguration space (collisions are in blue and free space is in red) and the associated condence ellipsoids of learned gmm distributions from collision samples ). level set ln of the gaussian distribution n is explicitly given by = l n = f n  log   det   . accordingly, since it lacks an exact closed-form expres- sion, we suggest approximating the condence region of a gaussian mixture distribution gm associated with a condence level as a union of ellipsoi- dal condence regions of individual gaussians, associated with condence levels := that satisfy pk k= kk = , as cgm := [k k= cn , = k [ k= n x rn|t k f n o , . observe that, by construction, we have z cgm gmdx . a standard choice of the condence levels of individual gaussians is k = for all k as shown in fig. ; however, this usually yields a poor approximation of the actual condence region of the mixture model because less accurate gaussians with high variances become more inuential in determining the condence region. a more accurate analytical choice for the individual condence levels is k = l n  k  based on a shared probability level = pk k= kln . alternatively, in this paper, we use an iterative search algorithm to nd a more accurate shared probability level as described in and set k = l n  k  for all k, as shown in fig. . with this approach, we obtain condence regions of gaussian mixture models that approximately represents conguration space obstacles, as illustrated in fig. -. b. probabilistically safe corridors suppose gm be a gaussian mixture mo- del constructed as described above for modeling the col- lision subspace o of a conguration space in rn and let cgm be the corresponding approximate condence region associated with a desired condence level = pko k= okok. accordingly, we dene the probabili- stically safe corridor around a conguration p rn to be sco:= ( x t ok ok min r f n ok , ! , k ) , = xrn (ok p) t ok ok(ok p) max r f n ok(ok p) , , k , which is constructed using tangent hyperplanes of con- dence ellipsoids of gaussians and is a closed convex polytope, as depicted fig. here, r is a scalar safety tolerance parameter, and .denotes the standard euclidean norm, and for any positive-denite covariance matrix rnn, a positive-denite choice of is = v  diag  , , . . . , n  vt where = v diagvt is the singular-value decompo- sition of . it is also useful to observe from that f n = ok for any condence region boundary point p cn. hence, the safety constraints encoded by sco are relaxed with increasing . proposition : for , the probabilistically safe corri- dor sco of a conguration p rn is a nonempty convex neighborhood of p; and for > , sco strictly contains p in its interior sco, i.e., for any p rn p sco , and p sco > proof: by denition , the probabilistically safe corridor sco is constructed as an intersection of half- spaces and so is a convex polytope. moreover, for any , these half-spaces are guaranteed to contain p . thus, the result follows. proposition : for , the probabilistically safe cor- ridor sco of a probabilistically safe state p rn \ cgm contains p in its interior sco and is also probabilistically safe, i.e., p rn \ cgm = p sco rn \ cgm. proof: for any p rn \ cgm, we have from that r f n ok< for all k. hence, the result directly follows from and the fact that for any safe conguration p rn \ cgm fig. : local steering via probabilistically safe corridors. example tree extension using a probabilistically safe corridor in d space, probabilistically safe corridor in d space. the probabilistically safe corridor sc is bounded by tangent hyperplanes of condence regions of individual gaussians that strictly separates the point p from the gaussian condence ellipsoids. note that the safe corridor sco around a probabili- stically unsafe conguration p cgm can be empty for < , especially for gaussian mixture models with signicant overlap. fortunately, many gaussian mixture learning algorithms yield proper mixture models with mini- mal overlap. moreover, in order to resolve this issue, one can consider using a nonnegative , which adaptively relaxes the safety constraints of sco depending on the safety level of the conguration p and yields a nonempty relatively safe corridor sco. thus, an optimal selection of is = , which ensures nonempty safe corridors for all congurations and exact probabilistically safe corridors for probabilistically safe congurations . c. guided steering via safe corridors we now describe a novel use of probabilistically safe corridors for guided local steering of sampling-based plan- ning, in particular, rrts. in the original rrts, a sample conguration qrand is randomly drawn in the conguration space, and then its nearest node qnear in the tree is found based on a distance measure, which is set to be the standard euclidean distance in this paper. then, a new conguration qnew is slightly extended from qnear towards qrand, say using the standard straight-line steering. if qnew is collision-free, it is added to the tree as a new node, which is connected to the nearest node. if qnew collides with an obstacle, then tree construction repeats with another qrand. in this paper, we propose a new approach for tree expan- sion where qnew is adjusted to head towards collision-free space using probabilistically safe corridors sco, as shown in fig. , by projecting qrand onto sco as follows: qproj = sco where a := arg minaax ais the metric projection of a point x rn onto a closed convex set a rn; that is to say, a returns the closest point of set a to the input point x. hence, the tree is extended towards qproj instead of qrand, as shown in fig. proposition : if a sampling-based motion planning algo- rithm is probabilistically complete for the standard straight- line steering, then the straight-line steering towards the pro- algorithm tree extension in conguration space require: : o, o : t .init; : while distance > dmin do : qrand getrandomsampling, iter = ; : while iter < max iter do : qnear getnearestneighbor; : qproj steeringguide; : qadj straightlinesteering; : if straightline is collision-free then : t .addtree, iter = iter + ; : else : break; : end if : end while : end while jected goal onto probabilistically safe corridors, as described in , preserves its probabilistic completeness for > proof: the result simply follows from proposition because the probabilistically safe corridor sco of a conguration p rn strictly contains p in its interior for > and the metric projection onto a probabilistically safe corridor locally behaves as the identity map. in other words, for > , the straight-line steering toward the projected goal onto probabilistically safe corridors is locally equivalent to the standard unconstrained straight-line steering. one computational challenge of our guided steering ap- proach is that it requires to recompute the metric projection of qrand onto sco for each new selection of qrand and so qnear. metric projection onto a convex polytope can be solved using any state-of-the-art quadratic optimization solver. for efciency, we apply the active-set method for qua- dratic optimization, which is an iterative solver that ensures a feasible solution and a decrement on the objective function at each iteration. this enables us to inherit some useful information from prior computation and stop its computation after some desired number of iterations. in order to reduce to computational cost, we keep qrand the same until a maximum number of iteration max iter is reached. this enables us to warm-start the active set method with the active constraints of the previous computation. if active constraints at the optimal solution are given, then a quadratic optimization problem with inequality constraints can be converted into a quadratic problem with equality constraints, which requires signicantly less computational time to solve the optimizati- on problem. for example, previous active constraints could be still active for slightly changed qnear if the sample goal qrand is kept the same. therefore, to increase computational efciency, we always check rst if the quadratic optimization is feasible with previously active hyperplane constraints of probabilistically safe corridors. ) tree extension in the conguration space: algorithm presents the pseudocode for the proposed tree extension methods in the conguration space. here, the nearest node algorithm tree extension in task space require: : o, o : t .init; : while distance > dmin do : qrand getrandomsampling; : qnear getnearestneighbor; : qnew straightlinesteering; : xrand, xnear, xnew fwdkin; : xproj steeringguide; : xadj xprojxnear ||xprojxnear|| ||xnew xnear||; : qadj qnear + jxadj ; : if straightline is collision-free then : t .addtree; : end if : end while qnear of a random goal qrand in tree t is extended by a new node qadj towards the projected goal qproj through the probabilistically safe corridor sco of qnear. if the random goal qrand satises the safety corridor constraints, then the tree is directly extended to the random goal, just like the standard straight-line extension method. in our implementa- tion, we set the maximum number of iterations, max iter , for using the same random goal qrand to be , and we select the maximum stepsize of the straight-line planner, , manually depending on the desired accuracy level of collision checks. ) tree extension in the task space: for task space planning, we also use probabilistically safe corridors for guiding the end-effector of a manipulator as described in algorithm using forward kinematics, we dene xrand to be the end-effector position of the random goal qrand and xnear to be the end-effector position of the nearest node qnear of qrand in tree t . here, our objective is to steer the end-effector position xnear towards xrand via the projection xproj of xrand onto the sco along the safe corridor sco in d space, as shown in fig. accordingly, we select a steering step that is proportional with the stepsize of the standard straight-line steering of the end-effector as xadj = xproj xnear ||xproj xnear|| ||xnew xnear||, and determine the corresponding conguration as: qadj = qnear + jxadj, where j is the pseudoinverse of manipulator jacobian j, satisfying j = jt in fig. , we illustrate the guided steering of a manipulator using probabilistically safe corridors in task space: the new conguration , suggested by the standard straight line planner, collides with obstacles, whereas the adjusted conguration , consistent with probabilistically safe corridors, moves in the tangent direction of obstacles. ) gmm-based biased sampling: in our experiments, we also compute the mixtures of gaussian gm fig. : examples of task-space steering of a robotic manipulator. here, the new conguration , suggested by the straight line planner from the nearest conguration , is adjusted to a better conguration based on the associated probabilistically safe corridor. for modeling the free space, which is used for biased samp- ling over the free space as described in . for the settings where biased sampling is used, instead of uniform sampling in line in algorithms and , we randomly sample a conguration from the collision-free gaussian mixture distribution gm. this sampling method increases the likelihood of a new sample being collision-free, and so can increase the computational efciency of planning as discussed below.","The text discusses various approaches to improve the computational efficiency of sampling-based motion planning in complex environments. It mentions biased sampling methods, rejection sampling methods, lazy collision checking, and fast probabilistic collision checks as strategies to reduce computational time. The use of steering approaches to address narrow passages is also highlighted. A new approach called safety-guided RRT via probabilistically safe corridors is proposed, which utilizes Gaussian mixture models for approximate probabilistic modeling of configuration spaces. Probabilistically safe corridors are defined around configurations to ensure safe motion planning. The text also introduces a guided steering method using probabilistically safe corridors to extend motion planning trees. The use of Gaussian mixture models for biased sampling over free space is also discussed to improve planning efficiency."
,
"we evaluate sg-rrt in various environments using both a simulator and a real robot. we analyze the performance of sg-rrt by comparison with several existing rrt ap- proaches. in addition, we demonstrate sg-rrt on a real humanoid robot and provide results under real settings. all experiments are performed on a ghz pc, and all planners are implemented in matlab. a. learning gaussian mixture models in all our experiments, we learn gaussian mixture models ofine by using the samples generated during the standard rrt planning (which was rich enough for accurate modeling, see fig. ) and by manually selecting the kernel band- width for the meanshift clustering so that the desired level of representation resolution is guaranteed. in particular, we select the gaussian kernel sizes for the meanshift clustering as degrees for dof manipulator planning, degrees for dof manipulator planning, and cm for task space planning. gmm learning takes seconds for clusters from , collision samples for dof manipulator, seconds for , clusters from , collision samples for dof manipulator, and seconds for clusters from a d point cloud for task space planning. for probabilistically safe corridors, we set the desired condence level = and the safety tolerance = for all cases. in future work, we plan to consider online gmm learning for adaptive motion planning in dynamic environments. b. dof planar manipulator for ease of visual presentation, we rst consider motion planning of a dof planar manipulator whose rst link is units long and second link is units long as illustrated workspace rrt sg-rrt execution time time rrt srrrt biasedrrt biasedsrrrt birrt bisrrrt corridortime execution time collision check number rrt srrrt biasedrrt biasedsrrrt birrt bisrrrt collision number number of collision checks fig. : rrt planning performance for a dof planar manipulator in fig. in fig. , we compare the computational perfor- mance of several variants of rrt planners (the standard rrt, the biased-rrt with % goal bias, and the bidirectional rrt) with and without our proposed safety guided steering. here, gmms are learned ofine along the collision space boundary ) using collision samples obtained during the standard rrt planning (green points in fig. ) and they are used online for constructing probabilistically safe corridors. in our quantitative evaluation, we consider the total execution time and the total number of collision checks as a performance measure, and we obtain the statistics of these performance measures by running each planning algorithm the number of samples for gmm learning iteration for a path to goal number of collision checks collision noncollision total fig. : safety-guided rrt planning performance with respect to the number of collision samples used for gmm learning fig. : prm with the standard straight-line planner, prm with our safety guided local planner for times for different start and goal pairs. in overall, we observe that our safety guided steering increases compu- tation performance signicantly over the standard straight- line steering by dramatically reducing the required number of planning iterations to nd a path between any given start and goal pair, as shown in fig. because safety guided steering via probabilistically safe corridors minimizes collision risk by adaptively adjusting steering direction and stepsize. as a result, our safety guided local planner yields steering action that are signicantly less likely to be in collision; whereas the standard straight-line planner ends up being in collision with more than % chance, as seen in fig. finally, we nd it useful to emphasize that the construction of and the projection onto a probabilistically safety corridor takes around msec in average for each new sample (denoted by corridortime in figure ), which is in the same order of magnitude as the computation cost of a collision check that takes around msec. in fig. , we demonstrate how the average number of rrt iterations , required for nding a path between any given start and goal pair, changes with the number of sample collision congurations (i.e., training data) used for gaussian mixture learning. as expected, the performance of rrt planning with safety guided steering increases with the increasing size of training data as a result of increasing accuracy of the gaussian mixture model. in fig. , we present an application of our safety guided steering to the probabilistic roadmap planning of the dof planar manipulator. as seen in fig. , our safety guided steering noticeably increases the connectivity of a prm as compared to the standard straight-line planner. here, two vertices of a prm is said to be connected if safety guided steering can joining them in at most table i: gmm and prm computation times gmm construction time prm construction time num. of sampling gmm total num. of prm collision connected samples time time time vertices time checks prm , no , no , , no , , no , , yes , , yes , , yes , , yes execution time case number time rrt wssgrrt sgrrt gmmrrt gmmwssgrrt gmmsgrrt total collision check case number number rrt wssgrrt sgrrt gmmrrt gmmwssgrrt gmmsgrrt total , fig. : rrt planning performance for a dof manipulator: sequential planning tasks, average execution time, average number of collision checks steps. finally, to briey compare the computation cost of the learning phases of the gmm and prm methods, we provide in table i the average computation time for the gmm and prm constructions for the dof planar manipulator planning. as expected, for the same number of samples, gmm learning is around two orders of magnitude faster then the prm construction because the connectivity test of prms is signicantly computationally costly than the nearest neighbor search and the statistics computation of gmm. c. dof manipulator in d space in order to validate the performance of sg-rrt quan- titatively in high dimensional space, we compare it with traditional approaches with a dof manipulator in d space using the webots simulator of the cyberbotics ltd. company. fig. shows the simulation scenario that is composed of seven sequential planning tasks. this scenario includes a difcult task, where the robot must remove its arm from the lower shelf and then insert it into the upper shelf. the simulation trials are repeated times for accurate evaluation, and we use the average execution time and the number of collision checks as the evaluation criteria. for the comparison, we evaluate the standard rrt, safe- guided rrt , and safe-guided rrt in the task space . in addition, since we can apply gmm- based sampling as described in section iii-c., we also evaluate gmm-based rrt , gmm-based safe- guided rrt , and gmm-based safe-guided rrt in the task space . note that we apply a bidirectional method in all approaches. the gmm-rrt can be faster than the standard rrt, and the gmmsg-rrt is the fastest among all approa- ches. the wssg-rrt and the gmmwssg-rrt are faster than the rrt and gmm-rrt. this demonstrates that the end-effector of the manipulator is effectively guided by the safe corridor in the high dimensional space, and it can reduce the computational time and the number of collision checks compared to traditional approaches. we also observe in fig. that sgrrt planning is faster and requires less collision checks in conguration spaces than in task spaces, because probabilistically safe corridors are geometrically more infor- mative when constructed in conguration spaces than in task spaces. therefore, the tree extension with the safe corridor is signicantly more efcient than the traditional methods. d. physical robot experiments we demonstrate the performance of sg-rrt on a dof manipulator of an actual humanoid robot and an rgbd camera with the scenario shown in fig. . the robot is positioned cm from the shelf on the table. figure presents the comparison results of gmmsg-rrt and the standard rrt in terms of the execution time and the number of collision checks. note that we apply a bidirectional method and give % goal biased samples. since the gmmsg-rrt adjusts a new node in the direction that avoids obstacles using probabilistically safe corridors and also utilizes biased sampling over collision-free space, the sample connectivity increases around narrow spaces, and tree expansion efciently avoids obstacles. gmmsg-rrt is signicantly efcient even when the robot needs to insert its arm onto the shelf. on the other hand, the computational time and the number of collision checks for the standard rrt planner dramatically increases in such complicated tasks. execution time case number time rrt gmmsgrrt total collision check case number number rrt gmmsgrrt total , , fig. : rrt planning performance with an actual physical robot: experiment with a physical robot, average execution time, average number of collision checks","The section evaluates safety-guided rapidly exploring random tree (SG-RRT) in various environments using a simulator and a real robot. Performance is analyzed by comparing SG-RRT with existing RRT approaches. Gaussian mixture models are learned offline using samples from standard RRT planning, with different kernel sizes selected based on the planning task complexity. Offline GMM learning takes seconds for different scenarios. SG-RRT is demonstrated on a real humanoid robot, showing improved computational performance and collision avoidance compared to traditional methods. In high-dimensional space, SG-RRT shows faster execution times and fewer collision checks compared to standard RRT planning. Physical robot experiments further validate the efficiency of SG-RRT in complex tasks."
"in this paper, we present an effective local steering ap- proach for sampling-based motion planning using probabi- listically safe corridors of learned gaussian mixture models of conguration spaces. we construct a probabilistically safe corridor around a conguration using tangent hyperplanes of condence ellipsoids of gaussian mixture models that are learned using collision history to approximate conguration space obstacles. accordingly, we propose a probabilistically safe local steering primitive that extends a random motion planning graph towards a sample goal using its projection onto the associated probabilistically safe corridor, which heu- ristically minimizes collision likelihood. we observe that the proposed local steering approach improves the performance of sampling-based planning in challenging regions, especi- ally narrow passages, by adjusting steering direction and stepsize. in our simulations and experiments with a real robot manipulator, we demonstrate that our proposed safety guided local planner shows signicant performance improvement over the standard straight-line planner for randomized motion planning of dof and dof manipulators. in a future paper, we plan to extend our work using online gmm learning for uncertainty-aware adaptive planning.","The text presents a local steering approach for sampling-based motion planning using probabilistically safe corridors around learned Gaussian mixture models of configuration spaces. This approach constructs safe corridors around configurations and proposes a local steering primitive to guide motion planning towards sample goals, minimizing collision likelihood. The method improves planning performance in challenging areas, such as narrow passages, by adjusting steering direction and step size. Simulations and experiments with a real robot manipulator show significant performance gains over standard planners. Future work aims to extend this approach by incorporating online Gaussian mixture model learning for uncertainty-aware adaptive planning."
,
"arxiv:v jan improved bethe-heitler formula wei zhu department of physics, east china normal university, shanghai , china abstract the bremsstrahlung cross section of electron in the atomic electric eld is re- derived using the time ordered perturbative theory. the results are compared with the bethe-heitler formula. we indicate that both the topt-description and a soft version for the bremsstrahlung process predict a strong screening parameter- dependent cross section, which is missed by previous bremsstrahlung theory. keywords: bremsstrahlung; qed; screening eect pacs numbers: .-m; .-t; .jx","The text presents an improved derivation of the Bremsstrahlung cross section of an electron in an atomic electric field using time-ordered perturbative theory. The results are compared with the Bethe-Heitler formula, revealing a strong screening parameter-dependent cross section not accounted for in previous theories. The study highlights the significance of both TOPT-description and a soft version in predicting this phenomenon. Key terms include Bremsstrahlung, Qed, and screening effect."
"when electrons scatter oelectric eld of proton or nucleus, they can emit real pho- tons. this is bremsstrahlung . bremsstrahlung appears in nearly all branches of physics. bethe and heitler rst gave a quantum-mechanical description of the bremsstrahlung emission at the coulomb potential of an innite heavy atom . the bethe-heitler formula is an elementary and important equation in quantum electromag- netic dynamics and astrophysics. the bremsstrahlung process of electron in the coulomb eld is a second-order process, which involves the photon emission of electron and the coulomb scattering. however, these two sub-processes have divergences. the former is infrared divergence, while the later origins from the long-range /r potential. for a neutral atom, the nuclear coulomb potential is completely screened by the elec- tron cloud, which reduces a signicant contribution from scattering distance larger than the atomic radius. therefore, the coloumb potential in the s-matrix element should be replaced by a phenomenological screening potential. however, the complex correlations between the above mentioned two sub-processes hinder us to obtained an exact analytical solution for the integrated cross section. bethe and heitler take an extra model to intro- duce the screening radius r in the solution and the result predicts a slower ln r-dependent cross section. the bethe-heitler formula was broadly applied in astrophysics. recently, a puzzled dierence of the energy spectra of electron/positron at gev-tev energy band in cosmic-ray raises our doubts to the validity of the bethe-heitler formula . we found that the bremsstrahlung cross section at the soft photon limit (the photon energy = ) contradicts with the predictions of the bethe-heitler formula. it means that the bethe-heitler formula should be improved at a general case of = for this sake, we re-derive the cross section formula of the bremsstrahlung emission with the screening potential in this work, where the time ordered perturbative theory will be used to separate the scattering and radiation processes at the equivalent photon (or weizs acker-williams) approximation. this method was successfully used to decompose a complex feynman diagram to a several simpler sub-processes in our previous works . we consider the scattering of electron on a light nucleus, where the recoil eect is not negligible, since most targets in the interstellar medium are light nuclei. besides, we will show that the contributions of interference terms between scattering and radiation sub-processes can be neglected due to the recoil eect, thus we can further decompose the process. the new bremsstrahlung formula predicts a strong r-dependent cross section. the result will inspire us to review the traditional electromagnetic shower theory at the extreme conditions. the paper is organized as follows. in sec. we detail the derivation of the bremsstrahlung formula using the topt. then we compare the results with the bethe-heitler formula in sec. a short summary is given in sec. the bremsstrahlung cross section with screening potential the bethe-heitler formula assumes that the target atom is innitely heavy. for using the topt in the following derivation, we consider a more general case: electron scattering oa nite heavy atom. the dierential cross section of the bremsstrahlung emission (fig. ) in the covariant perturbation theory at the leading order approximation is d = mem q m em |mpipipfpf k| d k med pf ef md pf ep f , where the screening photon propagator in the matrix takes ig q + i. the screening parameter has the dimension of mass and / r, r is the atom radius for a neutral atom. according to the topt, a covariant feynman propagator in mpipipfpf k s = z dl i l + me l m e + i, may decompose to a forward and a backward components: sf = i e l l + me + ef e l , forward and sb = i e l l + me + ef e l . backward figure : two elemental bremsstrahlung amplitudes. figure : the topt decomposition of fig. dashed lines indicate the time ordered of the process. note that l(el, lt, ll) is o-mass shell l = m e, while l = (e l, lt, ll) or l = (e l, lt, ll) are on-mass shell, i.e., l = m e. it seems that the topt decomposition complicates the calculation with increasing the propagators . however, the backward component will be suppressed at higher energy and small emitted angle. for example, we take l along the z-direction, and dene v as the momentum fraction of l carried by the longitudinal momentum kl of photon, v = kl l e l , where we neglect the electron mass me at high energy and note that l is on-mass shell. at high energy and small emitted angle we denote l = (e l, lt, ll) = (e l, , e l), k = (, kt, kl) = ve l + k t ve l , kt, ve l , and pf = (ef, pf,t, pf,l) = e l + k t e l , kt, e l . if v = and one can nd that sf e l + ef e l v k t , which is much larger than sb e l ef + e l ve l . therefore, the contributions of the backward propagator are negligible. this not only reduces the number of diagrams, but also allows us to factorize the complex feynman graph due to the on-mass shell of the forward propagator. this is the theoretical basic of the equivalent photon approximation. we use the processes in fig. to show this approximation. we take the laboratory frame, where the target atom is at rest, but the incident elec- tron has a high energy. this is an innite momentum frame for the electron. note that the physical picture of the same process has dierent appearances in the dierent coor- dinate frames, even in the dierent innite momentum frames. in the above mentioned frame, both the longitudinal and vertical momenta of the virtual photon generally does figure : four topt diagrams after neglecting the contributions of the backward com- ponents at high energy and small scattering angle. not disappear. thus, the contributions of figs. b and c are not negligible due to the coherence between fig. a and c. now we consider the recoil eect. the electron-atom interaction time is , is the energy loss of the incident electron. the radiation time is t t ef + e l = e lv k t . we set = ei. one can nd that at high energy ei and small scattering angle a small energy loss may suciently lead to < t,, i.e., the scattering time can not cover two time periods t and t in a same bremsstrahlung event. in this case, the contributions of the interferant processes in figs. b and c are figure : the factorized bremsstrahlung processes with the recoil eect at high energy and small scattering angle. inhibited. after removing these coherent diagrams, using the on-mass shell of the mo- mentum l, the process can further decompose to two sub-processes . we discuss the process involving in fig. , eq. becomes da = mem q m em |mpipi lpf| md pf ep f med pf ef e l ! ef + e l ! |m lpfk| d k d adpa, where d a = mem q m em |mpipi lpf| md pf ep f med pf ef , and dpa = e l ! ef + e l ! |m lpfk|d k . we calculate d a using |mpipi lpf| = e m em h lp i + p i l g(l pi m e) i h p f p i + p i p f g(pf pi m ) i , where we use l to replace l in the matrix since el e l for the small emitted angle. the result is d a = z e i (sin + /ei) cos q m sin + ei m sin d = z e i (( + eim) sin + /e i ) cos q m sin + ei m sin d z e i (sin + /e i ) cos q m sin + ei m sin d. where we used the -transfer momentum q = ei sin , the energy momentum conservation = ei ef = ei m sin , and ef + = ei + ei m sin ! . note that a q-dependent term in eq. is absent when the target is a spin- particle, however, it is does not change the following results. on the other hand, through a simple calculation, we obtain dpa padvdk t = ) v dvd lnk t. , where v = / = /. in the calculation, we turn the z-axis direction from pi to l. combining eqs. and , we have da = z e i (sin + /e i ) cos q m sin + ei m sin ) v ddvd lnk t. using z dv v = ln max min = ln ei min , and k t with since k t origins from q, we have z d ln k t = ln k t,max k t,mim = ln q , where we introduce a cut-o to regularize the collinear divergence. usually, the value relates to the measurement resolution. thus, we have the bremsstrahlung cross section in the dierential form da = z e i ln q (sin + /ei) cos q m sin + ei m sin ) v dvdd padvd, where we reorganize d a = z e i ln q (sin + /ei) cos q m sin + ei m sin d, and pa = ) v . we calculate the integrated bremsstrahlung cross section at a given initial energy through the angle-integral. note that q implies sin = /(e i ) . we decompose ln q = ln e i ln  + ei m sin /  + ln. the contribution of the second term on the right side is negligible comparing with that of the rst term. the rst term can be integrated and it contributes z/( + ) ln(e i /) at the leading order approximation. however, the third integral is not so lucky. through the numeric computations we nd that the contribution of the third term is almost z/ ln(e i /) with . thus we have da z) v dv ln e i , > ln e i . if keeping a leading term /v in pa, we obtain the total cross section a z ln ei min ln e i , > ln e i . one can understand the physical sense of as follows. the parameters and appear in a same factor / of the cross section. since / is a space scale of the screening coulomb potential, / should also be related to a space character about the process. in fact, min in eq. relates to a maximum impact parameter bmax for the scattering of electron in a central coulomb potential. therefore, / bmax and eq. has two space scales. the results and show that as long as < , the bremsstrahlung cross section is almost proportional to the geometric area of the atomic coulomb eld r, rather than a weaker ln r-dependence that the bethe-heitler formula predicted. now we calculate the process involving in fig. its dierence from da is not only the interpretations of pi l and l pf, but also they have the dierent phase spaces. corresponding to eq. we have db = mem q m em |m lpipfpf| md pf ep f med pf ef e l ! e l + ei ! |mpi lk| d k d b pdvd, where d b = z e i ln q (sin + /ef) cos q m sin + m sin d = z e i ln q (( ef m) sin + /e f) cos q m sin + m sin d z e i ln q (sin + e f ) cos q m sin + m sin d. note that ei = ef ef m sin ! , q = ef sin , and the energy momentum conservation = ei ef = ef m sin are used. after integral, we have b = a ef ei  < a.","Electrons emitting real photons due to scattering by protons or nuclei is known as bremsstrahlung, a phenomenon present across various fields of physics. Bethe and Heitler introduced a quantum-mechanical description of bremsstrahlung, leading to the Bethe-Heitler formula, widely used in quantum dynamics and astrophysics. However, a recent discrepancy in cosmic-ray energy spectra questions the formula's validity. A new bremsstrahlung formula with screening potential is derived, using time-ordered perturbative theory to separate scattering and radiation processes. This new formula predicts a stronger radius-dependent cross section, challenging traditional electromagnetic shower theories under extreme conditions. The study compares results with the Bethe-Heitler formula and highlights implications for astrophysics."
,
"the dierential cross section of the bethe-heitler formula for the bremsstrahlung is dbh = z | pf| | pi| d dcdk | q| "" pf sin f (ef | pf| cos f)(e i q) + pi sin i (ei | pi| cos i)(e f q) + p i sin i + p f sin f (ei | pi| cos i)(ef | pf| cos f) | pi|| pf| sin i sin f cos (ei | pi| cos f)(ef | pf| cos f)(e i + e f q) # , where i, f are the angles between k and pi, pf respectively; the angle between ( pi k) plane and ( pf k) plane. the bethe-heitler formula does not consider the recoil eect, therefore the scattering time of the electron with an innite heavy target is = . thus, we can not distinguish whether the photon is emitted from the incoming or outgoing electrons. in opposite to the bethe-heitler formula, eq. presents two factorized sub-processes, because of the recoil eect suppresses the contributions of figs. b and c. after integral over angles, the dierential cross section at high energy ei, ef me can be simplied as dbh z m e d e i (e i + e f eief)  log eief me  . unfortunately, eq. does not present the screening eect since it used a pure coulomb potential. bethe and heitler dene a term eief/m e in eq. as a screening radius r, i.e., dbh = z m e d e i (e f + e i efei)  log  , or dbh = z m e d e i (e f + e i efei)  log  , where the thomas-fermi model is used. however, this method freezes unreason- ably the variables eief/ and has following uncertainty: if setting any factor = /)n//)n into eq. , one can get the dierent r-dependent results. for further comparison, we refer eq. to rewrite eq. as dbh = bh pbhdv, where bh = z m e  log  , and pbhdv = d e i  e i + e f eief  . using v = /ei and d/ = dv/v, we have pbh = "" + v v # . thus, the bethe-heitler formula becomes dbh z m e  ln  "" + v v # dv. figure : the bremsstrahlung process on a coulomb potential at the double lines are the eikonal form of the electron propagators. figure : the factorized bremsstrahlung process for a soft photon version. this result has a similar leading behavior /v as eq. . however, there is a completely dierent r-dependence in bh. it is well known that at limit any process leading to photon emission can be factorized . the bremsstrahlung cross section has its soft version. two electron propagators in fig. , which are indicated by the double lines, are the eikonal form at the soft photon limit. a corresponding factorized dierential cross section is dsoft d = druth. d ln ei min v sin , nr ln q m e er using the screening potential, we have nr soft z e i ln ei min "" ln + e i # , at the nonrelativistic limit, which has the ln r-dependent cross section similar to eq. but with a /e i -suppletion. we emphasize that eq. at the nr-limit is valid only at a very narrow kinematics range near ei me, where the contributions of order o) are almost vanished. any term without scattering angle in eq. will appear a strongly r-dependent bremsstrahlung cross section, if they can not be completely canceled at ei me. we consider the integral of eq. at the er limit. because of m e in ln(q/m e) is not introduced as a cut-oparameter , the theory itself does not have any restrictions on the value of q therefore, q < m e is allowed. it implies a negative cross section. taking a step back, if we regard this me as a cut-oparameter, the result shows that the scattering will be restricted inside a small range r /| q| < /me fm, which is smaller much than atomic radius fm, and the cross section becomes irrelevant to the screening parameter . according to eq. , m e in ln(q/m e) should be replaced by a general cut-oparameter using the following substitution ln(q/m e) ln[(q/m e)/(q min/m e)] ln q , is the process-dependent and we request q > thus, the cross section at the er limit reads er soft = z e i z min d sin (sin + e i ) "" ln q ! # ln ei min , where sin /(e i ). we get er soft z lne i ln ei min, > z lne i ln ei min. it is compatible with eq. . the coecients of eq. are smaller than that of eq. . the reason is that the contributions of db are neglected in eq. . besides, the contribution of the third term in eq. is more negative due to the recoil eect. therefore, our formula is consistent with the soft version of bremsstrahlung, but both contradict with the bethe- heitler formula. according to the qed-results either eq. or eq. , we conclude that the screening parameter or /r is dened in a wrong location in the bethe-heitler formula ) for high energy bremsstrahlung. now we try to answer why a strong screening scale-dependence of the bremsstrahlung cross section has not been discovered in a long time? if , for example, me, eq. has structure m e ln e i m e ln ei min , which is similar to eq. with = me. the result is irrelevant to the screening param- eter and without a strong r-dependent eect. we think that the measurements of the dierential bremsstrahlung cross sections belong to this example. they detect the angular or energy distributions for the photons. the separation of the detected photon from the electron is restricted by the instrument resolution, which has a larger parameter . on the other hand, the r-dependent eect may obviously appear in the total bremsstrahlung cross section, where the angle and energy of the projected particles are integrated over all possible phase space and they have a minimum value of . the measurement of the ra- diation length a /a is such an example. in practical applications, there are several uncertainties: a quantitative relationship r , the value of min and the corrections of the approximations. we have suggested to measure the high energy electron spectra when they pass through the completely ionized and extremely thin atmosphere . where the atomic coulomb potential may expand to a macroscopic spatial scale cm, which is much larger than an atomic radius cm and can provide a big r-dependent eect. besides, the above mentioned uncertainties can be eectively canceled though the comparison with a normal radiation length. this application can be simplied as a = ! , or for the radiation length a = ! , where / is the radius of a referring neutral atom and or are xed by the corre- sponding data. we should mention the classical bremsstrahlung theory. it is the radiation of acceler- ated charged particle during its collisions with atomic electric eld. analogy to the soft photon limit of quantum theory, under low frequency limit the intensity of radiation is written as a factorized form d ddq di d druth dq , where is radiation frequency and ruth the classical rutherford cross section. using lim di d = m e q and druth dq = z ! q , we have d d = z m e z qmax qmin qdq = z m e "" ln qmax ! # , where qmin is used. the result is ln r. this is not surprising, since eq. corresponds really to the nonrelativistic limit in eq. . therefore, the description of bremsstrahlung in the classical electrodynamics using at high energy is not sucient.","The Bethe-Heitler formula for bremsstrahlung does not consider the recoil effect, leading to ambiguity in determining whether photons are emitted from incoming or outgoing electrons. A modified formula in the text presents two factorized subprocesses to address this issue. The formula introduces a screening radius to account for the recoil effect, but it is noted that this may introduce uncertainties in the results. The text discusses implications for high-energy bremsstrahlung, including limitations of the Bethe-Heitler formula in accounting for strong screening scale-dependence. The importance of measuring high-energy electron spectra in different environments to observe the effects of bremsstrahlung is highlighted. Additionally, classical bremsstrahlung theory is compared with the quantum theory, showing limitations in describing bremsstrahlung at high energies."
"the bethe-heitler formula describes bremsstrahlung of high energy electrons in a pure coulomb potential, which may lead to an innite total cross section since the coulomb scattering is a long-range interaction. a natural method is to use a screening potential to replace the coulomb potential. however, the complex interference eect between scatter- ing and radiation sub-processes makes a diculty for us to get an analytical solution if considering the screening potential. it brings the uncertainty in the bethe-heitler formula. for this sake, we re-derive the formula for the bremsstrahlung cross section of elec- tron in the atomic eld using the topt framework. we prove that the recoil correc- tions of a nite mass atom at high energy may further decompose the bremsstrahlung cross section to two sub-processes at the equivalent photon approximation. the im- proved bremsstrahlung formula contains the screening potential and predicts a strong r-dependent bremsstrahlung cross section. the results remind us to review the tradi- tional electromagnetic shower theory at the extreme conditions. acknowledgments author thanks l. feng, p. liu, j.h. ruan and f. wang for useful discussions. this work is supported by the national natural science of china . references h. bethe and w. heitler, proc.roy. soc., , . ams collab. , phys. rev. lett. , . . fermi lat collab. , phys. rev. d, . dampe collab. , nature, , .. s. torii , the calorimetric electron telescope : high energy astroparticle physics observatory on the international space station, the th international cosmic ray conference, july- august, the hague, the netherlands. https://www.lsu.edu/physics/les/icrc-torii.pdf. w. zhu, j.h. ruan, p. liu, l. feng and f. wang, arxiv:. m.d. scadron, advaced quantum theory and its applications through feynman dia- grams, springer-verlag, w. zhu, nucl. phys. b, ; w. zhu and j.h. ruan, nucl. phys. b, ; w. zhu, z.q. shen and j.h. ruan, nucl. phys. b, ; w. zhu, z.q. shen and j.h. ruan, b, . w. greiner and j reinhardt, quantum electrogynamics, springer-verlag f.e. low, phys. rev. , . j.d. jackson, classical elelctodynamics, wiley, new york,","The Bethe-Heitler formula describes bremsstrahlung of high-energy electrons in a pure Coulomb potential, potentially leading to an infinite total cross section due to the long-range nature of Coulomb scattering. To address this, a screening potential can be used, but the complex interference between scattering and radiation sub-processes complicates obtaining an analytical solution. By re-deriving the formula using the TOPT framework, it is shown that recoil corrections of atoms at high energy decompose the bremsstrahlung cross section into two sub-processes, leading to an improved formula that includes the screening potential. This predicts a strong distance-dependent bremsstrahlung cross section, prompting a reevaluation of traditional electromagnetic shower theory under extreme conditions."
"arxiv:v jan on polyhedral formula for kirillov-reshetikhin modules chul-hee lee abstract. we propose a method to prove a polyhedral branching formula for kirillov- reshetikhin modules over a quantum ane algebra. when the underlying simple lie algebra is of exceptional type, such a formula remains mostly conjectural. we con- vert a polyhedral formula into an identity between two rational functions of a single variable with only simple poles at known locations. it is then sucient to check the equalities of the residues at those poles, which are explicitly computable quanti- ties. by following this strategy, we obtain a computer-assisted proof of a conjectural polyhedral formula in type f","The text presents a method to prove a polyhedral branching formula for Kirillov-Reshetikhin modules over a quantum affine algebra. The formula, mostly conjectural for exceptional simple Lie algebras, is converted into an identity between rational functions of a single variable. This conversion simplifies the verification process to checking equalities of residues at known pole locations. By employing this strategy, the text achieves a computer-assisted proof of a conjectural polyhedral formula in type F."
"let g be a complex simple lie algebra. the kirillov-reshetikhin modules constitutes an important family of nite-dimensional irreducible representations of the quantum ane algebra uq(b g). an interesting problem is to understand how a kr module or their tensor product decomposes into irreducible uq-modules. the fermionic formula by kirillov and reshetikhin , proven through a series of works , gives an answer to this question by expressing the multiplicity of each irreducible summand as a certain combinatorial rule. however, it is dicult to use in practice, and it is often desirable to have a more explicit and computationally cheaper way of decomposing a single kr module. if g is of classical type, there is a well-known explicit formula called the domino removal rule. it is a polyhedral formula in the sense that the highest weight of an irreducible summand with non-zero multiplicity is characterized as a lattice point in a suitable bounded polyhedron. even when g is of exceptional type, a polyhedral formula still seems to exist, but an irreducible summand with multiplicity greater than one may appear. such a formula with multiplicity remains largely conjectural , and furthermore, even a conjectural formula has not been written completely (for example, in type e, e, or f). in fact, the only known polyhedral formula with multiplicity is when g is of type g by chari and moura. since their method is rather specic to type g, it seems dicult to adapt it to other cases in general. in this paper, we propose a method to prove a polyhedral formula. the key objects in our approach are the coecients that appear when the characters of kr modules date: january , chul-hee lee are written in some exponential form. they are essentially the residues at the poles of the generating function of the characters of kr modules. it turns out that it is possible to decompose a polyhedral formula into a nite list of identities involving these coecients; see . our method seems quite appropriate for a computer-aided mechanical approach. the diculty of the actual implementation, of course, varies according to the type of g. as our main objective in mind is of exceptional type, such a mechanical approach could be justied. after presenting the general strategy, we consider a special case when g is of type f for each node a of the dynkin diagram of g and m z, let us denote the corresponding kirillov-reshetikhin module, as a uq-module, by res w m . we obtain a computer-aided proof of the following polyhedral formula conjectured in : theorem . let g be of type f for every m z, the following holds : res w m = m j+j+j+jm j,j,j,jz pl , where p = min , and = . here we have used the same convention for enumerating the nodes of the dynkin diagram as in . this paper is organized as follows. in section we review the necessary background for our approach such as the q-system, and linear recurrence relations satised by the characters of kr modules. in section , we explain the steps for proving a polyhedral formula for kr modules. in section , we follow the procedures described in section to give a proof of theorem .","Kirillov-Reshetikhin (KR) modules are essential representations of the quantum affine algebra uq(b g) associated with a complex simple Lie algebra g. Understanding how a KR module decomposes into irreducible uq-modules is a key problem. While the fermionic formula by Kirillov and Reshetikhin provides an answer, it can be challenging to use practically. The domino removal rule offers an explicit formula for classical type g, while a polyhedral formula exists for exceptional type g, although formulas with multiplicities remain largely conjectural. This paper proposes a method to prove a polyhedral formula using coefficients derived from characters of KR modules, potentially suitable for computer-aided approaches. A computer-aided proof of a polyhedral formula for type f g is presented, demonstrating the effectiveness of the proposed method. The paper covers necessary background in section 2, outlines the steps for proving a polyhedral formula in section 3, and provides a proof in section 4."
"notation. we will use the following notation throughout the paper. g : simple lie algebra over c of rank r h : cartan subalgebra of g i = {, . . . , r} : index set for the dynkin diagram of g a, a i : simple root ha, a i : simple coroot a, a i : fundamental weight c = a,bi : cartan matrix with cab = b p = aiza : weight lattice p + p : set of dominant integral weights q : root lattice on polyhedral formula for kirillov-reshetikhin modules = p ai a : weyl vector q : highest root + : set of positive roots h r := aira : h r h r r : r-bilinear form induced from the killing form with = z : integral group ring of p (which is the same as the ring zji of laurent polynomials in ej) k := cji : eld of rational functions in ej with coecients in c ta := / {, , } []a z : coecients in the expansion = p ai[]aa sa : simple reection acting on h r by sa = a w : weyl group generated by {sa : a i} w, p : isotropy subgroup of w xing wj, j i : standard parabolic subgroup of w generated by {sa : a j} l, p + : irreducible highest weight representation of uq z : character of a nite-dimensional uq-module v = v with weight spaces v, i.e. = p pe o, p : w-orbit of","The text outlines important mathematical notations and concepts related to Lie algebras in the context of the Kirillov-Reshetikhin modules. These include definitions for simple Lie algebras, Cartan subalgebras, Dynkin diagrams, roots, weight lattices, dominant integral weights, root lattices, Weyl vectors, group rings, Laurent polynomials, field of rational functions, isotropy subgroups, parabolic subgroups, and irreducible highest weight representations. The notation and terminology established here will be crucial for understanding the subsequent technical details in the presentation on Kirillov-Reshetikhin modules."
"kr modules form a family of irreducible nite-dimensional representations of the quantum ane algebra uq(b g), where q c is not a root of unity. for every i z c, there exists a corresponding kr module w m . by restriction, we obtain a nite-dimensional uq-module res w m , which can be denoted by res w m since its isomorphism class does not depend on u, the spectral parameter, as a uq-module. let q m := m ). the q-system m ) = q m+q m + y b:cab< cab y k= q  cbamk cab , a i, m is a dierence equation that the characters of the kr modules satisfy. nakajima and hernandez proved the q-characters of kr modules satisfy the t-system from which we obtain the q-system by ignoring the spectral parameter. in , we studied a linear recurrence relation with constant coecients that the sequence m ) m= satises. we can summarize its main properties in terms of its generating function q := p m= q m tm as follows : theorem . let g be a simple lie algebra which is not of type e or e for each a i, there exist w-invariant nite subsets a and a of p with the following properties : chul-hee lee if we set d := q a q a, then n := qd is a polynomial in t with coecients in z and deg n < deg d. taa a = , where taa = {ta | a}. a a. let us x a and a as in the appendix of . when g is simply-laced, a is simply the set of weights of the fundamental representation l. note that shows that d has only simple roots. from the partial fraction decomposition of q = n/d, we deduce that for each p c z> there exists c, , , l) kji such that q m = x c, , , l)mem/l, m z, and it vanishes unless either = with a; or, = with a and ta = due to the w-symmetry of q m , we have w  c, , , l)  = c, w, , l), w w. let c := c, , , ) for a and c , := c, , , ta) for a and ta = we can rewrite as q m = x a c em + x a x :ta= c ,mem/ta. for c a we have an explicit product formula. theorem . for each a i, c a = q +[]a . here, []a z denotes the coecient in the expansion = p ai[]aa. we call the mukhin-young formula, which is originally conjectured in . we note that, in general, coecients other than c, , , ) do not seem to admit an expression as compact as . on polyhedral formula for kirillov-reshetikhin modules","KR modules are irreducible finite-dimensional representations of the quantum affine algebra uq(b g), with q not being a root of unity. Each KR module corresponds to a finite-dimensional uq-module denoted as res w m, independent of the spectral parameter u. The characters of KR modules satisfy a difference equation called the q-system. Nakajima and Hernandez proved that the q-characters also satisfy the t-system. The text examines properties of the linear recurrence relation found in the sequence m) and its generating function q = Σ q m tm. It presents the main properties of this relation in terms of w-invariant finite subsets a and a, concluding with the Mukhin-Young formula and a polyhedral formula for Kirillov-Reshetikhin modules."
"the fermionic formula, proposed by kirillov and reshetikhin , concerns the decomposition of a tensor product of kirillov-reshetikhin modules into irreducible uq-modules. let m )ai,m be a family of non-negative integers such that m is zero for all but nitely many . consider w = o  res w m  m , a tensor product of kirillov-reshetikhin modules, and its decomposition into irreducible uq-representations w = m p + ml, m z, where l denotes an irreducible uq-representation with highest weight . the fermionic formula provides an explicit combinatorial description of the multiplicity m in terms of m )ai,m and . since this formula is somewhat complicated and not essentially used in this paper, we refer the reader, for example, to for its precise statement.","The fermionic formula, introduced by Kirillov and Reshetikhin, addresses the decomposition of a tensor product of Kirillov-Reshetikhin modules into irreducible Uq-modules. It involves a family of non-negative integers and provides a combinatorial description of the multiplicity in terms of these integers. The formula is complex and not a central focus in the paper, so readers are directed to external sources for precise details."
"let = p ai caa, ca z> be the highest root of g. fix a i such that ca it is shown in that there exist positive integers jja, and dominant integral weights jja for some nite set ja such that res w m = m xf m l where f m = {jja | p jja bjxj = m, xj z}, and x = p jja xjj for each x f m . when ca > , we still expect to have a similar formula, but now with multiplicity, of the form res w m ? = m xf m pl, where p is a piecewise step-polynomial. a polyhedral formula for the decomposition of kr modules will mean a formula of the form . the fermionic formula can be used to decompose res w m for small individual ms, from which we can observe patterns and guess the form of . and then, we need a separate argument to prove since it is now a formula which is supposed to be true for all m z in the next section, we explain an approach for a proof of . chul-hee lee","The text introduces the polyhedral formula for the decomposition of kr modules in the context of dominant integral weights. It discusses the existence of positive integers and weights for a finite set, presenting a formula for the decomposition using step-polynomials. The fermionic formula is used to decompose specific values of m, leading to patterns that inform the overall formula. The text mentions the need for a separate proof argument to validate the formula for all integer values of m."
"let p m denote the character of the right-hand side of , i.e., p m = x xf m p). here the letter p is chosen from the word polyhedron. when there is such a polyhedral formula, consider its generating function p := x m= p m tm, which is expected to be a rational function in t in general. then, is equivalent to p = q, an identity between two rational functions. we can state the steps necessary to prove as follows : prove that p has at most simple poles, and the set of poles of p is a subset of the set of poles of q. also make sure that the degree of the denominator of p is greater than that of its numerator. prove the equality of the coecients c, , , l) = c, , , l) when belongs to one of the following cases : = with a p +; = with a p + and ta = recall that we already know explicitly where the poles of q are located, which are always simple. we can deduce from that p m tm can be written in the form p m = x c, , , l)mem/l with c, , , l) kji, which vanishes unless the non-vanishing conditions for c, , , l), stated after , are satised. step is equivalent to showing that p and q have the same residues at their poles, which are simple at most. since both p and q are w-invariant, the coecients follow the same w-symmetry in . hence, it is enough to consider weights in p + because every element of p has a unique element in p + in its w-orbit. once p m is explicitly given as in , it is more or less straightforward to compute p and c, , , l). for computing c, , , l), we can use the q-system along with previously known c, , , l) with b i such that cab < ; when there is a known polyhedral formula for b i, we can explicitly compute c, , , l). on polyhedral formula for kirillov-reshetikhin modules suppose that we already have proved . then another way to nish the proof of p = q is by showing q m = p m for m = , , . . . , |a| + ta| a| since now we know that both sequences satisfy the same linear recurrence relation of order |a| + ta| a| although it is a purely mechanical task to check q m = p m for given m using the fermionic formula, we have found that it is still computationally challenging for m large. by considering c, , , l) and c, , , l), we localize the problem in the sense that we are looking at a single pole at a time, and thus obtain further simplications. in a nutshell, it is possible to check both and algorithmically. in the next section, we follow this strategy to prove a conjectural polyhedral formula in type f, where we discuss some practical issues in our method in detail. remark . we know that c, , , l) is invariant under w from . when we explicitly compute c, , , l), it is given as a sum over w; see for an example. thus we can regard as a summation formula over w for c, , , l).","The text presents a framework for proving polyhedral formulas, focusing on the equality of rational functions denoted by p and q. It emphasizes the steps needed to prove this equality, including analyzing poles, coefficients, and symmetries. The text also mentions techniques for computing coefficients and verifying the equality of the two functions for specific values. It concludes by highlighting the computational challenges faced in the process and suggests strategies for simplification. Overall, the text outlines a systematic approach for proving polyhedral formulas and hints at practical issues that may arise in the process."
"let g be a simple lie algebra of type f when a = or , there is a known polyhedral formula for res w m . the formula for a = is given in and will be used later. the main goal of this section is to prove theorem , namely, the polyhedral formula for a = for m z, let p m be the character of the right-hand side of and p = p m= p m tm. by following the strategy outlined in section , we will show that q m = p m , that is, q = p as rational functions in t. before turning to proofs, we present a table for a and a from . since they are w-invariant, they are given as a disjoint union of w-orbits of elements of p + : a a p + a p + , , , , , , , , , + , . throughout the section, = p wwew denotes the weyl denominator and = . we often need to explicitly deal with w or its subgroups. one may refer to for an algorithm to nd the weyl orbit of a weight or a minimal coset representative for a coset of a standard parabolic subgroup chul-hee lee of a weyl group. the accompanying mathematica notebook le for some computer calculations is available at https://github.com/chlee-/kr-polyhedral-formula.","The section proves Theorem ?? by demonstrating the polyhedral formula for a simple Lie algebra of type f when a = or through rational functions in t. It introduces a polyhedral formula for a = and utilizes a character, p m, to illustrate the result. The text presents a table for a and a, highlighting their properties as w-invariant. Throughout the proof, the Weyl denominator and other relevant terms are defined. Additionally, reference is made to an algorithm for finding Weyl orbits and a GitHub repository for associated computer calculations."
". from , it is clear that q can have only poles of order at most and they can only be found at t = e, let us nd the poles of p. to write p explicitly, dene a sequence {am} m= by am = x j+j+j+jm j,j,j,jz p xj xj xj xj , whose generating function is x m= amtm = by combining this with the weyl character formula, p can be written as x ww ew t) t) t) t) t) at this point, it is not entirely clear whether p has only simple poles at t = e, or not. for example, p may have a double pole at t = e consider the partial fraction decomposition of a summand in : t) t) t) t) t) = d w; t + d w; ewt + d w; ewt + d w; ewt + e w; ewt + e w; ewt + e w; t), where d w;, e w; k, and e w; and e w; k are polynomials of degree at most they are uniquely determined by this form of decomposition. for example, d w; = ) ) ) ) because these expressions are long but easy to nd, we do not write them here; one can refer to the accompanying le for an explicit description. proposition . we have p = x ww ew d w; t + d w; ewt + d w; ewt + d w; ewt ! . on polyhedral formula for kirillov-reshetikhin modules proof. it is sucient to show that for {, , }, x ww ewe w; = we may use computers to verify this directly. below, we will explain how to reduce the amount of calculation to check . while this reduction is not essential as long as we focus on type f whose weyl group is manageable in size, it might be useful for treating a similar vanishing sum over bigger groups in other types. for = = , w = w{,,}. the parabolic subgroup w{,} of w{,,} satises x ww{,} ewe w; = which implies the vanishing of the sum over w{,,} since can be written as x ww w x ww{,} ewe w; where w is the set of minimal coset representatives for cosets in w{,,}/w{,}. similarly, when = = , we have w = w{,,} and the following sum over the parabolic subgroup w{,} vanishes : x ww{,} ewe w; = when = , we have not found any proper parabolic subgroup of w = w{,,}, over which the sum vanishes. however, if we let e := x ww{,} ewe w;, then the left-hand side of becomes x ww w ) where w is the set of minimal coset representatives for cosets in w{,,}/w{,}. the size of w is , and it is possible to partition this set into pairs of distinct elements so that the contribution from each pair to the above sum is zero. in other words, for each w w , there exists w w , w = w such that w ) + w ) = this proposition immediately implies the following : chul-hee lee proposition . the rational function p has only simple poles, possibly at t = e, and no other poles. now we know that the poles of q and p can only appear at t = e,","The text explains how to determine the poles of a rational function p using a sequence and the Weyl character formula. It discusses the possibility of p having double poles at t = e but shows through partial fraction decomposition that p only has simple poles at t = e. Using a polyhedral formula for Kirillov-Reshetikhin modules, the text simplifies calculations to verify the vanishing sum property. Ultimately, it concludes that the rational function p has only simple poles, potentially at t = e, and no other poles, confirming that the poles of p and q can only be found at t = e."
". it remains to carry out the second step of our strategy in section to prove . as is empty, we have to show c, , , ) = c, , , ) for {, , , } p +. we rst explain how to compute both sides, and then check their equality. let us write c = c, , , ) and d = c, , , ). how to calculate c . let us explain how to calculate c . recall the q-system rela- tion q m = m ) q mq m+ for a = by rewriting this relation using , we obtain an expression for c in terms of c , : c = x s c c , where s := { : = , + = }. to handle explicitly, we need a way to compute c and c . and these are all we need to nd c , because c with non-zero o is given by c w = w ). lemma . we have c = p wwew/) , and c = / y + [] proof. note that c is given by theorem , the mukhin-young formula. to nd c , we can exploit the known polyhedral formula from q m = m x k= ) . on polyhedral formula for kirillov-reshetikhin modules by the weyl character formula, q = x ww ew ) = x ww ew d w; t + d w; tew ! , where d w; = ew, and d w; = ew. therefore, c = p wwewd w; . how to calculate d . recall that we have p m = x d em. by proposition , we can write d as d = p wwewd w; . for a dominant weight , w is a standard parabolic subgroup of w. once we enumer- ate the elements of w, it is straightforward to compute d . of course, it becomes computationally easier to manipulate when w is a proper subgroup of w. in this sense, the most dicult case arises when = now we can compute both and and thus, are ready to check c = d for {, , , }. to use we need s = { : = , + = }, as described below. the cases of {, , } do not bring much diculty, and a computer can easily simplify c d and return zero. we give further comments on the = case, which is the most dicult one. . = case. s = n , , , , , , , , , o . chul-hee lee . = case. in this case, s = {, }. thus gives c = c c in fact, this identity is a special case of . . = case. s = n , , , , , o . . = case. our goal is to check whether c d is actually zero, but this calculation is not quite straightforward as before, since they are quite huge rational functions. can be rewritten as c = x o c c , and |o| = and d involves an alternating sum of orbits of over the entire weyl group w and hence, it is obtained by adding |w| = rational functions in e, . . . , e it is slightly better to work with c and d to simplify their denominators. let us consider d = x ww ewd w; we can rewrite the above as d = x ww {,,} w x ww{,,} ewd w; where w {,,} denotes the set of minimal coset representatives of cosets in w/w{,,}. note that the size of w {,,} is let fw = w x ww{,,} ewd w; , w w {,,}. in our computer calculation, we further considered a partition of w {,,} into disjoint subsets w {,,} i , i = , . . . , , say, w {,,} = f i= w {,,} i . we can write as d = x i= x ww {,,} i fw . on polyhedral formula for kirillov-reshetikhin modules finally, we start with c , subtract (p ww {,,} i fw), and simplify the expression at each step i = , . . . , once we subtract every summand, the result becomes be zero, as we wanted. on our desktop computer with a ghz cpu and gb of ram, it took about seconds to complete this calculation. we note that the partition for w {,,} we used is simply found through many computer experiments to reduce the time required to complete the calculation, and may be hardly optimal.","The text discusses the computational steps involved in proving the equality between two mathematical expressions, denoted as c and d. The expressions are calculated using a known polyhedral formula and involve manipulating rational functions. The most challenging case is described for a specific scenario. The calculations are performed using a computer and involve dividing the Weyl group into subsets to simplify the expression and verify that c equals d. The process is detailed, including the use of specific subsets and the time taken for the computer calculations to complete."
,
,
,
"symmetric integrators based on continuous-stage runge-kutta-nystr om methods for reversible systems wensheng tanga,b, jingjing zhangc,,, acollege of mathematics and statistics, changsha university of science and technology, changsha , china bhunan provincial key laboratory of mathematical modeling and analysis in engineering, changsha , china cschool of science, east china jiaotong university, nanchang , china abstract in this paper, we study symmetric integrators for solving second-order ordinary dierential equations on the basis of the notion of continuous-stage runge-kutta-nystr om methods. the construction of such methods heavily relies on the legendre expansion technique in conjunction with the symmetric conditions and simplifying assumptions for order conditions. new families of symmetric integrators as illustrative examples are presented. for comparing the numerical behaviors of the presented methods, some numerical experiments are also reported. keywords: continuous-stage runge-kutta-nystr om methods; reversible systems; symmetric integrators; simplifying assumptions; legendre polynomials.","The text discusses the development of symmetric integrators using continuous-stage Runge-Kutta-Nyström methods for reversible systems. The integrators are designed for solving second-order ordinary differential equations, employing Legendre expansion technique along with symmetric conditions and simplifying assumptions for order conditions. The paper introduces new families of symmetric integrators and includes numerical experiments to compare their numerical behaviors. Key words include continuous-stage Runge-Kutta-Nyström methods, reversible systems, symmetric integrators, simplifying assumptions, and Legendre polynomials."
"numerical integration that preserves at least one of geometric properties of a given dynamical system has attracted much attention in these years . as suggested by kang feng , it is natural to look forward to those discrete systems which preserve as much as possible the intrinsic properties of the continuous system this is a truly ingenious idea for devising good integrators to properly simulate the evolution of various dynamical systems with geometric features. it is evidenced that numerical methods with such a special purpose can not only perform a more accurate long-time integration than those traditional methods without any geometric-feature preservation, but also produce an improved qualitative behavior . such type of methods, generally associated with the terminology geometric integration, are distinguished by the geometric properties they inherit, including symplectic methods for hamiltonian systems, symmetric methods for reversible systems, volume-preserving methods for divergence-free systems, invariant-preserving methods for conservative systems, multi-symplectic methods for hamiltonian partial dierential equations etc. for more details, we refer the interested readers to and references therein. reversible systems and reversible maps are of interest in both aspects of theoretical study and numerical simulation for many dierential equations . let be an invertible linear transfor- corresponding author. email addresses: tangws@lsec.cc.ac.cn , jjzhang@outlook.com preprint submitted to elsevier june , arxiv:v jun mation in the phase space of a rst-order system given by z = f, then the system is called -reversible if f = f, for z, and a map is called -reversible if = . particularly, it is shown in that all second-order systems with the form z = f are reversible as they can be transformed into reversible rst-order systems. in addition, notice that the exact ow of a reversible system is a reversible map, it is therefore natural to nd a numerical method h, which is better referred to as a reversibility-preserving integrator, such that it is also a reversible map (i.e., h = h ). it is known that a number of symmetric integrators automatically possess this property, e.g., all symmetric runge-kutta methods, some partitioned runge- kutta methods for special partitioned systems, some composition and splitting methods, and standard projection methods for dierential equations on special manifolds . to be specic, we quote the following result from . theorem . a runge-kutta method or a runge-kutta-nystr om method is reversible iit is symmetric. thanks to the property of reversibility preservation, symmetric integrators often have an ex- cellent long-time numerical behavior than those non-symmetric integrators for reversible systems . so far, a wide variety of eective symmetric integrators have been proposed (see [, , , , , , ] and references therein). in the context of geometric integration, the greatest interest has been given to the develop- ment of symplectic integrators for solving hamiltonian systems over the last decades . however, if the hamiltonian h satises h = h, then the system is reversible with respect to the linear transformation : particularly, a well-known class of separable hamiltonian systems determined by the hamiltonian h = pt mp + u happens to be such type of reversible systems. therefore, it makes sense for devising a numerical method that preserves symplecticity and reversibility at the same time, and fortunately, this has been shown to be an attainable goal . besides, a numerical method which is energy-preserving and reversibility-preserving can also be of interest . in recent years, numerical methods with innitely many stages including continuous-stage runge-kutta methods, continuous-stage partitioned runge-kutta methods and continuous-stage runge-kutta-nystr om methods are presented and discussed by several authors, see . they can be viewed as the natural generalizations of numerical methods with nite stages . it is shown in that by using continuous-stage methods many classical rk, prk and rkn methods of arbitrary order can be derived, without resort to solving the tedious nonlin- ear algebraic equations in terms of many unknown coecients. if the system is non-autonomous, we can introduce an extra equation namely t = to rewrite the original system as an autonomous system. the construction of continuous-stage methods seems much easier than that of those traditional methods with nite stages, as the associated butcher coecients are continuous or smooth functions and hence they can be treated by using some analytical tools . moreover, as presented in , numerical methods serving some special purpose including symplecticity-preserving methods for hamiltonian systems, sym- metric methods for reversible systems, energy-preserving methods for conservative systems can also be established within this new framework. besides, a well known negative result we have to mention here is that no rk methods is energy-preserving for general non-polynomial hamilto- nian systems , in contrast to this, energy-preserving csrk methods can be easily constructed . in addition, as presented in , some galerkin variational methods can be interpreted as continuous-stage rk methods, but they can not be completely understood in the classical rk framework. therefore, continuous-stage methods have granted us a new insight for numerical integration of dierential equations and some subjects in this new area need to be investigated. since symmetric integrators possess important theoretical and real values in numerical ordinary dierential equations , we are concerned with the development of new symmetric integrators for solving second-order ordinary dierential equations . the construction of such methods in this paper is on the basis of the notion of csrkn methods and heavily relies on the legendre polynomial expansion technique. furthermore, by using gaussian and lobatto quadrature formulas we show that new families of symmetric rkn-type schemes can be easily devised. moreover, by theorem , these methods are also reversibility-preserving and therefore very suitable for solving reversible systems. this paper will be organized as follows. in section , we introduce the exact denition of csrkn methods for solving second-order odes and the corresponding order theory previously developed in will be briey revisited. in section , by using legendre expansion technique, we present some useful results for devising symmetric integrators which is then followed by giving some illustrative examples for deriving new symmetric integrators in section some numerical experiments are reported in section at last, we give some concluding remarks in section to end this paper.","The text discusses the development of numerical integration methods that preserve geometric properties of dynamical systems. Methods such as symplectic, symmetric, volume-preserving, and reversible integrators are highlighted, each tailored to specific system characteristics. Symmetric integrators, particularly runge-kutta methods, are emphasized for their ability to preserve reversibility, leading to improved long-term numerical behavior. Continuous-stage methods are introduced as generalizations of traditional finite-stage methods, offering advantages in construction and efficiency. The discussion also covers the construction of new symmetric integrators for second-order ordinary differential equations, showcasing the use of legendre polynomial expansion and quadrature formulas. The text provides a structured overview of methods for solving reversible systems, including the formulation of csrkn methods and numerical experiments to demonstrate their effectiveness."
"in this section, we will recall the notion of the so-called continuous-stage runge-kutta-nystr om methods and review some known results which are useful for constructing such methods of arbitrarily high order. for more details, see .",The section discusses continuous-stage Runge-Kutta-Nyström methods and their order theory. It emphasizes recalling the concept and reviewing established results that aid in developing methods of high order.
"consider the following initial value problem governed by a second-order system q = f, q = q, q = q , where f : r rd rd is a smooth vector-valued function. a well-known numerical method for solving is the so-called rkn method with s stages, which can be depicted as qi = q + hciq + h s x j= aijf, i = , , s, q = q + hq + h s x i= bif, q = q + h s x i= bif, and it can be characterized by the following butcher tableau c a b b where a = ( aij)ss, b = ( b, , bs)t , b = t , c = t . compared with an s-stage rk method applied to the corresponding rst-order system deduced from , the rkn method is preferable since about half of the storage can be saved and the computational work can be reduced a lot . as a counterpart of the classical rkn method, the csrkn method can be formally dened. denition . let a, be a function of variables , and b, b, c be functions of . for solving , the continuous-stage runge-kutta-nystr om method as a one-step method mapping (q, q ) to (q, q ) is given by q = q + hcq + h z a,fd, , q = q + hq + h z bfd, q = q + h z bfd, which can be characterized by the following butcher tableau c a, b b","The continuous-stage RKN method is a numerical method used to solve initial value problems governed by a second-order system. It is an improvement over traditional methods as it requires less storage and reduces computational work significantly. This method is depicted through equations and a butcher tableau, and compared to other methods for solving first-order systems. Additionally, a derivative method called the CSRKN method is formally defined as a counterpart to the classical RKN method."
"denition . a rkn-type method is of order p, if for all regular problem , the following two formulas hold, as h , q q = o, q q = o. we introduce the following classical simplifying assumptions for rkn methods b : s x i= bic i = , , cn : s x j= aijc j = c+ i , i s, , dn : s x i= bic i aij = bjc+ j bjcj + bj + , j s, theorem . if the coecients of the rkn method - satisfy the simplifying assumptions b, cn, dn, and if bi = bi holds for all i = , . . . , s, then the method is of order at least min{p, + , + }. analogously to the classical case, we have the following simplifying assumptions for csrkn methods b : z bc d = , , cn : z a, c d = c+ , , , dn : z bc a, d = bc+ bc + b + , , theorem . if the coecients of the csrkn method - satisfy the simplifying assumptions b, cn, dn, and if b = b holds for , then the method is of order at least min{p, + , + }. let us introduce the normalized shifted legendre polynomial pk of degree k by the following rodrigues formula p = , pk = k + k! dk dxk , k = , , , . a well-known property of legendre polynomials is that they are orthogonal to each other with respect to the l inner product in z pjpk dx = jk, j, k = , , , , where jk is the kronecker delta. for convenience, we list some of them as follows p = , p = , p = , . theorem . for the csrkn method - denoted by ( a,, b, b, c) with the assumption b = , c = , the following two statements are equivalent to each other: both cn and dn hold true; a, possesses the following form in terms of legendre polynomials a, = p + p + n x = +pp+ n x =   + +  pp + n x = +p+p + x i j pipj. where = , n = max{ , }, n = max{ , }, n = max{ , } and are arbitrary real numbers. recall that we have b by using b = , c = , thus theorem implies that we can easily construct a csrkn method with order min{, +, +} = min{+, +} . however, for the sake of deriving a practical csrkn method, we need to dene a nite form for the coecient a, , which can be easily realized by truncating the series . in such a case, we get a, which is a bivariate polynomial. consequently, by applying a quadrature formula denoted by s i= to -, it leads to an s-stage rkn method qi = q + hcciq + h s x j= bj aci,cjf, i = , , s, q = q + hq + h s x i= bi bcif, q = q + h s x i= bibcif, whose butcher tableau is cc b ac,c bs ac,cs . . . . . . . . . ccs b acs,c bs acs,cs b bc bs bcs bbc bsbcs if we additionally assume b = b, b = , c = , then it gives an s-stage rkn method with tableau c b ac,c bs ac,cs . . . . . . . . . cs b acs,c bs acs,cs b bs b bs where bi = bi, i = , , s. in view of theorem , we have the following result for analyzing the order of the rkn method with tableau . theorem . assume a, is a bivariate polynomial of degree a in and degree a in , and the quadrature formula s i= is of order p. if the coecients of the underlying csrkn method - satisfy b = b, b = , c = , and both cn, dn hold true, then the rkn method with tableau is of order at least min, where = min(, p a + ) and = min(, p a + ).","An RKN-type method of order p satisfies specific formulas for all regular problems. Various simplifying assumptions are introduced for RKN and CSRKN methods, with the latter involving Legendre polynomials properties. The equivalence of certain statements and the construction of a CSRKN method with order min{p, +, +} are discussed. By applying a quadrature formula and truncating series, a practical CSRKN method is derived with a bivariate polynomial coefficient. The order of the RKN method is analyzed based on the degree of the polynomial and the quadrature formula's order. If certain conditions are met, the RKN method is of at least a certain order."
"now let us introduce the denition of symmetric methods and then show the conditions for a csrkn method to be symmetric. denition . a numerical one-step method h is called symmetric if it satises h = h, where h = h is referred to as the adjoint method of h. symmetry implies that the original method and the adjoint method give identical numerical results. an attractive property of symmetric integrators is that they possess an even order . by denition, a one-step method z = h is symmetric if exchanging h h, z z and t t leaves the original method unaltered. theorem . if the coecients of the csrkn method - satisfy c = c, a, = b b + a,, b = b b, b = b, for , , then the method is symmetric. proof. firstly, let us establish the adjoint method. from -, by interchanging t, q, q , h with t, q, q , h respectively, we have q = q hcq + h z a,fd, , q = q hq + h z bfd, q = q h z bfd. notice that t ch = t + h, thus becomes q = q + h z bfh, q)d. substituting it into yields q = q + hq + h z (b b)fh, q)d. next, by inserting and into , it follows that q = q + hq + h z b + a,)fh, q)d. by replacing and with and respectively, we can recast , and as q = q + hc q + h z a ,f(t + c h, q )d, , q = q + hq + h z b f(t + c h, q )d, q = q + h z b f(t + c h, q )d, where q = q, and c = c, a , = b b + a,, b = b b, b = b, for , . therefore, we have get the adjoint method dened by and . given that a csrkn method can be uniquely determined by its coecients, hence if we require the following condition c = c , a, = a ,, b = b , b = b , namely the condition , then the original method is symmetric. in the following we present a preferable result for ease of devising symmetric csrkn methods. theorem . suppose that b = b, b = , c = , then the csrkn method de- noted by ( a,, b, b, c) is symmetric, if a, possesses the following form in terms of legendre polynomials a, = p + p + x i+j is even i+j> pipj, , , where = and are arbitrary real numbers. proof. by noticing b = b, b = , c = , it suces for us to consider the second condition given in . by using a simple identity = p + p, it implies a, a, = = p). next, let us consider the following expansion of a, in terms of the legendre orthogonal basis {pipj : i, j }, a, = x i,j pipj, r, and then by replacing and with and respectively, with the help of p = p , we have a, = x i,j i+jpipj. substituting the above two expressions into and collecting the like basis, follows = , = , = , when i + j is odd and i + j > , which completes the proof. by putting theorem and theorem together, we can devise symmetric integrators of arbitrarily high order. besides, as an alternative way, we can use the same technique as presented in to construct symmetric integrators for arbitrary order, that is, substituting into the order conditions one by one and determining the corresponding parameters . as symmetric methods possess an even order, it is sucient to consider those order conditions for odd orders, so we can increase two orders per step. we present the the following result without a proof . theorem . suppose that a, is in the form and b = b, b = , c = . then the corresponding csrkn method is symmetric and of order at least. if we additionally require = , then the method is of order at least. moreover, if we further require that = , = , = = , = , for even i > , then the method is of order at least.","Symmetric methods in the context of csRKN methods involve satisfying specific conditions for symmetry. A numerical one-step method is called symmetric if it satisfies symmetry conditions that ensure the original method and its adjoint method give identical results. The conditions for a csRKN method to be symmetric include satisfying specific coefficient relationships. By meeting these conditions, a csRKN method can be proven to be symmetric. Specific forms of coefficients, like those based on Legendre polynomials, can be used to devise symmetric integrators of arbitrarily high order. The process involves iteratively substituting into order conditions to determine corresponding parameters, allowing for the construction of symmetric integrators for arbitrary order. Symmetric methods have an even order, enabling an increase of two orders per step. Additional requirements on coefficients can further increase the order of the method."
"in this section, we show that symmetric rkn methods can be easily derived from symmetric csrkn methods by using quadrature formulas. theorem . if the coecients of the underlying symmetric csrkn method satisfy , then the associated rkn method is symmetric, provided that the weights and abscissae of the quadrature formula satisfy bs+i = bi and cs+i = ci for all i. proof. the symmetric condition for an s-stage classical rkn method denoted by ( aij, bi, bi, ci) is known as ci = cs+i, aij = bs+j bs+j + as+i,s+j, bi = bs+i bs+i, bi = bs+i, + table : two families of symmetric and symplectic rkn methods of order , by using gaussian and lobatto quadrature formulas respectively. for all i, j = , , s. by using , we have cci = cci, aci,cj = bcj bcj + aci,cj, bci = bci bci, bci = bci, for all i, j = , , s. in view of bs+i = bi and cs+i = ci for all i, the coecients (bj aci,cj, bi bci, bibci, ci) of the associated rkn method satisfy cci = ccs+i, bj aci,cj = bs+jbcs+j bs+j bcs+j + bs+j acs+i,cs+j, bi bci = bs+ibcs+i bs+i bcs+i, bibci = bs+ibcs+i, for all i, j = , , s, which completes the proof by the classical result. corollary . if a, takes the form and b = b, b = , c = , then by using a quadrature formula s i= with bs+i = bi and cs+i = ci for all i, the resulting rkn method is symmetric. since the weights and abscissae of gaussian-type and lobatto-type quadrature formulas satisfy bs+i = bi and cs+i = ci for all i, they can be used for devising symmetric rkn methods. example . if we take the coecients ( a,, b, b, c) as a, = p + p, b = , b = , c = , with one parameter being introduced, then we get a family of symmetric csrkn methods with order by theorem presented in , such methods are also symplectic and thus suitable for solving general second-order hamiltonian systems. by using suitable quadrature formulas with order p we can get symmetric rkn methods of order the resulting symmetric rkn methods are shown in table . this can be easily checked by the classical order conditions that listed in . + + + + + ++ + + + + ++ table : two families of symmetric rkn methods of order , by using gaussian and lobatto quadrature formulae. example . if we take the coecients ( a,, b, b, c) as a, = p + p + pp + pp + pp, b = , b = , c = , then we get a family of symmetric csrkn methods with order by using suitable quadrature formulas with order p we get symmetric rkn methods of order , which are shown in table . remark . we point out that: the left family of rkn methods in table are always symmetric and symplectic, while the right family of rkn methods of table are symmetric and symplectic when = . the classical -stage lobatto iiia method induces the following rkn method, which can be retrieved by taking = , = , = in table . the classical -stage lobatto iiib method induces the following rkn method, which can be retrieved by taking = , = , = in table . + + + + + + + + + + + + + + + + + + + + + + + + + + + table : two families of symmetric and symplectic rkn methods of order , by using gaussian and lobatto quadrature formulas respectively. example . if we take the coecients ( a,, b, b, c) as a, = x i+j pipj + pp, b = , b = , c = , where = , = , and the remaining satisfy , then we get a family of -order symmetric and symplectic csrkn methods. by using suitable quadrature formulas with order p we get symmetric and symplectic rkn methods of order , which are shown in table .","Symmetric RKN methods can be derived from symmetric CSRKN methods using quadrature formulas. A theorem states that if the coefficients of a symmetric CSRKN method satisfy certain conditions, then the associated RKN method is symmetric. The proof involves matching weights and abscissae of the quadrature formula. By applying Gaussian and Lobatto quadrature formulas, two families of symmetric and symplectic RKN methods of order s are obtained. It is noted that these methods are suitable for solving second-order Hamiltonian systems. Examples and a remarkable point regarding the symmetry and symplectic properties of the resulting RKN methods are provided."
"in this section, we perform some numerical results for comparing the numerical behaviors of the presented methods. for this aim, we consider the -order method and the following three -order methods: by taking = , = = in table it leads to a diagonally implicit symplectic and symmetric rkn method by taking = , = , = in table it gives the following symmetric rkn method by taking = , = , = in table it gives the following symmetric rkn method for convenience, we denote four symmetric rkn methods , , and by rkn- iiib, rkn-diagsymp, rkn-a and rkn-b methods respectively. these methods are applied to the following perturbed pendulum equation q = sin q cos, q = , q = , where the initial values are taken the same as that given in . the system is reversible with respect to the reection p p and the corresponding hamiltonian function is given by h = p cos q + sin. global errors of the numerical solutions by the above four methods with six small step sizes are shown in fig. with log-log scales, which veries the order of all the methods. from fig. , it is seen that rkn-iiib method and rkn-b method produce obvious energy drifts, though these methods are symmetric. this shows that not all symmetric rkn methods nearly preserve the energy over long times even if the system is reversible this observation has been shown for symmetric runge-kutta methods in . it is observed that the energy error keeps bounded for the rkn-diagsymp method. besides, it seems that the non-symplectic rkn-a method gives a better behavior. however, when we integrate the system on a much longer time interval , it gives a worse result compared with the rkn-diagsymp method . from these numerical tests we may conclude that symplectic-structure preservation is more essential than the reversibility preservation of the reversible hamiltonian systems in long-term numerical simulation. nevertheless, for general reversible non-hamiltonian systems, symmetric methods are also preferable.","In this section, numerical experiments were conducted to compare the behaviors of different methods. Four symmetric RKN methods were analyzed in relation to a perturbed pendulum equation. Results showed that while symmetrical methods may not always preserve energy over long periods, symplectic-structured methods are crucial for long-term numerical simulations in reversible Hamiltonian systems. Symmetric methods remain preferable for general reversible non-Hamiltonian systems."
"we develop symmetric integrators by means of continuous-stage runge-kutta-nystr om methods in this paper. the crucial technique based on legendre polynomial expansion combining with the symmetric conditions and order conditions is fully utilized. as illustrative examples, new - - - - - - - - - - - - - - - - - - - - figure : global errors of the numerical solutions by rkn-iiib method, rkn-diagsymp method (blue line), rkn-a method and rkn-b method for the perturbed pendululm equation . the reference line has slope in every subplots. - - - - - - figure : energy errors of the numerical solutions by rkn-iiib method, rkn-diagsymp method (blue line), rkn-a method and rkn-b method for the perturbed pendululm equation : step size h = , integration interval . - - - - - figure : energy errors of the numerical solutions by rkn-diagsymp method , and rkn-a method (red line) for the perturbed pendululm equation : step size h = , integration interval . families of symmetric integrators are derived in use of gaussian- type and lobatto-type quadrature formulas. it is worth observing that other quadrature formulas can also be considered for devising symmetric integrators and more free parameters can be led into the formalism of the butcher coecients. acknowledgements the rst author was supported by the national natural science foundation of china , china scholarship council and scientic research fund of hunan provincial education department . the second author was supported by the foundation of nsfc and phd scientic research foundation of east china jiaotong univer- sity. references l. brugnano, f. iavernaro, d. trigiante, hamiltonian boundary value methods: energy pre- serving discrete line integral methods, j. numer. anal., indust. appl. math., , l. brugnano, f. iavernaro, d. trigiante, analysis of hamiltonian boundary value methods : a class of energy-preserving rungeckutta methods for the numerical solution of polynomial hamiltonian systems, commun. nonlinear. sci. numer. simulat., , l. brugnano, f. iavernaro, line integral methods for conservative problems, monographs and research notes in mathematics, crc press, boca raton, fl, l. brugnano, f. iavernaro, d. trigiante, a simple framework for the derivation and analysis of eective one-step methods for odes, appl. math. comput., , l. brugnano, f. iavernaro, line integral solution of dierential problems, axioms, , https://doi.org//axioms c. burnton, r. scherer, gauss-runge-kutta-nystr om methods, bit, , j. r. cash, a variable step runge-kutta-nystr om integrator for reversible systems of second order initial value problems, siam j. sci. comput., , e. celledoni, r. i. mclachlan, d. mclaren, b. owren, g. r. w. quispel, w. m. wright., energy preserving runge-kutta methods, man , r. p. k. chan, on symmetric rungekutta methods of high order, computing, , e. faou, e. hairer, t. l. pham, energy conservation with non-symplectic methods: examples and counter-examples, bit numerical mathematics, , k. feng, on dierence schemes and symplectic geometry, proceedings of the -th inter., sym- posium of dierential geometry and dierential equations, beijing, , k. feng, k. fengs collection of works, vol. , beijing: national defence industry press, k. feng, m. qin, symplectic geometric algorithms for hamiltonian systems, spriger and zhejiang science and technology publishing house, heidelberg, hangzhou, first edition, e. hairer, s. p. nrsett, g. wanner, solving ordiary dierential equations i: nonstiprob- lems, springer series in computational mathematics, , springer-verlag, berlin, e. hairer, c. lubich, g. wanner, geometric numerical integration: structure-preserving algorithms for ordinary dierential equations, second edition. springer series in computa- tional mathematics, , springer-verlag, berlin, e. hairer, energy-preserving variant of collocation methods, jnaiam j. numer. anal. indust. appl. math., , j. hong, a survey of multi-symplectic runge-kutta type methods for hamiltonian partial dier- ential equations, frontiers and prospects of contemporary applied mathematics, , y. li, x. wu, functionally tted energy-preserving methods for solving oscillatory nonlinear hamiltonian systems, siam j. numer. anal., , r. i. mclachlan, g. r. w. quispel, g. s. turner, numerical integrators that preserve symme- tries and reversing symmetries, siam j. numer. anal., , y. miyatake, an energy-preserving exponentially-tted continuous stage runge-kutta methods for hamiltonian systems, bit numer. math., , y. miyatake, j. c. butcher, a characterization of energy-preserving methods and the con- struction of parallel integrators for hamiltonian systems, siam j. numer. anal., , d. okunbor, rd. skeel, explicit canonical methods for hamiltonian systems, math. comput., , g. r. w. quispel, d. i. mclaren, a new class of energy-preserving numerical integration methods, j. phys. a: math. theor., j. m. sanz-serna, m. p. calvo, numerical hamiltonian problems, chapman & hall, d. stoer, variable steps for reversible integration methods, computing, , w. tang, y. sun, a new approach to construct runge-kutta type methods and geometric numerical integrators, aip. conf. proc., , - w. tang, y. sun, time nite element methods: a unied framework for numerical discretiza- tions of odes, appl. math. comput. , w. tang, y. sun, construction of runge-kutta type methods for solving ordinary dierential equations, appl. math. comput., , w. tang, g. lang, x. luo, construction of symplectic runge-kutta methods with continuous stage, appl. math. comput. , w. tang, y. sun, w. cai, discontinuous galerkin methods for hamiltonian odes and pdes, j. comput. phys., , w. tang, j. zhang, symplecticity-preserving continuous-stage runge-kutta-nystr om methods, appl. math. comput., , w. tang, y. sun, j. zhang, high order symplectic integrators based on continuous-stage runge- kutta-nystr om methods, arxiv: , w. tang, a note on continuous-stage runge-kutta methods, appl. math. comput., , w. tang, continuous-stage runge-kutta methods based on weighted orthogonal polynomials, arxiv: , w. tang, an extended framework of continuous-stage runge-kutta methods, arxiv: ,","The section presents the development of symmetric integrators using continuous-stage Runge-Kutta-Nyström methods. The technique involves Legendre polynomial expansion combined with symmetric and order conditions. New numerical solutions for the perturbed pendulum equation are illustrated using various methods. Families of symmetric integrators are derived using Gaussian-type and Lobatto-type quadrature formulas. Other quadrature formulas can also be considered for devising symmetric integrators. Acknowledgements and references are provided, including works related to Hamiltonian systems and numerical integration methods for preserving energy."
"arxiv:v jan time series classication based on triadic time series motifs wen-jie xiea,b, rui-qi hanc, wei-xing zhoua,b,c, adepartment of finance, east china university of science and technology, shanghai , china bresearch center for econophysics, east china university of science and technology, shanghai , china cdepartment of mathematics, east china university of science and technology, shanghai , china abstract it is of great signicance to identify the characteristics of time series to qualify their similarity. we dene six types of triadic time-series motifs and investigate the motif occurrence proles extracted from logistic map, chaotic logistic map, chaotic henon map, chaotic ikeda map, hyperchaotic generalized henon map and hyperchaotic folded-tower map. based on the similarity of motif proles, we further propose to estimate the similarity coefcients between different time series and classify these time series with high accuracy. we further apply the motif analysis method to the ucr time series classication archive and provide evidence of good classication ability for some data sets. our analysis shows that the proposed triadic time series motif analysis performs better than the classic dynamic time wrapping method in classifying time series for certain data sets investigated in this work. keywords: time series analysis, classication, time series motifs, motif proles, dynamic time wrapping jel: c, p, z","The text introduces a method for time series classification based on triadic time series motifs. Six types of motifs are defined and studied, which are extracted from various chaotic maps. By comparing motif profiles, similarity coefficients between different time series can be estimated, leading to accurate classification. The method is tested on the UCR time series classification archive and shows better performance than traditional dynamic time warping for certain datasets. Key terms include time series analysis, classification, motif profiles, and dynamic time warping."
"quantifying the similarity of time series has always been a very useful primitives for time series analysis, with ap- plications to many elds (hu et al., ; silva et al., ; gomes and batista, ; mueen et al., ; mcgovern et al., ; chiu et al., ; mueen and keogh, ; tataw et al., ). the key point of measuring similarity is to de- ne a suitable and effective distance between two time series (hu et al., ; tarango et al., ; mi skiewicz and ausloos, ). the widely adopted denitions of distance include the euclidean distance and correlation measures (mi skiewicz and ausloos, ). however, in terms of measuring the similarity of time series, the euclidean distance is often average, some- times bad . for most time series analysis problems, the dynamic time warping provides a highly competitive distance metric . to get the best performance of dtw, we need to regulate its unique parameter to optimize the dynamic time warpings window width . the complexity of the dtw method is relatively high, so many researchers provide some improved methods to have bet- ter performance . moreover, practitioners generalize the dtw to some multi-dimensional time series classication experiments . similar subsequences in time series can be dened as time series motifs, which characterize the temporal proper- ties and dynamics of the corresponding long time series (mcgovern et al., ; chiu et al., ; mueen and keogh, ). it is useful for exploratory data mining and often used as inputs for classication of time series, clustering, segmentation . time series motif anal- ysis has been widely used in diverse elds (yeh et al., ; zhu et al., ; linardi et al., a,b; yeh et al., ; zakaria et al., ). gomes and batista presented a sax-based motif discovery method to classify the urban sound . wang et al. proposed a method to automatically detect repeating segments in music and two time series data sets . son and anh introduced two novel methods to discover approximate corresponding to: meilong road, p.o. box , school of business, east china university of science and technology, shanghai , china. email address: wxzhou@ecust.edu.cn preprint submitted to elsevier january , k-motifs in time series data and their methods play an important role in several time series data mining tasks by using motif discovery. lots of researchers have used time series motifs analysis for applications in many different domains . triadic time series motifs are inspired by the network motifs in visibility graph (lacasa et al., , ; ni et al., ; yang et al., ; elsner et al., ; qian et al., ) and horizontal visibility graphs mapping from time series (lacasa et al., ; elsner et al., ; lacasa and toral, ; shao, ; dong and li, ; ahmadlou et al., ; tang et al., ; xie et al., , a). the six triadic time series motifs are similar in some features with sequential hvg motifs and ordinal patterns (keller and sinn, ; mccullough et al., , ; zhang et al., ). the permutation entropy based on ordinal patterns (bandt and pompe, ; amig o, ) is a natural complexity measure and useful in the presence of dynamical or observational noise. similarly, the triadic time series motif analysis can also mine the dynamical characteristics of time series from com- plex system. xie et al. used the triadic time series motif analysis to uncover the different dynamics in the heartbeat rates of healthy subjects, congestive heart failure subjects, and atrial brillation subjects and identify the bullish and bearish markets from the price uctuations of nancial markets. in this work, we identify six triadic time series motifs and investigate their occurrence proles in time series from logistic maps with different control paratemers and chaotic time series generated from chaotic logistic map, chaotic henon map, chaotic ikeda map, hyperchaotic generalized henon map and hyperchaotic folded-tower map. it is of great signicance to be able to discover the characteristics of time series from different types of chaotic maps. we also apply the triadic time series motif analysis to classify the time series in data sets from ucr time series classication archive .","Time series analysis involves quantifying the similarity between time series, which is crucial for various applications. Different methods such as euclidean distance and dynamic time warping are used to measure similarity, with dynamic time warping often providing better performance but requiring parameter optimization. Time series motifs, representing similar subsequences in time series, are used for exploratory data mining and classification tasks. Triadic time series motifs, inspired by network motifs and horizontal visibility graphs, are increasingly utilized to analyze complex system dynamics. Researchers have used triadic time series motifs to study heartbeat rates, financial market trends, and chaotic time series from various maps. This approach is valuable for uncovering the characteristics of time series from different chaotic maps and for classifying time series in datasets."
"triadic time series motifs are determined by the relative magnitude and ordinal order of three data points that are randomly chosen from the time series xie et al. . for three arbitrary data {xi, x j, xk} with i < j < k in the time series {xi}i=,,l, a time series motif forms if the following conditions is fullled : ( xi > xn and x j > xn, n x j > xm and xk > xm, m . we obtain six triadic time series motifs, which are denoted as m, m, m, m, m, m in fig. this denition does not consider situations where two or three data points of {xi, x j, xk} are equal. when two data points are identical, we treat it as if the latter data point is larger than the former one. figure : illustrative example showing the six types of triadic motifs in time series. the time series motifs are different from the conventional motifs of horizontal visibility graphs (lacasa and toral, ; lacasa et al., ; shao, ; dong and li, ; ahmadlou et al., ; elsner et al., ; tang et al., ; xie and zhou, ; xie et al., , a). considering the triadic hvg motif, there are only two admissible motifs in undirected hvgs, one being a chain and the other being a triangle. as shown in fig. , the open triadic motif can be mapped from the time series , , and and the close triadic motif can be mapped from the time series and . time series motifs consider not only the visibility between data points, as hvg motifs, but also the order and relative magnitudes of the points. hence, time series motifs explore ner structures of hvg motifs .",Triadic time series motifs are defined by the relative magnitude and ordinal order of three data points randomly chosen from a time series. The motifs are represented by six different configurations based on specific conditions being fulfilled. This concept differs from traditional motifs in horizontal visibility graphs as it considers the order and magnitude of data points in addition to visibility. Time series motifs reveal more intricate structures compared to horizontal visibility graph motifs.
". chaotic maps we perform triadic time series motif analysis numerically for different time series in continuous and discrete dynamic systems. through extensive numerical experiments, we investigate the motif distribution extracted from the logistic map, the chaotic logistic map, the chaotic henon map, the chaotic ikeda map, the hyperchaotic generalized henon map, and the hyperchaotic folded-tower map. the logistic map is a representative example of how complex, chaotic behaviour can arise from very simple nonlinear dynamical equation. mathematically, the logistic map is written as xn+ = rxn. to distinguish chaotic maps and hyperchaotic maps, we generate four types of time series from chaotic henon map, chaotic ikeda map, hyperchaotic generalized henon map and hyperchaotic folded-tower map. the specic equations for these four types of dynamic systems are given below . mathematically, the chaotic henon map is written as xn+ = yn + ax n, yn+ = bxn. where a = and b = . the chaotic ikeda map is written as xn+ = + , yn+ = . where tn = /( + x n + y n) and = . the hyperchaotic generalized henon map is written as xn+ = a y n bzn, yn+ = xn, zn+ = yn. where a = and b = . the hyperchaotic folded-tower map is written as xn+ = axn , yn+ = ), zn+ = zn + byn. where a = and b = .","The text presents a study on triadic time series motif analysis applied to chaotic maps in continuous and discrete dynamic systems. Various chaotic maps such as the logistic map, chaotic Henon map, chaotic Ikeda map, hyperchaotic generalized Henon map, and hyperchaotic folded-tower map are analyzed through numerical experiments. The logistic map is used as a simple example to demonstrate how complex chaotic behavior can emerge from a nonlinear dynamical equation. Different types of dynamic systems are mathematically defined, including the chaotic Henon map, chaotic Ikeda map, hyperchaotic generalized Henon map, and hyperchaotic folded-tower map, each characterized by distinct equations and properties."
"we rst generate time series by using the logistic map with control parameter r. the parameter r ranges in the interval of (, ]. when r = , x will approach permanent oscillations between two values from almost all initial conditions. when r = , x will approach permanent oscillations among four values from almost all initial conditions. when r = , , or , x exhibits chaotic behaviour. for each parameter r, we generate time series with length , determine the occurrence frequencies fi of the six triadic time series motifs, and obtain the occurrence frequency distribution of each motif. fig. shows the distributions of the occurrence frequency fi of the motif i in the logistic time series with r = , , , , and we nd that the ve classes of time series have very different occurrence frequency distributions of time series motifs. f p f p r = r = r = r = r = f p f p f p f p a b c d e f figure : the probability distribution of occurrence frequency fi of motifs mi for different types of time series of the logistic map with parameter r = , , , , when the parameter r = , the time series is {xi}i=,,l = {a, b, a, b, a, b, }. without loss of generality, we assume that a > b. the set of motif m is the union of {xi, xi+, xi+} and {xi, xi, xi+} with i = , , , , so that the occurrence count of m is o = the sets of motifs m, m and m are respectively the union of {xi, xi+, xi+}, of {xi, xi+, xi+}, and of {xi, xi, xi+}, where i = , , , it follows that o = o = o = by denition, motifs m and m cannot appear. therefore, the occurrence frequencies fi := f = lim l o p i= o are obtained as follows f := = . this analytical result is veried by the numerical simulations, as shown in fig. . when the parameter r = , the time series is slightly more complicated. as shown in fig. , for the same motif, the distribution of occurrence frequency fi of the motif mi is concentrated and the variance is small, even when r > , the oscillation period becomes longer and longer, until about r = , the period tends to innity, and the system becomes a chaotic system. when r > , the result of the iterative run will switch between the period type and the chaotic type. until r = , the system is complete chaos. although they are all chaos, the distribution of occurrence frequency fi for r = , , in fig. has a big difference. we further perform triadic time series motif analysis of the four types of discrete chaotic time series: chaotic henon map, chaotic ikeda map, hyperchaotic generalized henon map and hyperchaotic folded-tower map. in fig. , the length of time series is and there are big difference in the occurrence frequency of motif m and motif m between the four types of discrete chaotic time series. it can be imagined that the longer the time series is, the larger the difference of the occurrence frequency of the individual motifs will be, and the easier it is to distinguish the the four types of discrete chaotic time series. in fig. , we cannot use one motifs occurrence frequency to distinguish different types of chaotic time series, but in fig. , we can use a single indicator f to distinguish the ve types of logistic time series. this method can be understood as a dimension reduction method, which reduces the time series with length n to -dimensional space for different time series, because p i= fi = f p chaotic henon map chaotic ikeda map hyperchaotic generalized henon map hyperchaotic folded-tower map f p f p f p f p f p a b c d e f figure : the probability distribution of occurrence frequency fi of motifs mi for different types of time series: chaotic henon map, chaotic ikeda map, hyperchaotic generalized henon map and hyperchaotic folded-tower map . the length of time series is . classication of time series the triadic motif analysis is applied to the classication of time series to investigate the effectiveness of the similarity measure of time series. in order to classify different time series, we need to extract the features of time series. the triadic time series motifs are used as the features of time series, and then the time series are classied based on the motif occurrence frequency distributions. from a common sense, the longer the time series, the more information obtained by the method for extracting features of time series, the more accurately the time series can be classied. therefore, we consider the inuence of time series lengths on the accuracy of classication. we compare two classical methods for measuring the similarity of time series: one is the simple euclidean distance method, and the other is the dynamic time warping method . we select the nearest neighbor method to classify the time series based on the three similarity measures. we analyze respectively the time series generated from the logistic map with parameter r = , , , and and from the four chaotic maps. for each type of time series, we generate time series as the training set and time series as the test set. to analyze the accuracy of classication of time series with different lengths l, the length of time series is changed from l = to l = , and then the data sets are classied by the nearest neighbor method based on three similarity measures. the three colors in fig. and correspond to the three similarity measures: the motif occurrence prole, the dtw and the euclidean distance. the red dot indicates the accuracy of classication of the nn method based on the motif occurrence prole. the green dot indicates the accuracy of classication of the nn method based on the dtw. the blue dot indicates the accuracy based on the euclidean distance. the ordinate represents the discriminant correctness rate based on the training set and the test set. the abscissa represents the time series length l. in general, the dtw-based discriminant accuracy is the best and the motif prole method performs slightly worse, especially when the time series length is less than the euclidean distance method is the worst. usually, the longer is the time series, the more information is extracted by the methods. however, the accuracy of the classication method based on euclidean distance decreases with the increase of the time series length. this is mainly because that the euclidean distance calculation is simple. when the time series is longer, there is more noise, which is not l accuracy e accuracy l accuracy e accuracy a b c d figure : the results of classication based on similarity of occurrence frequency of motifs for different types of time series. the average classication accuracy rate , and for the ve types of logistic time series with parameter r = , , , and the time series contains data points. for each type of logistic time series, we generates time series training sets and test sets. the ordinate represents the average classication accuracy rate. the abscissa represents the length of time series. the three colors correspond to three similarity measurements: the motif distribution, the dtw and euclidean distance. the relationship between the data deletion rate e and the average classication accuracy rate for the ve types of logistic time series. the average classication accuracy rate , and for the chaotic henon map, chaotic ikeda map, hyperchaotic generalized henon map and hyperchaotic folded-tower map, respectively. the relationship between the data deletion rate e and the average classication accuracy rate for the four chaotic maps. conducive to depicting the similarity between time series. the euclidean distance method has a relatively good effect when the time series length is less than when the time series length is greater than , we nd that = and = , indicating that the dtw-based method and the motif prole method provided in this paper are able to distinguish completely different time series. the euclidean distance method is not good and it is the same as the random classication, from which the accuracy is /c, where c is the number of categories in the data set. the logistic map series has categories, we have /c = / the discrete chaotic time series has categories, we have /c = / in order to analyze the accuracy of classication in the case of data loss, we perform the same analysis on the time series after data deletion. the length of the original time series is l = we randomly delete a proportion of the data from each time series. we then classify the remaining data and calculate the classication accuracy rates. this process is repeated times and the average classication accuracy rates , and are obtained. fig. and show the relationship between the data deletion rate e and the classication accuracy rates , and . overall, , and decrease with increasing e. we observe that the euclidean distance method performs as the random classication, with = / for the logistic maps and = / for the chaotic time series. for the logistic maps, the motif prole method is more robust to data deletion than the dtw method. when the data deletion rate is close to %, the motif-based classication accuracy can still reach %, while the dtw-based classication accuracy rate drops to about %. in contrast, for the chaotic time series, the dtw method outperforms the motif prole method. the dtw classication method is very robust to data deletion and its accuracy rate is close to % even when the data deletion rate e is as high as %. not surprisingly, each method has its own advantages and disadvantages. different methods usually have different performances when they are applied to different time series. triadic time series motif analysis of the ucr time series classication archive to test the effectiveness of this method on similarity measures of time series, we use this method to classify real time series. the data source is from the ucr time series classication archive . the ucr time series classication archive contains data sets, each of which is divided into a training set and a test set. the dataset website also presents some results about classication accuracy of three methods. the rst method uses the nearest neighbor method to classify based on the euclidean distance. the accuracy rate is expressed by , where the highest rate is % and the lowest correct rate is % . the second uses the nearest neighbor method to classify based on the dtw method. the correct rate is represented by , where the highest is % and the lowest correct rate is % . the third method is based on the improvement of dtw. from previous research results, we found that the dtw method can describe the similarity of time series very well, which is more effective than the euclidean distance, but the complexity of the dtw method is too high. we apply our method to the data sets and compared the results with those obtained from the rst and second methods. a b c d e f figure : motif occurrence proles f for different categories of time series in six data sets: pigairwaypressure , pigcvp , phoneme , wafer , motestrain , and toesegmentation . each radar chart corresponds to a data set. each solid line in the radar map represents the average motif occurrence prole of a class of time series in the data set. fig. shows the radar charts of the motif occurrence proles averaged within different classes of time series in six representative data sets. each radar chart corresponds to a data set. each solid line in the radar map represents the average of the motif occurrence prole f of one category of time series in the data set. the time series belonging to the same class in the training set and the test set are included in the averaging process. it can be seen that the six radar charts are very different, implying that the motif prole method can classify different data sets effectively. for each data set, the difference between the prole lines in the corresponding radar chart represents the difference between different time series. the classication will be more accurate if the difference is larger. the three radar charts on the top panel of fig. have many prole lines that are not sufciently separated, which indicates that it would be hard to distinguish those categories. in contrast, each of the three radar charts on the bottom panel of fig. have only two prole lines that are well separated, which indicates that the two categories can be well distinguished. indeed, the classication accuracy is low for the former data sets and high for the later data sets . cincecgtorso ethanollevel forda ham motestrain pigairwaypressure pigartpressure rock screentype wafer wine birdchicken computers ethanollevel forda housetwenty inlineskate insectepgregulartrain phoneme pigairwaypressure pigartpressure pigcvp refrigerationdevices screentype smallkitchenappliances toesegmentation wafer wine worms , figure : results of classication based on the occurrence frequency fi of motifs. each data point in the gure corresponds to a data set. in the gure, the abscissa indicates the correct discriminant rate based on the motif distribution, , and the ordinate indicates the discriminant correctness rate, , based on the dtw distance. the abscissa in the gure indicates the correct discrimination rate based on the distribution of the motifs, , and the ordinate indicates the correctness rate based on the euclidean distance, . euclidean distance, and dtw based. we use the triadic motif occurrence prole as the characteristic time series feature to classify the data sets. the classication accuracy is shown in fig. for data sets, our method is better than the dtw method, since . the data sets is shown in the lower right triangle of fig. fig. also compares the classication accuracy of the motif prole method and the euclidean distance method. there are data sets satisfying , indicating that our method performs better than the euclidean distance method for these data sets. for instance, for the data set smallkitchenappliances, our method is % more accurate than the euclidean distance method. in general, the dtw method does a very good job in the measurement of time series similarity. our method is superior to the dtw method for some data sets.","The text discusses the analysis of occurrence frequency distributions of triadic motifs in different types of time series generated from logistic maps and chaotic maps. It explores how these motifs can be used as features for classifying time series, comparing the effectiveness of different similarity measurement methods such as Euclidean distance and Dynamic Time Warping (DTW). The study shows that the DTW method generally performs better in classification accuracy, especially for longer time series. Additionally, the effectiveness of these methods is tested on real time series data from the UCR Time Series Classification Archive, showing promising results in classifying different categories of time series."
,
"it is of great signicance to be able to discover the characteristics of time series from a unique perspective through novel methods. here, we studied the characteristics of time series through triadic time series motifs. we dened six different network motif. the simulation analysis nds that the distributions of the motif occurrence frequencies cor- responding to logistic maps and chaotic time series (chaotic henon map, chaotic ikeda map, hyperchaotic generalized henon map and hyperchaotic folded-tower map) all have their own characteristics. the motif occurrence proles can quantify the time series characteristics in different dynamical systems and show comparative classication power as the dtw method. we apply the motif analysis to the ucr data sets. the advantage of the euclidean distance method is that the calculation is simple and fast. the dtw method performs best, but in some data sets, the performance is not as good as the motif prole method. our method has better accuracy than the dtw method for data sets. the starting point of our method is completely different from the euclidean distance method and the dtw method. this study is based on the complex networks, and mines the features in the time series. it is expected to be effectively improved in future research and provide a more effective method for measuring the similarity of time series. indeed, there are many methods for extracting motifs from time series. different motif extraction methods can describe different time series features. in order to improve the practicality of our method, we will develop different motif recognition methods to measure time series similarity. acknowledgements this work was supported by national natural science foundation of china and fundamental research funds for the central universities . references ahmadlou, m., adeli, h., adeli, a., new diagnostic eeg markers of the alzheimers disease using visibility graph. j. neural transm. , amig o, j., permutation complexity in dynamical systems. springer-verlag berlin heidelberg. bagnall, a. j., lines, j., bostrom, a., large, j., keogh, e. j., the great time series classication bake off: a review and experimental evaluation of recent algorithmic advances. data min. knowl. discov. , bandt, c., pompe, b., permutation entropy: a natural complexity measure for time series. phys. rev. lett. , chiu, b., keogh, e., lonardi, s., probabilistic discovery of time series motifs. in: acm sigkdd international conference on knowledge discovery and data mining. acm, pp. dau, h. a., begum, n., keogh, e. j., semi-supervision dramatically improves time series clustering under dynamic time warping. in: proceedings of the th acm international conference on information and knowledge management, cikm , indianapolis, in, usa, october -, pp. dau, h. a., keogh, e., kamgar, k., yeh, c.-c. m., zhu, y., gharghabi, s., ratanamahatana, c. a., yanping, hu, b., begum, n., bagnall, a., mueen, a., batista, g., october a. the ucr time series classication archive. https://www.cs.ucr.edu/eamonn/time_series_data_/. dau, h. a., silva, d. f., petitjean, f., forestier, g., bagnall, a. j., keogh, e. j., judicious setting of dynamic time warpings window width allows more accurate classication of time series. in: ieee international conference on big data, bigdata , boston,ma, usa, december -, pp. dau, h. a., silva, d. f., petitjean, f., forestier, g., bagnall, a. j., mueen, a., keogh, e. j., b. optimizing dynamic time warpings window width for time series data mining applications. data min. knowl. discov. , dong, z., li, x., comment on network analysis of human heartbeat dynamics. appl. phys. lett. , elsner, j. b., jagger, t. h., fogarty, e. a., visibility network of united states hurricanes. geophys. res. lett. , l gomes, e., batista, b., classifying urban sounds using time series motifs. advanced science and technology letters , hu, b., chen, y., keogh, e. j., classication of streaming time series under more realistic assumptions. data min. knowl. discov. , hu, b., rakthanmanon, t., campana, b. j. l., mueen, a., keogh, e. j., establishing the provenance of historical manuscripts with a novel distance measure. pattern anal. appl. , iacovacci, j., lacasa, l., a. sequential motif prole of natural visibility graphs. phys. rev. e , iacovacci, j., lacasa, l., b. sequential visibility-graph motifs. phys. rev. e , keller, k., sinn, m., ordinal analysis of time series. physica a , lacasa, l., luque, b., ballesteros, f., luque, j., nu no, j. c., from time series to complex networks: the visibility graph. proc. natl. acad. sci. u.s.a. , lacasa, l., luque, b., luque, j., nu no, j. c., the visibility graph: a new method for estimating the hurst exponent of fractional brownian motion. epl , lacasa, l., toral, r., description of stochastic and chaotic series using visibility graphs. phys. rev. e , linardi, m., zhu, y., palpanas, t., keogh, e. j., a. valmod - scalable discovery of variable-length motifs in data series. in: proceedings of the international conference on management of data, sigmod conference , houston, tx, usa, june -, pp. linardi, m., zhu, y., palpanas, t., keogh, e. j., b. valmod: a suite for easy and exact detection of variable length motifs in data series. in: proceedings of the international conference on management of data, sigmod conference , houston, tx, usa, june -, pp. mccullough, m., small, m., iu, h. h. c., stemler, t., multiscale ordinal network analysis of human cardiac dynamics. philos. trans. a , mccullough, m., small, m., stemler, t., iu, h. h. c., time lagged ordinal partition networks for capturing dynamics of continuous dynamical systems. chaos , mcgovern, a., rosendahl, d. h., brown, r. a., droegemeier, k. k., identifying predictive multi-dimensional time series motifs: an application to severe weather prediction. data min knowl disc , mi skiewicz, j., ausloos, m., correlation measure to detect time series distances, whence economy globalization. physica a , mori, u., mendiburu, a., keogh, e. j., lozano, j. a., reliable early classication of time series based on discriminating the classes over time. data min. knowl. discov. , mueen, a., keogh, e., online discovery and maintenance of time series motifs. in: acm sigkdd international conference on knowledge discovery and data mining, washington, dc, usa, july. pp. mueen, a., keogh, e., zhu, q., cash, s., westover, b., exact discovery of time series motifs. in: proceedings of the siam international conference on data mining. siam, pp. mueen, a., keogh, e. j., extracting optimal performance from dynamic time warping. in: proceedings of the nd acm sigkdd interna- tional conference on knowledge discovery and data mining, san francisco, ca, usa, august -, pp. ni, x.-h., jiang, z.-q., zhou, w.-x., degree distributions of the visibility graphs mapped from fractional brownian motions and multifractal random walks. phys. lett. a , petitjean, f., forestier, g., webb, g. i., nicholson, a. e., chen, y., keogh, e. j., faster and more accurate classication of time series by exploiting a novel dynamic time warping averaging algorithm. knowl. inf. syst. , petitjean, f., forestier, g., webb, g. i., nicholson, a. e., chen, y.-p., keogh, e. j., dynamic time warping averaging of time series allows faster and more accurate classication. in: ieee international conference on data mining, icdm , shenzhen, china, december -, pp. qian, m.-c., jiang, z.-q., zhou, w.-x., universal and nonuniversal allometric scaling behaviors in the visibility graphs of world stock market indices. j. phys. a , shao, z.-g., network analysis of human heartbeat dynamics. appl. phys. lett. , shokoohi-yekta, m., hu, b., jin, h.-x., wang, j., keogh, e. j., generalizing dtw to the multi-dimensional case requires an adaptive approach. data min. knowl. discov. , silva, d. f., de souza, v. m. a., ellis, d. p. w., keogh, e. j., batista, g. e. a. p. a., exploring low cost laser sensors to identify ying insect species - evaluation of machine learning and signal processing methods. journal of intelligent and robotic systems , silva, d. f., giusti, r., keogh, e. j., batista, g. e. a. p. a., speeding up similarity search under dynamic time warping by pruning unpromising alignments. data min. knowl. discov. , son, n. t., anh, d. t., discovery of time series k-motifs based on multidimensional index. knowledge and information systems , tang, q., liu, j., liu, h.-l., comparison of different daily streamow series in us and china, under a viewpoint of complex networks. mod. phys. lett. b , tarango, j., keogh, e. j., brisk, p., accelerating the dynamic time warping distance measure using logarithmetic arithmetic. in: th asilomar conference on signals, systems and computers, acssc , pacic grove, ca, usa, november -, pp. tataw, o. m., reddy, g. v., keogh, e. j., roy-chowdhury, a. k., quantitative analysis of live-cell growth at the shoot apex of arabidopsis thaliana: algorithms for feature measurement and temporal alignment. ieee/acm trans. comput. biology bioinform. , wang, l., chng, e. s., li, h., a tree-construction search approach for multivariate time series motifs discovery. pattern recognition letters , wang, x.-y., mueen, a., ding, h., trajcevski, g., scheuermann, p., keogh, e. j., experimental comparison of representation methods and distance measures for time series data. data min. knowl. discov. , xie, w.-j., han, r.-q., jiang, z.-q., wei, l.-j., zhou, w.-x., analytic degree distributions of horizontal visibility graphs mapped from unrelated random series and multifractal binomial measures. epl , xie, w.-j., han, r.-q., zhou, w.-x., a. tetradic motif proles of horizontal visibility graphs. commun. nonlinear sci. numer. simul. xie, w.-j., han, r.-q., zhou, w.-x., b. triadic time series motifs. epl . xie, w.-j., zhou, w.-x., horizontal visibility graphs transformed from fractional brownian motions: topological properties versus the hurst index. physica a , xu, x.-k., zhang, j., small, m., superfamily phenomena and motifs of networks induced from time series. proc. natl. acad. sci. u.s.a. , yang, y., wang, j.-b., yang, h.-j., mang, j.-s., visibility graph approach to exchange rate series. physica a , yeh, c.-c. m., kavantzas, n., keogh, e. j., meaningful multidimensional motif discovery. in: ieee international conference on data mining, icdm , new orleans, la, usa, november -, pp. yeh, c.-c. m., zhu, y., ulanova, l., begum, n., ding, y.-f., dau, h. a., zimmerman, z., silva, d. f., mueen, a., keogh, e. j., time series joins, motifs, discords and shapelets: a unifying view that exploits the matrix prole. data min. knowl. discov. , zakaria, j., mueen, a., keogh, e. j., young, n. e., accelerating the discovery of unsupervised-shapelets. data min. knowl. discov. , zhang, j.-y., zhou, j., tang, m., guo, h., small, m., zou, y., constructing ordinal partition transition networks from multivariate time series. sci. rep. , zhu, y., zimmerman, z., senobari, n. s., yeh, c.-c. m., funning, g., mueen, a., brisk, p., keogh, e. j., exploiting a novel algorithm and gpus to break the ten quadrillion pairwise comparisons barrier for time series motifs and joins. knowl. inf. syst. ,","The study introduces a novel method for analyzing time series using triadic time series motifs, defining six different network motifs. Simulation analysis on various chaotic time series reveals unique motif occurrence frequency distributions. Motif occurrence profiles can quantify time series characteristics effectively, comparable to dynamic time warping (DTW) method. Motif analysis applied to UCR datasets shows superior accuracy compared to DTW. The method, based on complex networks, is expected to provide a more effective way to measure time series similarity. Further research will focus on developing motif recognition methods to enhance practicality and accuracy."
"a node-based sirs epidemic model with infective media on complex networks leyi zheng a and longkun tang , a, b afujian province university key laboratory of computation science, school of mathematical sciences, huaqiao university, quanzhou , china. bdepartment of mathematics & statistics, georgia state university, atlanta , usa. abstract in this paper, we focus on the node-based epidemic modeling for networks, introduce the propagation medium and propose a node-based susceptible-infected-recovered- susceptible epidemic model with infective media. theoretical investigations show that the endemic equilibrium is globally asymptotically stable. numerical examples of three typical network structures also verify the theoretical results. fur- thermore, comparison between network node degree and its infected percents im- plies that there is a strong positive correlation between both, namely, the node with bigger degree is infected with more percents. finally, we discuss the impact of the epidemic spreading rate of media as well as the eective recovered rate on the network average infected state. theoretical and numerical results show that network average infected percents go up with the increase of the infected rate of media ; the infected rate of media has almost no inuence on network average infected percents for the fully-connected network and nw small-world network; network average infected percents decrease expo- nentially with the increase of the eective recovered rate, implying that the percents can be controlled at low level by an appropriate large eective recovered rate. keywords: sirs; node-based; propagation medium; complex network; epidemic spreading; stability.","The paper presents a node-based SIRS epidemic model on complex networks, incorporating an infective medium. The model establishes global asymptotic stability of the endemic equilibrium. Numerical examples confirm theoretical findings, showing a positive correlation between node degree and infection percentages. Results demonstrate that the network's average infection levels increase with the spreading rate of the infective medium, remain unaffected by the media's infectivity in fully-connected and small-world networks, and decrease exponentially with an increase in the effective recovery rate. The findings suggest that effective recovery rates can control infection levels effectively."
"with the development of network science, the mathematical modeling of epidemic spread- ing has involved in a research area across many disciplines including mathematical biol- ogy, physics, social science, computer and information science, and so on. on the basis of classical epidemic spreading models, such as, susceptible-infected-susceptible corresponding author: tomlk@hqu.edu.cn arxiv:v jan model, susceptible-infected-recovered model, and susceptible-infected-recovered- susceptible model, a variety of epidemic spreading models in networks were developed. investigations on these models have important signicance in public-health domain, especially in infectious disease epidemiology, by providing a number of interesting and unexpected behaviors. the theoretical studies of epidemic spreading models in complex networks rely mostly on the mean-eld theory approaches, especially on degree-based mean-eld the- ory which was the rst theoretical approach presented for the analysis of general dynam- ical processes on complex networks . this approach assumes that all nodes of degree k are statistically equivalent, and any given vertex of degree k is connected with the same probability to any node of degree k . therefore, the epidemic spreading model based on dbmf theory depends in general on the statistical topological properties of the under- lying networks instead of the whole network structure, resulting into the loss of detailed features of network topologies such that it is dicult to deeply understand the eect of network structures on the disease propagation. to the best of our knowledge, in , mieghem et al. rstly proposed the continuous-time node-based sis epidemic spreading model for understanding the inuence of network characteristics on epidemic spreading. youssef and scoglio established a new individual-based sir model with the whole description of network structures. very recently, yang et al. suggested a node-based susceptible-latent-exploding-susceptible model, and in the same year they presented a heterogeneous node-based sirs model where each node has the dierent infected and recovered rates . the above models assume that disease transmission takes place between individuals in networks. however, diseases are propagated not only by the contact between individuals in the same population, but also by the contact between individuals and infective media. for instance, many human diseases, such as dengue fever, malaria, chagas disease, and so on, can be transmitted by the infective mosquito. for this case, shi et al. established a new sis epidemic model with an infective medium, which describes epidemics transmitted by infective media on various complex networks. by dierentiating the infective medium from individuals, yang et al. proposed a modied sis model. wang et al. pre- sented a modied sis with an infective vector by incorporating some infectious diseases. it is noteworthy that these existing models with infective media are degree-based instead of node-based. the motivation of this paper is to build a node-based sirs epidemic model with infective media on various complex networks by integrating the node-based approach and the infective medium, and investigate the stability of the equilibrium as well as the inuence of network structures, the infective medium and the eective recovered rate on the network infected steady state. the rest of this paper is organized as follows. some denitions and lemmas are introduced in sec. in sec. , a node-based sirs epidemic network model with infective media is built and then its equilibrium is given. the global asymptotical stability analysis with respect to the equilibrium is performed in sec. in sec. , numerical simulations of three typical network topologies are provided for further verifying the theoretical results. the correlation between the infected percents of nodes and its degree, as well as the impact of some critical parameters on network average infected percents, are studied theoretically and numerically. finally, some conclusions and discussions are given in sec.","The text discusses the development of epidemic spreading models in complex networks, drawing on disciplines such as mathematical biology, physics, and social science. Traditional epidemic models have evolved to include more nuanced features like infective media. The focus is on a node-based SIRS epidemic model with infective media, seeking to understand the stability and effects of network structures. The paper introduces key definitions, presents the model and equilibrium, conducts a stability analysis, and offers numerical simulations on different network topologies. The impact of critical parameters on infected percentages and correlations with node degrees are explored. The study aims to deepen the understanding of disease propagation dynamics in complex networks."
"first, some requisite denitions and lemmas are given as follows. denition : a matrix is metzler if its all o-diagonal entries are non-negative. denition : a matrix a is hurwitz stable if there exists a positive matrix d such that atd + da is negative denite. denition : a matrix a is diagonally stable if there exists a positive denite diagonal matrix d such that atd + da is negative denite. obviously, the diagonally stable matrix is hurwitz stable, and the opposite is also true for metzler matrices. lemma : a hurwitz and metzler matrix is diagonally stable. lemma : let a be a hurwitz and metzler matrix, d be a positive denite diagonal matrix, d and d be negative denite diagonal matrices. then,  a d d d  is diagonally stable. lemma : consider a smooth dynamical system x = g dened at least in a compact set c. then, c is positively invariant if g is pointing into c for any smooth table : description of parameters. parameters description si the percents that node i is susceptible at time t. ii the percents that node i is infected at time t. ri the percents that node i is recovered at time t. sm the percents that media is susceptible at time t. im the percents that media is susceptible at time t. m the probability that a susceptible node is infected by an infective media. the probability that a susceptible node is infected by an infected neighbor. the probability of infective node turns into an immunized one. the probability of infective node turns into a susceptible one. the probability that an immunized node loses immunity into a susceptible one. the birth rate of the medium. m the probability of a susceptible medium transforming into an infected one. point xon the boundary of c.","The section introduces key definitions and lemmas related to matrix stability, including Metzler, Hurwitz, and diagonally stable matrices. It establishes relationships between these stability concepts and presents lemmas regarding diagonal stability of Hurwitz-Metzler matrices. Additionally, it discusses parameters for a smooth dynamical system, such as percentages of nodes being susceptible, infected, or recovered, as well as probabilities related to node interactions and media transformations. The section also touches on the concept of a positively invariant set in the context of a dynamical system defined within a compact set."
"to begin with, we consider an underlying network denoted g = where v is the set of nodes and e is the set of edges. the nodes labeled from number to number n represent the individuals in propagation networks, and the edges stand for network links through which disease can propagate. in a simpler way, we denote a = nn the adjacent matrix of graph g describing network topological structures, where aij = if there is an edge between node i and node j, otherwise aij = assume that each node in the network has three possible states: susceptible, infected , and recovered , whereas the media has two possible states: susceptible and infected ; both states s and i convert each other with certain proba- bility, and the state sm is infected with the probability of m into the state im, but not vice versa; the state i is recovered with the probability of into the state r , and the state r is converted with the probability of into the state s after the immunity is out of work; the state s in the underlying network is infected with the probability of m by the infective media, and the media is with the birth rate of . for simplicity, the variables and parameters in this node-based sirs model with infec- tive media are summarized in table , and the schematic diagram of the model is shown in fig. let xi = , , and represent three states of node i at time t: the susceptible figure : the schematic diagram of node-based sirs network model with media. the sirs network part, the media part. ), the infected ) and the recovered ), respectively. xm = , , and represent three states of media at time t: the susceptible ), the infected ) and the dead, respectively. the state of individuals at time t can be expressed as by the vector x = . then si = p{xi = }, ii = p{xi = }, ri = p{xi = }, sm = p{xm = }, im = p{xm = } according to the assumptions, it implies the following probability of state transition: p{xi = |xi = } = t[mim + n x j= aijij] + o p{xi = |xi = } = t + o p{xi = |xi = } = t + o p{xi = |xi = } = t + o p{xm = |xm = } = mt + o p{xm = |xm = } = t + o p{xm = |xm = } = t + o by using the total probability law, one can obtain ii =p{xi = } =p{xi = }p{xi = |xi = } + p{xi = }p{xi = |xi = } + p{xi = }p{xi = |xi = } =si + n x j= aijij]) + iit + rit + o. let t , we get dsi dt = [msiim + si n x j= aijij] + ri + ii. similarly, it is easy to get the equations dominating ii, ri, rm and im. col- lecting them together, we have the following n + dimensional dynamical system: dsi dt = [msiim + si n p j= aijij] + ri + ii, dii dt = msiim + si n p j= aijij ii, dri dt = ii ri, dsm dt = sm msm, dim dt = msm im, with initial condition , , sn, ii, , in, ri, , rn, sm, im)t e , where e = {, , sn, i, , in, r, , rn, sm, im)t rn+ + | si + ii + ri = , sm + im = , i = , , n}. remark : from the view point of continuous-time markov chain , model is an approximation one on account of the linear transition rate instead of exact one from state s to state i. the performance examined in appendix c shows that model is able to well forecast the epidemic dynamics of model built by means of markov chain technique. furthermore, the dynamical behaviors of approximation models is more easily studied by applying the stability theory and method, and thus the similar approximation model is directly built and studied in a large number of related literatures. since si + ii + ri , sm + im , i n, system can be reduced into the following system: dii dt = m ri)im + ri) n p j= aijij ii, dri dt = ii ri, dim dt = m) im. with initial condition , , in, r, , rn, im)t , i = , , n, where = {, , in, r, , rn, im)t rn+ + |ii + ri , im , i = , , n}. let the right-hand terms in equal to zero, one gets an equilibrium e= (i i , r i , im) which is only one proven in appendix a, here im= m m + , r i = i i , i i = mm m+ + n p j= aiji j + + m( + ) m m+ + ( + ) n p j= aiji j . from the above equality, it is easy to get that i i < +/, implying that the percent with the infected state for any node is less than +/. remark : obviously, the equilibrium is not virus-free, implying that the virus exists persistently in each individual. the phenomenon can be understood by the fact that the endemic disease remains safely under cover in each individual in some local areas. although the equilibrium is given in the implicit form, it can be calculated out by the numerical iterative method. remark : when the infected rate of media m = , the model is reduced to the node-based sirs epidemic model with a virus-free equilibrium, to some extent implying that our extended model is rational and practical. the infected medium terms not only increase the dimension of sirs models, but more importantly make stability analysis more complicated, especially in the part of global attractivity.","The text discusses a mathematical model based on a network structure with nodes and edges to simulate disease propagation dynamics. The model considers nodes representing individuals and edges signifying network links for disease spread. Nodes can be in three states: susceptible, infected, or recovered, while the media can be susceptible or infected. State transitions occur with specific probabilities. The node-based SIRS model includes variables and parameters summarized in a table. The model equations describe the dynamics of susceptible, infected, recovered individuals, and media. The equilibrium point of the model is calculated numerically. The presence of infected states in the equilibrium implies the persistence of the virus in the population. Stability analysis of the model is challenging due to the complexity introduced by infected media terms."
"local stability to analyze the local stability of system at the equilibrium e, we start with its jacobian je=  b c m  , where c = m(s , , s n, , , )t, b =  d mimen diag en en  , i= (i , i , , i n)t, en is the identity matrix of order n, diag represents the diagonal matrix, and d = diag(s , s , , s n)a diag(s + i s , s + i s , , s n + i n s n ). theorem : system is asymptotically stable at the equilibrium e proof: obviously, m is a negative eigenvalue of je, and other eigenvalues are determined by matrix b. next, we show that all the eigenvalues of b have negative real part. for convenience, dene three matrices as follows k =a + diag  s , , s n  , k =k + + max i  s i  en, k =a + diag  s , , s n  diag s + i s , , s n + i n s n  . here we consider an undirected and connected graph, so the adjacent matrix a is an irreducible one, indicating that k is also an irreducible matrix. according to perron- frobenius theorem , k has a positive eigenvector v corresponding to the largest eigenvalue max, i.e., kv = maxv. then kv =  max + max i s i  v, and vtki=  max + max i s i  vti. on the other hand, it is easy from the second equality of system to get that mims+ diagaii= , where s= (s , , s n)t. thus, )ki= mims implies ki when s s s n = since v and iare positive vectors, it holds that max + max i s i , and max = max + max i s i from and , it follows that max < max  a + diag( s , , s n )  = max thus, k is a negative denite matrix, and then d = diagk is also a negative denite one. that is to say, d is a hurwitz and metzler matrix. according to lemma , matrix b is diagonally stable. therefore, the equilibrium eof system is asymptoti- cally stable.","The text discusses local stability analysis of a system at an equilibrium point using the Jacobian matrix. The system is proven to be asymptotically stable at the equilibrium point through showing that all eigenvalues have a negative real part. By utilizing Perron-Frobenius theorem and matrix properties, it is demonstrated that the system is diagonally stable and hence the equilibrium is asymptotically stable."
"to proof the global attractivity, it needs to determine the positively invariant set (in brief, once a trajectory of the system enters the set, it will never leave it again). next, it is not dicult to proof that = {, , in, r, , rn, im)t|ii + ri , im , i = , , n} is an invariant set. theorem : is a positively invariant set for system . proof: denote the boundary of , and then it consists of the following n + hyperplanes: i = {, , in, r, , rn, im)t |ii = }, i = , , n, n+i = {, , in, r, , rn, im)t |ri = }, i = , , n, n+i = {, , in, r, , rn, im)t |ii + ri = }, i = , , n, n+ = {, , in, r, , rn, im)t |im = }, n+ = {, , in, r, , rn, im)t |im = }, for simplicity and convenience, system is rewritten as: dz dt = g) with initial condition z . take the outer normal vectors corresponding to n + hyperplanes as follows: pi = (, , | {z } i , , , , , )t, pn+i = (, , , , , | {z } n+i , , )t, pn+i = (, , | {z } i , , , , , | {z } n+i , , )t, pn+ = (, , , , , , | {z } n+ )t, pn+ = (, , , , , , | {z } n+ )t, and let z= (i , , i n, r , , r n, im)t be a smooth point of . on the basis of these hyperplanes, ve dierent cases of zare discussed respectively. case : for i i = , ( dz dt |zi, pi) = m( r i )im( r i ) n p j= aiji j < case : for r i = , ( dz dt |zn+i, pn+i) = i i < case : for i i + r i = , ( dz dt |zn+i, pn+i) = i i r i < case : for im= , ( dz dt |zn+, pn+) = m < case : for im= , ( dz dt |zn+, pn+) = < therefore, g is pointing to , and is positively invariant according to lemma theorem : the equilibrium eof system is globally attractive on {}. proof: denote y = , , yn, yn+)t where yi = ii, yn+i = ri, yn+ = im , and then the virus equilibrium y= (y , , y n+), here yi= . to explore the asymptotic behavior of solutions of eq. , we dene two functions as follows: f) = max i yi y i : r, f) = min i yi y i : r. both functions are continuous and exist right-hand derivatives along solutions of eq.. let y is the solution of eq., and suppose that f) = yi y i , t , for some t and suciently small > then we have f )| = y i y i , where f )| lim h+sup f)f) h . next, we proof that the derivative of f) at t is non-negative. according to the denition of f), it follows that yi y i yi y i , i = , , , n + for f) > , three cases as below are discussed . case : i n. y i y i yi =y i yi {myn+ + n x j= aijyj yi} <m( y i y n+i)yn+ y i yi + ( y i y n+i)y i yi n x j= aijyj y i <m( y i y n+i)y n+ + ( y i y n+i) n x j= aijy j y i = case : n + i n. y i y i yi = y i yi < y in y i = case : i = n + y i y i yi = y i yi < y in y i = since y i > and yt > , one get y i < , implying f ) < similarly, f |) if f) = , f |) > if f) < , and f | if f) = denote u = max{f , }, y , v = min{ f, }, y . obviously, u and v are non-negative and continuous in , and u | and v | let hu = {y |u |) = } and hv = {y |v |) = }, then we have hu = {y : yj y j} {} and hv = {y : y j yj } {}. it follows from the lasalle invariance principle that any solution of system staring in approaches hu hv = {y} {}. therefore, any solution y with initial value y satises limty = y, i.e., yis globally attractive in . remark : together with local asymptotical stability, it is easily obtained that the equilibrium ein the sirs model with media is globally asymptotically stable. by the way, without the medium propagation, i.e., m = , system has a virus-free equilibrium e which is globally asymptotically stable if max < /, otherwise unstable. here max is the largest eigenvalue of the topological matrix a, and / represents the actual eective recovered rate.","To prove global attractivity, a positively invariant set needs to be determined for the system. The set = {, , in, r, , rn, im)t|ii + ri , im , i = , , n} is proven to be invariant. The system is rewritten as dz/dt = g, and different cases of solutions based on the hyperplane boundary of the invariant set are discussed. It is shown that the equilibrium of the system is globally attractive on {}. The asymptotic behavior of solutions is explored by defining two continuous functions, f) and f). The equilibrium is globally asymptotically stable in the SIRS model with media, while without media propagation, the system has a virus-free equilibrium that is stable if max < /, otherwise unstable."
"three typical network models in order to verify the above theoretical results, we choose three typical network struc- tures , and solve numerically system with m = , = , = , = , = , = , m = . here, the fully connected network means that all nodes are connected with each other, the small-world network is generated from nearest neighbor network with the probability p = of random adding edges , and the scale-free network is a ba scale-free one with m = and m = each type of network is with nodes, respectively. figures - show the evolution of ii, ri and im over time for the fully connected network, nw small-world network and ba scale-free network, respectively. obviously, the infected and susceptible states of each node in each network tends to their equilibrium states over time, namely, ii i i and si s i , implying that it further veries our theoretical results, and the infected percents of node is very low for sparsely connected networks, such as nw and ba networks. interestingly, for the fully connected network, all nodes tend to the same equilibrium state as time goes, namely, ii iand si s. but for the small-world network and scale-free network, all nodes approach their equilibrium but nonidentical states as time goes, namely, ii i i and si s i . in fact, the steady state of each node is closely related with its degree according to the formula of equilibria. for time t p ii ri im figure : the states ii, ri and im of time for fully connected networks and random initial conditions. time t p ii ri im figure : the states ii, ri and im of time for nw small-world networks and random initial conditions. time t p ii ri im figure : the states ii, ri and im of time for ba scale-free networks and random initial conditions. the fully connected network, each node has the same degree, resulting into that each node tends to the same equilibrium state, and the equilibrium states isatises + ( + ) < i + < + ( + ), and thus i +/ for enough large size networks. please refer to appendix b for the detailed derivation. for the nw small-world network, the node with large degree has large infected equilibrium state, as shown in fig. . however, the correlation is a little weaker for the ba scale-free network which is a heterogeneous one, please see fig. . in a word, the infected equilibrium state i i (s i ) is positively correlated with the degree of nodes, and the node with higher degree is easier to be infected, further verifying the result obtained by the article . i* i node i degree i* i degree i* i node i degree i* i degree figure : the correlation between the node degree and its infected equilibrium state for nw small-world and ba scale-free networks .","The section conducts numerical simulations on three different network models to validate theoretical results. The models include a fully connected network, a small-world network, and a scale-free network. The simulations demonstrate that nodes in all networks tend towards equilibrium states over time, with sparsely connected networks having lower infection rates. The steady state of each node is influenced by its degree. In fully connected networks, all nodes reach the same equilibrium, while in small-world and scale-free networks, nodes approach different equilibrium states based on their degrees. The infected equilibrium state is positively correlated with node degree, with higher degree nodes being more susceptible to infection. The results align with the theoretical findings in the article."
"to learn more about this proposed model, we now analyze the inuence of several impor- tant parameters, namely, m, , , and , on network average infected state. let us look back these parameters, m means the infected rate resulted from the medium, represents the infected rate from the nodes neighbour, and can be considered as the potential infected rate due to the fact that the state of nodes never transforms back once it changes to the susceptible state from the recovered state, and the susceptible state is the potential part transferred into the infected state. represents the recovered rate from the infected state to the recovered state. here we call / and / the actual and potential eective recovered rate, respectively. we rstly analyze the inuence of m, , and on network average infected state for the general topologies. according to the implicit dierentiation theorem, it is easy to obtain the following theoretical results through the formula of equilibria. theorem : under the assumptions -, it follows that i m > , i > , i > , i < , where i= n pn i= i i called network average infected state. the above theorem implies that the network average infected state rises with the increase of m, and decreases with the increase of / or /. furthermore, we numerically verify the theoretical implications for three typical net- work topologies, respectively. fig. shows that with xed = , = , = , = , m = , = , as m increases, the network average infected state rises gradually for ba scale-free network, but almost unchanged for the fully-connected network and nw small-world network, implying that the heterogenous network is sensitive to the infected rate of the medium, but the homogeneous network is the opposite. it is shown from fig. that the average infected state of three typical net- works decreases exponentially as the potential recovered rate / increases with xed = , and it can be located at low level when / is enough large. it implies that the epidemic spreading on networks can be signicantly suppressed by the even small increase of the potential eective recovered rate /. similarly, the average infected state goes down with the increase of /, and the virus decreases exponentially for nw small-world network, faster than those for the fully- connected network and ba scale-free network, as shown in fig. a possible reason is that the small-world network has the properties of the short average path length and small average degree.","The impact of system parameters on network average infected state is analyzed, focusing on parameters such as the infected rate from the medium (m), infected rate from neighbors (), recovered rate (), and potential effective recovered rate (/). Theoretical results show that the network average infected state increases with m but decreases with / or /. Numerical verification on different network topologies confirms these findings, with heterogeneous networks being more sensitive to the infected rate of the medium compared to homogeneous networks. Increasing the potential recovered rate (/) significantly suppresses epidemic spreading, especially in small-world networks. The average infected state decreases with increasing /, with small-world networks being more effective in reducing virus spread compared to fully-connected and scale-free networks, possibly due to their short average path length and small average degree."
"in summary, this paper has presented a node-based sirs epidemic model with media for understanding the disease spreading of networks with media propagation, where there is m i fcn ba nw / i = = = = / i = = = = / i = = = = figure : the inuence of the infected rate m of media on network average infected states, and the curves from top to bottom correspond to the fully-connected network, nw small-world network and ba scale-free network, respectively. the inuence of the potential spreading rate / on network average virus at dierent values of for the fully-connect network , nw small-world network and ba scale-free networks . the curves from bottom to top in each subplot correspond to the value of = , , , , respectively. i = = = = i = = = = / i = = = = figure : the inuence of the actual spreading rate / on network average infected steady state at dierent values of for the fully-connect network , nw small-world network and ba scale-free networks . the curves from bottom to top in each subplot correspond to the value of = , , , , respectively. only an equilibrium yet not virus-free one that is always globally asymptotically stable through the stability analysis. without the medium propagation, the model has a virus- free equilibrium which is globally asymptotically stable when the maximum eigenvalue of topological matrices is less than the eective recovered rate /. three typical networks, i.e., the fully-connected, small-world, and scale-free networks, are applied to numerical investigations for further verifying the theoretical results. numer- ical simulations also show that the sparse network has less infected percents. in addition, it shows that the infected percents of network nodes have the positive correlation with the degree of the node, in particular for the homogenous network, such as the fully-connected network and small-world network. finally, theoretical and numerical studies on the inuence of the eective recovered rate and medium propagation rate on network average infected percents imply that network average infected percents go up with the increase of the medium propagation rate . numerical investigations further show that the medium propagation rate does nothing with network average infected percents for homogenous networks, and the infected percents decease exponentially with the increase of the eective recovered rate. moreover, the percents can be controlled at low level only if the eective recovered rate is enough large, in other words, only if the eective infected rate is enough small. conicts of interest the authors declare that they have no conicts of interest regarding the publication of this paper. acknowledgements this work is supported in part by the national natural science foundation of china , in part by the promotion program for young and middle-aged teacher in science and technology research of huaqiao univer- sity , in part by the program for new century excellent talents in fujian province university in , and in part the project of education and scientic research for middle and young teachers in fujian province. appendix a: proof of uniqueness of equilibria here we now prove the equilibrium eis one and only equilibrium xed point. first of all, dene a continuous mapping h = : n n as below: hi = mm m+ + n p j= aijyj + + m( + ) m m+ + ( + ) n p j= aijyj , i = , , n. we can assert that the equilibrium is one and only if h is monotonic and exist a unique xed point. claim : h is monotonic. proof: let x, z n, x z, . then, hi = mm m+ + n p j= aijxj + + m( + ) m m+ + ( + ) n p j= aijxj mm m+ + n p j= aijzj + + m( + ) m m+ + ( + ) n p j= aijzj = hi, which implies h h, the proof of claim is completed. claim : h admits a unique xed point in n. proof: existence. since h is monotonic and continue for y [, )n, it follows that hi < hi < hi, n. on the other hand, hi > and hi < , implying )n and , )n, such that n n where and . we conclude that i hi i, i < i, i = , , n, so the restriction h on the compact convex set = . maps into . it follows from brouwer fixed point theorem that h exists a xed point u . uniqueness. suppose h exists the other xed point v = (v , , v n)t n. let = max i u i v i and i = argmax i u i v i , without loss of generality, we may assume > , it follows that u i = hi hi = mm m+ + n p j= aijv j + + m( + ) m m+ + ( + ) n p j= aijv j < mm m+ + n p j= aijv j + + m( + ) m m+ + ( + ) n p j= aijv j = hi = v i, which contradicts the assumption that u i = v i hence, the xed point is unique. this completes proof. appendix b: computation of the equilibrium in this appendix, we give the computation process of the equilibrium for the fully- connected network with n nodes. assume that i i = i, i = , , n, then i= mm m+ + n p j= aiji + + m( + ) m m+ + ( + ) n p j= aiji . denote a = ( + ), b = ( + + m( + ) ), c = m, and m = mm m+. then, the above equality can be rewritten as ai + bi+ c = as c a < , it follows from the hurwitz criterion that eq. has two opposite sign roots. it is easy to verify that the positive root i + satises < i + < , so i + is the equilibrium due to the uniqueness of solutions. since < b ac < , it follows that + + ( + ) < i + = b + b ac a < + + ( + ). therefore, the equilibrium i +/ for the large size fully-connected network. p approximationi markovi approximationr markovr p approximationi markovi approximationr markovr time t p approximationi markovi approximationr markovr figure : comparison of results between two models for fully-connected networks , nw small-world networks and ba scale-free networks . appendix c: comparison with exact markov models for the purpose of showing the performance of our model, the following exact markov model is established by means of continuous-time markov chain technique . dsi dt = si n p j= aijp{xi = , xj = } msiim + ri + ii, dii dt = msiim + si n p j= aijp{xi = , xj = } ii, dri dt = ii ri, dsm dt = sm msm, dim dt = msm im, where p{xi = , xj = } represents the probability of node i being state r and node j being state i. as matter of fact, model turns into model when p{xi = , xj = } will replaced with ij, namely, the transition rate from state r to state i is linear and equals to mim + pn i= aijij, as shown in fig. the relation between exact markov model and approximation model please refers to the reference . next, we select three typical networks with nodes, and use the gillespite algorithm to simulate the solution of the markov model where model parameters and initial conditions are the same as those in sec. . in the experiment, we select randomly initial nodes including susceptible, infected and recovered nodes, and make realizations for fully-connected networks, and realizations for nw small-world and ba scale-free networks. figure shows the comparison of network average states i = n pn i= ii and s = n pn i= si between the markov model and the approximation model. as time goes, both states i and s of approximation models are able to describe those of markov models, although there is a little overestimation. furthermore, the estimation for small-world and scale-free networks is better than that for the fully connected networks. on the whole, the performance of this new model is good for describing the real markov model. appendix d: proof of theorem proof of theorem : denote i = [++(+ )(m+ n x j= aiji j )]i i m n x j= aiji j , i = , , , n where m = mm m+, i= (i , , i n)t. then, it follows from the formula of equilibria in model that i = , i = , , , n taking the partial derivatives of i with respect to i j and , respectively, one gets j ( i (i j ))nn = diag( + i i ( + )) + diag(( + )i i )a, = [men + diag(i i )a]i. here diag = diag{, , , n}, = ( , , n )t, i = ( i , , i n )t and the same below. according to the implicit dierentiation formula, it follows that j i = . obviously, < , namely, i < . next, we prove that j is invertible and all the elements of are negative. it is easy to obtain that ( + )i i < due to i = i i + [( + )i i ](m + n x j= aiji j ) = denote m = a diag( + (+ )i i ) + diag( m i i ), and m = m + maxi{ + (+ )i i }en. obviously, m is non-negative matrix, and it is irreducible due to the fact that a is irreducible on account of the connectedness of the graph g. according to the perron-frobenius theorem , m has a simple positive eigenvalue and a positive eigenvector u, such that mu = u. so mu = [ maxi{ + (+ )i i }]u, implying that u is also eigenvector of m. on the other hand, it follows from eq. that mi= , indicating that iis a positive eigenvector of m belonging to eigenvalue as uti> , combining the simplicity of resulted from the simplicity of , one gets = , indicating that m has a zero eigenvalue and the other eigenvalues are negative. on the other hand, j = m diag(( + )i i )a diag( m i i ). it follows that all the eigenvalues of matrix j are negative, and j is metzler and irreducible. according to the result in , all the elements of matrix are negative. thus i = > , and consequently i = n pn i= i i > similarly, we get i = > , i m = m > , and i = < where = diag((+ )i i )ai, i m = m m+[(+ )i i ], and = [(+ m )en + diag(i i )a]i. therefore, i > , i m > , and i < the proof of theorem is completed. references pastor-satorras r, vespignani a. epidemic spreading in scale-free networks. phys- ical review letters, , : - mishra b k, saini d k. seirs epidemic model with delay for transmission of mali- cious objects in computer network. applied mathematics and computation, , : - yuan h, chen g. network virus-epidemic model with the point-to-group information propagation. applied mathematics and computation, , : - newman m e j. spread of epidemic disease on networks. physical review e, , : s elley f, besenyei a, kiss i z, et al. dynamic control of modern network-based epidemic models. siam journal on applied dynamical systems, , : - liu q, sun m, li t. analysis of an sirs epidemic model with time delay on hetero- geneous network . advances in dierence equations, , : wei x, liu l, zhou w. global stability and attractivity of a network-based sis epidemic model with nonmonotone incidence rate. physica a: statistical mechanics and its applications, , : - xiang wei, xiaoqun wu, shihua chen, jun-an lu, guanrong chen, cooperative epidemic spreading on a two-layered interconnected network, siam journal applied dynamical systems, , : - pastor-satorras r, castellano c,mieghem p, vespignani a. epidemic processes in complex networks. reviews of modern physics , :- mieghem pv, omic j, kooij r. virus spread in networks. ieee/acm transactions on networking , :- youssef m, scoglio c. an individual-based approach to sir epidemics in contact networks. journal of theoretical biology, , : - yang l x, draief m, yang x. the impact of the network topology on the viral prevalence: a node-based approach. plos one, , : yang l, draief m, yang x. heterogeneous virus propagation in networks: a theoreti- cal study. mathematical methods in the applied sciences, , : - shi h, duan z, chen g. an sis model with infective medium on complex networks. physica a: statistical mechanics and its applications, , : - yang m, chen g, fu x. a modied sis model with an infective medium on com- plex networks and its global stability. physica a: statistical mechanics and its applications, , : - wang y, jin z, yang z, et al. global analysis of an sis model with an infective vector on complex networks. nonlinear analysis: real world applications, , : - hom r a, johnson c r. topics in matrix analysis. cambridge up, new york, narendra k s, shorten r. hurwitz stability of metzler matrices. ieee transac- tions on automatic control, , : - yorke j a. invariance for ordinary dierential equations. mathematical systems theory, , : - shamash e r. fixed point theory: banach, brouwer and schauder theorems. california state university, northridge, robinson r c. an introduction to dynamical systems: continuous and discrete. american mathematical soc., yan g, zhou t, wang j, fu z q, wang b h. epidemic spread in weighted scale-free networks. chinese phys. lett. , :- yang l x, yang x, tang y y. a bi-virus competing spreading model with generic infection rates. ieee transactions on network science and engineering, , :- gillespie d t. exact stochastic simulation of coupled chemical reactions. the journal of physical chemistry, , : - yang l x, yang x, wu y. the impact of patch forwarding on the prevalence of com- puter virus: a theoretical assessment approach. applied mathematical modelling, , : - stewart w j , probability, markov chains, queues, and simulation: the mathemat- ical basis of performance mdeling , princeton university press, bhatia r, matrix analysis, springer-verlag, new york, usa, .","This paper presents a node-based SIRS epidemic model with media to study disease spreading in networks with media propagation. The influence of infected rate of media on network average infected states is explored for different network types. The study shows that the model has a virus-free equilibrium only without medium propagation, and equilibrium is globally asymptotically stable. The numerical investigations on various network types verify theoretical results, showing that sparse networks have lower infection rates. The study also indicates a positive correlation between node degree and infection rate for certain networks. The influence of effective recovered rate and medium propagation rate on network average infected rates is discussed, with infected rates increasing with medium propagation rate. Control over infected rates can be achieved with a sufficiently large effective recovered rate. Additionally, comparisons with exact Markov models show good performance of the proposed model. The appendix includes proofs for uniqueness of equilibria and computation for equilibrium in the fully-connected network."
"morse index and bifurcation for gure-eight choreographies of the equal mass three-body problem hiroshi fukuda, toshiaki fujiwara and hiroshi ozaki college of liberal arts and sciences, kitasato university, -- kitasato, sagamihara, kanagawa -, japan laboratory of general education for science and technology, faculty of science, tokai university, -- kita-kaname, hiratsuka, kanagawa, -, japan e-mail: fukuda@kitasato-u.ac.jp, fujiwara@kitasato-u.ac.jp and ozaki@tokai-u.jp april abstract. we report on the morse index and periodic solutions bifurcating from the gure-eight choreography for the equal mass three-body problem under homogeneous potential /ra for a , and under lennard-jones type potential /r /r, where r is a distance between bodies. it is shown that the morse index changes at a bifurcation point and all solutions bifurcating are approximated by variational functions responsible for the change of the morse index. inversely we observed bifurcation occurs at every point where the morse index changes for the gure-eight choreography under /ra, and for solution under lj type potential, where solution is a gure-eight choreography tending to that under /r for innitely large period. thus, to our numerical studies, change of the morse index is not only necessary but also sucient condition for bifurcation for these choreographies. further we observed that the change of the morse index is equal to the number of bifurcated solutions regarding solutions with congruent orbits as the same solution. submitted to: j. phys. a: math. theor.","The text discusses the morse index and bifurcation for gure-eight choreographies of the equal mass three-body problem. The study reports on how the morse index changes at a bifurcation point, with all solutions bifurcating approximated by variational functions responsible for the change. The research shows that the change of the morse index is a necessary and sufficient condition for bifurcation in these choreographies. Additionally, it is observed that the change in the morse index corresponds to the number of bifurcated solutions."
"choreographic motion of n bodies is a periodic motion on a closed orbit, n identical bodies chase each other on the orbit with equal time-spacing. moore found a remarkable gure-eight choreographic solution for n = under homogeneous potential /ra by numerical calculations, where r is a distance between bodies. chenciner and montgomery gave a mathematical proof of its existence for a = by variational method. the detailed initial conditions for three bodies are found in . arxiv:v apr morse index and bifurcation for gure-eight choreographies sbano , and sbano and southall , studied n-body choreographic solutions under an inhomogeneous potential ulj = r r, a model potential between atoms called lennard-jones-type potential. sbano and southall proved that there exist at least two n-body choreographic solutions for suciently large period, and there exists no solution for small period. we conrmed their theorem numerically for n = and unexpectedly found a multitude of three-body gure-eight choreographic solutions under lj-type potential . following shibayamas preliminary calculation for a = , we did accurate numerical calculation of the morse index, for the three-body gure-eight choreography in the domain of periodic function . here the morse index is a number of independent variational functions giving negative second variation of action functional. in our paper , a strong relationship between the morse index and h solution found by sim o , which is a periodic solution close to the gure-eight choreography but made up of three distinct orbits, was suggested. on the other hand gal an et al showed that the h solution bifurcated from gure-eight choreography by changing the masses of three bodies. they also found many dierent periodic orbits on gure-eight . there are several researches on the morse index for periodic solution of three- body problem. barutello et al calculated the morse index mathematically for the lagrangian circular solution, and hu and sun for elliptic lagrangian solutions, to discuss the linear stability. in this paper, we show a relationship between the morse index of the gure- eight choreographies and periodic solutions bifurcating for a system of three identical bodies interacting through a homogeneous potential or through lj-type potential . in section , we show the morse index changes at a bifurcation point and solutions bifurcating are approximated by variational functions responsible for change of the morse index. in section , we discuss the bifurcation from the gure-eight choreography under homogeneous potential, /ra, by changing a. in section , we show the h solution bifurcates at a = where the morse index changes. in section , another bifurcation at a = is shown. these bifurcations at a = and are rst found in by mu noz-almaraz et al using auto . there is no other point where the morse index changes for a in section , we discuss the bifurcation of the solution for the system under lj-type potential, where the solution is a gure-eight choreography tending to that under /r for innitely large period. there are seven points where the morse index changes for the solution. in section we show four points bifurcate periodic but non choreographic solutions, which are the same type of the bifurcations discussed in section in section , we show the rest three points yield choreographic solutions less symmetric than gure-eight. alain chenciner, in icm , asked a question about morse index and bifurcation for gure-eight choreographies the existence of less symmetric gure-eight . we can say yes, they exist under lj-type potential. section is a summary and discussions. our numerical results in this paper were calculated by mathematica .","The choreographic motion of n identical bodies on a closed orbit with equal spacing is studied. The existence of a gure-eight choreographic solution under homogeneous potential /ra was first found by Moore and later proved mathematically by Chenciner and Montgomery. Further research explored n-body choreographic solutions under an inhomogeneous potential, showing multiple solutions for large periods and none for small periods. Numerical calculations confirmed these findings, including the existence of three-body gure-eight choreographic solutions under an LJ-type potential. The Morse index, related to variational functions, was analyzed for these solutions, showing changes at bifurcation points. Various bifurcations are discussed, including the transition from gure-eight choreography to less symmetric solutions. Overall, this research sheds light on the complexity of periodic solutions in the three-body problem under different potentials."
". morse index and eigenvalue problem for a system of three identical bodies in classical mechanics, we consider periodic solutions to equations of motion, d dt l qi = l qi , i = , , . . . , , where dot represents a dierentiation in t. l is the lagrangian with the potential energy u, l = x i= q i u, and q = , q, . . . , q) a six component vector composed of position vectors rb = , yb)= , qb) for body b = , , in a plane, where represents transpose. the subscript of six component vector is assumed to be in the range between and for a periodic solution q = q and variation function q = q with period t, the morse index n is dened as a number of independent variation function q which make the second variation s with s = z t dt x i (qi qi + qi qi ) !k l of the action functional s = z t l(q, q)dt negative. since the s is written as s = (q, hq) by matrix operator h, ( h)ij = ij d dt u qiqj , the morse index is a number of negative eigenvalues of the eigenvalue problem, h = , morse index and bifurcation for gure-eight choreographies = , = , with the second variation s = and the variation function q = , where is the inner product dened by = z t dtf g and ij the kronecker delta.",The text discusses the Morse index and bifurcation in the context of a system of three identical bodies in classical mechanics. It considers periodic solutions to equations of motion with a Lagrangian and potential energy. The Morse index is defined as the number of independent variation functions that make the second variation of the action functional negative. This index is related to the number of negative eigenvalues of the eigenvalue problem. The analysis is applied to the case of figure-eight choreographies with specific parameters and definitions for the variation function and inner product.
"for a periodic solution of equation of motion with some parameter , q, suppose the other periodic solution qb bifurcates at = , that is, lim qb = q. since for the lagrangian q = u qi and qb = u qb i , q = qb q satises d dtqi = u qi q=qb + u qi = x j u qjqi qj + o, that is, hq for thus equation is a necessary condition for to be a bifurcation point. when , equation shows that q goes to the eigenfunction of h whose eigenvalue is zero. in other words, at a bifurcation point some eigenvalue of h has to go to zero. thus, using the normalized eigenfunctions of the eigenvalue we have an approximate expression by variated orbit q = q + h g x k ck for the bifurcating solution qb = q + q q for , where g is a degeneracy of the , ck and h are real coecients with pg k c k = and h since the variated orbit q has its own symmetry independent of and h , bifurcating solution qb will be found within the symmetry. morse index and bifurcation for gure-eight choreographies - - h - - h figure an action functional s) for n = bifurcation is supposed to be one side in > < >","A periodic solution qb bifurcates at a parameter value, q=, with q=q as a necessary condition for the bifurcation point. At the bifurcation point, an eigenvalue of the Hamiltonian h goes to zero, leading to an approximate expression for the bifurcating solution. The bifurcating solution qb will be found within its own symmetry, as indicated by the normalized eigenfunctions. This is important in understanding the morse index and bifurcation in choreographies."
"we dene change of the morse index n by n = lim + n lim n. thus g = |n| and an eigenvalue changes the sign at = , n around then the sign of the action, s = s) s), has the same sign as s and ns around since s is expanded in h with coecients as s = s + hs + h ! s + , and qb tends to q = q + hq with q = pg k ck for , s s s = h s + h ! s + = h + o. thus then are derived by . on the basis of n, and , we can picture bifurcation through manifold of action functional s in the subspace of corresponding . for example, suppose n = and bifurcation is one side in > thus, in the one dimensional subspace of , top of a local maximum in s where q locates for < , shown in gure , will slightly cave in for > , which yield critical points for qb in the both sides of the cave, shown morse index and bifurcation for gure-eight choreographies table n and symmetry of q for a a n q dy d in gure . we conrmed that the inequalities and corresponding to the picture of bifurcation as shown in gure hold in our numerical calculations. we dene an equivalent class, congruent class of bifurcated solutions b, by regarding the bifurcated solutions with congruent orbits as equivalent, and denote the number of elements by #b. then we dene number of incongruent bifurcated solutions as nb = lim #b + lim + #b. in the following sections we will show nb = |n| holds for = in our numerical calculations. note that at a bifurcation point the morse index may not change, n = , if = but d/d = however for gure-eight choreographies in this paper, such point never contribute to any bifurcation and belongs to = in the whole region of corresponding to the conservation laws. in other words, holds not only for = but also for all . we call the region < the left side of the bifurcation point and < the right side.","The Morse index and bifurcation are defined in the text by determining the change in Morse index n based on an eigenvalue changing sign. Bifurcation is described as occurring in the subspace of corresponding eigenvalues for the action functional. The text illustrates an example where bifurcation happens on one side of the subspace, resulting in critical points for the action. An equivalent class of bifurcated solutions is introduced, and the number of incongruent bifurcated solutions is calculated. The text confirms that the number of incongruent bifurcated solutions is equal to the absolute value of the Morse index. Additionally, it is noted that at a bifurcation point, the Morse index may not change if a certain condition is met."
"in this section we investigate the bifurcation of the gure-eight choreography by using a as the parameter for a system under homogeneous potential u = x b>c ua|), ua = ra, for a in table , a, n = and symmetry of the variated orbit q are tabulated. the symbol d means that the q is not choreographic, and the subscript y indicates that the orbits are symmetric in the y axis. there is no other point with n = for a than tabulated in table , a = and . . bifurcation at a = at a = , the morse index n changes by n = as shown in table for g = |n| = , the variated orbit q is written as q = q + h + sin ) morse index and bifurcation for gure-eight choreographies - - - - - - figure variated orbit q in for a = close to a = ; t = , h = , = . s of . q with dxy symmetry for a local maximum at = in s. note that the value of does not have universal meaning since it depends on the choice of orthonormal basis, and . filled circles are isosceles triangle conguration at t = and open circles euler conguration at t = t/ with c = cos and c = sin . the coecients are, thus, found as critical points in by s = as shown in gure at h = , there are six critical points in s of , three local maximums and three local minimums, which are independent of h. at all six critical points in , the variated orbits qs have the same symmetry higher than the dy indicated in table they consist of three distinct orbits symmetric in the y axis and in the x axis with exchange of two bodies; one orbit is symmetric itself but two collectively. thus we denote orbits with this symmetry by dxy. in gure , the variated orbit q at a local maximum is shown. black orbit is symmetric itself but two gray orbits collectively. consequently all bifurcating solutions from a = will be searched within dxy symmetry. note that since it is numerically dicult to calculate the q just at the bifurcation point a = and its symmetry does not depend on a, in gure calculation for a = is shown. conditions for q to be dxy, derived in appendix a., are that q takes an isosceles triangle conguration at t = shown by lled circles in gure , q = , q = v, with = tan y x, by parameters , and an euler conguration at t = t/ shown by open circles in gure , (q, q, q q q q) = for given period t, the three conditions determine the three parameters . morse index and bifurcation for gure-eight choreographies - - - - - - - - - - - - - - - - figure dxy solutions bifurcated from a = ; a = , a = , and a = , for t = parameters for are , and , respectively. in order to distinguish dxy from gure-eight choreography which are very close around a bifurcation point, we use the y component of body on the x axis, d = q, for t t/ with q = since d is zero if and only if dxy is choreographic. mu noz-almaraz et al rst found the bifurcation at a = using auto , and recently we re-found it using the equations with newtons method: three dxy solutions corresponding to the three local maximums in s of bifurcate in the right side of the bifurcation point, and three to the local minimums in the left side. the six critical points in gure are written by a position of local maximum as +j/+k, j = , , , k = , thus the q for the six solutions bifurcating in the both sides are represented by = + j/ with a smooth increasing function h of a. using the choreographic operator c dened by cfi = fi+ since for the doubly degenerate eigenvalue c + sin ) = cos + sin, the q for dxy is written as qdxy = q + h cj + sin ), j = , , since c = and cq = q, representation for j = , , dier only in cyclic permutation of bodies with time shift. thus their orbits are congruent and the number of incongruent solutions nb is counted as nb = + = in gure and , one of three congruent dxy solutions in the both sides of bifurcation point, a = , are shown with parameters for initial condition and the index d. as we showed in , the sim os h solution is in good agreement with the variated orbit q because it is the solution dxy bifurcating from a = very close to a = actually the solution dxy at a = shown in gure coincides with the sim os h solution. morse index and bifurcation for gure-eight choreographies - - - - - - - - - - figure variated orbit q for a = close to a = ; t = , = , h = . s of . q with dx symmetry for a local maximum at = . note that the does not have universal meaning since it depends on the choice of orthonormal basis, and . filled circles are isosceles triangle conguration at t = and open circles t = t/ q with d for a local minimum at = + / filled circles are euler conguration at t = and open circles at t = t/ . bifurcation at a = for a , there is one more point changing the morse index at a = as shown in table in this section we investigate this point in similar manner as in section . the eigenvalue which goes to zero at a = is doubly degenerate, g = |n| = , and the variated orbit q in are expected as bifurcating solutions. however its symmetry d shown in table is lower than for dy at a = , and its action s as a function of exhibits twelve critical points; six local maximums and six local minimums as shown in gure . in gure , s and q for a = are shown as a close point to a = because of numerical convenience. at local maximums, the variated orbit q consists of three distinct orbits: one orbit is symmetric itself in the x axis and the other two are collectively, shown in gure . on the other hand, at local minimums: one orbit is symmetric itself at origin and the other two collectively, shown in gure . we denote former by dx and latter d here the orbits of dx can have non zero total angular momentum l since sum of signed area of the three orbits can be non zero whereas it is zero for solutions d, dxy and the gure-eight choreography because of two fold symmetry at origin. conditions for q to be dx, derived in appendix a., are that q takes an isosceles triangle conguration shown by lled circles in gure , and with = tan y x tan l v p x + y, at t = by parameters , and opposite isosceles triangle conguration at t = t/ shown by open circles in gure , (q, q, q q, q q) = for given period t, four conditions determine four parameters . an index to distinguish dx from gure-eight choreography is = , morse index and bifurcation for gure-eight choreographies - - - - - - - - - - figure dx and d solutions at a = with t = bifurcated from a = . d is rotated by in . parame- ters: = , = . since dx with l = is the dxy solution. conditions for q to be d, derived in appendix a., are that q takes an euler conguration at t = shown by lled circles in gure , q = , q = by parameters , and another euler conguration at t = t/ shown by open circles in gure , (q, q, q q q q) = for given period t, three conditions determine three parameters . an index to distinguish d from gure-eight choreography is = , where i = i i and i = |q| = p i |qi|, since d with i = is the dxy solution. using auto , mu noz-almaraz et al found the solution dx bifurcated at a = in together with dxy at a = . knowing the existence of dx, we recently re-found dx and d using the above equations with newtons method: six dx and six d solutions bifurcate in the right side of bifurcation point. the six dx solutions are written by the q with at local maximums, and the six d at local minimums. the s at local maximums are written by a position of local maximum as + j/ with integer j, and local minimums as + / thus, by , the q for dx and d solutions are represented by qdx = q h cj + sin ), and qd = q h cj + sin), respectively, with j = , , and a > . in and , the sign in front of h eects inversion of orbits in the y axis. then the orbits of six dx solutions are all congruent, in direct isometry or mirror inversion, and the orbits of six d solutions are so. thus the number of incongruent bifurcating solutions nb is counted as nb = + = morse index and bifurcation for gure-eight choreographies table n and symmetry of q for lj . symbols, cx, cy, cxy and c mean that the q is symmetric in the x axis, in the y axis, in both the x and the y axes and at origin, respectively. + t n q t n q cxy d cx dy dy cy d c in gure , one of six congruent dx and d solutions at a = are shown with parameters for initial conditions and with i for d, where d is rotated by = tan q q, to make the x axis bisector of two euler congurations.","The text explores the bifurcation of the figure-eight choreography in a system under a homogeneous potential. It investigates the Morse index changes and bifurcations for gure-eight choreographies at different parameter values. Symmetries and variations in orbits are analyzed, with specific focus on dxy and dx solutions. Critical points and the behavior of variated orbits are detailed, along with conditions for different symmetries. The bifurcation points and corresponding solutions are discussed, highlighting the distinctions between dxy, dx, and gure-eight choreographies. Various parameters and conditions are outlined to distinguish between these solutions. The text presents in-depth analysis and numerical explorations of these bifurcations in figure-eight choreographies."
"in this section, we discuss the bifurcation of the solution by using t as the parameter for the system under lj-type potential u = x b>c ulj|). the solution bifurcates at t = tmin = , thus there exists no solution for t < tmin and two solutions for t > tmin. one branch from t = tmin of solution tends to the gure-eight choreography under homogeneous potential with a = for t , and the other branch + gourd-shaped for t . in table , t, n = and symmetry of the variated orbit q for + and are tabulated. symbols, cx, cy, cxy and c mean that the variated orbit q is choreographic and is symmetric in the x axis, in the y axis, in both the x and the y axes, and at origin, respectively. note that though the symbols cx and cxy were written as c and ce in , respectively, we redened them since c and ce have the same symmetry as dx and dxy, dened in section , respectively. there is no other point with n = for solution than seven points tabulated in table . bifurcation yielding dxy, dx and d solutions the points indicated by dy in table , t = for + and t = for , yield dxy solutions represented by as in the section . the bifurcations are the both sides and the number of incongruent bifurcating solutions nbs are both two. in gure , s for bifurcation from + at t = is plotted. though bifurcation is both sides, the bifurcating solution also bifurcate soon at t = . thus there exist two bifurcated solutions for t > . for < t < morse index and bifurcation for gure-eight choreographies t -- -- - - - - - - - - - - - - - - figure bifurcation of + at t = yielding dxy solutions. s, dxy for t = from the right side of bifurcation point, and from the left side. parameters ; , . t - -- - - - - - - - - - figure bifurcation of at t = yielding dxy solutions. s, dxy for t = from the right side of bifurcation point, and from the left side. parameters : , . both solutions are bifurcated from left side of bifurcation point but for t > one from right side and the other from left side. in gure and , the two bifurcated solutions for t = from both sides are shown with parameters and d. in gure , bifurcation of dxy solution from at t = is shown as gure for +. the points indicated by d in table , t = for + and t = for , yield dx and d solutions represented by and , respectively, as in the section . the bifurcations are one side in the right side and the number of incongruent bifurcating solutions nbs are both two. in gures and , s and the bifurcated solutions for t = are shown with parameters.","The section discusses Morse index and bifurcation for the LJ system under lj-type potential. The solution bifurcates at t = tmin, resulting in no solution for t < tmin and two solutions for t > tmin. One branch tends towards a figure-eight choreography under homogeneous potential with a = for t < tmin, while the other branch forms a gourd shape for t > tmin. The text includes tables detailing t, n values, and the symmetry of variated orbits. Bifurcation points yielding dxy, dx, and d solutions are indicated. The bifurcations occur on both sides, resulting in incongruent solutions. Graphs illustrate the bifurcation process for both t < tmin and t > tmin scenarios."
"the points with |n| = in table , t = and t = for +, and t = for , bifurcate choreographic solutions since the variated orbit q for g = |n| = , q = q + h, is choreographic, cq = q . at t = for , the variated orbit q is symmetric in the x axis as shown in gure . we denote this orbit by cx. conditions for q to be cx, derived morse index and bifurcation for gure-eight choreographies t - - - -- - - - - - - - - - - figure bifurcation of + at t = yielding dx and d solutions. s for dx solution and for d . dx solution and d for t = d is rotated by in . parameters; = , = . t - - - - - - - - - - - - - figure bifurcation from at t = yielding dx and d solutions. s for dx solution and for d . dx solution and d for t = d is rotated by in . parameters; = , = in appendix a., are that q takes an isosceles triangle conguration shown by lled circles in gure , and with at t = by parameters , and opposite isosceles triangle conguration at t = t/ shown by open circles in gure , (q, q, q q, q q) = for given period t, four conditions determine four parameters . an index to distinguish solution cx from gure-eight choreography is l = in gure , the orbit of the solution cx for t = bifurcated at t = from is shown with parameters . at t = for +, the variated orbit q is symmetric at origin as shown in gure . we denote this orbit by c conditions for q to be c, derived in appendix a., are that q takes an euler conguration shown by lled circles in gure , and at t = by parameters , and another euler conguration at t = t/ shown by open circles in gure , (q, q, q q q q) = for given period t, three conditions determine three parameters . an index morse index and bifurcation for gure-eight choreographies - - - - - - - - - - - - - - - - figure variated orbit q for lj solution. cx in ; t = , h = and = . filled circles are isosceles triangle conguration at t = and open circles at t = t/ c in +; t = , h = and = . filled circles are euler congurations at t = and open circles at t = t/ cy in +; t = , h = and = . filled circles are conguration where a body on the y axis at t = and open circles at t = t/ - - - - - - - - - - - - - - - - figure choreographic solutions for t = ; cx bifurcating from at t = , c from + at t = , cy from + at t = . c is rotated by in . parameters; = , = , = . to distinguish solution c from gure-eight choreography is i = in gure , the orbit of the solution c for t = bifurcated at t = from + is shown with parameters and i. at t = for +, the variated orbit q is symmetric in the y axis as shown in gure . we denote this orbit by cy. conditions for q to be cy, derived in appendix a., are that a body is on the y axis at t = shown by lled circles in gure , q = , q = , where s = + xw yu y + y by parameters , and its inversion in the y axis at t = t/ shown by open circles in gure , (q, q + x, q y, q + u, q v, q y) = for given period t, the six conditions determine six parameters . an index to distinguish solution cy from gure-eight choreography is y = in gure , morse index and bifurcation for gure-eight choreographies the orbit of the solution cy for t = bifurcating at t = from + is shown with . for these three choreographies c = cx, c, cy, the variated orbit q in is written as qc = q h. the sign in front of h eects inversion of the orbit in the y axis for cx, and in the x axis for c and cy. these couples of congruent orbits bifurcate in the right side of the bifurcation point and nb =","The text discusses choreographic bifurcation, where the variated orbits q for different choreographic solutions bifurcate at specific points in time, leading to symmetric configurations in both the x and y axes. Different conditions and configurations are described for each solution, such as isosceles triangle and Euler configurations. Multiple parameters are involved in determining these choreographies, with each solution distinguished by specific indices. The orbits of the solutions show distinct behaviors and rotations at different time points. The text also presents the morse index and bifurcation for gure-eight choreographies, detailing how the solutions differentiate from one another."
"in this paper, we showed that the morse index changes at a bifurcation point for periodic solution and inversely all points where the morse index changes are bifurcation points for gure-eight choreography under homogeneous potential with a and for the solutions under lj-type potential. thus, for these choreographies, change of the morse index, n = , is not only necessary but also sucient condition for bifurcation point. further we observed that n determines the number of incongruent bifurcating solutions nb as . the bifurcations are conrmed numerically by newtons method. if the number of parameters is three as in the dxy, d and c cases, the parameters of the solution are represented graphically like . at t satisfying one of the three conditions under some restriction f = const where f is a function of the three parameters, the rest conditions are given by two curves in the plane of two parameters. in gure , for solution dxy determined by three parameters in a = homogeneous system, at t satisfying q = , the two conditions q = and q q q q = are shown in plane with total energy e = x i= q i + u as a restriction. the parameter for the dxy solution and the gure-eight choreography are observed as crossing points of two curves in gure newtons method sometimes does not converge unless initial parameters are good enough. we introduced another graphically assisted method: draw one condition as a function of one parameter by newtons method in one lower dimension. in gure , for solution dx determined by four parameters in a = homogeneous system, q in are shown as a function of v which is obtained by dimensional newtons method for the rest of three parameters with the rest of three conditions (q, q q, q q) = zero about v = corresponds to gure-eight choreography and two other zeros dx solutions in gure it is useful to evaluate the euler characteristics = x q n morse index and bifurcation for gure-eight choreographies - - - - figure map to search dxy solution. full and dashed curves are q = and q q q q = , respectively, at t satisfying q = for q starting from the initial conditions with e = for a = homogeneous potential. horizontal and vertical axes are x x and y y, respectively, where is for gure-eight choreography with t = crossing points of two curves show the parameters for the dxy solutions and for the gure-eight choreography. v - - figure q in as a function of v. zero about v = corresponds to gure- eight choreography and two other zeros bifurcating solution dx. for the manifold of action functional in the domain of periodic functions, where n is the morse index at a periodic solution q. since the will conserve, sometimes comparison of at the both sides of the bifurcation point helps to nd bifurcating solution. we assume the euler characteristics conserves at the both sides of bifurcation point, and holds. then we denote a number of congruent solutions which belong to the ith incongruent class by ni, and their morse index by ni, i = , , . . . , nb. for a bifurcation point with |n| = , bifurcation is one side since nb = from the conservation of at the both sides of bifurcation point, n = n + nn we obtain n = and n = n. for a bifurcation point with |n| = , bifurcation can be one side or both sides since nb = for one side bifurcation, conservation of , n = n + nn + nn morse index and bifurcation for gure-eight choreographies table restrictions of bifurcation by conservation of the euler characteristics and . symbols ni and ni are a number of congruent solutions which belongs to the ith incongruent class, and their morse index, respectively, i = , . . . , nb, nb = |n|. |n| bifurcation restriction for ni restriction for ni one side n = n = n one side n = n n = n both sides n = n n = n leads n = n and n = n for both sides bifurcation, n + nn = n + nn leads n = n and n = n in table , these restrictions on bifurcation are tabulated. according to the restriction for one side bifurcation at |n| = we found d solution after nding dx. the procedure to nd bifurcation numerically by the morse index is summarized as follows: ) find a point the morse index changes, n = ) investigate the corresponding variated orbit q which is an approximation of bifurcating solution. indeed, symmetry of q is useful for numerical calculation. ) if |n| > , variated orbit is chosen to make action critical. ) check conservation of the euler characteristics at both sides of bifurcation point by table we leave the followings for future works: calculation of the morse index for bifurcating solution; calculation of the morse index for gure-eight choreography under homogeneous potential with negative a, that is, ra for a < ; tracking solutions bifurcated as much as possible; calculation of linear stability for gure-eight choreography and bifurcated solutions; conditions for the point the morse index changes to be bifurcation point; and conditions for observation on the number of bifurcating solutions to hold. acknowledgments we thank kazuyuki yagasaki for his valuable comment on variated orbits at symposium on celestial mechanics and n-body dynamics . this work was supported by jsps grant-in-aid for scientic research k and k . appendix a. conditions for solutions we derive conditions for q to be dxy, dx, d, cx, c and cy solutions. we assume an inertia frame that total linear momentum p is zero, p = p b rb = , and center of mass g is at origin, g = p b rb = the subscript and index for body is assumed to be in the range and for dxy, d, c and cy, total angular momentum l = p b rb rb is zero since sum of signed area of three orbits is zero from the symmetry of orbits. morse index and bifurcation for gure-eight choreographies a conguration that a body b is in the x axis and the other two bodies b have the same x coordinate is called isosceles triangle conguration. for q symmetric in the x axis conditions for isosceles triangle conguration is written as (qb, qb, qb+ qb, qb+ qb) = by p = for b = with x = q, y = q, v = p q + q and total angular momentum l, is written as , and . the motion beginning from is time reversal motion from with inversion in the x axis and exchange of bodies b a conguration that a body b is at origin is called as euler conguration. for q symmetric at origin a condition for euler conguration is written as (qb, qb, qb+ qb qb+ qb) = by g = for b = with x = q, u = q, v = q is written as and . the motion beginning from is time reversal motion from with rotation and exchange of bodies b appendix a. dxy solution the dxy solution takes initial conditions and with and takes euler conguration when body reaches at origin at t = t. thus with b = , holds at t = t. thus at t = t, three bodies take initial conditions with rotation and exchange of bodies and then at t = t, three bodies take initial conditions again and the motion is periodic with period t = t. since the orbits were constructed by those from t = to t = t/ and their rotation, the orbits are symmetric in the x and the y axes. appendix a. dx solution the dx solution takes initial conditions and with and takes isosceles triangle conguration when body reaches in the x axis again at t = t. then with b = , holds at t = t. then at t = t, three bodies take initial conditions again and the motion is periodic with period t = t. since the orbits were constructed by those from t = to t = t/ and their inversion in the x axis, the orbits are symmetric in the x axis. appendix a. cx solution the cx solution takes initial conditions and with and takes isosceles triangle conguration when body reaches in the x axis again at t = t. then with b = , holds at t = t. then at t = t, three bodies take initial conditions again but with cyclic permutation of bodies, and the motion is choreographic with period t = t = t. since the orbits were constructed by those from t = to t = t/ and their inversion in the x axis, the orbits are symmetric in the x axis. morse index and bifurcation for gure-eight choreographies appendix a. d solution the d solution takes initial conditions and , and takes euler conguration when body reaches at origin at t = t. then with b = , holds at t = t. thus at t = t, three bodies take initial conditions again and the motion is periodic with period t = t. since the orbits were constructed by those from t = to t = t/ and their rotation, the orbits are symmetric at origin. appendix a. c solution the c solution takes initial conditions and , and euler conguration when body reaches in the x axis again at t = t. then with b = , holds at t = t. thus at t = t, three bodies take initial conditions again with cyclic permutation of bodies, and the motion is choreographic with period t = t = t. since the orbits were constructed by those from t = to t = t/ and their rotation, the orbits are symmetric at origin. appendix a. cy solution the cy solution takes initial conditions and by g = p = with by l = where body is on the y axis, which are represented by six parameters. at t = t when body reaches in the y axis, the relation between the positions and velocities at t = t have to be inversion in the y axis with exchange of bodies and thus we have twelve relations but six conservation quantities, g, p, l and total energy, reduce it to six as . thus at t = t if is satised, three bodies take initial conditions again with cyclic permutation of bodies, and the motion is choreographic with period t = t = t. since the orbits were constructed by those from t = to t = t/ and their inversion in the y axis, the orbits are symmetric in the y axis. references moore c braids in classical gravity phys. rev. lett. chenciner a and montgomery r a remarkable periodic solution of the three-body problem in the case of equal masses annals of mathematics sim o c dynamical properties of the gure eight solution of the three body problem contemporary mathematics sbano l symmetric solutions in molecular potentials proceedings of the international conference spt, symmetry and perturbation theory sbano l and southall j periodic solutions of the n-body problem with lennard-jones-type potentials dynamical systems fukuda h, fujiwara t, ozaki h figure-eight choreographies of the equal mass three-body problem with lennard-jones-type potentials j. phys. a: math. theor. shibayama m numerical calculation of the second variation for the choreographic solution proceedings of symposium on celestial mechanics and n-body dynamics ed. saito m, shibayama m and sekiguchi m morse index and bifurcation for gure-eight choreographies fukuda h, fujiwara t, ozaki h morse index for gure-eight choreographies of the planar equal mass three-body problem j. phys. a: math. theor. gal an j, mu noz-almaraz f j, freire e, doedel e and vanderbauwhede a stability and bifurcations of the figure- solution of the three-body problem phys. rev. lett. mu noz-almaraz f j, gal an j and freire e families of symmetric periodic orbits in the three body problem and the gure eight monografas de la real academia de ciencias de zaragoza barutello v, jadanza riccardo d and portaluri a morse index and linear stability of the lagrangian circular orbit in a three-body-type problem via index theory a. arch rational mech anal hu x and sun s morse index and stability of elliptic lagrangian solutions in the planar three-body problem advances in mathematics hu x and sun s index and stability of symmetric periodic orbits in hamiltonian systems with application to figure-eight orbit commun. math. phys. mu noz-almaraz f j and vanderbauwhede a private communication mu noz-almaraz f j, gal an j, freire e and vanderbauwhede a numerical explorations in a modied potential of the tbp zenodo. http://doi.org//zenodo. doedel e, keller b h and kernevez j p numerical analysis and control of bifurcation problems : bifurcation in nite dimensions international journal of bifurcation and chaos no. doedel e, keller b h and kernevez j p numerical analysis and control of bifurcation problems : bifurcation in innite dimensions international journal of bifurcation and chaos no. chenciner a some facts and more questions about the eight topological methods, variational methods and their applications, proceedings of the icm satellite conference on nonlinear functional analysis shibayama m private communication fujiwara t, fukuda h and ozaki h decomposition of the hessian matrix for action at choreographic three-body solutions with gure-eight symmetry arxiv:","The text discusses the changes in the Morse index at bifurcation points for periodic solutions, particularly for gure-eight choreography under homogeneous potential and LJ-type potential. The change in the Morse index is identified as a necessary and sufficient condition for bifurcation points. The numerical confirmation of bifurcations using Newton's method is highlighted, along with graphical representations of solution parameters. The text also mentions the conservation of the Euler characteristics and conditions for observing the number of bifurcating solutions. Future work includes calculations for gure-eight choreography under negative potential, linear stability analysis, and obtaining conditions for observing bifurcation points and the number of solutions. Various solutions such as dxy, dx, d, cx, c, and cy are described along with their initial conditions and characteristics. The text also includes references to related works and acknowledges contributions from researchers in the field."
,
"robust policy search is the problem of learning policies that do not degrade in performance when subject to unseen environment model parameters. it is particularly relevant for transferring poli- cies learned in a simulation environment to the real world. several existing approaches involve sampling large batches of trajectories which reflect the differences in various possible environments, and then selecting some subset of these to learn robust policies, such as the ones that result in the worst performance. we propose an active learning based framework, effacts, to selectively choose model parameters for this purpose so as to collect only as much data as nec- essary to select such a subset. we apply this framework using linear bandits, and experimentally validate the gains in sample efficiency and the performance of our approach on standard continuous con- trol tasks. we also present a multi-task learning perspective to the problem of robust policy search, and draw connections from our proposed framework to existing work on multi-task learning. keywords deep reinforcement learning, robust learning, active learning, ro- botics","Robust policy search addresses the challenge of learning policies that maintain performance across different environment parameters, crucial for transferring policies from simulation to the real world. Existing methods involve sampling varied trajectories and selecting subsets for learning robust policies. An active learning framework, effacts, is proposed to choose model parameters effectively and minimize data collection. Linear bandits are used in this framework, demonstrating improved sample efficiency and performance on continuous control tasks. A multi-task learning perspective is introduced, linking effacts to existing work in multi-task learning. Key terms include deep reinforcement learning, robust learning, active learning, and robotics."
"recent advances in deep reinforcement learning algorithms have achieved remarkable performance on continuous control tasks . traditionally, these algorithms are used to learn policies to perform a given task in simulation. however, it has been found that policies learned in simulation often do not perform well in, or transfer to, a real-world system that the simulation models . indeed, the prospect of being able to deploy policies learned in simulation on real-world systems such as physical robots is one of the major drivers for research in reinforcement learning. one class of approaches towards this goal that has gained trac- tion is to learn from multiple simulated domains that approximate the real target domain. these usually correspond to an ensemble of environment models with various parameters such as the mass of a part of a robot or the coefficient of friction between the robots foot and the ground. given such an ensemble, the problem of robust policy search is to learn policies that perform well across this ensemble. one prominent group of approaches in this class involves sam- pling model parameters from the ensemble and collecting batches work performed when author was at the robert bosch centre for data science and ai, iit madras. preprint: this is the authors own version of their work that is published in the th joint international conference on data science and management of data (th acm ikdd cods and th comad) . doi:/ of trajectories simulated using these parameters , which are then used for training a policy, typically by a model-free rl algorithm. these approaches differ mainly in the way in which they choose subsets of these trajectories to focus on for policy learning. although robust policy search is inevitably a harder learning prob- lem than standard policy search, the amount of data collected by these methods is still quite large, up to almost orders of magnitude more than is typically required by the usual policy optimization al- gorithms, regardless of the method used to choose a subset of these trajectories for learning. therefore, although these approaches are shown to be effective for learning robust policies, and offer other advantages such as reduced modeling burden, the requirement for an abundance of data makes them computationally expensive. in this work, we demonstrate a novel way to improve their sam- ple complexity while maintaining the performance and robustness of the learned policy through the use of active learning for intelli- gently selecting model parameters for which to sample trajectories for learning. active learning is used to directly acquire some de- sired subset of the trajectories (such as the subset resulting in the worst performance), while collecting as little additional data as pos- sible. in contrast, existing methods sample parameters directly from the ensemble, and possibly discard large portions of the collected trajectories . the resulting framework, effacts, offers greatly improved scalability, thus broadening its applicability to real-world problems. the structure of the framework and the use of active learning for trajectory sampling results in some connections between robust policy search and multi-task learning. we discuss the relation between the two problems, as well as the differences in their solution approaches. thus, the contributions of this paper are as follows: we in- troduce a novel active learning framework that performs more judicious collection of trajectories for training robust policies, re- sulting in low sample complexity; we present an instantiation of the framework using linear bandits, and perform experimental validation on environment ensembles from standard continuous control benchmarks to empirically demonstrate significant reduc- tions in sample complexity while still being able to learn robust policies; we explore connections to multi-task learning that are revealed upon casting robust policy search as a multi-task learning problem and discuss its relation to existing work in the area.","Recent advances in deep reinforcement learning have led to impressive performance on continuous control tasks, but there is a challenge in transferring policies learned in simulation to real-world systems. To address this, research focuses on learning from multiple simulated domains that mimic the real environment. A key approach is robust policy search, where policies are learned to perform well across this ensemble of simulated domains. However, current methods require a large amount of data, making them computationally expensive. In this work, a novel active learning framework called effacts is introduced to improve sample complexity while maintaining performance. This framework intelligently selects model parameters for sampling trajectories, resulting in reduced data collection needs. Experimental validation shows significant reductions in sample complexity while still achieving robust policy learning. Connections to multi-task learning are explored, highlighting the potential of this approach for real-world applications."
"learn controllers with a specific functional form using trajecto- ries sampled for parameters drawn from an ensemble, and optimize for the average case performance. propose epopt, which learns arxiv:v nov narayanaswami et. al. a neural network policy using a model-free drl algorithm, but on simulated domains sampled from an ensemble of models. an adversarial approach to training is taken that involves selectively exposing to the model-free learner only data from those sampled models on which the learner exhibits the least performance. epopt optimizes the conditional value at risk, which has also been used for learning robust options. even though this is a more so- phisticated approach than the former and is demonstrated to have greater performance and robustness, the number of trajectories collected is still very large. a form of adversarial training is also employed in and , but in these works, external disturbances are applied to the agent, rather than the model itself changing. propose an approach that optimizes the average case perfor- mance, but additionally performs explicit system identification, and the estimated model parameters are fed to a nn policy as additional context information alongside the original observations. also uses system identification on data from the real world to decide the parameters on which to train. also perform system identifica- tion, but operate in a belief space over the model parameters. again, the data requirements are quite large, both for policy learning as well as system identification. a recent work that learns from an ensemble of models is , but the ensemble here consists of learned dnn models of the dynamics for use in model based rl, rather than being induced by changing physical properties of the environment. a similar ensemble gener- ated by perturbing an already learned model is used for planning through in . this work also does not deal with model uncertain- ties with physical meaning. approaches related to learning from an ensemble of models have also been studied under dynamics randomization and domain randomization . although uses only an appropriate subset of models to train on, none of the above approaches consider ways to sample trajec- tories only as necessary. our proposed framework employs active learning to decide with data from only a few model parameters the models for which the agent requires more training. active sampling approaches have also been explored for task selection in multi-task learning by , a viewpoint we discuss in more detail in section","The section discusses various approaches for learning controllers with specific functional forms using trajectories sampled for parameters drawn from ensembles, and optimizing for average case performance. One approach, epopt, uses a neural network policy learned with a model-free deep reinforcement learning algorithm on simulated domains sampled from an ensemble of models. It employs an adversarial training approach to selectively expose the learner to data from models where it exhibits poor performance. The approach optimizes the conditional value at risk for learning robust options. Other techniques discussed involve explicit system identification and using real-world data to train policies. Some works focus on learning from an ensemble of models generated through dynamics randomization and domain randomization. The proposed framework employs active learning to determine the models for which the agent requires more training from only a few model parameters, optimizing trajectory sampling."
"rl on an ensemble of models we work with the same setting described in where the model ensemble is represented as a family of parametrized mdps on a fixed state and action space. following the same notation, this is the set m = s, a, t , r,, s, for each parameter in the space of parameters p, whose elements are respectively the state and action spaces, transition functions, reward functions, discount factor and the initial state distribution. those items that are subscripted with depend on, i.e., different parameters induce different dynamics and rewards. we note here that we say parameter even if it is a vector rather than a real number. further, there is a source distribution p that indicates the likelihood of any particular p in the model ensemble. we denote the typical trajectory from any of these mdps by = {,,} =, where is the time horizon, and the discounted return from the start state = = . these trajectories are generated by following a policy which are parameterized by a vector , which we denote by . we define the performance at parameter as the expected discounted return from the start state in m = e "" = # robust policy learning via cvar optimization robust policy search seeks policies that perform well across all parameters in p, and do so without knowing the parameter for the mdp on which they are being tested. this translates to being able to perform well on some unknown target domain, and also potentially handle variations not accounted for in p. the intuitive objective for this is to consider the average performance of the policy over the source distribution = ep . however, this objec- tive could be close to the maximum even if there are sharp drops in performance in some regions of p. a different objective used by approaches such as is the conditional value at risk formulation from , which considers the performance across only the subset of p that corresponds to the bottom percentile of returns from p, for a given (, ]. this has the effect that policies which have such sharp drops in performance (i.e bad worst case performance) are no longer considered good solutions.","In this section, the text discusses the background of working with an ensemble of models represented as a family of parametrized Markov Decision Processes (MDPs) on a fixed state and action space. The parameters in the space of parameters include state and action spaces, transition functions, reward functions, discount factor, and initial state distribution. The dynamics and rewards of the models differ based on different parameters. The source distribution indicates the likelihood of any particular parameter in the model ensemble. Trajectories are generated by following policies parameterized by a vector. Robust policy learning via Conditional Value at Risk (CVaR) optimization seeks policies that perform well across all parameters without knowing the specific parameter values. The objective is to perform well on unknown target domains and handle variations not accounted for. Approaches such as CVaR optimization consider the performance across only a subset of parameters corresponding to the bottom percentile of returns, filtering out policies with sharp drops in performance."
"here, we provide a quick overview of linear stochastic bandits since they play an important role as solutions to the active learning problem in section the lsb problem is one of finding the optimal arm from a given set of arms x similar to the standard multi-armed bandit problem, but with the average reward from each arm being an unknown linear function of the features associated with that arm. that is, if x is an arm, and we also denote its features by , the reward is given by = + , where is some zero-mean noise, and gives the parameters for said linear function. thus, finding the optimal arm amounts to estimating . although it may seem restrictive to assume a linear dependence on the arms, more expressiveness can be achieved by using a feature transformer, in a manner similar to the practice for linear regression. the feature transformer is a function : x rthat maps each arm x to a feature vector , and the lsb learner would be estimating so that = + . there have been several approaches to solving the lsb problem under various objectives. one group of works are based on the principles of the upper confidence bound algorithm for mab problems. the other popular class of approaches is based on thompson sampling. active learning for efficient trajectory sampling existing approaches for robust policy search are based on col- lecting trajectories at various parameters from p, and using some subset of these trajectories to optimize the policy being learned. an active learning framework for efficient robust policy search use active learner to learn about policy performance by collecting trajectories as necessary sample parameters according to given objective based on learned performance prole use active learner to learn about policy performance by collecting trajectories as necessary perform policy update using trajectories collected at each of these parameters. figure : main loop of the effacts framework. as mentioned before, the number of trajectories required can be very large, up to orders of magnitude more than required for a standard rl problem. although robust policy search is expected to require more data than standard rl, improvements to its sample efficiency are still necessary in order for it to be viable in complex real-world systems. to motivate our developments to improve on the sample effi- ciency, we start with the observation that there is some functional dependence of the performance of a given policy on the model parameter corresponding to a task under consideration. existing approaches all disregard this dependence when evaluating their objective, leading to increased sample complexity. the increase in sample complexity is more severe when using the cvar objective such as in , due to having to discard most of the trajectories collected in order to estimate the cvar. in fact, for a standard value of = , % of the collected trajectories need to be discarded . here, we wish to devise a strat- egy to utilize the information from the aforementioned functional dependence effectively so as to minimize such wastage. active learning and the effacts framework active learning is a paradigm where the agent chooses data to learn from based on its previous experience (see for a comprehensive survey). it has been used to speedup learning tasks, especially in situations with limited data. an active learner not only needs to work with as few samples as possible, it also needs to account for the uncertainty in whatever data it has collected. these are exactly the desiderata of the required strategy, as it must be able to fit the performance function across p by collecting as few trajectories as possible, which come with noisy evaluations of the performance. thus, quite clearly, the problem of efficiently performing such sam- pling is connected to active learning. the case of active learning that is of interest to us is when the agent is allowed to sample output for arbitrary points in the input space (instead of having to choose points from a finite dataset). the input here is some parameter from p, and the output is the return from one trajectory collected at . we now outline our active learning framework for learning robust policies. each policy update involves selectively generating algorithm the effacts framework : for = . . . do : learnperf : p selectparams( ) : {} : for each p do : trajectory collected at using : : end for : + batchpolopt : end for : return trajectories to be sent to the batch policy optimization algorithm. this is done in two phases as follows: performance assessment: in the first phase, active learning is used to assess the performance of the current policy. to do this, an active learner sequentially picks some parameters and trajecto- ries are sampled for each of them by setting the environment to these parameters and running the current policy. after a particular number of trials, it is in theory expected to have a reasonably good approximation of the performance as a function of the parame- ters, say . note that we have used as a subscript since this function is specific to the current policy with parameters . the active learner and this process are encapsulated in a subrou- tine learnperf that takes as input some policy parameters, and returns the function as described above. learnperf could be equipped with persistent memory (e.g. to store data from previous iterations), and no assumptions are made about the form of the output other than that it can be evaluated at any given p. parameter selection: the second phase uses this assessment to decide which parameters from p trajectories need to be collected for. this selection is guided by some given objective that enforces robustness. another subroutine, selectparams carries out this selection, taking as input a performance profile . based on this performance profile, it returns a set p of parameters at which the policy needs to be trained. the policy update is then performed using trajectories collected for each parameter in p, and this is done for a given number of iterations. we call the resulting framework effacts (efficient active trajectory sampling), and summarize it in algorithm and figure any particular instantiation of effacts is defined by the choice of active learning algorithm and also the scheme used to select which parts of p to sample from, i.e by specifying the learnperf and selectparams subroutines. applying effacts we analyze one such instantiation based on the cvar objective for parameter selection. to generate samples of the bottom percentile of trajectories, a batch of parameters is sampled from p, and trajec- tories are collected only for those that are in the worst percentile of performance according to . the use of bandit algorithms for active learning is well studied in both multi-armed bandit as well as linear stochastic bandit settings. the parameter narayanaswami et. al. spaces involved in robust rl are invariably continuous. lsbs and gaussian process regression are two well-known approaches that can perform active regression on continuous spaces. lsbs, however, are specialized in the sense that they quickly seek out areas that lead to high reward, a property that we make use of as described shortly. considering that gaussian processes are more computa- tionally expensive, and also that lsbs are simpler to implement and can be very efficient, especially when data is scarce, we turn to lsbs as the active learner in our experiments. its arms are simply a large enough collection of parameters spread across p. pulling an arm p results in a trajectory being sampled at . feedback is given to the bandit in an adversarial manner, being proportional to the negative of the return obtained on that sampled trajectory. this causes it to seek out regions with low performance, and in the process learn about the performance across p. this behaviour is especially useful for identifying the worst-case parameters that are used in optimizing the cvar objective. we note that although the bandits learning phase is inherently serial, it is still possible to collect the trajectories for the estimated worst percentile of parameters in parallel. we call this algorithm effacts-c-b and its learnperf and se- lectparams subroutines described in algorithm the following hyperparameters are introduced: , the number of trajectories sampled by the bandit in the course of its learning, , the total number of parameters chosen (this means that l m parameter values are drawn from the source distribution and their perfor- mance is estimated using the bandit, but only the bottom of those are used to collect trajectories). the most critical component is the lsb learner which incorporates internally a feature trans- former that takes in a parameter from the source distributions support and applies some transformation on it (including possibly standardization), and also scales the negative returns given to it appropriately. sample efficiency due to the fact that effacts-c-b strategically chooses parameters at which to collect trajectories, we expect it to be able to maintain robustness and performance while collecting fewer samples than existing approaches. consider for instance epopt, which dis- cards a fraction of the trajectories it collects. if epopt collects trajectories, and effacts-c-bs bandit learner is allowed arm pulls, with the same number of trajectories being used for learning as epopt , the ratio of the total amount of data collected by the two algorithms is  +  . for a nominal setting of = , = and = , this results in a dramatic % reduction in the amount of data collected. we later show that efficiency gains of this order are indeed attainable in practice. connections to multi-task learning the problem of robust policy search on an ensemble of models can also be viewed as a form of transfer learning from simulated domains to an unseen real domain (possibly without any training on the real domain, which is referred to as direct-transfer or jumpstart ). further, the process of learning from an ensemble of models can be viewed as a multi-task learning problem with the set of tasks corresponding to the set of parameters that constitute the source domain distribution. learning a robust policy corresponds to maintaining performance across this entire set of tasks, as is usually the goal in mtl settings. mtl, which is closely related to transfer learning, has been studied in the drl context in a number of recent works . however, these works consider only discrete and finite task sets, whereas model parameters form a (usually multi-dimensional) continuum. more generally, we can think of mtl with the task set being generated by a set of parameters, and refer to such problems as parameterized mtl problems, with robust policy search being an instance of this setting. has employed a bandit based active sampling approach sim- ilar to what we have described here to intelligently sample tasks to train on for each iteration. the feedback to the bandit is also given in an adversarial manner. however, we note several differences when it comes to a parameter- ized mtl setting. the first is the functional dependence of the task performance to an underlying parameter as discussed earlier. in the discrete mtl settings usually studied (such as atari game playing tasks), there is no such visible dependency that can be modeled. this means that any algorithm that is adapted to the parameter- ized setting need to be reworked to utilize such dependencies as effacts does. the task selection procedure in effacts differs from the one in in that the performance is sampled for several tasks in between iterations of policy training. additionally, one single task is not chosen in the end, rather the active learner is used to inform the selection of a group of tasks as necessary for the given objective.","Linear stochastic bandits (LSBs) aim to find the optimal arm from a set of arms, where the average reward from each arm is an unknown linear function of its features. LSB learners estimate the parameters of this linear function to find the optimal arm. Feature transformers can enhance expressiveness by mapping arms to feature vectors. LSB problems can be solved using upper confidence bound or Thompson sampling approaches. The efficiency of LSBs is crucial for robust policy search, where active learning strategies like effacts framework help in sampling trajectories to optimize policies. Effacts efficiently samples trajectories based on assessed performance and selected parameters. Bandit algorithms, particularly LSBs, are used in active learning due to their simplicity and efficiency in seeking regions with high rewards. Effacts-C-B algorithm strategically selects parameters to collect trajectories, resulting in significant data efficiency gains compared to existing approaches. Additionally, LSBs in robust policy search can be viewed as a form of transfer learning or a parameterized multi-task learning problem."
,
,
"as epopt uses the cvar objective, it is a very suitable baseline against which to compare effacts-c-b for demonstrating the ben- efits of introducing the active learner. we conduct experiments to answer the following questions, which we will reference as rq, rq etc. in later discussions: do the policies learned using effacts-c-b suffer any degra- dation in performance from that of epopt? is robustness preserved across the same range of model parameters as in epopt? does the bandit active sampler identify with reasonable ac- curacy the region corresponding to the worst percentile of performance, and does it achieve a reasonable fit to the performance across the range of parameters (i.e has it ex- plored enough to avoid errors due to the noisy evaluations it receives)? how much can the sample efficiency be improved upon? this mainly boils down to asking how few trajectories are sufficient for the bandit to learn well enough. implementation details and hyperparameters the experiments are performed on the standard hopper and half- cheetah continuous control tasks available in openai gym , simulated with the mujoco physics simulator . as in , some subset of the following parameters of the robot can be varied: torso an active learning framework for efficient robust policy search algorithm definitions of learnperf and selectparams subroutines for effacts-c-b : function learnperf : for = . . . do : optimal arm estimate by . : trajectory collected at using : return from : update with : end for : function parameterized by s current weights : return : end function : function selectparams( ) : p {}, r {} : for = . . . l m do : parameter sampled from p : : add to r and to p : end for : psubset of p giving rise to the bottom per- centile of returns in r : return p : end function parameter low high mass friction damping inertias parameter low high mass friction damping inertias table : description of the source domain distribution for the hopper and half-cheetah tasks. the values given here specify the means and standard deviations for normal distributions truncated at the low and high points mentioned here. this is equivalent to the probability density of a normal distribution with parameters being zeroed outside the interval , and being normalized so that it integrates to figure : performance as a function of torso mass for = for the hopper and half-cheetah tasks. the bands indicate the confidence intervals of the performance measured across runs of the entire training procedure. all of the configurations tested use at least % fewer trajectories than epopt, and with the exception of = and = in the hopper task alone, perform to the same extent and are as robust as epopt . mass, friction with the ground, foot joint damping and joint iner- tias. we also use the same statistics for the source distributions of the parameters which are described in table we emphasize that because we are using the same environments in and also the same policy parameterization, results reported there can be used directly to compare against effacts-c-b. we use the hyperparameter settings for trpo suggested in ope- nai baselines on which our implementation is also based. these are shown in table one difference from epopts implementation is that we use generalized advantage estimation instead of subtracting a baseline from the value function. for value function estimation narayanaswami et. al. hyperparameters median %tile avg %tile std. dev. max %tile =, = =, = =, = =, = table : statistics for the percentiles described in section for the hopper tasks -d model ensemble with = , which are measured every fifth iteration from the th to the th iterations. hyperparameter value timesteps per batch max kl cg iters cg damping vf iterations vf stepsize e- table : hyperparameters for trpo hyperparameter value table : thompson sampling hyperparameters using a critic, we use the same nn architecture as the policy, hidden layers of units each. policies are parameterized with nns and have two hidden layers with units each, and use tanh as the activation function.","The experiments section compares effacts-c-b with the baseline epopt using the cvar objective. The experiments aim to answer questions such as whether policies learned using effacts-c-b degrade in performance compared to epopt, if robustness is maintained across varying model parameters, and how much sample efficiency can be improved. The experiments are conducted on standard hopper and half-cheetah continuous control tasks simulated with the mujoco physics simulator. Various parameters of the robot are varied during the experiments. Results show that configurations tested with effacts-c-b use fewer trajectories than epopt, with similar performance and robustness. The same environments and policy parameterization as epopt are used for direct comparison. Hyperparameters for trpo are utilized, with differences from epopt's implementation including the use of generalized advantage estimation instead of subtracting a baseline from the value function. Thompson sampling hyperparameters and neural network architectures for policies are also discussed in the experiments section."
"for all our experiments, we implement the bandit learner using thompson sampling due to its simplicity, following the version described in . the hyperparameters introduced by this are as in table . the first two parameters control the amount of exploration per- formed during thompson sampling, while is a regularization parameter for the parameter estimates. the arms of the bandit are model parameters taken uniformly across the domain and converted to feature values. in order to allow for some degree of expressiveness for the fit, we apply polynomial transformations of some particular degree to the model parameters. the features input to the bandit are th degree polynomial terms generated from the model parameters. this amounts to terms in the -d case and in the -d case. these arm representations are then standardized before being used by the bandit. the negative returns given as feedback to the bandit are scaled by a factor of performance and robustness in this section, we perform the following experiment to evaluate effacts-c-b for the objectives of rq in this experiment, only one environment parameter is varied, creating a -d model ensemble on which to evaluate the algorithm. the torso mass is varied in both the hopper and the half-cheetah domains keeping the rest of the parameters fixed at their mean values. the performance of the effacts-c-b learned policy is then tested across this range, and the results are shown in figure we use trust region policy optimization for batch policy optimization and run it for iterations. for this part, we use th degree polynomial transformations. we see that the policy is indeed robust as it maintains its perfor- mance across the range of values of the torso mass, and it achieves near or better than the best performance for both tasks as reported in in all but one case, with = and = for hopper. this setting samples the least number of trajectories per iteration, , which is just one eighth of the drawn in epopt. although there is one region where it is unstable, it is still able to maintain its performance everywhere else, thus attaining the same level of robustnes as epopt. further, the performance achieved is almost as good as, and possibly better than epopt. for the other settings which use more trajectories, this does not happen, and even at = and = which samples the most trajectories, a % reduction in samples collected is achieved over epopt . the other two settings which perform almost as well in both tasks collect each, which amounts to an even larger reduction of %. in the case of = and = with the half-cheetah task, this number is pushed even further to % while still retaining performance and robustness. performance on a -d model ensemble in this question, we further investigate the scalability of the algo- rithm to larger model ensembles, again in the context of rq in this experiment, we vary the friction with the ground in addition to the torso mass, thus creating a two dimensional ensemble of parameters. here, we run trpo for iterations, and again use th degree polynomial transformations. figure shows the results obtained. full performance is maintained over almost all of the parameter space in both domains, again being comparable to or better than in . notably, with =, =, the same % reduction in an active learning framework for efficient robust policy search figure : performance as a function of torso mass and ground friction on the hopper and half-cheetah tasks for = for one run or effacts-c-b. in each row, left: =, = right: =, = collected trajectories is obtained even in a higher dimensional model ensemble. this is also despite the added challenge of an increased number of parameters for the bandit to fit ( as opposed to in the previous experiment).","The text discusses the implementation of a bandit learner using Thompson sampling for experiments. The bandit arms are model parameters converted to feature values with polynomial transformations applied for expressiveness. The bandit receives negative returns scaled by a performance factor. The algorithm is evaluated on varying environment parameters, maintaining robust performance across different settings. Trust region policy optimization is used for batch policy optimization with polynomial transformations. The algorithm demonstrates robustness and performance comparable to or better than other methods, even with reduced sample collection. The scalability to larger model ensembles is also investigated, showing maintained performance over different parameter spaces."
"in figure , we present the outcome from one particular iteration of training. the true performance profile is estimated by collecting trajectories to calculate the mean return at each parameter . along with this is shown the bandits fit, the trajectories it collects while learning and the trajectories that are sampled based on its estimate of the bottom = percentile . we see that the bandit takes exploratory actions (points to the left and in the middle) that dont lead to the worst returns. however, it quickly moves towards the region with low returns, and the final fit is close to the true mean performance. from comparison with the output trajectories, the trajectories in the learning phase are quite clearly not representative of the bottom = percentile according to the source distribution. thus, we cannot reuse these to perform learning with the cvar objective. we also note that a perfectly learned performance profile is not necessary to sample from the worst trajectories. as long as the fit is reasonably accurate in that region, the output trajectories will be of good quality. in practice, we expect lsb algorithms to be capable of doing this as they tend to focus on these regions. analysis of the bandit active learner here we validate one of our key assumptions, that the active learner learns well enough about the performance that it can produce a decent approximation of a sample batch of the bottom percentile figure : the bandits operation in the th iteration of training in the hopper task. individual trajectories are shown as dots or crosses, with the position indicating the parameter value and the return obtained. of trajectories. for this, we first evaluate the average return for a batch of samples from p by collecting a large number of trajectories at each parameter. we note that these trajectories are solely for the purpose of analysis and are not used to perform any learning. then, the percentile of the trajectory deemed to have the greatest return among those chosen based on the bandit learner is computed using the returns in this batch by using a nearest-neighbor approximation. this is done across several iterations during the training, and the median percentiles along with other statistics are reported in table for the hopper task. ideally, we would like this value to come out to around , i.e the parameter with the greatest performance among the worst percentile should be at the percentile. in our estimates, there are some outliers that cause the average to become large, but as the median value shows, it is indeed reasonably close to the desired value of for = . non-stationary bandits for data reuse in this section, we attempt to answer rq by considering modifica- tions that can be made to effacts-c-b, so that it uses lesser data, while also achieving a level of performance and robustness that is close to the results above. particularly, we investigate the use of a modified version of the thompson sampling algorithm above that is suited for non- stationary scenarios. this is done in order to reuse performance history from previous iterations to estimate the bandits parameters with the aim of achieving a further reduction in sample complexity. to implement this, past data is weighted down in the linear re- gression step of thompson sampling, and the weight decays after each iteration by a factor . that is, at the iteration, the data from the , . . .iterations would have weights , . . .respectively. we compare this version with the original setup in figure we see that the setting with the smallest number of trajectories narayanaswami et. al. figure : results for the non-stationary-bandit version of effacts-c-b, along with the original effacts-c-b results on the hopper -d ensemble. a value of = was used, with the other hyperparameters remaining unchanged. as before, the plot shows the perfor- mance as a function of torso mass for = , and the bands indicate the confidence intervals of the performance mea- sured across runs of the entire training procedure. this time, with % fewer trajectories than epopt, the configura- tion = , = with history is able to perform compa- rably to epopt, as well as effacts without history and using more trajectories. collected with history is more robust than the vanilla case with = , = and performs nearly as well . with more data, some loss of robustness is encountered, but the performance improves, and becomes comparable to the best original case (= , = ).","In a technical presentation, the section discusses visualizing the Bandit Active Learner iterations of training and its trajectory outcomes. The Bandit Active Learner explores different actions and quickly moves towards regions with low returns, eventually producing a fit close to the true mean performance. It is noted that the trajectories collected during the learning phase are not necessarily representative of the bottom percentile from the source distribution. The active learner is validated to produce a decent approximation of the bottom percentile sample batch. Modifications are explored to make the algorithms more efficient and robust, including a non-stationary bandit version of the Thompson sampling algorithm that weights down past data in iterations. Results show that the non-stationary bandit version can achieve comparable performance with fewer trajectories compared to the original setup."
"we note that we do not perform pre-training as in epopt where the entire batch of trajectories is used for policy learning for some iterations at the beginning (corresponding to optimizing for the average return over the ensemble). this has been reported to be necessary, possibly due to problems with initial exploration if using the cvar objective from the beginning. effacts-c-b on the other hand works without any such step. however, at the very beginning, we collect trajectories for parameters sampled directly from p until time steps have elapsed in that iteration. this is because algorithms like trpo have been known to require at least that much data per iteration. conclusions and further possibilities we developed the effacts framework for using active learning to make an informed selection of model parameters based on agent per- formance, which can subsequently be used to judiciously generate trajectories for robust rl. with an illustration of this framework based on linear bandits and the cvar objective, we have both demonstrated its applicability for robust policy search as well as established its effectiveness in reducing sample complexity by way of empirical evaluations on standard continuous control domains. we also discussed our work in the context of multi-task learning along with the similarities and differences between these settings. our work opens up requirements for active learning algorithms that can work well with even lesser data than we need here. methods like gaussian process regression are known to be efficient, but not in high dimensional spaces. for robust policy search methods to be effective for transfer from simulation to reality, they need to be able to handle the complexities of the real world, which necessitates methods that work with high dimensional model ensembles, which in turn entail frameworks such as effacts to help reduce the sample complexity. another possibility for robust policy search itself is to develop objectives that can speedup learning as well as make use of the features of effacts to maintain sample efficiency. with our interpretation of robust policy search as a parameter- ized version of multi-task learning, a natural next step would be to adapt developments in the usual discrete mtl setting to robust policy search. it would also be worthwhile to similarly investigate the applicability of meta learning, as it would prove useful for both dealing with large disparities between the source domains and the real world, as well as coping with unmodeled dynamics (which are unavoidable since it is not feasible to model the real world with complete accuracy).","The section highlights that the Effacts framework was developed to use active learning for selecting model parameters based on agent performance, aiding in generating robust trajectories for reinforcement learning. Empirical evaluations on continuous control domains show its effectiveness in reducing sample complexity. The text also discusses the potential of active learning algorithms with lesser data requirements and the need for methods that can handle high-dimensional model ensembles for real-world applicability. Additionally, it suggests exploring objectives to accelerate learning and leveraging Effacts for maintaining sample efficiency. Adapting multi-task learning developments and investigating meta learning are proposed for enhancing robust policy search in dealing with real-world complexities and unmodeled dynamics."
,
"the authors thank the robert bosch center for data science and ai, iit madras for funding this work and providing the requisite com- puting resources. we also thank aravind rajeswaran for valuable pointers and suggestions, and openai for their excellent codebase of deep rl algorithms.","The authors express their gratitude to the Robert Bosch Center for Data Science and AI and IIT Madras for funding and providing computing resources for the project. They also acknowledge Aravind Rajeswaran for valuable pointers and suggestions, as well as OpenAI for their deep reinforcement learning algorithms codebase."
,
"arxiv:v jan approximation forte sur un produit de vari et es ab eliennes epoint e en des points de torsion yongqi liang r esum e. consid erons lapproximation forte pour les vari et es alg ebriques d e- nies sur un corps de nombres k. soit s un ensemble ni de places de k contenant les places archim ediennes. soit e une courbe elliptique de rang de mordellweil non nul et soit a une vari et e ab elienne de dimension strictement positive et de groupe de mordellweil ni. pour un ensemble ni quelconque t de points de torsion de ea, notons par x son compl ementaire. en supposant la nitude de x, nous d emontrons que x v erie lapproximation forte avec lobstruction de brauermanin hors de s si et seulement si la projection de t sur a ne contient aucun point k-rationnel. strong approximation for products of abelian varieties punctured at torsion points abstract. consider strong approximation for algebraic varieties dened over a number eld k. let s be a nite set of places of k containing all archimedean places. let e be an elliptic curve of positive mordellweil rank and let a be an abelian variety of positive dimension and of nite mordellweil group. for an arbitrary nite set t of torsion points of e a, denote by x its complement. supposing the niteness of x, we prove that x satises strong approximation with brauermanin obstruction os if and only if the projection of t to a contains no k-rational points.",The text discusses strong approximation for products of abelian varieties truncated at torsion points. It explores the concept of strong approximation for algebraic varieties defined over a number field and presents conditions under which strong approximation holds with a specific obstruction. The findings demonstrate the relationship between the complement of a set of torsion points and the projection of this set on an abelian variety in terms of k-rational points.
"soit k un corps de nombres. on sint eresse ` a etudier les points entiers dune k- vari et e lisse. on note par xc une compactication lisse de x. en g en eral, m eme si on conna t le comportement des points rationnels de xc, on ne peut quen d eduire tr` es peu dinformations sur les points entiers de x. depuis tr` es longtemps, on sint eresse ` a la densit e de zariski de lensemble des points entiers. on sint eresse egalement ` a lapproximation forte la densit e de lensemble des points entiers plong e dans lensemble des points ad eliques. dans la litt erature, de vari et es de types divers ont et e etudi ees. dans cet article, on se limite aux ouverts de certaines vari et es ab eliennes. dans larticle de b. hassett et y. tschinkel , ils ont discut e la question de la densit e potentielle de zariski des points entiers. ils ont d emontr e que tout ouvert dun produit de vari et es ab eliennes simples dont le compl ementaire a grand codimension v erie cette densit e potentielle. a. kresch et y. tschinkel mots cl es : approximation forte, obstruction de brauermanin, vari et es ab eliennes, points de torsion, points entiers. msc : g, g janvier yongqi liang ont aussi les r esultats num eriques qui soutiennent une r eponse armative ` a cette question pour certaines jacobiennes epoint ees en un point rationnel. dun autre point de vue, dans larticle r ecent de y. cao, f. xu et lauteur, nous avons etudi e la densit e des points entiers au sens de la topologie ad elique. nous avons r epondu partiellement ` a une question de o. wittenberg, cf. [aim, problem ] et . parmi nos r esultats, nous avons montr e le th eor` eme suivant. th eor` eme . soit e une k-courbe elliptique de rang de mor- dellweil non nul. soit a une k-vari et e ab elienne de dimension strictement positive et de rang de mordellweil z ero. si o d esigne l el ement neutre de e a, alors louvert \ o ne v erie pas lapproximation forte avec lobstruction de brauermanin hors de . sa d emonstration est bas ee sur une g en eralisation dun argument de d. harari et j. f. voloch et sur une id ee qui remonte ` a b. poonen . le but de cet article est de g en eraliser ce th eor` eme. nous gardons les m eme hypoth` eses sur e et a, consid erons le compl ementaire x dun ensemble quelconque de points de torsion de e a. en am eliorant lancien argument, nous trouvons une description compl` ete pour la propri et e dapproximation forte sur x. th eor` eme . soit k un corps de nombres. soit s un ensemble ni de places de k contenant les places archim ediennes. soit e une k-courbe elliptique de rang de mordellweil non nul. soit a une k-vari et e ab elienne de dimension strictement positive et de rang de mordellweil z ero. pour t un ensemble quelconque de points de torsion de e a, on note par x son compl ementaire. si la projection de t sur a contient un point k-rationnel, alors x ne v erie pas lapproximation forte avec lobstruction de brauermanin hors de s. en supposant la nitude du groupe de tateshafarevich x, si la projection t sur a ne contient pas de points k-rationnels, alors x v erie lapproximation forte avec lobstruction de brauermanin hors de s. dans , nous rappelons le contexte concern e et pr esentons un enonc e plus pr ecis, dont le th eor` eme est une consequence. dans , nous d etaillons sa d emonstra- tion. enonc e pr ecis du r esultat dans cet article, le corps de base k est toujours un corps de nombre quelconque. pour une place v appartient ` a lensemble des places de k, on note par kv le compl et e de k. on note par lensemble des places archim ediennes. pour v \ , on note par okv lanneau des entiers de kv. on note par a lanneau des ad` eles de k, et par as lanneau des s-ad` eles si s est un sous-ensemble ni de places. soit x une k-vari et e (sch ema s epar e de type ni, g eom etriquement int` egre sur k) lisse. laccouplement de brauermanin est d eni par x br q/z v, b x v invv), o` u invv : br q/z d esigne linvariant local en v provenant de la th eorie des corps de classes locaux. pour un sous-ensemble b brx, le sous-ensemble xb des familles de points locaux qui sont orthogonales ` a tous les el ements appartenant ` a b est ferm e dans lespace des points ad eliques x. il contient lensemble des points rationnels x dapr` es la th eorie des corps de classes globaux, et il contient ainsi son adh erence x. rappelons que x v erie lapproximation forte avec lobstruction de brauermanin par rapport ` a b hors de s si x x est dense dans prsb) x, o` u prs : x x est la projection en oubliant les composantes des places dans s. si b = br, on dit simplement que x v erie lapproximation forte avec lobstruction de brauermanin hors de s. dans lintroduction, nous avons enonc e le cas particulier o` u a et e sont des vari et es ab eliennes du corollaire de . en eet, ce dernier corollaire etait enonc e et d emontr e dans pour les vari et es semi-ab eliennes. dans le reste de cet article, nous d emontrons le th eor` eme suivant qui g en eralise au sens que le ferm e f nest pas forc ement de dimension et que f ne contient m eme pas n ecessairement un point k-rationnel. th eor` eme . soit k un corps de nombres. soit s un ensemble ni de places de k contenant les places archim ediennes. soient a et e des vari et es semi- ab eliennes d enies sur k. supposons que a est de dimension strictement positive et que a est discret dans a. supposons que e est de dimension et que e nest pas discret dans e. soit f e a un ferm e de codimension supposons que la projection de f sur a contient au moins un point k-rationnel p tel que la bre f a p, consid er ee comme un ferm e de zariski de e, ne consiste quen points de torsion (pas forc ement k-rationnels) de e. alors x = \ f ne v erie pas lapproximation forte avec lobstruction de brauermanin hors de s. lorsque e est une courbe elliptique, sur laquelle lhypoth` ese est equivalente ` a la condition que son rang de mordellweil est non nul. la premi` ere conclusion du th eor` eme en d ecoule directement. expliquons la seconde conclusion comme suit. remarque . en supposant la nitude des groupes de tateshafarevich x du quotient ab elien aab de a et x quand e est une courbe elliptique, lhypo- th` ese partielle du th eor` eme que la projection de f sur a contient au moins un point k-rationnel est n ecessaire. si la projection de f ne contient aucun point k- rationnel, alors x v erie lapproximation forte avec lobstruction de brauermanin hors de s. en eet, il sut de consid erer lhypoth` ese la plus faible et la conclusion la plus forte, disons s = . ceci r esulte dun argument simple de bration : si v est orthogonale ` a br, sa projection sur a est alors orthogonale ` a br, cela en- tra ne que cette projection (en oubliant les composantes archim ediennes) provient dun point k-rationnel p dapr` es un r esultat de d. harari [har, th eor` eme ] car a est suppos e discret dans a. comme la projection de f ne contient aucun k-point, la bre x ap est identique ` a e. elle contient donc vbr quitte ` a modier les composantes archim ediennes si n ecessaire. la famille de points locaux v\peut etre approxim ee par un point global dapr` es [har, th eor` eme ]. yongqi liang d emonstrations du enonc e on d emontre le th eor` eme dans cette section. tout dabord, on compare certains groupes de brauer dans le lemme . le th eor` eme est une cons equence directe des lemmes , et de la proposition . ` a la n, on d emontre la proposition . lemme . soient a, e, x, f, et p comme dans le th eor` eme . le point p a induit une immersion ferm ee ip : x a p x. si on identie x a p avec louvert e \ de e, alors im[i p : br br] = im. d emonstration. consid erons le diagramme commutatif suivant br /  br  br i p / br, o` u les ` eches verticales sont induites par les immersions ouvertes et o` u la ` eche horizontale en haut est induite par la section associ ee au point rationnel p a. comme f = \ x est de codimension , la ` eche ` a gauche est un isomorphisme dapr` es le th eor` eme de puret e pour les groupes de brauer. l egalit e voulue r esulte alors de la surjectivit e de la ` eche en haut. lemme . soit k un corps de nombres. soit s un ensemble ni de places de k. soit f : x a un morphisme entre de vari et es alg ebriques d enies sur un corps de nombres k. supposons que a a est discret. soit p a un point rationnel de a tel que la bre xp = x a p = . si x v erie lapproximation forte avec obstruction de brauermanin hors de s, alors xp v erie lapproximation forte avec obstruction de brauermanin par rapport ` a i p ) hors de s. d emonstration. cest un argument standard de bration. il existe un ouvert u de a tel que u a = {p}. soit vp xp un ouvert tel que [vp y vs xp ] xp i p ) = . louvert vp est alors la restriction dun ouvert v f x ` a la bre xp . dapr` es la fonctorialit e de laccouplement de brauermanin, [v y vs x] xbr = . comme x v erie lapproximation forte avec obstruction de brauermanin hors de s, lensemble des points rationnels x intersecte avec v . si q x v , alors f = p et q xp vp . donc xp v erie lapproximation forte par rapport ` a i p ) hors de s. le th eor` eme r esulte directement des lemmes pr ec edents et de la proposition suivante. proposition . soit k un corps de nombres. soit s un ensemble ni de places de k contenant les places archim ediennes. soit e une vari et e semi-ab elienne de dimension d enie sur un corps de nombres k. supposons que e nest pas discret dans e. soit m = {m, m, . . . , ms} e un sous-ensemble ni non- vide de points de torsion de e. si on note par e le compl ementaire de m dans e, alors e ne v erie pas lapproximation forte avec lobstruction de brauermanin par rapport ` a br hors de s. d emonstration. la preuve g en eralise largument de harari et voloch sur la courbe elliptique y = x + d enie sur q. une partie de largument suivant a et e etablie dans pour traiter le cas o` u m consiste en un seul point rationnel et pour l enonc e sur un corps de nombres k quelconque. la nouveaut e ici est que m peut contient plusieurs de points ferm es non n ecessairement rationnels. dabord, on d emontre la proposition pour le cas o` u e est une courbe elliptique. lorsque e est un tore de dimension , la preuve est essentiellement la m eme. on explique ` a la n les modications n ecessaires pour le cas de tores. ` a partir de maintenant, supposons que e est une courbe elliptique. soit e son mod` ele de n eron sur lanneau des entiers ok. pour tout j n, j s, soit mj ladh erence de zariski de mj dans e. la normalisation de mj est spec, o` u okj est lanneau des entiers du corps r esiduel kj de mj. soit e le compl ementaire dans e de la r eunion de mj. soit n n tel que nmj = o e pour tout j. dapr` es lhypoth` ese, e est de rang de mordellweil strictement positif. fixons un point rationnel q e dordre inni. il d enit une section de e not ee encore par q e. les sous-ensembles suivants de sont alors nis. t = {v \ : q intersecte avec mj au-dessus de v pour un certain j} t = {v \ : spec spec est rami e au-dessus de v pour un certain j} t = {v \ : spec mj admet une bre non triviale au-dessus de v pour un certain j} t = t t t s dapr` es le th eor` eme de seigel, lensemble des points t -entiers e = {p = q, p, , pt} est ni. choisissons une place non-dyadique v \ t telle que q pi mod v pour i t et telle que e admet une bonne r eduction en v soit q la caract eristique du corps r esiduel fv de ok en v on trouve soit pgcd| + , q) = , soit pgcd| , q) = d enissons a = ( n|e| + , si pgcd| + , q) = ; n|e| + , si pgcd| , q) = alors a est toujours premier avec qn|e|. dapr` es le th eor` eme de la progres- sion arithm etique de dirichlet, soit lensemble inni des nombres premiers l qui v erient l a mod qn|e|. donc n|e| divise l et la valuation valq = valq|) est une constante pour tout l . yongqi liang pour v , d esignons ) le groupe des composantes connexes du groupe de lie e. consid erons la suite l plong ee dans lespace compact y v ) y v / e. il existe alors une sous-suite convergente vers un el ement not e par vqui est ainsi orthogonal ` a br dapr` es la continuit e de laccouplement de brauermanin. pour toute v , le compos e e e ) est surjectif. quitte ` a modier xv si n ecessaire, on peut supposer que xv e pour toute place archim edienne sans aecter lorthogonalit e avec le groupe de brauer. pour toute v / , d emontrons que xv e en eet, il sut de consid erer le cas o` u il existe une place w de kj au-dessus de v telle que lextension kj,w/kv est triviale, sinon le corps r esiduel de chaque point du support du -cycle mj,kv = mj spec spec contient strictement kv ainsi que xv nest jamais contenu dans mj,kv. soit w une telle place et soit mj,w limage de mj par e e. comme n divise l , on trouve que lmj,w = mj,w dans e, ou bien lmj,w = mj,w dans e o` u mj,w = mj spec spec. puisque q nest pas de torsion q = mj,w e, il existe un entier r strictement positif tel que les r eductions de q e et de mj,w e sont di erentes dans e(okj,w/(r w)) o` u w est une uniformisante de kj,w. si la limite xv de lq est egale ` a mj,w, il existe alors un nombre inni de premiers l tel que lq co ncide avec mj,w = lmj,w dans e(okj,w/(r w)). cela contredit au fait que les r eductions de q et de mj,w sont di erentes dans e(okj,w/(r w)). pour toute v t , d emontrons que xv e, autrement dit la r educ- tion xv de xv modulo v ne se trouve pas dans ss j= mj. dapr` es la choix de t , la r eduction mj spec spec est une sous-sch ema ferm e r eduit de ev = e spec spec. si elle contient xv, il existe alors une place w de kj non- rami ee et de degr e au-dessus de v. la r eduction mj,w ev de mj,w mod w co ncide avec xv. comme n divise l et lordre de mi divise n, on trouve que mj = lmj e, do` u mj,w = l mj,w ev. pour l susamment grand, la r eduction l qv de lq mod v est egale ` a xv = mj,w = l mj,w ev. ceci entra ne que l qv = l mj,w ev pour un nombre inni de premiers l, do` u qv = mj,w ev qui contredit le fait que q et mj nintersectent pas en dehors de t . par cons equent xv e pour toute v / t . nous concluons que v[( y vt e) ( y v / t e)]br. an de compl eter la preuve, il reste ` a d emontrer que pour tout s la troncation vs prs [( y vt e) ( y v / t e)]br ! ne se trouve pas dans ladh erence de e e comme e [( y vt \s e) ( y v / t e)] = e = {p = q, p, , pt} est ni, e est discret dans e il sut de d emontrer que v / s nest limage daucun des points pi. observons que lq q mod v car |e| divise l pour i t, on trouve que q pi mod v dapr` es la choix de v, et ainsi que xv pi mod v donc v / s nest pas limage de pi pour i t. par labsurde, supposons que v / s = p = q. alors une sous-suite de l converge vers q dans e, disons q o pour les premiers l apparus comme indices de la sous-suite convergente. de lautre c ot e, e contient un sous- groupe dindice nie qui est isomorphe ` a comme groupes topologiques dapr` es . comme q nest pas un point de torsion, si on note cette derni` ere indice par n, l el ement q = nq e est alors non nul. en plus, q se trouve dans le sous-groupe qui peut etre identi e avec . la convergence q = nq o implique que l dans okv qui contredit le fait que la valuation valq = valq|) est une constante pour tout l . enn, expliquons ladaptation n ecessaire lorsque e est un tore de dimension au lieu dune courbe elliptique. dans la preuve, nous avons besoin dun ok-mod` ele e qui est un sch ema en groupe de type ni et lisse sur ok, pour un tore nous prenons la composante connexe de lidentit e du lft-mod` ele de n eron, cf. [blr, theorem ]. dapr` es le th eor` eme de dirichlet g en eralis e , le groupe des unit es e est un groupe ab elien de type ni. il est de rang strictement positif car e e nest pas discret. nous xons q e un point dordre inni. un point de torsion mj m s etend en une section mj e. tout reste de la preuve fonctionne dans ce contexte. r ef erences open problem session of the amer. inst. math. workshop : rational and integral points on higher-dimensional varieties. disponible sur http ://ai- math.org/pastworkshops/ratlhigherdimvarproblems.pdf. s. bosch, w. l utkebohmert, and m raynaud. n eron models, volume of ergebnisse der math. springer-verlag, y. cao, y. liang, and f. xu. arithmetic purity of strong approximation for homogeneous spaces. disponible sur arxiv : . d. harari. le d efaut dapproximation forte pour les groupes alg ebriques commutatifs. algebra number theory, :, b. hassett and y. tschinkel. density of integral points on algebraic va- rieties. in rational points on algebraic varieties, number in progr. math., pages birkh auser, d. harari and j. f. voloch. the brauermanin obstruction for integral points on curves. math. proc. cambridge philos. soc., :, a. kresch and y. tschinkel. integral points on punctured abelian surfaces. in algorithmic number theory, number in lecture notes in comput. sci., pages springer, a. mattuck. abelian varieties over p-adic groud elds. annals of math., :, b. poonen. insuciency of the brauer-manin obstruction applied to etale covers. ann. of math., :, v. platonov and a. rapinchuk. algebraic groups and number theory. academic press, yongqi liang o. wittenberg. rational points and zero-cycles on rationally connected varieties over number elds. pr epublication ` a paraitre dans the procee- dings of the ams summer institute in algebraic geometry, disponible sur arxiv : . yongqi liang university of scinece and technology of china, school of mathematical sciences, jinzhai road, hefei, anhui, china e-mail address: yqliang@ustc.edu.cn","The section discusses studying integral points on smooth k-dimensional varieties in the context of number fields. It explores Zariski density of integral points and strong approximation for integral points in the setting of adelic topology. The article focuses on open subsets of certain abelian varieties and discusses potential Zariski density. Results are presented related to strong approximation, Brauer-Manin obstruction, abelian varieties, torsion points, and integral points, with numerical support for specific Jacobi varieties. The theorems address conditions under which certain sets do not satisfy strong approximation with the Brauer-Manin obstruction. The proofs involve various mathematical arguments and considerations for semi-abelian varieties and elliptic curves in the context of adelic topology."
,
,
,
"date of publication , , date of current version , digital object identier /access..doi fpga-based accelerators of deep learning networks for learning and classication: a review ahmad shawahna, sadiq m. sait, , , and aiman el-maleh, department of computer engineering, king fahd university of petroleum & minerals, dhahran-, saudi arabia center for communications and it research, research institute, king fahd university of petroleum & minerals, dhahran-, saudi arabia corresponding author: sadiq m. sait . this work was supported by the king fahd university of petroleum & minerals, dhahran, saudi arabia. abstract due to recent advances in digital technologies, and availability of credible data, an area of articial intelligence, deep learning, has emerged, and has demonstrated its ability and effectiveness in solving complex learning problems not possible before. in particular, convolution neural networks have demonstrated their effectiveness in image detection and recognition applications. however, they require intensive cpu operations and memory bandwidth that make general cpus fail to achieve desired performance levels. consequently, hardware accelerators that use application specic integrated circuits , eld programmable gate arrays , and graphic processing units have been employed to improve the throughput of cnns. more precisely, fpgas have been recently adopted for accelerating the implementation of deep learning networks due to their ability to maximize parallelism as well as due to their energy efciency. in this paper, we review recent existing techniques for accelerating deep learning networks on fpgas. we highlight the key features employed by the various techniques for improving the acceleration performance. in addition, we provide recommendations for enhancing the utilization of fpgas for cnns acceleration. the techniques investigated in this paper represent the recent trends in fpga-based accelerators of deep learning networks. thus, this review is expected to direct the future advances on efcient hardware accelerators and to be useful for deep learning researchers. index terms adaptable architectures, convolutional neural networks , deep learning, dynamic reconguration, energy-efcient architecture, field programmable gate arrays , hardware accelerator, machine learning, neural networks, optimization, parallel computer architec- ture, recongurable computing.","Recent advancements in digital technologies have led to the emergence of deep learning, a branch of artificial intelligence showing promising results in solving complex problems. Convolutional neural networks (CNNs) have proven effective in applications like image detection but require significant computing and memory resources beyond what traditional CPUs can provide. To address this, hardware accelerators, specifically Field-Programmable Gate Arrays (FPGAs), are being utilized to enhance the performance of CNNs by maximizing parallelism and energy efficiency. This paper reviews various techniques for accelerating deep learning networks on FPGAs, highlighting key features and offering recommendations for improving acceleration performance. The study aims to guide future developments in efficient hardware accelerators and benefit researchers in the field of deep learning."
"i n recent years, due to the availability of massive amounts of credible data , and tremendous advances in the area of digital electronics technologies that provide immense computing power, there has been a revival in the area of articial intelligence , particularly in the area of deep learning , a sub- eld of machine learning . the eld of dl emerged in after a long pause in the area of neural networks research . a key aspect in dl is that the networks and/or their weights are not designed by human beings. instead, they are learned from data using a general purpose learning procedure , . while ml uses algorithms to parse and learn from data, to make informed decisions, dl structures algorithms in layers to create an articial neural network that can learn, and similar to human intelligence, can make accurate decisions on its own . therefore, instead of designing algorithms by hand, systems can be built and trained to implement concepts in a way similar to what comes naturally to humans, and with accuracy sometimes exceeding human-level performance , . in dl, each layer is designed to detect features at different levels. a layer transforms the representation at one level (starting from input data which maybe images, text, or sound) to a representation at a higher, slightly more abstract volume , arxiv:v jan shawahna et al.: fpga-based accelerators of deep learning networks for learning and classication: a review level . for example, in image recognition, where input initially comes in the form of pixels, the rst layer detects low level features such as edges and curves. the output of the rst layer becomes input to the second layer which produces higher level features, for example semi-circles, and squares . the next layer assembles the output of the previous layer to parts of familiar objects, and a subsequent layer detects the objects. as we go through more layers, the network yields an activation map that represents more and more complex features. the deeper you go into the network, the lters begin to be more responsive to a larger region of the pixel space. higher level layers amplify aspects of the received inputs that are important for discrimination and suppress irrelevant variations. a. applications of deep learning networks with the now widely used convolution neural networks , and deep neural networks , , it is now possible to solve problems in domains where knowledge is not easily expressed explicitly and implicit information is stored in the raw data. solutions to multifarious problems in the domain of sciences, business, etc., have been possible that were not conceivable for several years, in spite of best attempts by the ai community. this has been primarily possible due to the excellent abil- ity of deep learning in discovering intricate structures in high-dimensional data. examples include character recog- nition , gesture recognition , speech recognition (e.g., in google now, siri, or click-through prediction on an advertisement) , document processing , natural language processing , , video classi- cation , image classication , face detection and recognition , , robot navigation , real- time multiple object tracking , nancial forecasting , and medical diagnosis systems , to name a few. other recent areas of applications include automated driving (e.g., learning to detect stop signs, trafc lights, pedestrians, etc.), aerospace and defense (e.g., identify ob- jects from satellites and identify safe or unsafe zones), medical research , industrial automation (e.g., to improve worker safety by detecting when people or objects are within an unsafe distance of machines), and electronics (used in automated hearing, speech translation, etc.) , . b. emergence of deep learning networks convolutional neural networks are considered as one of the most inuential innovations in the eld of computer vision . the success of deep learning networks grew to prominence in when krizhevsky et al. uti- lized cnns to win the annual olympics of computer vision, imagenet large-scale vision recognition challenge . using alexnet model, they achieved an astounding improvement as the image classication error dropped from % to %. imagenet is a stan- dard benchmark dataset used to evaluate the performance figure imagenet competition results . of object detection and image classication algorithms. it consists of millions of different images distributed over tens of thousands of object classes. cnns have achieved even better accuracy in classica- tion and various computer vision tasks. the classication accuracy in ilsvrc improved to % , % , and % in the , , and competitions, respectively. fig. shows the accuracy loss for the winners of imagenet competitions before and after the emergence of deep learning algorithms. thereafter, large host companies started using cnns at the core of their services. google, microsoft, facebook, amazon, pinterest, and instagram are currently using neural networks for their photo search, bings image feeds, auto- matic tagging algorithms, product recommendations, home feed personalization, and for their search infrastructure, respectively . however, the classic use-case of cnns is for image and speech processing . a typical cnn is a multi-layered feed-forward ann with a pipeline-like architecture. specically, each layer performs a well-known computation on the outputs of the previous layer to generate the inputs for the next layer. in general, cnns have two types of inputs; the data to be tested or classied , and the weights. images, audio les, and recorded videos are examples of the input data to be classied using cnns. on the other hand, the network weights are the data generated from training the cnn on a dataset containing similar inputs to the one being tested. c. hardware acceleration of deep learning networks to provide more accurate results as well as real-time object recognition, for example in applications such as robots and auto-piloted cars, the size of the convolution neural network needs to be increased by adding more neural network layers . however, evolving more and new type of nn layers results in more complex cnn structures as well as high depth cnn models. thus, billions of operations and millions of parameters, as well as substantial computing re- sources are required to train and evaluate the resultant large- scale cnn , , . such requirements represent volume , shawahna et al.: fpga-based accelerators of deep learning networks for learning and classication: a review a computational challenge for general purpose processors . consequently, hardware accelerators such as appli- cation specic integrated circuit , eld programmable gate array , and graphic processing unit have been employed to improve the throughput of the cnn. in practice, cnns are trained off-line using the back- propagation process . then, the off-line trained cnns are used to perform recognition tasks using the feed-forward process . therefore, the speed of feed-forward process is what matters. gpus are the most widely used hardware accelerators for improving both training and classication processes in cnns . this is due to their high memory bandwidth and throughput as they are highly efcient in oating-point matrix-based operations . however, gpu acceler- ators consume a large amount of power. therefore, their use in cnn-based applications implemented as a cloud service on large servers or in battery operated devices becomes a challenge. furthermore, gpus gain their performance from their ability to process a large image batch in parallel. for some applications like a video stream, input images should be processed frame by frame as the latency of the result of each frame is critical to the applications performance. for some tracking algorithms, the result of one frame affects the process of the next frame . nurvitadhi et al. recently evaluated emerging dnn algorithms on latest generations of gpus and fpgas (i.e., intel arria gx and intel stratix ). the experimental results show that current trends in deep neural networks favor fpga platforms as they offer higher power efciency . fpga and asic hardware accelerators have relatively limited memory, i/o bandwidths, and computing resources compared with gpu-based accelerators. however, they can achieve at least moderate performance with lower power consumption . the throughput of asic design can be improved by customizing memory hierarchy and assigning dedicated resources . however, the development cycle, cost, and exibility are not satisfactory in asic-based acceleration of deep learning networks , . as an alternative, fpga-based accelerators are currently in use to provide high throughput at a reasonable price with low power consumption and recongurability , . the availability of high-level synthesis tools, using c or c++, from fpga vendors lowers the programming hurdle and shortens the development time of fpga-based hardware accelerators . convolutional neural networks have a very useful prop- erty, that is, each feature map neuron shares its weights with all other neurons . the authors in , proved that the highest energy expense results from accessing the off-chip dram memory for data movement rather than computation. in other words, the energy cost of the increased memory accesses and data movement due to the large number of cnn operations often exceeds the energy cost of computation , . thus, cnn accelerators need to carefully consider this to achieve efcient architecture in terms of time and power. in this paper, we review the current status of using fpgas as accelerators for implementing deep learning networks. we highlight the implementation challenges and design directions used to tackle those challenges. we also provide future recommendations to maximize the performance of fpgas as accelerators for deep learning networks and simplify their use. the remainder of the paper is organized as follows. section ii provides background information about cnns, their key operations, and some well-known deep learning networks. in addition, it introduces the basic structure of fp- gas and highlights their features enabling them to acceler- ate computationally intensive applications. it also discusses the implementation challenges of deep learning networks on fpgas and how these challenges can be overcome. section iii reviews existing cnns compression techniques and presents the current status of accelerating deep learning networks using asic-based and fpga-based accelerators. section iv describes the use of metaheuristics in the de- sign and optimization of cnns implementation. section v summarizes existing design approaches for accelerating deep learning networks and provides recommendations for future directions that will simplify the use of fpga-based accelerators and enhance their performance. finally, section vi concludes the paper.","Recent advancements in digital electronics have led to a resurgence in artificial intelligence, particularly in deep learning, a subfield of machine learning. Deep learning involves neural networks learning from data to make informed decisions, surpassing human-level performance in accuracy. Applications of deep learning networks include image and speech recognition, document processing, natural language processing, and more. Convolutional neural networks have been pivotal in computer vision and have been widely adopted by major companies for various tasks. To improve the accuracy and real-time performance of deep learning networks, hardware accelerators like field-programmable gate arrays (FPGAs) are being employed. FPGAs offer higher power efficiency compared to graphics processing units (GPUs) and can achieve moderate performance with low power consumption. The paper reviews the use of FPGAs as accelerators for deep learning networks, discussing implementation challenges, design strategies, and future recommendations for maximizing performance and simplifying their use."
"this section gives an overview of the key operations and terminology used in convolutional neural networks and provides examples of well-known deep learning net- works. in addition, it illustrates the basic structure of eld programmable gate arrays and how deep learning methods can benet from the capabilities of fpgas. the last subsection highlights the challenges of implementing deep learning networks on fpgas. a. convolutional neural networks in this subsection, we describe the key operations and terminology involved in the construction of cnns including convolution, activation functions, normalization, pooling, and characteristics of fully connected layers. ) convolution a convolution operation can be thought of as the production of a matrix smaller in size than the original image matrix, representing pixels, by sliding a small window (called lter, feature identier, or kernel) of size k k over the image ), to produce an output feature neuron value . the lter is an array of numbers called weights or parameters. these weights are computed during the training phase. as the lter slides over the feature map, it multiplies the values in the lter with the original pixel values, that is, it rst performs element-wise multiplication, and then sums the products, to produce a volume , shawahna et al.: fpga-based accelerators of deep learning networks for learning and classication: a review single number. the inputs and outputs of the conv layer are a series of fm arrays. this operation, starting from the top left corner of the fm, is repeated by moving the window s strides at a time, rst in the right direction, until the end of the fm is reached, and then proceeding downwards until the fm is completely scanned and all the elements of the fm are covered. the sliding of the lter window and performing the operation is known by the verb convolving, hence the noun convolution , . normally, the size of the kernel is very small, less than or equals to each output- input fm pair has a set of weights equal to the kernel size and each output fm is computed based on the sum of the convolution operations performed on all input fms. note that different conv layers in the same cnn model vary considerably in their sizes. in summary, the convolution operation comprises four levels of loops; the output fms loop , the loop across the input fms , the loop along the di- mensions of a single input fm , and the kernel window size loop (multiply-and-accumulate operation, loop-). conv layers are dominant in cnn algorithms since they often constitute more than % of the total cnn operations , , , , , . therefore, many attempts have been made to speedup conv operations using loop unrolling technique , , as will be discussed later. loop unrolling maximizes the parallelism of conv macs computation which requires a special consideration of processing elements and reg- ister arrays architecture. fig. illustrates the loop unrolling of conv loops levels. ) activation functions activation function in neural networks is similar to action potential in animal cells such as neurons. a neuron is said to re if it emits an action potential. a popularly used activation function is the sigmoid function which can be expressed as fpxq {p ` exq where x represents the weighted sum of the neuron inputs and if it is a sufciently large positive number, the sigmoid function approximates to unity. for sufciently large nega- tive values of x, the sigmoid function is close to another popular activation function is fpxq tanhpxq the above standard sigmoid and tanh non-linear functions require long training time . a recently proposed and commonly used af in cnns is rectied linear unit which is dened as fpxq maxpx, q relu activation function is known to converge faster in training, and has lesser computational complexity , figure conv loops unrolling : unrolling loop-; unrolling loop-; unrolling loop-; unrolling loop-, where, p kx, p ky, p ix, p iy, p if, and p of are loop unrolling design variables for the kernel window width, kernel window height, input fm width, input fm height, number of input fms, and the number of output fms, respectively. than standard sigmoid and tanh functions. in addition, it does not require input normalization to prevent it from saturating , , . ) normalization in real life, a phenomenon called lateral inhibition appears, which refers to the capacity of an excited neuron to subdue its neighbors, thereby creating a contrast in that area. in cnns, to accomplish this, local response normalization or simply normalization is used, particularly when dealing with relu neurons, because they have unbounded activation that needs normalization. it detects high frequency features with a large response. if we normalize around the local neighborhood of the excited neuron, it becomes even more sensitive as compared to its neighbors. at the same time, it will dampen the responses that are uniformly large in any given local neighborhood. if all the values are large, then normalizing those values will diminish all of them. so, basically it performs some kind of inhibition and boosts the neurons with relatively larger activations. normalization can be done within the same feature or across neighboring features by a factor that depends on the neighboring neurons. expressions to compute the response normalized activity can be found in , . ) pooling pooling, also known as subsampling, is employed to pro- gressively reduce the spatial size of the representation, thereby reducing the amount of parameters and computation in the network. pooling layers are periodically inserted in between successive convolutional layers. they operate independently on every depth slice of the input and resize it spatially using the max operation. the most common form is a pooling layer with lters of size applied where the max operation would be taking a maximum over samples thereby discarding percent of the activations . in volume , shawahna et al.: fpga-based accelerators of deep learning networks for learning and classication: a review figure alexnet cnn architecture . addition to the popular max pooling, the pooling units in some cnns are also used to perform other functions, such as avg and min operations . ) fully connected layer a common form of a convolutional neural network archi- tecture comprises stacks of a few convolutional and relu layers, followed by layers for pooling, and this pattern is repeated until the image has merged spatially to a small size. this is followed by one or more fully connected layers, also known as inner-product layers, whose neurons have full connections to all activations in the previous layer, hence the name. the last fully connected layer is the classication layer and it holds the output such as the class scores . b. examples of deep learning networks we list in this subsection some of the well-known deep learning networks. alexnet is a convolutional neural network consisting of convolutional layers, interspersed by normalization layers, as well as fully connected layers . each convolutional layer performs the activation function using relu. in addition, pooling layers are employed with the rst, second, and last convolutional layers. the architecture of alexnet cnn is shown in fig. alexnet won the imagenet challenge by classifying input color images to , different output classes. vgg is a convolutional neural network model similar to alexnet in terms of the number of fully connected layers. however, it consists of groups of convolutional layers , . the exact number of conv layers in each group depends on the version of the vgg, visual geometry group, model. table shows the number of conv and fc layers for the most commonly used vgg models. resnets are deep residual networks with ex- tremely irregular and complex structures compared to alexnet and vgg cnn models , , . this is due to having more types of layers, where non-adjacent layers incorporate shortcuts to compute the residual functions, as well as having highly deep structures, that is, between and conv layers. unlike alexnet and vgg models where the layers are connected in sequence, the interconnections in resnet layers are in the form of a directed acyclic graph . resnet- and resnet- are widely used, especially for image classication. resnet-/ structure contains / conv (most of them are followed by batch normalization , scale, and relu layers), / max pooling, / average pooling, / fc, and, / element-wise layers, respectively. c. field programmable gate arrays fpgas are off-the-shelf programmable devices that provide a exible platform for implementing custom hardware func- tionality at a low development cost. they consist mainly of a set of programmable logic cells, called congurable logic blocks , a programmable interconnection network, and a set of programmable input and output cells around the device . in addition, they have a rich set of embedded components such as digital signal processing blocks which are used to perform arithmetic-intensive operations such as multiply-and-accumulate, block rams , look-up tables , ip-ops , clock management unit, high speed i/o links, and others. fig. shows a basic structure of an fpga. fpgas are widely considered as accelerators for computationally-intensive applications as they enable mod- els with highly exible ne-grained parallelism and as- table cnn layers for vgg models. layers vgg- vgg- vgg- conv conv conv conv conv conv fc total volume , shawahna et al.: fpga-based accelerators of deep learning networks for learning and classication: a review figure fpga basic structure . sociative operations such as broadcast and collective re- sponse . in , , fpga computing models used for applications acceleration are presented, including data streaming, associative computing, highly parallel memory access, use of standard hardware structures such as rst in rst out buffers, stacks and priority queues, and functional parallelism. fpgas have the advantage of maximizing performance per watt of power consumption, reducing costs for large scale operations . this makes them an excellent choice as accelerators for battery operated devices and in cloud services on large servers. fpgas have recently been widely used for deep learning acceleration given the exibility in implementing architectures with large degree of parallelism resulting in high execution speeds . the adoption of software-level programming models such as the open computing language standard , in fpga tools made them more attractive to use for deep learning , . in addition, the feed-forward nature of deep learning algorithms makes fpgas offer a clear advantage as they can create customized hardware circuits that are deeply pipelined and inherently multithreaded . fpgas also have the capability of partial dynamic cong- uration, which allows part of the fpga to be recongured while the rest is being used. this could be of potential benet to deep learning methods where the next layer could be recongured while the current layer is being used. d. challenges of fpga-based implementation of deep learning networks implementation of deep learning networks and, in particular, cnns on fpgas has a number of challenges including the requirement of a signicant amount of storage, external memory bandwidth, and computational resources on the order of billions of operations per second . for example, alexnet cnn has over million model parameters which need mb of memory for storing the weights based on -bit oating-point representation as well as requires around billion operations for each input image . this large amount of storage required is not supported by existing commercial fpgas and hence the weights have to be stored on external memory and transferred to the fpga during computation. without careful implementation of deep learning networks and maximizing resource sharing, the implementation may not t on fpgas due to limited logic resources. the problem exacerbates with more complex models such as vgg cnn model which have layers. for example, the vgg- cnn model has million weights and needs over gops . although the current trends in implementing cnns is going toward compressing the entire cnn model with dramatically reducing data bit-width , it is expected that future cnn models will get more complex with larger number of layers as the amount of training data continues to grow and the problems to be solved get more complex. in addition, different layers in cnns have different characteristics which result in different parallelism and memory access requirements. different layers in a cnn network exhibit vastly different amounts of intra-output and inter-output parallelism . intra-output parallelism parallelizes the computation of a single output image since it is the sum of n input-kernel convolutions. however, inter-output parallelism is based on computing multiple output fms in parallel. furthermore, convolutional layers are computational-centric while fully connected layers are memory centric . for example, the number of operations in each group of convolutional layers in vgg- model are on the order of to gops while the number of weights are on the order of to million. however, the number of operations in fully connected layers are in the order of to gops, while the number of weights are on the order of to million. thus, the developed cnn accelerator must be designed carefully to meet the varying requirements of different layers and needs to be exible to maximize the performance for each cnn layer. as technology advances, fpgas continue to grow in size and capabilities. it is crucial to have some mechanisms for addressing the requirements for efcient implementations of deep learning networks. addressing hardware resource limitations requires reuse of computational resources, and storing of partial results in internal memories. data transfer and computational resource usage are signicantly impacted by the ordering of operations and selection of parallelism in the implementation of cnns on fpgas. careful scheduling of operations can result in signicant reduction in external memory access and internal buffer sizes. external memory bandwidth requirements can be also decreased by using reduced precision for representing the weights with minimal impact on solution quality, which also results in a better energy efciency. in addition, the number of external mem- ory accesses can be reduced by utilizing on-chip memory and exploiting data reuse. furthermore, the large number of weights in the fully connected layer can be reduced, based on utilizing singular value decomposition with a small impact on accuracy. in the next section, we will volume , shawahna et al.: fpga-based accelerators of deep learning networks for learning and classication: a review review various design approaches used to cope with those challenges for implementing deep learning networks. iii. acceleration of deep learning networks: current status in this section, we will start by covering convolutional neural networks compression techniques as they have a signicant impact on the implementation complexity of cnns. cnns compression techniques target the min- imization of the number of operations and the memory footprint with minimal impact on accuracy. then, we discuss hardware acceleration techniques for deep learning algorithms and cnns based on both application specic integrated circuit and eld programmable gate array implementations. in general, hardware accelerators focus on designing specic modules and architectures that ensure data reuse, enhance data locality, and accelerate convolutional layer operations based on performing needed operations in parallel. a. cnns compression in this subsection, we review techniques that target the com- pression of cnns which results in signicantly reducing their implementation complexity with minimal impact on accuracy. denton et al. proposed a technique to reduce the memory footprint for the network weights in object recog- nition systems. they used singular value decomposition and lter clustering methods for this purpose. the results for convolutional model of layers in show that the proposed technique speeds up the operations in convolutional layers by a factor of , compared to cpu eigen-based library implementation . in addition, it successfully achieved memory footprint reduction for the fully connected layers while preserving the recognition accuracy within %. in another work, han et al. employed network pruning techniques to reduce the over-tting and complexity of neural network models. their results demonstrated that pruning redundant connections as well as less inuential connections achieved and compres- sion for alexnet and vgg- models, respectively, while achieving zero accuracy loss for both. in a subsequent work, han et al. proposed a deep compression technique for more reduction of the storage requirements of cnns through the enforcement of weights sharing. deep compression basically consists of pruning, trained weights quantization, and huffman coding pipeline stages. the experimental results show that the proposed compression technique successfully reduced the storage requirement of alexnet and vgg- cnn models by and , respectively, without affecting their accuracy. this also improved the power efciency (a.k.a., performance per watt) by to b. asic-based accelerators in this subsection, we present some recent work in the area of hardware-based accelerators . an asic-based hardware accelerator referred to as dian- nao was designed for large-scale convolutional neural networks and deep neural networks. diannao accelerates neural networks by minimizing memory transfers, which opened a new paradigm for hardware accelerators. since the weights are repeatedly used in the computations of con- volution layers, frequent memory access can signicantly degrade the overall performance. therefore, the authors exploited the locality properties of neural network layers to design custom storage structures that take advantages of these properties. in addition, they employed dedicated buffers and tiling techniques to reduce the overall external memory trafc through increasing data locality. chen et al. also observed that using short xed- point representation of feature maps and weights can also signicantly reduce computation resources and memory footprint. they found that the area and power of a - bit multiplier can be reduced by a factor of and , respectively, using -bit multipliers. consequently, diannao has been implemented using nm fabrication technology with -bit xed-point arithmetic units, bits of which are used for the integer part and the remaining for the fractional part. the experimental results demonstrated that diannao has an average performance of gops with power consumption of mw. the results depicted that using -bit arithmetic units instead of -bit ones in- troduced only % accuracy loss on mnist dataset . on the other hand, the scalability and efciency of diannao accelerator are severely limited by the bandwidth constraints of the memory system. in a related research work, chen et al. , pro- posed dadiannao multi-chip supercomputer which offers sufcient memory capacity suitable for on-chip storage of all weights in cnns. this system is mainly important for todays large-scale deployments of sophisticated industry and consumers services. dadiannao uses -bit xed-point numbers in the inference process like diannao, but it is implemented using nm technology. the results show that dadiannao outperforms the performance of a single gpu architecture by up to and reduces the average energy consumption by with only % accuracy error rate on mnist dataset for a -chip system. another member of the diannao family, called pudian- nao , has been designed using tsmc nm process to support multiple techniques and scenarios of machine learning . pudiannao accelerates different ml tech- niques through extracting their critical locality properties and computational primitives with the use of on-chip storage as well as novel functional units. experimental results show that pudiannao is and faster and energy-efcient, respectively, than nvidia km gpu architecture. however, both of dadiannao and pu- volume , shawahna et al.: fpga-based accelerators of deep learning networks for learning and classication: a review figure processing element architecture in; flexflow, d-mapping . diannao architectures have not been optimized to be used for embedded applications. to improve the scalability and energy efciency of di- annao design discussed in , shidiannao accelerator was proposed . shidiannao is designed especially for real-time object recognition applications such as self- driving cars, smartphones, and security using nm cmos technology. the proposed accelerator directly connects with a cmos/ccd sensor in the image processing chip. in addition, all the weights of cnn layers are stored in sram on-chip memory, as the target here is small cnn models. shidiannao is embedded inside the processing chip to elim- inate off-chip dram memory accesses and minimize data movements between the sram holding the cnn model and the individual processing elements from the sensor. shidiannao has a power consumption of mw with a peak performance of gops under ghz working frequency. moreover, shidiannao has speedup and is more energy-efcient than diannao . however, diannao , dadiannao , , pu- diannao , and shidiannao are not implemented using fpga or any other recongurable hardware. there- fore, they cannot be efciently adapted to different appli- cation demands . in addition, asic designs have a long development cycle and lack exibility for handling varying dl network designs. finally, cnn accelerators, which store all weights on-chip such as shidiannao , will not be able to support realistic large-scale cnn models. similar approaches to the diannao family of techniques are presented in with similar limitations. isaac and prime have explored in-memory processing to design an acceleration architecture for neural networks. the proposed isaac architecture has achieved better improve- ments of , , and in throughput, energy, and computational density, respectively, than the state-of-the-art dadiannao architecture. in cnn models, ne-grained parallelism appears at fea- ture map level, in the neuron level, and in the synapse level. lu et al. reviewed current accelerators that exploit the intrinsic parallelism and observed a mismatch between the parallel types supported by the computing engine and the dominant parallel types that appear in cnn workloads. they identied that most of the previous techniques pro- posed solutions that fall into one of the three representative architectures: systolic, d-mapping, and tiling. due to limitations of dataow of each of the above three architectures, most existing accelerators support only one specic parallelism. systolic architectures exploit synapse parallelism , d-mapping architectures exploit neuron parallelism , and tiling architectures exploit feature map parallelism . however, in a practical cnn, the dominant parallel type depends on the number of input fms, the number of output fms, the size of the output fms, and the size of the kernel. with three components that can be either left serial, or parallelized, we get possible combinations. an example of processing style could be sfsnms, meaning, single feature map, single neuron, and multiple synapse. to address the above problem, and support all possi- ble processing styles, lu et al. proposed a exible dataow architecture, called flexflow, with minimal con- trols. flexflow supports all types of data paths in each type of parallelism in different layers efciently. as a rst step, a modication to the processing element micro-architecture, and the interconnections between pes, is proposed. pes are arranged in rows where each row can complete one convolution and serve one output neuron. the adders in each pe row are connected to form the adder tree. fig. illustrates the proposed pe in flexflow and that in d-mapping architecture. by eliminating de- pendency between adjacent pes, the proposed convolutional unit supports the comprehensive mfmnms parallelisms. to cater to different types of parallelisms, they also proposed a hierarchical dataow with high data routability and low control overhead. the entire dataow can be divided into three sub-ows: distribution to local storage in each volume , shawahna et al.: fpga-based accelerators of deep learning networks for learning and classication: a review pe, fetching of data from local storage for operators , and, dataow from neuron and kernel buffers to the distribution layer. they also presented a method to determine parallelization type and degree (i.e., the unrolling parameters) for each conv layer. flexflow architecture was evaluated for computing re- source utilization, performance, power, energy, and area. comparison was made with three typical architectures (i.e., systolic, d-mapping, and tiling) using six practical work- loads, including alexnet and vgg. they also examined the scalability of flexflow in terms of resource utilization, power, and area with growing scales of computing engine. from experimental results, it was found that computing resource utilization of each baseline was over % across all workloads in contrast to other baselines that utilized less than % . in terms of performance, flexflow demonstrated over gops performance with ghz working frequency. it also out- performed others in terms of data reusability and power efciency. c. fpga-based accelerators in this subsection, we will review recent techniques employ- ing fpgas for the acceleration of deep learning networks. for each reviewed technique, we will highlight the key features utilized to maximize performance and throughput in the acceleration process. fpga implementations of cnns appeared in the mid- s when cloutier et al. designed the virtual image processor on altera epf fpga. vip is a single-instruction stream multiple-data streams multiprocessor architecture with a d torus connection topology of processing elements . vip improves the performance through the use of low-accuracy arithmetic to avoid implementing full-edged multipliers. fortunately, recent digital signal processing -oriented fpgas in- clude large numbers of multiply-and-accumulate units which allow for extremely fast and low power cnn implementations. thereafter, fpga implementations of deep learning net- works have mainly focused on accelerating the computa- tional engine through optimizing conv layer operations. several studies in the literature have reported fpga-based implementations of convolution operation. farabet et al. presented an fpga implementation of cnn that uses one dedicated hardware convolver and a soft- processor for data processing and controlling, respectively. the proposed implementation is referred to as convolutional network processor . cnp exploits the parallelism of conv layers to accelerate the computational engine of cnns while fully utilizing the large number of dsps, the mac hardware units on fpga. the proposed architecture consists of virtex sx fpga platform and external mem- ory. the authors designed a dedicated hardware interface with the external memory to allow simultaneous read/write accesses transparently. in addition, they used rst in rst figure d convolution module of kernel . out buffers between the fpga and the external memory chip in both directions to guarantee the steadiness of dataow. the vector arithmetic and logic unit in cnp implements d conv, pooling, and non-linear activation function op- erations of convolutional networks. the implementation of d conv with kernel of size is shown in fig. , where x is the data from input feature map , y is the partial result to be combined with the current result, z is the result to the output fm, wij is the weight value in the convolution kernel, and w is the width of the input image. it can be seen that the proposed convolutional module accomplishes k mac operations simultaneously in each clock cycle. cnp represents fms and weights using -bit xed-point format. the proposed accelerator has been implemented for a face detection system with lenet- architecture . it utilized % and % of the general logic and multipliers, respectively. in addition, cnp consumed less than watts of power. sankaradas et al. proposed a massively parallel coprocessor to accelerate cnns using virtex lxt fpga platform. the proposed accelerator mainly focused on optimizing computation engine by employing the paral- lelism within convolution kernel and fms. the coprocessor can be considered as parallel clusters of vector processing elements where each cluster is designed using d convolvers, adders, sub-samplers, and look-up tables. each vpe consists of multiplier-accumulator and programmable register units to hold kernel weights and fm data. to hold the massive intermediate data of cnns, the authors employed a dedicated off-chip memory ( ddr memory banks) with a large bandwidth on the coprocessor card. moreover, the proposed accelerator uses a low precision data representation feature with memory packing to further improve the memory bandwidth as well as the throughput. -bit and -bit xed-point representations were utilized for kernel weights and fms, respectively. volume , shawahna et al.: fpga-based accelerators of deep learning networks for learning and classication: a review figure maple processing core architecture . the authors examined their architecture on cnn with conv layers and without any fully connected layer for a face recognition application. the results show that the proposed coprocessor is faster than a software implementation on a ghz amd opteron processor with less than watts of power dissipation. however, the proposed accelerator cannot be used to accelerate full cnns as it uses few conv layers without any fc layer. a full cnn model consists of both conv layers and fc layers. thus, an efcient cnn accelerator for real-life applications is needed to consider both. similar approaches to the work of sankardas et al. are presented in , to accelerate support vector machines . maple is a programmable fpga prototype sys- tem presented to accelerate both learning and classication tasks in applications with unstructured large amount of data. the authors analyzed ve workload domains to help in designing maple. these workloads are svm , supervised semantic indexing , k-means , generalized learning vector quantization , and cnns . they found that their computations can be structured as parallel streams of vector or matrix operations. thus, they architected maple as a d grid of vector pro- cessing elements as shown in fig. to efciently perform matrix multiplication, they allocate a private local storage to each pe which is used to store a column, or part of it, from the multiplier matrix. in this way, matrix multiplication is accomplished by streaming the multiplicand matrix rows through the pes where each pe performs a mac operation. the pes are organized in clusters, where each group is served by a separate memory bank of the banked off-chip memories, which create independent streams for processor- memory computation. moreover, maple uses on-chip smart memory blocks to process the large intermediate data on-the-y using in- memory processing. fig. shows the architecture of the smart memory block. to illustrate the idea of on-the-y in-memory processing, lets consider nding the maximum k elements. the lter compares the input data with the figure maple smart memory block . threshold value . if the input value is greater than val, it updates the list by replacing val at address addr with the input value. then, the scanner searches for the new minimum value in the list and updates the threshold val and addr accordingly. it should be mentioned here that the employment of in-memory processing reduced the off-chip memory trafc by , , and for ssi, k-means, and cnn workloads, respectively. maple pro- totype has been implemented on virtex sxt platform running at mhz. the experimental results for face and digit recognition cnns show that maple is % faster than that for ghz nvidia tesla c gpu implementation. chakradhar et al. proposed a dynamically cong- urable cnn architecture on fpga. the proposed system consists of three main components; a coprocessor, a dy- namically congurable cnn processing core, and -bank memory subsystem. the coprocessor is designed such that it automatically congures the software and the hardware elements to fully exploit the parallelism at the workload level. dc-cnn is responsible for executing cnn applications and its architecture is shown in fig. it consists of m computational elements (each with n d figure the architecture of dc-cnn . volume , shawahna et al.: fpga-based accelerators of deep learning networks for learning and classication: a review convolvers as well as sub-sampling and non-linearity pipelined units), m adders , and input/output switches. the internal structure of the switches vector encloses m n selectors which are used to help in exploring the entire design space and to provide the congurability function across different layers of cnn model. to determine the best feasible combination for each layer, the system analyzes the workload using integer factorization techniques because it is considered fast for small numbers , . dynamic programming is also used to quickly prune infeasible combinations. the authors compared the proposed dc-cnn architec- ture, considering d convolvers as well as a mem- ory subsystem of -bit port width, with a ghz nvidias gpu implementation. the results show that dc- cnn achieved , , , , and speedup for face recognition , face detection , mobile robot vision , video surveillance , and automotive safety workloads, respectively. it is worth mentioning that dc-cnn is the rst architecture that achieves a perfor- mance suitable for real-time processing for video streaming as it processes up to frames per second. in addition, dc- cnn is more energy-efcient than the gpu implementation as it consumes watts, while more than watts are consumed by the gpu. on the other hand, the authors modeled their architecture on a cnn with conv layers only without any fc layer which makes it unsuitable for todays other real-life applications. a second-generation of cnp architecture has been proposed in by designing a stream processor sys- tem. the proposed design replaces the dedicated hardware convolver in cnp with multiple parallel vector processing units, named as alus, laid out in a d grid. each alu is composed of four local routers, one global router, and a streaming operator. the local routers are used to stream data to/from the neighbors. streaming data to and from global data line is done through the global router. the streaming operators in the alu are fully pipelined to produce a result per clock cycle as described in with the use of q coding to represent fms and weights. the proposed system also uses a multi-port direct memory access streaming engine to allow individual streams of data to operate seamlessly within processing blocks. the results show that the proposed stream processor system can run small cnns at up to fps while consuming about watts. an improved version of cnp architectures given in , was presented in and referred to as neuflow. particularly, neuflow has replaced the d grid of alus with a d grid of processing tiles . the proposed architecture contains a d grid of pts, a control unit, and a smart dma module, as shown in fig. each pt consists of local operators and a routing multiplexer . the top three pts have been implemented to perform mac operation. thus, they can be used to perform d convolution, simple dot-products, and spatial pooling. general-purpose operations, such as dividing and squaring, figure the architecture of neuflow . have been implemented at the middle three pts. therefore, the middle row of neuflow can be used for normalization. finally, neuflows bottom pts row implements non-linear operations. moreover, each operator employed input and output fifos to stall its pipeline when required. on the other hand, pts mux is used to connect its local operators with the neighboring pts streaming operators and off-chip memory instead of the used local routers and global router discussed in . neuflow uses a dataow compiler, named luaflow, to translate a high-level ow-graph representation of cnns in torch into hdl scripts with different levels of parallelism. in addition, luaflow produces a binary code conguration le and holds it in the embedded control unit. thereafter, the control unit congures the d grid of pts and the dma ports through run-time conguration buses. a smart memory module has been designed to support multiple asynchronous accesses of off-chip memory through its recongurable ports. by targeting the larger xilinx virtex vlxt fpga, neuflow achieved gops at watts for street scene parsing cnn in with the use of bits to represent fms and weights. peemen et al. utilized the exible off-chip memory hierarchy method to design a congurable memory-centric accelerator template for a variety of models of cnns. this accelerator exploits data reuse in complex access patterns to reduce off-chip memory communication, which minimizes the bandwidth requirements. the memory-centric accelera- tor maximizes the efciency of on-chip memories for better data locality using loop transformation (to optimize the tiling parameters) and block ram -based multi- bank on-chip buffers . at the same time, it minimizes the size of fpga on-chip memories to optimize energy and area usage, which are key requirements for embedded platforms. the memory-centric accelerator uses a simd cluster of mac pes with exible reuse buffers to accelerate the conv layer. the acceleration template has been imple- mented on virtex fpgas. in addition, a microblaze pro- volume , shawahna et al.: fpga-based accelerators of deep learning networks for learning and classication: a review cessor has been utilized to congure and communicate with the accelerator via fifo-based fast simplex link . the proposed accelerator has been analyzed for a cnn vision task of size gmac and the results show that the memory-centric accelerator is faster than the standard implementation of similar fpga resources. neural network next is a real-time system- on-chip computing system for deep learning networks on mobile devices. the architecture of nn-x consists of a host processor, a co-processor, and external memory. the co-processor accelerates the learning networks by paral- lelizing their operations throughout arrays of congurable processing elements referred to as collections. each collec- tion contains one convolution engine, one pooling module, and one non-linear operator. the conv engine accelerates the conv operation by fully pipelining the incoming data with the use of cache memories. the collections are able to communicate with one another using the collection route component to achieve cascaded pipelining, which results in reducing accesses to external memory. the data transfer between the collections and the external memory is ac- complished throughout the co-processor full-duplex memory router, which provides independent data streams. the nn- x has been prototyped on xilinx zc which contains zynq xcz, two arm cortex-a processors, and gb ddr eight collections have been employed to achieve large parallelism. the results for face recognition model in show that nn-x is faster than the two embedded arm processors. zhang et al. proposed a rooine-based model to accelerate convolutional neural networks on fpgas. the rooine model is an intuitive visual performance model used to relate the attainable performance to the peak performance that can be provided by the hardware platform and the off-chip memory trafc . the focus in their work is primarily on accelerating the convolutional layers as it consumes more than % of the computational time during the prediction process . in doing so, the authors optimized both the computation operations and the memory access operations in convolutional layers. they considered a cnn application composed of ve convolutional layers that won the imagenet competition in . the proposed accelerator uses polyhedral-based data dependence analy- sis to fully utilize all fpga computational resources through loop unrolling, loop pipelining, and loop tile size enumeration. note that loop unrolling maximizes the par- allel computation of conv mac operations. on the other hand, local memory promotion and loop transformation are used to reduce redundant communication operations and to maximize the data sharing/reuse, respectively. subsequently, the rooine performance model is used to identify the optimal design from all possible solutions in the design space. specically, the authors model all possible legal designs delivered from the polyhedral analysis in rooine to nd the optimal unrolling factor xtm, tny for every convolutional layer, where tm and tn are the tile size figure zhang et al. accelerator architecture. for the output fms and input fms, respectively. however, designing a cnn accelerator with different unrolling factors to each convolutional layer is challenging. therefore, the proposed architecture enumerates all possible valid designs to nd uniform cross-layer unrolling factors. thereafter, the hardware accelerator is implemented based on the cross- layer optimal unrolling factors. the proposed accelerator composed of a computational engine and memory sub-system is depicted in fig. the computation engine is designed as tm duplicated tree- shaped poly structures with tn inputs from the input fms, tn inputs from the weights, and one input from the bias. on the other hand, the memory sub-system is implemented as four sets of on-chip buffers; two sets to store the input fms and weights, each with tn buffer banks, and two buffer sets of tm independent banks for storing the output fms. to overlap data transfer with computation, on-chip buffers are operated in a ping-pong manner. in addition, two independent channels are implemented for load and off- load operations to increase the bandwidth utilization. more- over, microblaze processor is used to send conguration parameters and commands for the accelerator over axilite bus. the cnn accelerator communicates with external data transfer engines through fifo interfaces, where the data transfer engines are used to access ddr dram memory through axi bus. the accelerator is designed using vivado high level synthesis tool and implemented on xilinx vc fpga board clocked at mhz. the experimental results depict that the proposed implementation achieves a peak performance of gflops as well as a speedup over the software implementation on intel xeon cpu e- at ghz with mb cache and threads. in addition to this, the results show that the proposed fpga architecture is more energy-efcient than the software implementation as the total power consumption is only watts. the proposed implementation has some limitations such as designing the accelerator with new cross-layer unrolling factors for different architectures of cnns. fur- volume , shawahna et al.: fpga-based accelerators of deep learning networks for learning and classication: a review figure top-level archeticture of microsoft cnn accelerator . thermore, using the cnn accelerator with uniform unrolling factors might be sub-optimal for some conv layers, which affects the overall performance. in , microsoft research team of catapult project integrated fpga boards into data center applications to successfully achieve speedup for bing ranking (the large-scale search engine) . a year later, ovtcharov et al. at microsoft research utilized catapult hardware infrastructure, a dual-socket xeon server equipped with stratix-v gsmd fpga, to design a specialized hardware for accelerating the forward propagation of deep cnns in a power-constrained data center. the top-level architecture of the proposed cnn accelera- tor is shown in fig. multi-banked input buffer and kernel weight buffer are used to provide an efcient buffering scheme of fms and weights, respectively. to minimize the off-chip memory trafc, a specialized network on-chip has been designed to re-distribute the output fms on the multi-banked input buffer instead of transferring them to the external memory. the d convolution operations (such as the dot-product) and other cnn operations are indepen- dently performed using spatially distributed scalable vectors of pes. the controller engine is responsible for streaming and data delivery of multi-banked input buffer and kernel weight buffer data to each of the pe vectors. in addition, it supports conguring multiple cnn layers at run-time. the results show that the proposed design is able to classify images/sec, while consuming about watts, for alexnet model on imagenet-k dataset , which is better than the published throughput results for the rooine-based fpga accelerator . the authors mentioned that using top-of-the-line fpgas such as arria gx improves the throughput to around images/sec. qiu et al. proposed an fpga design to accelerate cnns for a large-scale image classication challenge on embedded systems. the focus was on accelerating both conv and fc layers, since they are considered as the most computational-centric and the most memory-centric operations in cnns, respectively. the proposed accelerator reduces the resource consumption using specic design of convolver hardware module. in addition, the authors applied singular value decomposition to the weight matrix of fc layer in order to reduce memory footprint at this layer . to further reduce memory footprint and bandwidth requirement of cnn, they proposed a dynamic- precision data quantization ow component. this compo- nent is responsible for nding the optimal fractional length for weights in each layer as well as the optimal fractional length for fms in adjacent layers, while achieving minimal accuracy loss. then, it converts the oating-point numbers representing weights and fms into xed-point numbers. in addition, the authors proposed a data arrangement scheme that maximizes the burst length of each transaction to the external memory to accelerate conv and fc layers, as well as to avoid unnecessary access latency. note that maximizing the dram burst length raises up the effective dram bandwidth , . the proposed architecture consists of a processing system and programmable logic . cnn computations are performed through special design of processing element modules in fpga. the main modules in the processing element are convolver complex, max-pooling, non-linearity, data shift, bias shift, and adder tree, as shown in fig. the convolver complex is designed as a classical line buffer , as shown in fig. , to achieve convolution operations as well as to compute fc layer multiplication of matrix-vector. the pooling layer implemented in the max- pooling module is used to output the maximum value in the input data stream with a window of size the activation function of cnn is applied to the input data stream using the non-linearity module. the adder tree accumulates the partial sums generated from the convolvers. finally, data shift and bias shift modules are responsible for accomplishing dynamic quantization. the proposed embedded fpga platform has been im- plemented using vgg--svd network with -bit xed- figure processing element module of qiu et al. embedded accelerator architecture. volume , shawahna et al.: fpga-based accelerators of deep learning networks for learning and classication: a review figure convolver complex design of qiu et al. embedded accelerator architecture. point numbers on zynq xcz platform. the results demonstrate that applying svd technique reduces memory footprint of fc layer by % with a compression rate of % while introducing an accuracy loss of only %. finally, the overall performance of the proposed acceler- ator reported is gops under mhz working frequency with the top- accuracy of % and a total power consumption of watts. deepburning is an fpga-based neural network design automation tool. it allows for building learning accelerators for specic nn with optimized performance and custom design parameters conguration using a pre- constructed register transfer level module library. the rtl library holds the hardware descriptive scripts for nn recongurable components as well as their conguration scripts. in addition, it contains other rtl building blocks for logical and arithmetic operations such as the connection box (used to exchange data between nn layers as well as to approximate the division operation) and approximate look- up table (used to simplify a function or operation to allow it to be mapped into hardware). in order to design an optimized hardware, deepburning compresses the passed nn model to the greatest extent using temporal and spatial folding which helps also in satisfying the resource constraints and minimizing the re- quired hardware modules. deepburning not only generates the hardware description for neural network scripts, but also analyzes the complex access pattern and data local- ity using an integrated compiler to generate a run-time control ow which provides energy-efcient, and, better data reuse implementation. in addition, the deepburning compiler investigates the accelerator on-chip memory size and throughput to properly tile and partition the nn weights and feature data layouts. moreover, deepburning uses the address ow component to automatically fetch and store off-chip memory and on-chip memory data. the authors compared the performance of deepburning with that in , considering alexnet cnn model, as they both operate at mhz. they considered a high budget resources constrained deepburning on zynq- device. the results show that deepburning is slower but more energy-efcient. an opencl-based optimization framework to accelerate large-scale convolutional neural network models was pro- posed by suda et al. . they found that the number of performed conv mac operations in parallel , simd vectorization factor , normalization layer loop unrolling factor , the number of parallel pooling outputs in one cycle , and the number of parallel fc mac operations are the key variables that determine the parallelism of the design. subsequently, they analytically and empirically modeled the execution time for each layer as a function of the above mentioned variables. then, genetic algorithm was used to explore the design space for nding the optimal combination of the key design variables considering the resources constraints. the authors implemented the scalable conv block in a similar fashion to that in as a matrix multipli- cation by attening and on-the-y rearrangement of the feature data. the opencl software has been utilized in their work due to its parallel programming model as well as its ability to integrate the compiled rtl design with external memory interfacing ips , which uses memory coalescing technique with complex load and store units. in addition, it has optimized matrix multiplication and cpu- fpga communication libraries , . the framework is used on both vgg- and alexnet cnn models which are implemented on p-d and de-net fpga boards with xed-point opera- tions according to their precision study. they compared the proposed implementation with ghz core i- cpu implementation that uses caffe tool with atlas optimized library for matrix/vector operations. the results show that the opencl optimized framework on p- d achieved and speedups for vgg- and alexnet models, respectively. on the other hand, de-net fpga achieved less throughput speedup than the p-d for vgg-, and for alexnet) as it has less dsps than what is available on p-d zhang et al. , analyzed the transformation of conv and fc layers to regular matrix multiplication presented in prior work . for vgg- model, they found that such transformation necessitates up to duplication of input fms. to address this problem and improve the bandwidth utilization, they designed a uniformed matrix multiplication kernel that uses either input-major mapping or weight-major mapping techniques while computing fc layer. in imm, the designed kernel batches a group of different input fms together, and then performs the matrix multiplication. imm technique improves the data reuse of fc weights. on the other hand, the designed kernel with wmm technique makes use of the fact that the fc layer is communication-bound in which the weight matrix volume , shawahna et al.: fpga-based accelerators of deep learning networks for learning and classication: a review is much larger than the input fm matrix. in particular, it loads input fm matrix to a weight buffer and loads weight matrix to input fm buffer. subsequently, a regular matrix multiplication is performed on these matrices. as a result, wmm may allow for a higher data reuse than imm, especially for input fms that can be reused multiple times considering the limited hardware resources. for the above, the rooine model was applied to identify the optimal mapping technique under different batch sizes and data precisions. the results demonstrate that wmm is better than imm in term of data reuse and bandwidth uti- lization, especially in small batch sizes which is required for real-time inference. hence, the same matrix multiplication kernel is utilized for the computation of both conv and fc layers, but with the use of imm in conv layer and wmm in fc layer. based on this, the authors proposed a software/hardware co-design library, which they named caffeine, to accelerate cnns on fpgas. with an easy-to-use developed tool, caffeine aids in automatically choosing the best hardware parameters, using the model les from caffe and fpga device specications obtained from the user. caffeine fpga engine uses a high- level synthesis -based systolic-like architecture to implement matrix multiplication kernel. it allows changing parameters such as number of pes, precision, and fm size. caffeine further maximizes the fpga computing capability by optimizing multi-level data parallelism discussed in and pipeline parallelism using polyhedral-based optimiza- tion framework given in . caffeine framework also handles the weights and biases reorganization in off-chip dram to maximize the underlying memory bandwidth utilization. in addition, the double-buffering technique is employed to prefetch the next data tile for each pe. caffeine has been evaluated by implementing alexnet and vgg- cnns on ultrascale ku and on virtex t considering different precisions. the vgg- implementation with - bit xed-point on ultrascale ku and virtex t provided and overall throughput enhancement, respectively, compared to implementation on a two-socket server, each with a -core intel cpu . a special case of dataow, referred to as synchronous dataow , is a paradigm of computation that allows for representing a computing system as a stream- ing problem. in this way, sdf model can represent the hardware implementation of cnns using linear algebra and directed sdf graph . each node of sdfg represents a hardware building block that can immediately start its computation as soon as the data are available through its input arcs. such representation of cnn model offers a fast design space exploration. venieris and bouganis employed sdf model to optimize the mapping of cnns onto fpgas based on hls. in particular, the proposed fpgaconvnet framework in takes as input a high-level script programmed by dl expert describing the cnn model, along with specications figure sdf graph partitioning . of the targeted fpga platform. thereafter, it parses the input script through a developed domain-specic language processor to model the cnn in the form of a directed acyclic graph where each node corresponds to a cnn layer. then, the dag-based cnn is transformed into an sdfg representation and modeled as a topology matrix. the topology matrix contains the number of incoming parallel streams, the width of each data stream, and the production or consumption rates at each node. in addition, the dsl processor extracts information about the platform- specic resource constraints. unlike other attempts, instead of exploring the design space for the optimal parameters of loop unrolling and tiling, fpgaconvnet explores the design space of the topology matrix components while considering the resource con- straints. in doing so, fpgaconvnet performs graph parti- tioning, coarse-grained folding, and ne-grained folding. the graph partitioning splits the original sdfg into sub- graphs and each subgraph is then mapped to a distinct bitstream as shown in fig. note that the proposed multi- bitstream architecture might have multiple conv layer processors , as in the provided example. this away, on-chip ram is used for intermediate results and data reuse within the subgraph, while accesss of off-chip memory is minimized and limited for input and output streams of the subgraph. however, this scheme adds reconguration penalty due to the need for reconguring the fpga when the data ows between adjacent subgraphs. to amortize this overhead, several input data streams are processed in a pipelined manner. thereafter, each bitstream architecture is optimized using coarse-grained folding and ne-grained folding. in coarse- grain folding, conv, pooling, non-linear, and other major operations of each layer are unrolled to provide the highest possible throughput by having several parallel units of each operation. the ne-grain folding controls the unrolling and pipelining of the dot-product operations inside conv and average pooling units. instead of fully unrolling the imple- mentation of dot-product which produces a dot-product per cycle, with the use of a high number of multipliers and adders, fpgaconvnet uses a smaller number of mac units and schedules the execution of different operations using time-multiplexing. a trade-off between the performance and the required hardware resources can be achieved by changing the unroll factor and the degree of multiplex- volume , shawahna et al.: fpga-based accelerators of deep learning networks for learning and classication: a review ing. therefore, fpgaconvnet employed simulated annealing to nd the optimal partitioning points and folding factors. finally, fpgaconvnet uses optimal components to derive the conguration of pes and buffers, and generates a synthesizable vivado hls hardware design. fpgaconvnet framework has been evaluated by mapping lenet- and scene labelling small cnn models with q xed-point representation onto a zynq- xcz fpga platform working at mhz. in mapping lenet-, fpgaconvnet achieves up to the performance density of cnp . compared to tegra k gpu implementation of scene labelling cnn, fpgaconvnet surpasses tegra ks power efciency by . ma et al. proposed a python-based modularized rtl compiler to accelerate cnns by employing loop unrolling optimization , for conv layer operations. a de- tailed review article of this work has been recently published and referred to as alamo . the proposed compiler integrates both the rtl ner level optimization and the exibility of hls to generate efcient verilog parameterized rtl scripts for asic or fpga platform under the available number of parallel computing resources (i.e., the number of multipliers ). if nm is greater than the number of input fms , the proposed compiler fully unrolls loop- while it partially unrolls loop- to exploit the data reuse of shared features among nm{nif output fms. otherwise, it partially unrolls loop- which results in nif{nm repeated sliding of kernel window. on the other hand, loop- is serially computed after loop- to minimize the number of partial sums. the overall modules of the proposed cnn accelerator are shown in fig. the controller is responsible for directing and ensuring in-order computation of cnn modules for each layer. the data routers oversee the selection of data read and data write of two adjacent modules as well as the assignment of buffer outputs to shared or pool multipliers of the multiplier bank. the feature buffers hold the fms using on-chip rams. the weight buffers are used to ensure the availability of conv and fc layers weights before their computation as well as to overlap the transfer of fc layer weights with its computation. the conv module consists of control logic, groups of adder trees, and relu components. the control logic component parametrizes the loop unrolling factors based on the conguration of each layer . the conv module contains nm{nif adders to sum nif parallel multiplier results and accumulate them. moreover, the adder trees can be shared by layers with identical nif to be as one single module. the relu component checks the input pixel sign bit to either output zero or the data pixel itself. the pool module contains accumulators or comparators to perform average or maximum operation, respectively. the norm module maintains the required components to perform the operations of local response normalization such as square, non-linear , and multiplication oper- figure alamo overall acceleration modules . ations. finally, the fc module shares the multiplier bank module with the conv module to perform the matrix- vector multiplication . alamo architecture permits the output pixels to be only stored in the feature buffers, which makes alamo suitable for cnns with only small intermediate data volumes. the proposed rtl compiler has been tested by accelerating two cnn models; alexnet and nin . the generated parameterized rtl scripts for alexnet and nin are synthe- sized using altera quartus synthesis tool and implemented on de-net fpga board. the experimental results for alexnet model are compared with the results for opencl- based design as both use the same fpga board with similar hardware resources for alexnet. alamo achieved and improvement for throughput and power consumption, respectively. moreover, the overall throughput of nin model is better than that of alexnet. this is because nin has more conv layers and many of them have the same nif. liu et al. proposed a parallel framework for fpga- based cnn accelerators that exploits four levels of par- allelism; task level, layer level, loop level, and operator level. task-level parallelism involves executing multiple im- age prediction tasks simultaneously. layer-level parallelism exploits pipelining across layers to enable parallel execution of all layers with different images. loop-level parallelism utilizes loop unrolling in performing convolutions and this can be achieved either through intra-output or inter-output parallelism. finally, operator-level parallelism is achieved by parallelising the k k macs operations needed for convolution operation in convolutional layers or the n macs needed for inner-product computation in fully connected layers. fig. shows the parallel framework exploiting these four levels of parallelism. the authors have used -bit xed-point format for rep- resenting pixels in input feature maps and output feature maps. however, they have used bits for intermediate results which get truncated to bits. in addition, they have used bits for representing kernels and weights. they have presented a systematic methodology for design space volume , shawahna et al.: fpga-based accelerators of deep learning networks for learning and classication: a review figure parallel framework exploiting four levels of parallelism . exploration to nd the optimal solution that maximizes the throughput of an fpga-based accelerator under given fpga constraints such as on-chip memory, computational resources, external memory bandwidth, and clock frequency. the proposed technique has been evaluated by imple- menting three cnn accelerators on the vc board for lenet, alexnet, and vgg-s. it has achieved a throughput of gops, gops, and gops for lenet, alexnet, and vgg-s accelerators, respectively. in addition, the performance has been compared with matconvnet tool running the cnn models on intel core i-k cpu ( ghz) and nvidia gtx- gpu (, cuda cores, gb gddr, gb/s memory bandwidth). compared to the cpu implementations, the accelerators for lenet, alexnet, and vgg-s achieved , , and in performance, respectively, and , , and in power efciency, respectively. compared to the gpu implementations, the accelerators achieved better per- formance in the small-scale network lenet , com- parable performance in the medium-scale network alexnet , and worse performance in the large-scale network vgg-s . however, the accelerators achieved higher power efciency than the gpu implementations in all three networks with for lenet, for alexnet and for vgg-s. fp-dnn is an end-to-end framework that auto- matically generates optimized fpga-based implementations of deep neural networks using an rtl-hls hy- brid library. fp-dnn compiler, programed using c++ and opencl, takes tensorflow symbolic descriptions of dnns, and then performs model inference through the use of model mapper, software generator, and hardware gener- ator modules. the model mapper extracts the topological structure and layers congurations of dnn model from the tensorflow descriptions and generates an execution graph for the target model. the execution graph shows layer-by- layer operations and read/write data transactions. fp-dnn compiler allocates off-chip dram data buffers to store intermediate data, weights, and model parame- ters and congurations. the model mapper maximizes the storage resource reuse through minimizing the number of required physical buffers. specically, it formulates the data reuse problem as a graph coloring problem , and then the left-edge algorithm is applied to generate kernel conguration and kernel schedule. subsequently, the software generator uses the kernel schedule to generate a host c++ program which initializes the model, manages the data buffers, and schedules the kernel execution. on the other hand, the hardware generator uses the kernel conguration and the execution graph to generate the fpga hardware codes by instantiating the corresponding optimized templates from an expandable rtl-hls hybrid library. each template is comprised of verilog-based computational engine and opencl-based control logics engine. the architecture of the proposed fpga-based accelerator consists of matrix multiplication and data arranger modules. matrix multiplication module is a hand-written verilog code that is designed and optimized based on the hardware constraints of altera stratix-v gsmd fpga. it applies tiling and ping-pong double buffers techniques to improve the throughput. on the other hand, data arranger is an opencl-based module that is responsible for mapping the computational part of a layer to matrix multiplication as well as performing data communication with off-chip memory and matrix multiplication module. mapping dnns compu- tational operations to matrix multiplication has been widely applied in prior studies , , . fp-dnn maps fc layer to matrix multiplication by batching input vectors together. before model deployment, fms and weights are rearranged in dram using the channel-major scheme to optimize the communication between the accelerator and off-chip dram. on the other hand, both oating-point and xed-point representations have been supported for implementation, and they can be adjusted by the user. the proposed rtl-hls hybrid framework has been eval- uated by accelerating vgg-, lstm-lm , resnet- dnns on stratix-v gsmd fpga. note that this is the rst work that implements resnet- on fpga. the volume , shawahna et al.: fpga-based accelerators of deep learning networks for learning and classication: a review experimental results demonstrated that the speedup of fp- dnn for -bit xed-point implementations are about - compared with the server that includes processors each with -core intel xeon e-v at ghz. in line with the current trends towards compressed neural networks, with dramatically reduced weights and activations bit-width using -bit or -bit quantization , umuroglu et al. conducted a set of experiments to estimate the trade-off between the network size and preci- sion using the rooine model. they found that binarized neural networks require to times more operations and parameters than an -bit xed-point cnn to achieve a comparable accuracy on mnist dataset. however, the performance of bnn is found to be faster than the xed-point network. subsequently, the authors proposed a framework, referred to as finn , that maps a trained bnn onto fpga. finn generates a synthesizable c++ network description of a exible heterogeneous streaming architecture. the architecture consists of pipelined compute engines that communicate via on-chip data streams. each bnn layer has been implemented using dedicated compute engines with -bit values for weights and fms; + and - are used to represent a set bit and unset bit, respectively. the authors have optimized accumulation, batch normal- ization , activation, and pooling operations of bnns. in particular, the accumulation of a binary dot- product has been implemented as a counter of set bits . the popcount-accumulate reduces the number of required look-up tables and ip-ops by a half, compared to the implementation of signed- accumulation. bnn batchnorm and activation operations have been simplied and implemented together as unsigned comparison with a threshold k, + is produced when the input value is greater than or equals to k, and - otherwise. the value of k is computed during run- time. such an implementation of batchnorm-activation op- erations requires much smaller number of luts, without the need for dsps and ffs, compared to regular imple- mentation of batchnorm-activation. max-pooling, average- polling, and min-pooling have been effectively implemented with boolean or-operator, boolean majority function, and boolean and-operator, respectively. the accelerator architecture is composed of building blocks from the finn hardware library. the matrix-vector- threshold unit is the core computational building block as matrix-vector operations followed by thresholding form the majority of bnn operations. the design of mvtu consists of an input buffer, an array of p parallel pes each with s simd lanes, and an output buffer. bnn weight matrix is distributed across the pes and stored locally in on- chip memory. subsequently, the input images are streamed through the mvtu and multiplied with the weight matrix. particularly, the pe computes the dot-product between an input vector and a row of weight matrix, each of s-bits wide, using an xnor gate, as shown in fig. then, it figure the architecture of mvtu pe . compares the number of set bits to a threshold and produces a -bit output value as previously discussed. umuroglu et al. implemented the conv layer using a sliding window unit and an mvtu, where convo- lutional operation is transformed to matrix-multiplication of image matrix and lter matrix. swu generates the image matrix to mvtu by moving the sliding window over the input fms, while the lter matrix is generated by packing the weights from the convolution lters as shown in fig. in order to meet the user throughput requirement, mvtu is folded by controlling the values of p and s. folding of mvm decides partitioning of the matrix across pes. every row of matrix tile is mapped to a distinct pe and every column of pe buffer is mapped to a distinct simd lane. in this away, the required number of cycles to compute one mvm is obtained as pxy q{pp sq, where x and y are the dimensions of the matrix. the folding factors of bnn layers have been determined such that every bnn layer takes nearly the same number of cycles. to evaluate finn, the authors implemented cnv topol- ogy on xilinx zynq- board at mhz to acceler- ate bnns inference on cifar- . cnv contains figure transforming conv to matrix-multiplication , where, ifm and ofm are the input and output feature maps, respectively. volume , shawahna et al.: fpga-based accelerators of deep learning networks for learning and classication: a review figure conv acceleration architecture and dataow , where, p ix p ox , p iy p oy , and p of three repetitions of two convs and max- pooling layers. its topology is inspired by vgg- and binarynet . although cnv accepts images with - bits/pixel as an input and produces a -element vector of -bit values, -bits are used for representing intermediate results while -bit is used for representing conv and fc weights. experimental results demonstrated that the proposed design provides high performance while incurring low energy consumption . finn outperforms the design by ovtcharov et al. by over for throughput. in , loop optimization techniques , have been employed in fpga to design a customized cnn accelerator through speeding up conv layer operations. firstly, an in- depth analysis is provided to numerically characterize loop unrolling, loop tiling, and loop interchange optimization techniques. in doing so, conv dimensions parameters , loop unrolling design variables , and loop tiling design variables have been used with a con- straint, as for a specic loop level, p t n . note that unrolling loop- and loop- requires pkxpky and pif multipliers, respectively, an adder tree with fan-in of pkx pky and pif, respectively, and an accumulator. on the other hand, unrolling loop- requires pixpiy par- allel units of mac to reuse the same weight for pixpiy times, while the input feature pixel can be reused by pof times when unrolling loop- with the use of pof parallel mac units. thus, pkx pky pif pix piy pof multipliers are required. please refer to fig. for more details on conv loops levels and their parameters. in loop tile optimization, the authors have numerically set the lower bound on the required size of the input pixel buffer, the weight buffer, and output pixel buffer that ensures reading each input feature pixel and weight from the off- chip memory only once. on the other hand, loop interchange technique has a great impact on the times of memory access as well as the number of partial sums since it determines the order of computing conv loops. secondly, the authors have provided a quantitative anal- ysis of the design variables to minimize each of computing latency, partial sum storage, on-chip buffer access, and off- chip dram access. subsequently, matlab scripts are used to randomly sample a subset of the solution space to nd the optimal design congurations. this is due to the large solution space, more than possible congurations for loop tiling variables of width and height output fm alone. according to the randomly sampling results for vgg- cnn model on arria gx fpga, uniform unrolling factors for conv layers are used with pix pox piy poy and pof for loop- and loop-, respectively, to reuse input feature pixels and weights. on the other hand, loop- and loop- are serially computed to prevent the movement of the partial sums between the mac units and consume them asap since both loop- and loop- need to be nished in order to obtain one nal output pixel. more importantly, the order of loops computation has been found to be as follows. loop- is computed rst, then comes loop-, and nally loop- and loop- are computed in any order. finally, a customized convolution accelerator module with efcient dataow has been designed based on the previous results and used for all vgg- conv layers. the conv accelerator consists of , independent mac units and input pixel buffers. fig. shows an example of the designed conv accel- erator when pix, piy, and pof are all equal to the input pixels are shifted after fetching them out of the input pixel buffers. subsequently, they can be reused among the input register arrays. then, the input pixels are fed into the associated mac units. the gure also shows that the input pixels and weights are shared by pof and pixpiy mac units, respectively. the overall cnn acceleration system mainly consists of two sdram banks that hold the input feature pixels and weights, two modular scatter-gather dma engines to facilitate the simultaneous read/write from/to the sdrams, and a controller to govern the sequential computation of layers as well as the iterations of the four volume , shawahna et al.: fpga-based accelerators of deep learning networks for learning and classication: a review conv loops. on the other hand, dual weight buffers have been used to increase the throughput of fc layer through overlapping the inner-product computation with off-chip communication. the acceleration system has been written as parametrized verilog scripts. the experimental results show that the proposed accelerator has a throughput of gops, which is more than enhancement compared to prior vgg- fpga-based implementations , . venieris and bouganis further extended fpga- convnet framework to allow for optimizing either throughput or latency depending on the size of the workload. for large workloads, weights reloading transformation has been introduced to efciently design latency-critical cnns on fpga. in contrast with fpgaconvnet, where a distinct architecture is designed for each subgraph, the weights reloading transformation allows for generating a single exible architecture, named as the reference architecture and derived using pattern matching, to execute the workloads of all subgraphs by transitioning to different modes. upon the execution of a new subgraph, the subgraphs weights are read into the on-chip memory and the multiplexers are congured to form the appropriate datapath. fig. demon- strates how weights reloading is applied. the authors have mentioned that the required time for transferring subgraphs weights is much smaller than the average time for full fpga reconguration, less when loading mb of weights for a vgg- layer on zynq xcz in the situation discussed above, due to limited on-chip memory capacity, it might not be possible to load all weights required for a single conv layer. to handle this, the authors introduced an input fms folding factor with each conv layer. a conv layer is partitioned into fini subgraphs in which each subgraph executes a fraction of conv i to produce a fraction of the output fms. the proposed latency-driven methodology has been evaluated by implementing alexnet and vgg- with - bit xed-point precision for both on zynq xcz at figure weights reloading . figure overall dla architecture . mhz. the experimental results showed and higher conv throughput than deepburning and the embedded fpga accelerator in for alexnet and vgg- implementations, respectively. lavin and gray demonstrated that cnn algorithms with small lters can be efciently derived using winograd algorithm and fast fourier transform algo- rithm due to their advantages in improving resource efciency and reducing arithmetic complexity. winograd computation involves a mix of element-wise and general-purpose matrix multiplication, where some of the matrices need to be transformed. in particular, winograd algorithm exploits the structure similarity among nn tiled input fm pixels given a lter of size rr to generate mm tiled pixels of the output fm, where m represents the stride between winograd tiles , while minimizing the number of required conv multiplications from mr for conventional conv algorithm to n in another work, zhang et al. implemented fft algorithm for cnn on fpga platform. however, their proposed implementation shows little reduction of computation complexity with small lters such as aydonat et al. presented a deep learning architec- ture based on opencl. their proposed architecture reduces the external memory bandwidth requirements by an order-of-magnitude for both the convolutional and fully con- nected layers. this is achieved by caching all intermediate feature maps on-chip in stream buffers. for fully connected layers, image batching is used where a batch of images are processed together through the fully connected layers. the approach utilizes the winograd transformation to reduce the multiply-accumulate operations, which could reduce the number of needed operations by about %. in addition, it uses half-precision oating-point operations with shared exponents, which signicantly reduces the needed computational resources. the overall dla architecture is shown in fig. each pe consists of dot-product units, accumulators, and caches, for performing dot-products for convolution and fully connected layers. caches are used for storing lter weights. to avoid idle computation cycles, double-buffering is used such that lter weights for the next convolution volume , shawahna et al.: fpga-based accelerators of deep learning networks for learning and classication: a review layer are prefetched onto the caches while lter weights are loaded from the caches for a particular convolution layer. stream buffers store feature data and stream it to pes. each stream buffer is double-buffered similar to lter caches. images are loaded from the ddr and are stored in stream buffers before the rst convolution layer starts execution. during a convolution layer execution, while feature data for a convolution layer is being streamed into the pes, the outputs of convolutions are simultaneously stored in the buffers. the streambuffer unit applies the winograd transformations to features, and streams the transformed features to the rst pe which are forwarded through all the pes via the daisy-chained input connections between them. the relu unit receives the outputs of the pes via daisy-chained output connections. then, the normalization unit receives the outputs of the relu unit and applies the normalization formula across the feature maps. the pooling unit receives the outputs of the normalization unit and computes the maximum value in a window. the output of the pooling unit is stored back in the stream buffer for further processing, if more convolution layers are to follow. otherwise, the outputs of the pooling unit are stored in external memory. for the fully connected layers, features data are stored on pes caches while lter weights are stored in stream buffers. for the rst fully connected layer, features data are read back from external memory and loaded onto the pe caches. the relu output is sent directly to ddr, without applying normalization or pooling. the sequencer generates the control signals to control the operation of the various blocks in dla according to the topology of the executed cnn. executing a different cnn requires just changing the sequencer conguration. the dla has been evaluated by implementing alexnet cnn on intels arria dev kit which contains a a- device using a batch size for the fully connected layers. it achieved a performance of images/s. in addition, it achieved x more gflops than the latest ultrascale result reported in , which uses a batch size for the fully connected layers, and more gflops than the latest stratix v result reported in . furthermore, it has achieved energy efciency at images/s/w, which is similar to what is achieved with the best publicly known implementation of alexnet on nvidia titan x gpu. unlike dla architecture where a d winograd algorithm was employed to reduce arithmetic complexity, lu et al. implemented a novel fpga architecture with a two-dimensional winograd algorithm to ac- celerate convolutional computation of cnns. the overall architecture consists of line buffer structure and winograd pe engine, as shown in fig. particularly, n ` m input lines and m output lines of on-chip buffers are used to effectively reuse fm data among different tiles. while winograd pe engine reads the rst n input lines to perform winograd computation, the next m input lines load pixels from off-chip memory using fifos to overlap the data figure winograd-based cnn accelerator , where, m is the size of the input fm tile, n is the size of the output fm tile, m is the number of input channels, n is the number of output channels, w is the maximal width of all input fms, c is the width of the output fms. transfer and computation. thereafter, the input lines are rotated in a circular fashion to make the next n input lines ready. on the other hand, winograd pe engine composed of pipelined stages performs transformation, element- wise matrix multiplication, additional transformation, and accumulation of output tiles, respectively. a vector of pes is employed to achieve parallelism through unrolling loop and loop similar to that in . to implement fc layer, the proposed accelerator uses the input line buffers to hold fc weights while input neurons are stored on the lter buffers. then, winograd pe engine is reused to implement fc operation but with bypassing the transformation stages. moreover, a batch of input fms are assembled and processed together in order to improve the memory bandwidth. an analytical model has been proposed for a fast design space exploration of optimal design parameters (n, pof, pif, nbatch) constrained by fpga conguration with a -bit xed-point representation for both fm data and lter. the proposed accelerator has been evaluated by imple- menting alexnet and vgg- on xilinx zcu fpga. alexnet conv layers have different lters. conventional conv algorithm has been applied to the rst conv layer as it has a lter of size while a uniform lter of size for winograd algorithm has been used to implement the rest of the layers. the design parameters are found to be equal to and for alexnet and vgg-, respectively. the experimental results demon- strated that the proposed winograd-based cnn accelerator has an average performance of gops and gops for alexnet and vgg-, respectively, with power consumption of watts for both. the proposed accelera- tor has also been evaluated on xilinx zc platform where the design parameters are found to be as and for alexnet and vgg-, respectively. the ex- perimental results demonstrated that winograd-based cnn accelerator has an average performance of gops and gops for alexnet and vgg-, respectively, with power consumption of watts for both. compared to the implementation of vgg- on nvidia titan x with volume , shawahna et al.: fpga-based accelerators of deep learning networks for learning and classication: a review figure compute unit with a d bram-to-pe interconnection . the latest cudnn , titan x gives better performance than xilinx zc but the implementation on xilinx zc achieves higher energy efciency. zhang et al. presented an opencl-based architec- ture for accelerating cnns on fpga. they also proposed an analytical performance model to identify the bottleneck in opencl-based acceleration of vgg- ccn model on modern fpga platforms such as altera arria gx based on rooine mode analysis, it is shown that the bandwidth requirement of vgg- workload is higher than what is provided by the fpga board. thus, they identied on-chip memory bandwidth as the key performance bottle- neck. in addition, they observed that exploited data-level parallelism in the existing altera opencl library leads to wasteful replication of on-chip memory . this is due to connecting each pe with a dedicated bram port. therefore, a verilog-based accelerator kernel has been designed and warped to an opencl ip in order to opti- mally balance on-chip memory bandwidth with workload computational throughput and off-chip memory accesses. in particular, the proposed kernel consists of a compute subsystem, a local memory subsystem, and a d dispatcher. the compute subsystem is organized hierarchically into compute units and pes. at pe level, the authors have figure d dispatcher , where, x is the column size of kernel buffer as well as the row size of the input feature buffer, and x is the row size of kernel buffer. figure line buffer design . designed a d multi-cast interconnection between brams and pes to improve the efciency of on-chip bram usage by sharing the data of one bram port with several pes as shown in fig. the cu has been designed as a d pe array of size to match the computational bandwidth with the maximum streaming bandwidth provided by off-chip memory. the d dispatcher divides the work items into work groups each of size as shown in fig. thereafter, it adaptively schedules the work items within each work group to the cus starting with the lowest dimension to balance the memory bandwidth with capacity. the d dispatcher is also responsible for host/device memory data transfers. in addition, the authors have limited the maximum fan-out for registers to in order to guarantee a higher frequency. the conv layer has been implemented as a matrix multiplication by attening and rearranging the data using line buffer , as shown in fig. , in a similar fashion to that in . the line buffer converts continuous address stream from external memory into a stream conducive for conv operation to substantially reduce the bandwidth requirement of off-chip memory. to implement fc layer, the proposed accelerator uses one column of pes in the cu. the proposed implementation has achieved gops and gops with the use of -bit oating-point and - bit xed-point, respectively, under mhz and mhz working frequencies, respectively. all previously discussed fpga-based cnn accelerators, except the ones discussed in , , have employed a single clp to maximize the aggregate throughput of performed consecutive convolutional operations. however, shen et al. noted that using a single globally-optimized clp design for the computation of conv layers of radi- cally different congurations and dimensions leads to sub- optimal performance and insufcient utilization of fpga resources. fig. a demonstrates the use of a single clp to iteratively process l , l, and l conv layers where the dimensions of the hardware and the layers are represented by the size and shape of the boxes. it is clear that computing l and portions of l leaves fpga resources unutilized as their dimensions are smaller volume , shawahna et al.: fpga-based accelerators of deep learning networks for learning and classication: a review figure operation of conv layer processors on cnn with three conv layers . than the dimension of the used clp. note that processing a conv layer with a dimension bigger than the dimension of clp, such as l, requires the repeated use of clp to process different portions of the layer. the authors have also followed the methodology in to derive an optimal single clp, through nding the optimal unrolling factor xtm, tny, for implementing squeezenet and alexnet on virtex t fpga with a single precision oating-point and -bit xed-point arithmetic units, respectively. they found that one quarter of dsp slices of squeezenets clp remain unused. even more worse utilization has been observed for alexnet. the optimal single clp has not utilized, on average, more than one quarter of the arithmetic unit resources. on the other hand, they also noted that using one clp for each stage of conv layer in a fashion similar to that in is not efcient due to three reasons. first, it reduces the on-chip bram buffer size of each clp which minimizes overall data locality. second, such one-to-one mapping of conv layers and clps requires orchestrating many off-chip memory accesses which incurs latency and bandwidth overheads. third, the overall control overhead scales with the number of clps which leaves insufcient resources for the computation of cnn. to address the above inefciencies, shen et al. proposed a multi-clp accelerator system for cnns where the available pfga hardware resources are partitioned across multiple smaller clps. each clp is tailored with a dimension that closely matches the dimensions of a subset of conv layers. thereafter, these specialized clps are used to concurrently operate on a batch of images to achieve a higher overall throughput, as shown in fig. b, where the same hardware in fig. a is partitioned into two parallel clps; clp and clp shen et al. developed an optimization search algo- rithm that uses dynamic programming to nd optimal de- signs. for given congurations of cnn model (i.e., conv layers descriptions) and resource constraints of the targeted fpga platform (i.e., number of dsp slices, bram-kb units, and off-chip memory bandwidth), it derives the opti- mal number of clps as well as the optimal mapping between conv layers and clps that maximize the performance. the assignment of cnn layers to clps is static, where each cnn layer is mapped and bounded to a particular clp. subsequently, cnn layers are pipelined to their clp, as shown in fig. b, where l and l are pipelined to clp while l is re- peatedly processed on clp with very little idle hardware which improves the performance compared to single clp approach. moreover, the optimization algorithm also nds the optimal partition of on-chip bram resources of each clp that minimizes the overall off-chip memory accesses. note that the optimal dimension of each clp is found based on the work in . subsequently, c++ templates are parameterized to design clps and to form a complete implementation of cnn. a standard axi crossbar is used to intercon- nect the independent clps. the ping-pong double-buffering technique is also used for input fms, output fms, and weights to allow for transferring data while computation is in progress. the experimental results of implementing alexnet with a single precision oating-point using multi- clp accelerator on virtex t and t fpgas at mhz demonstrate and higher throughput than the state-of-the-art single clp design in , respectively. for the more recent squeezenet network, the proposed multi-clp accelerator results in speedup of and on virtex t and t fpgas at mhz with -bit xed-point, respectively. wei et al. presented a systolic architecture for automatically implementing a given cnn on fpga based on opencl description, maximizing clock frequency and resource utilization. the proposed systolic architecture is shown in fig. each pe shifts the data of the weights and inputs horizontally and vertically to the neighboring pes in each cycle. the d structure of pes is designed to match the fpga d layout structure to reduce routing complexity and achieve timing constraints. volume , shawahna et al.: fpga-based accelerators of deep learning networks for learning and classication: a review figure systolic array architecture for cnn . the technique rst nds a feasible mapping for the given cnn to the systolic array to guarantee that proper data is available at specic locations in the pe array at every cycle. then, the size of pe array is determined which has an impact on the required number of dsps, the clock frequency, and the dsps efciency. finally, the data reuse strategy is determined by choosing proper tiling sizes. the proposed technique has been evaluated using alexnet and vgg on intels arria gt board. the technique has explored the use of both -bit oating- point and xed-point using -bits for weights and -bits for data. evaluation results show that, for the vgg cnn, the technique achieves up to , gops on intels arria device with a clock frequency of mhz and -bit xed-point representation. in another recent research work, ma et al. general- ized the previously proposed accelerator in to efciently accelerate resnet- and resnet- on arria gx fpga. in doing so, they designed exible and scalable conv, relu, batchnorm, scale, pooling, fc, and eltwise primitives. in addition, local control logic and registers have been used with each primitive to control their computation order and to hold their congurations, respectively. by doing so, resnets primitives can be efciently reused for different parameters of each layer. for resnets scalable conv primitive, there are four size congurations; , , , and . therefore, a similar architecture and dataow to that shown in fig. has been used for conv but with the use of two sets of register arrays; with shifting between the registers (which is shown in fig. , set-), and without shifting between the registers . the conv primitive with kernel and stride of uses set- register array, while set- is used with , , and congurations. in conv primitive with set-, the input pixels are fed from the input pixel buffers into the corresponding registers without shifting, and then to mac units. the skipped input pixels in conguration are not stored to the input pixel buffers. on the other hand, the conguration of the kernel and stride sizes is retained as the case while transferring repeated input pixels into the input pixel buffers and rearranging their storage patterns. the conv primitive also takes care of zero-paddings for different size congurations. the loop unrolling and tiling techniques in have also been employed to accelerate conv primitive with a uniform mapping of pes to all resnets conv layers. however, designing of efcient cnn modules is not enough, as the memory accesses and data movements between these modules must also be minimized. therefore, the authors have designed a layer-by-layer computation ow. the global control logic is responsible for governing the sequential op- erations of primitives and their dataow through predened and preloaded layered-based execution owchart, as shown in fig. in addition, it has been modeled to recongure resnet primitives according to the parameters of each layer during runtime. for instance, it maps a particular number of pes to conv layer based on loop unrolling parameters as well as it controls the selection of register array type (set- or set-) based on conv parameters. on the other hand, a custom dma manager has been designed to control the operations of dma. note that the dma is responsible for transferring the input fm pixels, weights, and output fm pixels between off-chip memory and on-chip buffers. unlike alamo architecture where the output pixels are only stored in on-chip buffers, this work as well as the work discussed in store the output pixels in off-chip memory with the use of loop tiling technique in order to have a exible architecture that can process large-scale cnns. the dual weight buffers technique has not been used in this work due to the current trend in cnns where either the size of fc weights has been signicantly reduced ( m in resnet compared with m in vgg) or the fc layers are completely removed such as in nin. the experimental results demonstrated that the achieved throughput for resnet- and resnet- are gops and gops, respectively. finally, the authors mentioned that higher throughput can be achieved figure execution flowchart of resnets layers . volume , shawahna et al.: fpga-based accelerators of deep learning networks for learning and classication: a review figure dlau accelerator architecture . using batch computing . wang et al. proposed a scalable design on fpga for accelerating deep learning algorithms. in order to provide a scalable architecture and support various deep learning applications, the proposed architecture utilizes the tiling technique in which the large-scale input data is partitioned into small subsets. the size of the tile is congured to leverage the trade-off between the hardware cost and the speedup. moreover, the authors explored hot spots prol- ing to determine the computational parts that need to be accelerated to improve the performance. the experimental results illustrated that matrix multiplication and activation functions are the key operations in deep learning algorithms as they consume about % and % of the overall execution time, respectively. thus, the proposed accelerator is responsible for speeding up both matrix multiplication and activation function computations. the main components of the proposed architecture are the embedded processor, the ddr memory controller, the dma module, and the deep learning acceleration unit , as shown in fig. the embedded processor utilizes the jtag-uart to communicate with the accel- eration unit . the dlau unit accesses the ddr memory to read the tiled input data and to write the results back through the dma module during the execu- tion. the dlau utilizes three fully pipelined processing units to improve the throughput, while minimizing the memory transfer operations. these units are tiled matrix multiplication unit , partial sum accumulation unit , and activation function acceleration unit . tmmu is responsible for multiplication and generating the partial sums. to optimize the performance, tmmu is structured as a pipelined binary adder tree. moreover, it uses two sets of registers alternately to overlap the computation with the communication, one group is used for the computation, while in parallel, the other group is loaded with the next node data every clock cycle. on the other hand, psau is responsible for accumulating the partial sums generated from tmmu. finally, afau implements the sigmoid function using piecewise linear interpolation to speedup the computation with negligible accuracy loss. since the processing units in dlau might have inconsistent throughput rates, each unit has input fifo buffer and output fifo buffer to prevent data loss. the authors implemented the proposed architecture on xilinx zynq zedboard with arm cortex-a processors clocked at mhz. in addition, they used the mnist dataset as a benchmark considering the network size as , , and the experimental results demonstrated that the speedup of the dlau accelerator is up to compared with the intel core processors at network size. in addition, the results depict that the proposed architecture is quite energy-efcient as the total power consumption was only mw. in , a generalized end-to-end acceleration system of the previously proposed accelerators in , , , has been developed to support diverse cnn models. in doing so, a user-friendly interface and an rtl-level compiler have been proposed to automatically generate customized fpga designs. the authors have developed an expandable optimized rtl-based library containing the most commonly used cnn operations. these operations have been coded in verilog and designed based on the quantitative analysis and optimization strategies discussed in . the compiler generates a dag-based structure for the used cnn model and then compiles it with rtl mod- ules in the library. the proposed compiler allows the user to input the high-level information of the used cnn model as well as the design variables (i.e., loop unrolling and loop tiling variables) with the resource constrains of the targeted fpga platform. such utility facilitates the exploration of the best trade-off between the resource usage and the performance. unlike the architecture in where individual conv module is assigned to each conv layer, the scalable rtl computing module proposed in this work is reused by all cnn layers of the same type for different cnns as shown in fig. note that it is not necessary to have all these modules in the architecture. for instance, the rtl compiler figure overall architecture and dataow . volume , shawahna et al.: fpga-based accelerators of deep learning networks for learning and classication: a review figure conv recongurable computing module . will not compile or synthesize eltwise and combined batch normalization with scale modules for vgg- model which greatly saves the hardware resources. on the other hand, the authors categorized cnn layers into key layers and afliated layers (e.g., relu, bnorm, eltwise, and all other layers). they have also dened layer combos, where each combo is composed of a key layer and several afliated layers. layer combos are sequentially executed according to their order in the dag. moreover, the layer combo is also divided into several sequential tiles. the computation of each combo tile starts by reading its input pixels from off-chip dram and ends by writing back its output pixels to off-chip dram. the global control logic, inter-tile control logic, and intra-tile control logic are responsible for governing the sequential operations of layer combos and reconguring the modules, combo tiles, and tile layers , respectively, through predened exible execution schedule similar to that in . the authors have also employed special storage pattern of both input pixels and weights on off-chip memory before the acceleration process to maximize data reuse and minimize of data communication. the architecture of conv module is designed based on the acceleration strategies in , but with a different organization of mac units as shown in fig. the mac units of conv module have been organized into piy pof independent mac blocks, with each mac block containing pix mac units to further minimize the buffer read operations and the partial sums movements. moreover, such organization enables to handle varying sizes congurations through generating different variants of conv register arrays during the compilation. experimental results demonstrated that the achieved throughput on intel stratix v gxa for nin, vgg-, resnet-, and resnet- are gops, gops, gops, and gops, respectively. on the other hand, the achieved throughput on intel arria gx was gops, gops, gops, and gops for nin, vgg-, resnet-, and resnet-, respectively. more than throughput improvements have been achieved on intel arria gx since it has and more logic elements and dsps than the intel stratix v gxa, respectively, which allows for larger loop unrolling variables. recently, the programmable solutions group at intel has developed an fpga software-programmable and run-time recongurable overlay for deep learning inference . the developed overlay is referred to as the deep learning accelerator . for the hardware side of intels dla, the team has partitioned the congurable parameters into run- time and compile-time parameters. the run-time parameters allow for easy and quick use of different neural network frameworks, while the compile-time parameters provide a tunable architecture for performance. intels dla uses a lightweight very long instruction word network, an -bit unidirectional ring network, to support the control and reprogramming logic. compared with typical overlays, intels dla comes with only % overhead while other typical overlays tend to always come with larger over- heads . the reprogramming of intels dla overlay allows for consecutive runs of multiple nns in a single application run without the need for reconguring and recompiling the fpga. fig. shows that a d array of pes is used to perform convolution, multiplication, or any other matrix operations. each pe contains a double-buffered lter cache allowing for pre-loading of next lters while computing. the stream buffer employed the double-buffering mechanism as well to store the inputs and the intermediate data on-chip. to have exible nn architecture, intels dla employs an xbar interconnect that connects all the core functions required. thus, deep learning functions can be easily added to the overlay through the xbar by picking them from a suite figure intels dla: neural network inference accelerator . volume , shawahna et al.: fpga-based accelerators of deep learning networks for learning and classication: a review of pre-optimized functions of the select frameworks that intels dla uses. the width adaptation module has been used to control the throughput of the function. in addition, intels dla supports vectorization across the input width , input height , input depth , output depth , lter width , and other dimensions as depicted in fig. the authors mention that vectorization depends on the layers dimensions of the considered framework. however, they did not provide a systematic way for nding the optimal balance for the number of used pes and the size of the caches. for efcient use of resources, intels dla maps avg pooling and fc primitives to convolutions in order to avoid having under- utilized dedicated auxiliary functions. for the software side of intels dla, the proposed accel- erator uses a graph compiler to map a nn architecture to the overlay for maximizing the hardware efciency through slicing, allocation, and scheduling. in the slicing pass, the graph compiler breaks down the architecture into subgraph in such a way that they t within the computing and storage resources of the overlay. a single conv layer followed by a pooling layer is an example of cnn subgraph. the graph compiler optimizes the external memory spill-points by group slicing technique. the group slicing allows several sequential convolutions, for instance, of a single slice to be computed before moving onto the next slice while using the whole stream buffer. during the allocation pass, the graph compiler optimizes the use of a custom developed lter caches and stream buffer by managing the read and write from the stream buffer for each slice. moreover, it assigns an external memory address when the stream buffer is not big enough to hold the slice data. finally, intels dla compiler schedules the execution of subgraphs using cost-based (the ratio of the output size figure design flow from cnn model to hardware acceleration . figure overall architecture of angel-eye . to the effective input size) priority queue. the authors utilized the software-programmable and run-time recon- gurable overlay to optimize the software and hardware implementation of googlenet and resnet cnns. the benchmark results on an arria gx fpga demonstrated that intels dla has a throughput of fps on googlenet. the team pointed that multi-fpga deployment might be used to further improve the throughput of intels dla. kaiyuan et al. proposed a complete design ow, referred to as angel-eye, for mapping cnns onto fpga. it includes a data quantization strategy, a parameterizable and run-time congurable hardware architecture to support various cnns, fpga platforms, and a compiler to map a given cnn onto the hardware architecture. it adopts the approach of using a exible hardware architecture and maps different cnns onto it by changing the software. the proposed design ow from cnn model to hardware acceleration is shown in fig. due to the large dynamic range of data across different layers, the best radix point is found for each layer for a given bit width. they demonstrated that their strategy can simplify state-of-the-art cnns to -bit xed-point format with negligible accuracy loss. although -bits are used for representing data, bits are used for representing inter- mediate data in layers, which is then aligned and quantized to bits. fig. and fig. show the overall architecture of angel-eye and the structure of a single pe, respectively. the architecture is designed for supporting an instruction interface that supports three types of instructions; load, save, and calc. the overall architecture is divided into four main compo- figure structure of a single pe . volume , shawahna et al.: fpga-based accelerators of deep learning networks for learning and classication: a review nents; pe array, on-chip buffer, external memory, and con- troller. the pe array implements the convolution operations in cnn and supports kernel level parallelism, and input and output channel parallelisms. it uses a convolution kernel, as this size is most popular in state-of-the-art cnn models. however, larger kernel sizes are supported based on the use of multiple kernels. the use of on-chip buffers allows data i/o and convolution calculation to be done in parallel. the controller is responsible for receiving the instructions and issuing them to the other components. the compiler maps the cnn descriptor to the set of instructions that will be executed by the hardware. it follows basic scheduling rules to fully utilize data localization in cnn and reduce data i/o. the block partition step partitions the calculation of one layer to t each block into the hardware. the memory mapping step allocates memory for communication between the host cpu and the cnn accelerator. based on the block partition result, on-chip memory is allocated for the input and output feature map blocks and for the convolution kernels and bias values. the dependency check step checks data dependency among instructions and sets appropriate instruction ags to maximize parallelism between convolu- tion calculation and data i/o. based on experimental results, it is shown that the -bit implementation of angel-eye on xcz achieves up to higher energy efciency than nvidia tk and higher than nvidia tx in addition, the -bit implementation of angel-eye on xcz is faster and higher in power efciency than peer fpga implementation on the same platform . in and , a special register array architecture has been designed to rearrange buffers data and direct them into pes for the purpose of implementing conv module that supports specic stride and zero-padding settings. although the designed conv module is not generalized for any (ker- nel, stride) size congurations, it is composed of complex wire routing and control logic as shown in fig. to have exibility in directing the dataow of conv pixels, ma figure conv acceleration architecture and dataow using data router , where, p ix p ox, and p iy p oy. et al. replaced the register array architecture in with a data router as shown in fig. the data router is a scalable set of data bus from buffer to pe . the bufpe data bus consists of simple register arrays with fifos in between to form a line buffer similar to that in . the register array uses the fifo to pass its input pixels to the adjacent registers. each bufpe data bus has different data movements within its register arrays to implement specic stride and kernel size settings. unlike the register array architecture in where the west zero-paddings are handled by changing the storage pattern within the input pixel buffer, the bufpe handles such kind of paddings by shifting the connection between the register arrays and the input pixel buffers to simplify the data transferring from off-chip memory to on-chip buffers. however, there is still a need for adjusting the storage pattern within the input buffers in order to handle other zero-paddings. the global control logic is responsible for selecting the suitable bufpe data bus from the data router as well as the suitable storage pattern within the input buffers based on the size conguration of conv layer. the conv module has also been optimized by reducing the required number of parallel adders that add the partial sums with biases as well as the number of parallel multipliers and adders needed to perform bnorm operation by serializing the parallel outputs using multipliers. in addition, -bit xed-point has been used to represent both weights and pixels, while dynamically adjusting the decimal point in different layers to fully utilize the existing data width . the proposed compiler in has been used to cong- ure the parameterized verilog scripts of the overall cnn acceleration system. experimental results show throughput degradation on both intel arria gx and intel stratix v gxa in comparison to the results in . in table and table , we summarize the reviewed fpga-based deep learning networks acceleration tech- niques. for each technique, we list the year the technique was introduced, the key features employed for acceleration, the used deep learning model, the number of needed op- erations per image, the fpga platform used to implement the technique, the precision used for the fms and weights, the clock frequency used, the design entry for describing the modeled deep learning network, the type of lut for the used platform, the number of resources available by the used platform in terms of brams, luts, ffs, and dsps, the percentage of each resource utilization, the performance in gops, the speedup in comparison to a given baseline model, and nally the power efciency . iv. metaheuristics in the design of convolutional neural networks currently, convolutional neural network structures are designed based on human expertise. for a given applica- tion, this consists of determining the number of convolution layers, number of fully connected layers, sizes of feature volume , shawahna et al.: fpga-based accelerators of deep learning networks for learning and classication: a review table implementation and performance summary of fpga-based accelerators. technique year key features dl model image operations platform precision frequency lut type design entry resources resources utilization performance speedup baseline power efciency brams/ mk luts/ alms ffs dsps brams/ mk luts/ alms ffs dsps cnp d conv modules, memory interface with simultaneous accesses lenet- virtex sx -bit xed-point -input luts lush , , n/a % % % n/a conv coprocessor accelerator parallel clusters of d convolver units, data quantization, off-chip memory banks conv layers virtex lxt -bit xed-point -input luts c , , % % % % ghz amd opteron maple in-memory processing, banked off-chip memories, d array of vpes conv layers virtex sxt xed-point -input luts c++ , , , n/a ghz c gpu n/a dc-cnn integer factorization to determine the best cong for each layer, input and output switches conv layers virtex sxt -bit xed-point -input luts rtl , , , n/a - ghz c gpu neuflow multiple full-custom processing tiles , pipelining, fast streaming memory interface conv layers in n/a virtex vlxt -bit xed-point -input luts hdl , , n/a ghz core duo cpu memory-centric accelerator flexible off-chip memory hierarchy, data reuse, loop transformation conv layers virtex vlxt xed-point -input luts c , , % % n/a % standard virtex implementation n/a nn-x cascaded pipelining, multiple stream processing conv layers zynq xcz -bit xed-point -input luts lua , , n/a mhz embedded arm cortex-a processors rooine-based fpga accelerator rooine-based model, loop transformation, loop tiling/unrolling alexnet virtex vxt -bit oat-point -input luts c , , , , % % % % ghz intel xeon microsoft specialized cnn accelerator multi-banked buffers, network on-chip for re- distribution of output data, software congurable alexnet stratix-v gsmd -bit oat-point -input luts c , , , , n/a rooine-based fpga accelerator embedded fpga accelerator data quantization and arrangment, svd vgg- zynq xcz -bit xed-point -input luts rtl , , % % % % ghz intel xeon deepburning nn model compression, compiler-based library, automatic partitioning/ tiling, function approx. alexnet zynq xcz -bit xed-point -input luts rtl , , n/a % % % ghz intel xeon opencl-based fpga accelerator design space exploration for all cnn layers, genetic algorithm, altera opencl sdk alexnetpaq stratix-vpq gxa -bit xed-point -input luts opencl , , , % % n/a % paq ghz intel i- n/a pbq n/a vgg-pbq stratix-vpq gsd , , ,, , % % n/a % paq n/a pbq n/a caffeine , systolic array architecture, loop unrolling, double buffering, pipelining, rooine model alexnetpaq virtexpq vxt -bitpaq xed-point -input luts c++ , , , , % % % % pbaq two-socket server each with a -core intel cpu e- at ghz % % % % pbaq vgg-pbq xilinxpq ku -bitpbq xed-point , , , , % % % % ,pbbq n/a paaq n/a fpgaconvnet datapath optimization, synchronous dataow, partitioning and folding, design space exploration lenet- zynq xcz -bit xed-point -input luts hls , , % % % % cnp n/a scene labelling % % % % tegra k gpu volume , shawahna et al.: fpga-based accelerators of deep learning networks for learning and classication: a review table implementation and performance summary of fpga-based accelerators. technique year key features dl model image operations platform precision frequency lut type design entry resources resources utilization performance speedup baseline power efciency brams/ mk luts/ alms ffs dsps brams/ mk luts/ alms ffs dsps throughput-optimized fpga accelerator four-levels of parallelism, memory-cache mechanism, design space exploration lenet virtex vxt -bit xed-point -input luts n/a , , , , % % % % ghz intel core i-k cpu alexnet % % % % vgg-s % % % % fp-dnn rtl-hls hybrid compiler, hand-written matrix multiply, quantization, tiling and double buffers vgg- stratix-v gsmd -bit xed-point -input luts c++ and opencl , , , , % % n/a % processors each ghz intel xeon -core e-v resnet- % % n/a % finn binarized cnn, pipelining, automatic partitioning and tiling, approximate arithmetic cnv zynq xcz -bit precision -input luts c++ , , % % n/a n/a , microsoft specialized cnn accelerator customized conv loops accelerator numerical analysis of conv loops opt., solution space exploration, dataow opt. vgg- arria gx -bit oat-point -input luts verilog , , ,, , % % n/a % energy-efcient cnn opencl-based fpga accelerator pbq latency-driven design for fpga-based cnns synchronous dataow, weights reloading and sdf transformations, design space exploration alexnet zynq xcz -bit xed-point -input luts hls , , n/a deepburning n/a vgg- embedded fpga accelerator dla winograd transformations, double stream buffers, pes double cache buffers, daisy-chained pes alexnet arria gx half-precision fp with shared exponent -input luts opencl , , ,, , % % % % , caffeine paaq opencl-based fpga accelerator paq winograd-based cnn accelerator winograd algorithm, loop unrolling, double buffers, batching, line buffers, design space exploration alexnetpaq zynqpq zcu -bit xed-point -input luts c , , , n/a paq paq ,pbq caffeine pbaq vgg-pbq xilinxpq zc -input luts , , paq paq pbq titan x opencl-based architecture for accelerating cnns d bram-to-pe interconnection, d dispatcher, rooine model, opencl vgg- arria gx -bit oat-point -input luts opencl , , ,, , % n/a n/a % altera opencl on arria platform -bit xed-point , , ,, , % n/a n/a % , n/a multi-clp accelerator for cnns multiple conv processors, pipelining, dynamic programming, double-buffering alexnetpaq virtexpq vxt -bit oat-point -input luts c++ , , , , % % % % paq single clp design based in % % % % paq squeezenetpbq virtexpq vxt -bit xed-point -input luts , , , , n/a pbq n/a % % % % pbq automated systolic array architecture for cnn d systolic array architecture, rooine model, automation ow design space exploration alexnetpaq arria gx -bitpq oat-point paq -input luts opencl , , ,, , % % n/a % paq n/a n/a vgg-pbq -bitpq xed-point pbq % % n/a % pbq pbq % % n/a % ,pbq end-to-end scalable fpga accelerator flexible and scalable resnet modules, conv loops opt., dataow opt., controlled execution ow of resnets layers resnet- arria gx -bit xed-point -input luts verilog , , ,, , % % n/a % n/a n/a resnet- % % n/a % dlau pipelined processing units, tiling, fifo buffers dnn n/a zynq xcz -bit oat-point -input luts n/a , , % % % % n/a ghz intel core n/a an automatic rtl compiler for high- throughput deep cnns library-based rtl compiler, flexible and scalable cnn modules, layer combo computation, conv loops and dataow opt., controlled execution ow of cnn layers ninpaq stratix-vpq gxa -bit xed-point -input luts verilog , , , % % n/a % paq deepburning n/a % % n/a % pbq n/a vgg-pbq % % n/a % pcq n/a % % n/a % pdq fp-dnn resnet-pcq arria pq gx -input luts , , ,, , % % n/a % paq n/a % % n/a % pbq caffeine pbaq resnet-pdq % % n/a % pcq n/a % % n/a % pdq n/a alamo , modularized rtl compiler, loop unrolling, loop tiling alexnet stratix-v gxa -bit xed-point -input luts verilog , , , % % n/a % rooine-based fpga accelerator nin % % n/a % n/a angel-eye quantization, parallel pes, compiler, on-chip buffers vgg- zynq xcz -bit xed-point -input luts n/a , , % % % % nn-x zynq xcz -bit xed-point , , % % % % optimizing the conv operation to accelerate dnns on fpga scalable set of data buses , optimized conv module for different size congurations, flexible and scalable cnn modules, conv loops and dataow opt. ninpaq stratix-vpq gxa -bit xed-point -input luts verilog , , , % % n/a % paq n/a n/a % % n/a % pbq n/a vgg-pbq % % n/a % pcq n/a % % n/a % pdq fp-dnn resnet-pcq arria pq gx -input luts , , ,, , % % n/a % paq n/a % % n/a % pbq n/a resnet-pdq % % n/a % pcq n/a % % n/a % pdq n/a volume , shawahna et al.: fpga-based accelerators of deep learning networks for learning and classication: a review maps in each layer, along with other operators. recent research has demonstrated that a large number of weights in fully connected layers could be eliminated with minimal impact on accuracy. in addition, although the suggested cnn structures by experts perform well for various appli- cations, the question arises whether the suggested structures could be optimized for performance with minimal impact on accuracy. since the designed cnn has a signicant impact on the complexity of its implementation, we review in this section some approaches attempting to optimize the design of cnns using metaheuristics. np-hard combinatorial optimization problems ap- pear in the design of cnns. some examples of areas include design of cnn structures, selection of weights and bias values to improve accuracy, and determination of optimal values of variables to reduce run-time. below, we briey touch upon some existing literature in these areas. a. cnn structure optimization in the design of cnns, the number of possible network structures increases exponentially with the number of layers. xie and yuille used genetic algorithm in learning deep network structures . the objective was to nd the best cnn structure that would minimize the error rate. the cost function was the cnn accuracy. they proposed an elegant encoding of chromosome using a xed length binary string to represent each network structure. a cnn string represents only the convolution layers. in each generation, using standard genetic operations new individuals are generated and weak ones eliminated. the quality of an individual was assessed by its recognition ac- curacy which is obtained via the time consuming operation of training the network, and evaluating it on a validation set. two small data sets were used to run the genetic implementation via which they demonstrated the discovery of new structures. b. cnn weights and bias values optimization an attempt to train cnns using metaheuristics (that is, determine weights and bias values) is presented in . the objective again was to improve accuracy and minimize the estimated error. the authors experiment with three metaheuristic algorithms, namely; simulated annealing, dif- ferential evolution, and harmony search. the algorihtms compute the values of weights and bias in the last layer. these values are used as the solution vector denoted by x which is to be optimized. the move comprised adding a small value of x to perturb the state. the cost function y is modeled as y n inpo uq n where, o is the expected output, u is the real output, and n is the number of used samples. the stopping criterion is when the iteration count is reached or when the cost function goes below a pre-specied value. c. cnn design variables optimization suda et al. presented a systematic methodology for design space exploration with the objective of maximizing the throughput of an opencl-based fpga accelerator for a given cnn model . fpga resource constraints such as on-chip memory, registers, com- putational resources and external memory bandwidth are considered. the optimization problem comprises nding the best combination of nconv , sconv , nnorm, np ool, and nf c variables, where nconv is size of the lter ; sconv is the factor by which computational resources are vectorized to execute in a single-instruction stream multiple-data streams fashion; nnorm represents the number of normalization oper- ations performed in a single cycle; np ool is the number of parallel outputs of the pooling layer in a single cycle to achieve acceleration; and, nf c is the number of parallel multiply and accumulate operations preformed in a single work-item within the fully connected layer. the objective function to be minimized is the run-time , and is given by t l i rtirnconv , sconv , nnorm, np ool, nf cs subject to digital signal processing slices, logic, and memory constraints, where tl represents the total number of cnn layers including the repeated layers. the convolu- tion layer run-time is analytically modeled as a function of design variables as rt conv i # of convolution opsi nconv sconv frequency as for the other layers, that is, normalization, pooling, and fully connected, the following general model is proposed rt layeri # of layer opsi unroll factor frequency the above analytical models are later validated by per- forming full synthesis at selective points and running them on the fpga accelerator. clearly, in order to determine the best values of the discussed design variables, exhaustive search, especially if the number of variables and or fpga resources is large, is infeasible. we have to resort to iterative non-deterministic heuristics such as simulated annealing, simulated evolution, tabu search, genetic algorithm, particle swarm optimization, cuckoo search, etc., or any of the modern metaheuristics, to efciently traverse the search space to nd acceptable solutions. volume , shawahna et al.: fpga-based accelerators of deep learning networks for learning and classication: a review the proposed methodology employing genetic algorithm was demonstrated by optimizing the implementation of two representative cnns, alexnet and vgg, on two altera stratix-v fpga platforms, de-net and p-d boards, both of which have different hardware resources. peak performance is achieved for both, for the convolution oper- ations, and for the entire cnn network. one major issue related to use of non-deterministic iter- ative heuristics in the design of neural networks and cnns is the large amount of memory required to store the state of solution and the amount of time taken to determine the cost of the solution, be it accuracy/error estimation, run-time, or any other objective. reasonable estimation techniques and analytical formulations are required to efciently traverse the design space in search of efcient solutions.","The text provides an overview of key operations and terminology related to convolutional neural networks (CNNs) and how deep learning methods can benefit from field programmable gate arrays (FPGAs). It discusses the challenges of implementing deep learning networks on FPGAs, focusing on concepts such as convolution, activation functions, normalization, pooling, and characteristics of fully connected layers. Various examples of well-known deep learning networks like AlexNet, VGG, and ResNets are highlighted, along with details about the basic structure of FPGAs and their advantages in accelerating computationally intensive applications. The text also explores different FPGA-based accelerators designed to optimize the performance of deep learning networks by leveraging parallelism, loop unrolling, loop tiling, and other optimization techniques. Examples of successful implementations and their performance metrics are described, showcasing the potential of FPGA-based solutions for accelerating deep learning tasks."
,
,
"in this section, we highlight the key features discussed in the acceleration of convolutional neural networks implemented on fpgas, and provide recommendations to enhance the effectiveness of employing fpgas in the ac- celeration of cnns. all reviewed techniques are centered around accelerating the convolution operation as it consumes around % of the computational time. this is achieved by utiliz- ing parallel multiply-accumulate operations bounded by re- source limitations. in addition, careful design of data access patterns are targeted to minimize the memory bandwidth requirements utilizing internal memory structures and max- imizing data reuse. this is crucial in the acceleration process due to the large memory data that needs to be accessed including feature maps and weights. to minimize the memory footprint and to achieve effective utilization of resources, some techniques optimize the number of bits used to represent the feature maps and weights with minimal impact on accuracy. this is combined with the optimized selection of the number of fraction bits used for each layer. other techniques optimize the number of used weights in the fully connected layers as they are memory-intensive. coprocessors are also employed to automatically congure both the software and the hardware elements to fully exploit parallelism . to optimize parallelization of convolution operations, several approaches have been attempted. work load analysis has been tried to determine computations that can be struc- tured as parallel streams . the rooine model based accelerator uses polyhedral-based data dependence analysis to nd the optimal unrolling factor for every convolutional layer , and to fully utilize all fpga computational resources through loop pipelining. to optimize performance, tiled matrix multiplication is structured as a pipelined binary adder tree for performing multiplication and generating partial sums . an optimization framework has been proposed by suda et al. who identied the key variables of the design and optimize them to maximize parallelism. to reduce computational complexity of conv layers and improve resource efciency, a number of approaches such as , , utilized winograd transformation in performing conv operations as this reduces the computa- tional complexity by around %. to maximize throughput, several techniques such as , , have used multiple conv layer pro- cessors instead of using a single clp that is opti- mized for all conv layers. this pipelines the operation of the multiple clps achieving layer-level parallelism which maximizes resource utilization and enhances performance in comparison to using a single clp. since the computational requirement of fc layers is signicantly less than that of conv layers, to improve performance, and maximize resource utilization, a number of techniques such as , , , create batches by grouping different input fms and processing them together in fc layers. complex access patterns and data locality are used in deepburning tool for better data reuse. in , the authors explored hot spots proling to determine the computational parts that need to be accelerated to improve the performance. acceleration is accomplished by reducing the memory bandwidth requirements. techniques proposed exploit data reuse to reduce off-chip memory communica- tions. loop transformations have also been used by reducing tiling parameters to improve data locality, and to reduce redundant communication operations to maximize the data sharing/reuse. efcient buffering, where the weight buffers are used to ensure the availability of conv and fc layers weights before their computation, as well as to overlap the transfer of fc layer weights with its computation, helps in improving performance , . in the catapult project, fpga boards were integrated into data center applications and achieved speedup. microsoft researchs catapult utilized multi-banked input buffer and kernel weight buffer to pro- vide an efcient buffering scheme of feature maps and weights, respectively. to minimize the off-chip memory trafc, a specialized network on-chip was designed to re- distribute the output feature maps on the multi-banked input buffer instead of transferring them to the external memory . to further reduce memory footprint and bandwidth re- quirement, optimal fractional length for weights and feature maps in each layer are used. singular value decomposition has also been applied to the weight matrix of fc layer in order to reduce memory footprint at this layer . tiling techniques have been proposed where large-scale input data is partitioned into small subsets or tiles whose size is congured to leverage the trade-off between the hardware cost and the speedup . automation tools have been developed that automatically build neural networks with optimized performance . they employ pre-constructed register transfer level module library that holds hardware (including logical and arithmetic operations) and conguration scripts. deepburn- volume , shawahna et al.: fpga-based accelerators of deep learning networks for learning and classication: a review ing, for example, generates the hardware description for neural network scripts. another modularized rtl compiler, alamo, integrates both the rtl ner level optimization and the exibility of high-level synthesis to generate efcient verilog parameterized rtl scripts for asic or fpga platform under the available number of parallel computing resources , . acceleration is achieved by employing loop unrolling technique for conv layer operations. some of the reviewed techniques also help minimize the size of fpga on-chip memories to optimize energy and area usage , . in table and table , we list the optimization mech- anisms utilized by each of the reviewed techniques to maximize performance and throughput of fpga-based deep learning networks. to enhance utilization of fpgas in cnns acceleration and to maximize their effectiveness, we recommend the development of a framework that includes a user-friendly interface that allows the user to easily specify the cnn model to be accelerated. this includes specifying the cnn model parameters in terms of number of convolution layers and their sizes, and number of fully connected layers along with other intermediate operations. the specied cnn model weights will be read from a le. in addition, the user should have the option of specifying the fpga platform that will be used for implementing the cnn accelerator and the maximum tolerable error, along with the selection of a library from a set of applications to be used for model optimization and evaluation. the framework then should perform optimizations to nd the minimum number of bits that need to be used for representing the weights and feature maps and the number of fraction bits to be used for each layer. in addition, optimization of fully connected layers is performed to minimize the memory requirements. all such optimizations are carried out bounded by the maximum error specied by the user for the specied application library. the framework should be designed based on the devel- opment of a scalable hardware architecture that works for any given fpga platform and achieves higher speedup with the availability of higher resources. based on the available resources, specied by the fpga platform, the tool will perform optimizations to maximize parallelism and data reuse, given the resource limitations. the tool will then automatically generate the cnn model that will t on the given fpga platform and will allow the user to evaluate the performance based on the chosen application library. this will allow the user to evaluate the performance gains while evaluating different fpga platforms with different resources. the tool should have the option to generate per- formance measures based on different performance metrics as selected by the user such as number of frames processed per second or number of operations performed per second. in addition, the tool will report other design metrics such as resource utilization, memory sizes and bandwidth, and power dissipation. furthermore, it is desired to have the option for the user to specify the desired performance for a given cnn model and have the tool perform necessary analysis and evaluation and recommend to the user candidate fpga platforms for achieving the desired performance levels. this will require the development of reasonably accurate analytical models that will estimate the needed resources for achieving the desired performance. the user can then choose the recom- mended fpga platform and perform complete evaluation to verify that the desired performance levels are met.","The section summarizes key techniques for accelerating convolutional neural networks (CNNs) on Field-Programmable Gate Arrays (FPGAs) by focusing on optimizing convolution operations and memory access. Techniques include utilizing parallel multiply-accumulate operations, optimizing data access patterns, selecting optimal bit representations for weight and feature maps, employing coprocessors for configuration, and optimizing parallelization through loop pipelining. Techniques like Winograd transformation and batch processing are used to reduce computational complexity and improve resource efficiency. Various optimization methods, including loop transformations and efficient buffering, help reduce memory bandwidth requirements. Integration of FPGAs into data centers and the use of specialized network on-chip are also discussed. Recommendations include developing a user-friendly framework for specifying and optimizing CNN models on FPGA platforms to achieve higher speedup and performance metrics. Tools should offer performance evaluation, resource utilization metrics, and platform recommendations based on user-defined performance goals."
"in this paper, we reviewed recent developments in the area of acceleration of deep learning networks and, in particular, convolution neural networks on eld programmable gate arrays . the paper begins with a brief overview of deep learning techniques highlighting their importance, key operations, and applications. special emphasis is given on cnns as they have wide applications in the area of image detection and recognition and require both cpu and memory intensive operations that can be effectively accelerated utilizing fpga inherent ability to maximize parallelism of operations. while the paper briey touches upon the acceleration techniques for deep learning algorithms and cnns from both software and hardware perspectives, the core of this article has been the review of recent techniques employed in the acceleration of cnns on fpgas. a thorough up-to- date review is provided that illustrates the employment of various possibilities and techniques such as exploitation of parallelism utilizing loop tiling and loop unrolling, effective use of internal memory to maximize data reuse, operation pipelining, and effective use of data sizes to minimize mem- ory footprint, and, to optimize fpga resource utilization. the paper also presented the use of tools for generating register transfer level scripts that not only help in automating the design process, but also help in exploring the design space and suggesting efcient hardware. the paper discusses the use of analytics such as: work load analysis in determining the computations that can be parallelized, optimal loop unrolling factors, determining access patterns to improve data locality, etc. in addition, a brief review of the use of non-deterministic heuristics in solving np-hard combinatorial optimization problems in the design and implementation of cnns has been presented. finally, the paper summarizes the key features employed by the various fpga-based cnn acceleration techniques and pro- vided recommendations for enhancing the effectiveness of utilizing fpgas in cnns acceleration. acknowledgment authors acknowledge king fahd university of petroleum & minerals, dhahran, saudi arabia for all support. we also like to acknowledge dr. blair p. bremberg and ms. sumaiya hussain sadiq for their help in professional english editing of this manuscript. volume , shawahna et al.: fpga-based accelerators of deep learning networks for learning and classication: a review table optimization mechanisms employed for fpga-based acceleration of deep learning networks. technique vip cnp conv coprocessor accelerator maple dc-cnn neuflow memory-centric accelerator nn-x rooine-based fpga accelerator embedded fpga accelerator deepburning opencl-based fpga accelerator caffeine , fpgaconvnet loop unrolling loop tiling loop interchange pipelining batching multi-clps fixed-point precision per-layer quantization singular value decomposition prefetching rearranging memory data in-memory processing line buffer double buffering approximating non-linear af eliminating fc layer rooine model polyhedral optimization dynamic programming graph partitioning volume , shawahna et al.: fpga-based accelerators of deep learning networks for learning and classication: a review table optimization mechanisms employed for fpga-based acceleration of deep learning networks. technique alamo , throughput- optimized fpga accelerator fp-dnn finn customized conv loop accelerator latency-driven design for fpga-based cnns dla winograd- based cnn accelerator opencl-based architecture for accelerating cnns multi-clp accelerator for cnns automated systolic array architecture for cnn end-to-end scalable fpga accelerator dlau an automatic rtl compiler for high-throughput deep cnns intels dla angel-eye optimizing the conv operation to accelerate dnns on fpga loop unrolling loop tiling loop interchange pipelining input batching fc layer batching multi-clps binarized cnn fixed-point precision per-layer quantization prefetching rearranging memory data line buffer double buffering padding optimizations winograd algorithm approximating non-linear af rooine model polyhedral optimization dynamic programming graph coloring graph partitioning pattern matching volume , shawahna et al.: fpga-based accelerators of deep learning networks for learning and classication: a review","The paper reviews recent developments in the acceleration of deep learning networks, specifically convolutional neural networks (CNNs), on field-programmable gate arrays (FPGAs). It emphasizes the importance of CNNs in image detection and recognition tasks due to their intensive CPU and memory operations that can be accelerated efficiently with FPGA's parallel processing ability. Techniques such as loop tiling, loop unrolling, internal memory usage optimization, operation pipelining, and data size optimization are discussed. The paper also covers tools for automating design processes and analytics for workload analysis and optimal loop unrolling factors. Various FPGA-based acceleration techniques and optimization mechanisms are presented, along with recommendations for enhancing FPGA utilization in accelerating CNNs."
,
"finitely dependent processes are finitary yinon spinka abstract. we show that any nitely dependent invariant process on a transitive amenable graph is a nitary factor of an i.i.d. process. with an additional assumption on the geometry of the graph, namely that no two balls with dierent centers are identical, we further show that the i.i.d. process may be taken to have entropy arbitrarily close to that of the nitely dependent process. as an application, we give an armative answer to a question of holroyd .","The section discusses finitely dependent processes in relation to transitive amenable graphs. It demonstrates that any such process on a transitive amenable graph is a finite factor of an i.i.d. process. Additionally, with the assumption that no two balls with different centers are identical, the i.i.d. process can have entropy close to that of the finitely dependent process. Furthermore, it provides an affirmative answer to a question posed by Holroyd."
"consider a random process x = vv living on the vertex set v of an innite graph g. the process x is said to be nitely dependent if its restrictions to sets which are suciently separated (at least some xed distance apart) are independent. a trivial example of a nitely dependent process is a process y = vv in which all random variables are independent. a natural question is then how close is a nitely dependent process to such an independent process? before addressing this question, we rst observe that local functions of an independent process y are always nitely dependent. that is, if x is obtained from y by computing each xv as a function only of the random variables yu for which u is at a uniformly bounded distance from v, then x is nitely dependent. suppose now that g is transitive and henceforth restrict attention to processes x which are invariant under all automorphisms of g . in particular, the independent process y considered above must now be an i.i.d. process that is, in addition to being independent, the {yv}v are also identically distributed. if x is obtained from y by applying the same local function at each vertex v (i.e., the function applied at u is the composition of the function applied at v with any automorphism taking u to v), then x is said to be a block factor of y . thus, block factors of i.i.d. processes provide a recipe for constructing invariant nitely dependent processes. it was a long-standing open problem to determine whether block factors of i.i.d. processes are the only nitely dependent processes on z, until nally an example was given by burtongouletmeester of a -dependent process which is not a block factor of any i.i.d. process. recently, holroyd and liggett showed that proper colorings distinguish between block factors of i.i.d. processes and nitely dependent processes no proper coloring of z is a block factor of an i.i.d. process, but nitely dependent proper colorings exist. thus, it is not true that every nitely dependent process is a block factor of an i.i.d. process. in other words, given a nitely dependent process x, one cannot in general hope to nd an i.i.d. process y and an invariant rule for computing x from y , which allows to determine the value of xv by looking at y on a xed-size window around v. the goal of this paper is to show the next best thing namely, that it is possible to determine xv by looking at y on a variable-sized window around v, where the size of the window, though always nite, may vary according to the input y . we say that x is a nitary factor of y if there is an invariant rule which allows to compute the value of x at any vertex v by only looking at variables yu for which u is within a certain nite, but random, distance from v . thus, a block factor is a nitary factor in which the required distance is not only nite, but is determistically bounded by some constant. the main contribution of this paper is to prove that every nitely dependent process is a date: january , arxiv:v jan yinon spinka nitary factor of an i.i.d. process. this result holds on any amenable graph g. when it is further assumed that no two balls in g with dierent centers are identical, it is also possible to control the entropy of the i.i.d. process involved, and the result becomes that every nitely dependent process is a nitary factor of an i.i.d. process with only slightly larger entropy. . denitions and main result. let v be a countable set, let g be a graph on vertex set v and let be a group acting on v. a random eld on g is a collection of random variables x = vv indexed by the vertices of g and dened on a common probability space. we say that x is -invariant if its distribution is not aected by the action of , i.e., if vv has the same distribution as x for any . we say that x is k-dependent if uu and vv are independent for any two sets u, v v such that dist > k for all u u and v v . we say that x is nitely dependent if it is k-dependent for some nite k. suppose now that g is a transitive locally nite graph and that is a subgroup of the automor- phism group of g. let s and t be two measurable spaces, and let x = vv and y = vv be s-valued and t-valued -invariant random elds. a coding from y to x is a measurable func- tion : t v sv that is -equivariant, i.e., commutes with the action of every element of , and satises that and x are identical in distribution. such a coding is also called a factor map from y to x, and when such a coding exists, we say that x is a -factor of y . suppose now that s and t are countable. let v be a distinguished vertex. the coding radius of at a point y t v, denoted by r, is the minimal integer r such that = for all y t v which coincide with y on the ball of radius r around in the graph-distance, i.e., y v = yv for all v v such that dist r. it may happen that no such r exists, in which case, r = . thus, associated to a coding is a random variable r = r which describes the coding radius. while s will always be at most countable, we will allow t to be a larger space, in which case the coding radius may be similarly dened a coding is called nitary if r is almost surely nite. when there exists a nitary coding from y to x, we say that x is a nitary -factor of y . a graph is said to be amenable if inf |v |/|v | = , where the inmum is over all nite non-empty subsets v of v, and where v denotes the edge-boundary of v . theorem . let g be a transitive amenable graph and let be a transitive group of automor- phisms of g. then any nitely dependent -invariant random eld on g is a nitary -factor of an i.i.d. process. with a minor additional constraint on the geometry of the graph g, we can further control the entropy of the i.i.d. process used in the coding . the condition we require is that r = r for any distinct u, v v and r , where r is the ball of radius r around u. theorem . let g be a transitive amenable graph satisfying , let be a transitive group of automorphisms of g, and let x be a nite-valued nitely dependent -invariant random eld on g. then for any > there exists an i.i.d. process y with entropy h < h + such that x is a nitary -factor of y . let us make some remarks about how the two theorems compare to one another. in theorem , s is nite and, in particular, x has nite entropy, while in theorem , s may be countable and x may have innite entropy. in theorem , y has nite entropy so that t is countable, whereas theorem may require a larger space t for we will only be concerned with spaces t which are nite, countable or of the form t t for nite sets . in the latter case, the coding radius is the smallest r for which there exists n such that = for all y having the property that y v,i = yv,i for all such that dist r and i n. finitely dependent processes are finitary the conclusion to hold. in fact, in the absence of condition , even when s is nite, it might not be possible to have t countable . let us also remark that, while the theorems do not assume connectivity of the graph, there is no loss of generality in assuming this, since transitivity implies that the connected components are isomorphic and nite dependence implies that the random eld is independent on dierent components. thus, the same coding can be used for all components.","The text discusses the concept of finitely dependent random processes on infinite graphs. It explores the relationship between finitely dependent processes and block factors of i.i.d. processes. The main contribution of the paper is the proof that every finitely dependent process on an amenable graph is a nitary factor of an i.i.d. process. The text also introduces the idea of coding from one random field to another, defining nitary coding and providing theorems that outline the conditions under which finitely dependent processes can be represented as nitary factors of i.i.d. processes. The text emphasizes the importance of graph properties and group actions in understanding the relationships between different types of random processes."
"finite dependence and nitary factors have applications in computer science. for example, if the graph g represents machines in a network and the random eld x represents a common plan in which each machine v is assigned a specic role xv, then nite dependence provides certain security benets in the face of an attacker (if someone gains access to some machines, they learn nothing about the roles of far away machines, thereby conning the security breach), and being a nitary factor of an i.i.d. process provides reliability as it means that the machines can determine their own roles in a distributed manner by following a common protocol, while using local randomness and communicating with nitely many other machines. see e.g. for more information. the nitary coding properties of nitely dependent processes on g = z, and in some cases on g = zd with d , have been studied in various contexts. we give a brief account of these works. we are unaware of any works regarding nitary factors for nitely dependent processes on other graphs. a result by smorodinsky shows that every stationary nitely dependent process on z is nitarily isomorphic to an i.i.d. process. this result is not for the full autormorphism group of the graph (which includes reections), but rather only for the group of translations. in this respect, theorem strengthens this result , as it provides a nitary factor which is also reection invariant whenever the nitely dependent process is such. the proof in is based on the so-called marker-ller method of keane and smorodinsky . unfortunately, only a brief sketch of the proof is provided in and the details seem to be missing (after some initial steps, smorodinsky says that the rest of the proof proceeds along the same lines as in with some necessary modications). our proof is based on a dierent approach; see section for an outline. the question of whether there exists a stationary nitely dependent process which is not a block factor of any i.i.d. process was raised by ibragimov and linnik in some progress on this question was made until it was nally resolved in by burtongouletmeester who gave the rst example of a stationary nitely dependent process which is not a block factor of an i.i.d. process. in fact, they showed such an example in which the nitely dependent process is a -dependent hidden-markov process with nite energy. some history about nitely dependent processes that cannot be written as block factors is given in . holroyd and liggett constructed a stationary -dependent -coloring and a stationary - dependent -coloring of z, neither of which is a block factor of an i.i.d. process (indeed, no coloring is such ). holroyd subsequently showed that the -dependent -coloring is a nitary factor of an i.i.d. process. regarding the analogous statement for the -dependent -coloring, holroyd writes in that one may attempt to apply our method to the -dependent -coloring, but we will see that it meets a fundamental obstacle in this case. our result shows that either coloring is a nitary factor of an i.i.d. process , answering armatively question in . in fact, the two colorings are also reection invariant, and hence the nitary factors may also be taken to commute with reections. related aspects of these two colorings were studied in . in a subsequent paper , holroyd and liggett constructed, for any q , a -dependent q-coloring of z which is invariant under translations and reections and is also symmetric under yinon spinka permutations of the colors. it was shown in that each of these colorings is a nitary factor of an i.i.d. process . our result shows that each of these colorings is a nitary factor of an i.i.d. process, where the factor map commutes with all automorphisms of z and the i.i.d. process has entropy only slightly larger than the coloring. in , holroyd and liggett also constructed stationary nitely dependent colorings of zd with d more specically, they constructed a stationary -dependent d-coloring of zd and a sta- tionary nitely dependent -coloring of zd. however, unlike the above one-dimensional colorings, these colorings are only translation-invariant and not automorphism-invariant. in fact, it is still unknown whether there exists a nitely dependent coloring of zd which is invariant under all automorphisms of zd. in the same paper , holroyd and liggett also investigated the existence of stationary nitely dependent processes on z which are supported on a given shift of nite type. they showed that for any reasonably non-degenerate shift of nite type s on z, there exists a stationary nitely dependent process which almost surely belongs to s. it was later shown that there exists such a process which is also a nitary factor of an i.i.d. process (with exponential tail on the coding radius). a block factor is precisely a nitary factor with bounded coding radius. given a nitary factor which is not a block factor, it is natural to wonder about the typical value of the coding radius. as we have mentioned, the -dependent -coloring of z from , which is not a block factor of any i.i.d. process, was shown in to be a nitary factor of an i.i.d. process. this nitary factor was shown to have power-law tail on the coding radius, thus yielding a perhaps innite expected coding radius. holroydhutchcroftlevy showed that there exist nitely dependent colorings of z which are nitary factors of i.i.d. processes with exponential tail on the coding radius. indeed, they showed that such a k-dependent q-coloring exists when is either , or . on the other hand, it is believed that when is or , no k-dependent q-coloring is a nitary factor of an i.i.d. process with nite expected coding radius. we mention that optimal tails for the coding radius of colorings of zd and shifts of nite type on z have been studied in . our main theorem gives no information about the coding radius beyond its almost-sure nite- ness. indeed, in light of the above discussion, it would seem that for an arbitrary nitely dependent process on z, there is not much hope to obtain a nitary factor with nite expected coding ra- dius. still, some information about the coding radius may be extracted from the proof given here . for example, for -dependent processes on z, the nitary factor provided by theorem has a coding radius r satisfying that p /r for all r. in the particular case of the -dependent -coloring of , this improves the power in the power-law bound shown in . to the best of our knowledge, beyond smorodinskys result on z, there do not exist any general results on the nitary coding properties of nitely dependent processes. in particular, theorem and theorem are new for any amenable graph g other than z, and also for g = z in the case when is the full automorphism group of z. finally, we mention that the situation for non- amenable graphs is still poorly understood for example, on a regular tree (of degree at least three), it is not even known whether every automorphism-invariant nitely dependent process is a factor of an i.i.d. process .","Finite dependence and nitary factors in computer science have practical applications, such as providing security benefits in network scenarios where machines are assigned specific roles. This concept ensures that if some machines are compromised, attackers cannot learn about the roles of other machines, enhancing security. Additionally, being a nitary factor of an i.i.d. process allows machines to determine their roles through a common protocol while using local randomness and communicating with a finite number of other machines. Various studies have explored the nitary coding properties of nitely dependent processes on different graphs, including Z and Z^d. Notably, research has investigated whether there are stationary nitely dependent processes that are not block factors of i.i.d. processes, with examples provided for hidden-Markov processes.

Holroyd and Liggett have constructed different colorings on Z and Z^d that are nitary factors of i.i.d. processes, with considerations for properties like translation and reflection invariance. They have also explored the existence of stationary nitely dependent processes on specific shifts of finite type layouts on Z. The coding radius, which represents the typical value of the proximity of roles in the assigned processes, has been examined, including exploring the power-law tail distribution for specific colorings on Z. 

Overall, existing results indicate challenges in obtaining nitary factors with finite expected coding radius for arbitrary nitely dependent processes on Z. The research highlights new findings on nitary coding properties for amenable graphs beyond Z and sheds light on the complexities related to non-amenable graphs like regular trees."
"i would like to thank omer angel, nishant chandgotia, tom meyerovitch and mathav murugan for useful discussions. i am especially grateful to nishant chandgotia for suggesting to extend the result from zd to transitive amenable graphs, and to omer angel for jointly proving lemma with me. i would also like to thank the referees for useful comments. finitely dependent processes are finitary","The text acknowledges Omer Angel, Nishant Chandgotia, Tom Meyerovitch, and Mathav Murugan for useful discussions. Nishant Chandgotia suggested extending the results to transitive amenable graphs, and Omer Angel collaborated on proving a lemma. The referees provided valuable comments. The text also mentions finitely dependent processes being finitary."
"throughout the paper, g is always assumed to be an innite, transitive, lo- cally nite, connected graph on a countable vertex set v, and is a subgroup of the auto- morphism group of g that acts transitively on v. the full automorphism group of g is de- noted by aut. the graph distance in g is denoted by dist. for sets u, v v, we write dist := minuu,vv dist and dist := dist. for r , denote v +r := {u v : dist r} and v r := {u v : dist > r}. the ball of radius r around v is denoted by r := {v}+r. the neighborhood of v is n := v + \ v and the edge-boundary of v is v := {{u, v} e : v v, u / v }. all logarithms are taken to be in base and we use the convention that log is","The paper assumes a graph g to be an infinite, transitive, locally finite, connected graph on a countable vertex set v, with g being a subgroup of the automorphism group that acts transitively on v. The notation used includes aut for the full automorphism group of g, dist for graph distance, and various symbols for sets and distances related to graph properties. Logarithms are considered to be in base e in this context."
"our goal is to express x, a nitely dependent invariant process, as a nitary factor of an i.i.d. process y . the construction of the nitary coding involves the use of three sources of randomness: a random number of random bits located at each vertex, a so-called cell process, and a random total order on v. the rst of these three will simply be given by an i.i.d. process, denoted y bits, while the latter two will be obtained as nitary factors of dierent i.i.d. processes, denoted y cell and y ord. in turn, y will be a triplet y = consisting of three mutually independent i.i.d. processes. to illuminate the main ideas behind our construction, we provide a sketch of the proof below, explaining separately three ingredients: constructing a nitary coding: the basic and most essential part of the construction is how to obtain x as a nitary factor of y when y is allowed to have innite entropy (this is the setting of theorem ). in this case, y bits v and y ord v may be taken to be uniform random variables in , and the total order may be taken to be the one induced by the usual order on y ord v . controlling the entropy: the second part is how to control the entropy of the y bits process, requiring only slightly more entropy than that of x. to postpone dealing with the issue of controlling the entropy of y ord, we shall assume in this part of the proof outline that the vertices of g can be deterministically ordered in a -invariant manner so that y ord may be disregarded entirely . for example, the lexicographical order is such an ordering when g is the graph zd and is the group of translations. constructing a random order: the third part is how to allow for graphs g and groups which do not admit such a deterministic order. this is of course the case for general graphs, but it may also be the case for simpler graphs, such as z or zd, when is the full automorphism group of g. in these cases, we are led to consider random orders with suitable properties. already the rst part above relies on the aforementioned cell process. before introducing this process, it is convenient to observe that it suces to prove theorem and theorem for -dependent random elds. to see this, let gk denote the graph on vertex set v in which two vertices are adjacent if their distance in g is at most k. it is immediate from the denitions that x is k-dependent as a random eld on g if and only if it is -dependent as a random eld on gk. since any automorphism of g is also an automorphism of gk, and since gk is amenable and satises whenever g is such, we see that we may indeed assume that x is -dependent. this assumption, though not at all essential, is convenient as it obviates the need to work with a dierent connectivity than the usual connectivity in g. a cell process is a random sequence a = of subsets of v satisfying the following properties almost surely: a a a . yinon spinka a a = v. for each n , all connected components of an are nite. we will obtain a cell process a as a nitary factor of an i.i.d. process y cell with arbitrarily small entropy. we do not explain here how this is done and refer the reader to section for more details and to figure for an illustration of the construction. constructing a nitary coding: we construct a realization of x as a nitary factor of y in innitely many steps with the idea that at the end of step n {, , . . . }, we will have dened x on the region an. in the rst step, we sample x on the set a this is particularly simple as the cells in a are at pairwise distance at least , and so, due to the -dependence assumption on x, each cell in a can be sampled independently. next, at each step n {, , . . . }, we sample x on the region an \ an, conditioned on the value of x on an, which has already been sampled in the previous steps the key observation here is that, due again to the -dependence assumption on x, the values of x on dierent cells in an are conditionally independent indeed, if {vi}i are at pairwise distance at least from one another, then for any sets ui vi, given {xui}i, one has that {xvi}i are mutually conditionally independent. since all the cells of every an are nite, the above steps can be carried out in a nitary manner that is, the conditional distribution of x on a given cell depends only on the previously sampled values within that cell. since an increases to v, the value at every given vertex will eventually be sampled, thus producing a realization of x from y in a nitary and -equivariant manner. let us be slightly more specic about the way in which we sample x on a cell. in each cell in a, we distinguish a vertex by choosing the smallest element in the cell according to the order given by y ord. we call these distinguished vertices level agents. similarly, for each n and each cell in an that is not contained in an, we select a level n agent in the cell by choosing the smallest element in the cell which is not in an note that the level n agents are obtained as a nitary factor of . with the notion of agents, we may now say more precisely that, in step n above, if c is a cell of an that is not contained in an, then we sample x on the region c \ an by accessing a sample of the desired distribution from the random variable y bits u , where u is the unique level n agent in c, using also the order induced by y ord on c \ an to break any symmetries which may be present in the graph structure of this region (for example, if g = z and includes reections, then when c \ an is a symmetric interval around u, the left and right sides of u cannot be dierentiated in a -equivariant way without some additional information; ordering all elements in the set is a simple way to get rid of such problems). in this interpretation, we regard y bits u as consisting of independent samples of p for all nite u, v zd and sv , most of which are never used in practice. controlling the entropy: it is clear from the last observation above that there is plenty of waste in the above construction . the problem is that we do not know ahead of time which samples of which distributions we will need access to. the basic solution to this is to place an innite sequence of random bits at each site, i.e., y bits v {, }n, from which we may easily construct samples of any desired distributions . of course, this idea alone still does not provide any control on the entropy of y bits. for this, we must place a nite number of random bits at each site, and somehow still be sure that we are able to construct the required samples. by a random number of random bits, we mean a random variable w taking values in {, }, the set of nite words over {, }, and having the property that, conditioned on the length |w| of the word, w is uniformly distributed on {, }|w|. suppose now that there exists a deterministic total order on v that is -invariant in the sense that u v implies that u v for any u, v v and . for instance, the lexicographical order is such an order when g = zd and is the translation group (but there is clearly no such order when is the full automorphism group of zd). for the purpose of this part of the proof outline, finitely dependent processes are finitary it is convenient to further suppose that every v v has a -successor, which we denote by v + , as is the case for the lexicographical order on zd ). the existence of such a deterministic order renders y ord unneeded, allowing us to focus now only on the task of controlling the entropy of y bits. the idea is to associate to each possible distribution we might require, a simulation which outputs a sample of the distribution in question from an input of unbiased random bits. the simulation is fed independent unbiased bits one-by-one, until at some point it halts and outputs the sample. such simulations may be done eciently: the expected number of input bits read by the simulation is bounded by the entropy of the target distribution, up to an additive universal constant. we shall use such simulations whenever we sample x on a cell. if the cell c is large, then the entropy of x on c is also large, and thus the above additive error is negligible. when the boundary of the cell is small in comparison to the size of the cell, the average entropy of x on c per site will also be close to h, the entropy of x itself. thus, it will be important that the cells in a are typically large with small boundary. this already shows that, in some sense, the average number of random bits needed to generate the samples required throughout the construction is very close to h. however, we must place a nite number of bits at each site < h + ), and even if we have more than h such bits at every site, it still may happen at some point during the construction that a simulation carried out by an agent u requires access to many more input bits than are available in y bits u . to solve this, we must allow to transfer bits from one location to another. this aspect of our construction is inspired by the algorithms in . the idea is that whenever an agent u requires access to an additional bit (beyond those available in y bits u ), it may look for an unused bit at u + . if there are no available unused bits at u + at that time, it may then proceed to look at u + , and so on. one consequence of the above description is that the steps of the construction cannot be directly related to the levels of the cell process. that is, it will no longer be the case that after step n of the construction, we will have dened x on the region an. instead, at any step of the construction, dierent regions of g will be at dierent levels of the cell process. we will continue to use n to denote the levels of the cell process, and will use t to denote the step of the construction (which we henceforth also refer to as time). the way this is done is as follows. initially, at time t = , all level agents are deemed active. an active level agent attempts to collect unused bits until its associated simulation halts, at which point in time the agent becomes inactive and is said to have completed level once all level agents contained in some level cell c have completed, the level agent associated to c becomes active. an active level agent proceeds in the same manner as an active level agent, attempting to read bits in order to complete its associated simulation. in general, a level n agent becomes active once all level n agents contained in its associated cell have completed. in our actual construction, it is more convenient to employ the following policy which makes the details simpler to write down: at time t, an agent u may read at most one bit, and this bit may only be read from site u + t. this has the advantage that it ensures that no two agents ever try to read bits from the same location simultaneously. constructing a random order: for a general graph g and group , there need not be a deterministic total order of v that is -invariant. instead, we construct a random total order on v whose distribution is -invariant. moreover, we construct as a nitary factor of an i.i.d. process y ord with arbitrarily small entropy. here nitary means that the order induced on any nite set of vertices is determined by a nite subset of {y ord v }vv. in addition, the constructed order will have the property that its order type is almost surely the same as that of z. that is, almost surely, every element v has a successor v+ and a predecessor v , and {v + n}nz = v. though it does not follow from the above denition of nitary, it will yinon spinka turn out to be the case that determining whether some vertex is the successor of some other vertex is also a nitary property (i.e., it is almost surely determined by a nite subset of {y ord v }vv). once such an order is at hand, the proof continues as outlined above. organization. in section , we introduce some preliminaries. in section , we prove the existence of a nitary cell process. in section , we prove the existence of a nitary random total order having the order type of z. in section , we give the construction of the nitary coding for the nitely dependent process x. finally, we end in section with some remarks and open problems.","The text outlines a proof for expressing a finitely dependent process x as a unitary factor of an i.i.d. process y. The construction involves using three sources of randomness: random bits, a cell process, and a random total order. The proof explanation is divided into three main parts: constructing a unitary coding, controlling the entropy of the random bits process, and constructing a random order for graphs that do not admit a deterministic order. The proof illustrates how x can be obtained as a unitary factor of y in infinitely many steps, with the construction focusing on sampling x in a unitary and -equivariant manner. Methods to control entropy and handle randomness in the process are discussed, along with the construction of a random total order on vertices. The proof is systematically organized and involves various technical details to achieve the desired outcome of expressing x as a unitary factor of y."
"recall that g is always assumed to be an innite, transitive, locally nite, connected graph on a countable vertex set v, and that is assumed to be a subgroup of the automorphism group of g that acts transitively on v.","In this section, the text defines the preliminary assumptions for the technical presentation. Specifically, it states that the graph g is infinite, transitive, locally finite, and connected on a countable vertex set v. Additionally, it assumes that is a subgroup of the automorphism group of g that acts transitively on v."
"the shannon entropy of a discrete random variable z is h := x z p log p, where the sum is taken over z in the support of z, or alternatively, we interpret log to be the measure-theoretic entropy of a -invariant random eld x on an amenable graph g is h := inf v v nite and non-empty h |v | . a flner sequence in g is a sequence n= of non-empty nite subsets of v such that lim n |fn| |fn| = it is well-known that the entropy of x may be computed along any flner sequence: h = lim n h |fn| for any flner sequence n= in g. it follows that for any > there exists > such that h |f| h + whenever f v is non-empty and nite and |f| |f|. of course, since entropy is maximized by the uniform distribution, we also have that h |f| log |s| whenever fv is non-empty and nite and e is an event with positive probability, where s is the nite set in which x takes values. we note that if y is an i.i.d. process, then its entropy h is equal to the entropy of its single-site distribution h.","The section discusses entropy in the context of discrete random variables and measure-theoretic entropy of a -invariant random field on an amenable graph. It presents the Shannon entropy formula and discusses how the entropy of a random field can be computed along flner sequences in a graph. Additionally, it mentions the relationship between entropy and the uniform distribution and highlights that for an i.i.d. process, the entropy is equal to the entropy of its single-site distribution."
"for u, v v, denote u,v := { : u = v}. note that u,u is the stabilizer of u. we say that is unimodular if |u,uv| = |v,vu| for all u, v v. it is well-known that is unimodular if and only if the following mass- transport principle holds: x u f = x v f for any diagonally -invariant function f : v . finitely dependent processes are finitary by diagonally -invariant, we mean that f = f for all u, v v and . we note the well-known fact that, when g is amenable, any transitive group of automorphisms is unimodular.","The text discusses the mass-transport principle in the context of unimodular structures. An unimodular structure is characterized by the equality of certain mass-transport relationships between elements. Specifically, the principle states that for a diagonally-invariant function f, the equality x u f = x v f holds for any u, v in the structure. The text also mentions that finitely dependent processes are finitary under the diagonally-invariant condition. Additionally, it notes that in the case where g is amenable, any transitive group of automorphisms is unimodular."
"we shall use a result about the simulation of a given distribution from unbiased random bits. let be a distribution on a countable set . a simulation of is a pair s = of measurable functions stime : {, }n n {} and sout : {, }n with the properties: if is a sequence of independent unbiased bits, then sout has distribution . if stime = n for some x {, }n and n n, then stime = n and sout = sout for any x {, }n which coincides with x on {, . . . , n}. the rst property says that we can use s to simulate the desired distribution from random bits. the second property may be interpreted as saying that stime is a stopping time and that sout is adapted to the -algebra generated by {i : i stime} that is, the simulation reads one input bit at a time, and once the stopping time is reached, the output is determined only by the bits that have already been read. the following theorem follows from a result of knuth and yao (see theorems and there and the corollary just after). theorem . let z be a discrete random variable. there exists a simulation s of z from inde- pendent unbiased bits satisfying that stime < almost surely and estime h + knuth and yao show that the above is in fact optimal in a strong sense (they provide a simulation whose stopping time is stochastically dominated by that of any other simulation). a version of this theorem was proved in via a more concrete construction. the results in also provide explicit exponential bounds on the probability that the simulation uses more than n bits, but we shall not need this.","A distribution on a countable set can be simulated from unbiased random bits using measurable functions. The simulation includes functions for reading input bits and generating output bits, ensuring that the desired distribution is replicated accurately. The simulation's stopping time and output are constrained by the bits read, guaranteeing accurate results. Knuth and Yao demonstrated the optimality of their simulation method, which outperforms other methods in terms of stopping time. The simulation of a discrete random variable from independent unbiased bits can be achieved with a stopping time less than n almost surely. Additional results offer exponential bounds on the number of bits used in the simulation."
"recall from section that a cell process is a random sequence a = of subsets increasing to v and satisfying that the connected components of each an are nite. we may identify a cell process a with the n-valued random eld vv. in particular, when we say that a is -invariant or a nitary factor, we mean that this latter process is such. in this section, we show that nitary cell processes with arbitrarily small entropy exist on any transitive amenable graph. proposition . let g be a transitive amenable graph and let > there exists an i.i.d. process y of entropy at most and a cell process a which is a nitary aut-factor of y . let us mention that in the special case of g = zd, several parts of the argument below can be skipped or simplied, thereby leading to a shorter proof. in the general case, certain technicalities arise which make the proof somewhat longer. the reader may therefore wish to have in mind the case of g = zd on a rst reading. before giving the proof, let us explain the idea behind it; see figure for an illustration. the main idea of the construction is to use the points of a low-density bernoulli process to construct voronoi cells , which are then used as the cells of a . using another bernoulli process of even lower density, we again construct voronoi cells, which are then used to obtain a from a by lling in some of the empty space between the cells of a, taking care not to connect cells of a which are not in the same voronoi cell (thus ensuring that an innite cluster is not created). repeating in this manner, we obtain an increasing sequence a a of sets as a nitary factor of a small entropy i.i.d. yinon spinka a going from a to a a figure constructing the cell process. the cells of a are simply the voronoi cells of a bernoulli process. to get from a to a, we consider the voronoi cells of a lower-density bernoulli process, and merge cells of a which are entirely contained in any such voronoi cell. repeating this procedure produces the cell process. the green shade depicts regions belonging to the cell process. the dark green depicts cells of a which are not cells of a process. it will then only remain to show that an increases to v. this is where amenability comes into play. to ensure that an increases to v, we must be careful in how we dene the voronoi cells. when the graph g has a flner sequence consisting of balls (as is the case when g has subexponential growth), the voronoi cells may be taken with respect to the graph distance in g. however, for the general case considered here, we must adapt the usual voronoi cells to a certain metric (which is not necessarily a true metric) given by a suitable flner sequence. since we will need this metric to be diagonally -invariant, we need to choose a flner sequence n having the property that each fn is invariant under the stabilizer of some xed vertex v, i.e., fn = ,fn for all n. it is a simple observation that any graph g of subexponential growth has such a sequence for any , and that the cayley graph g of a nitely generated amenable group also has such a sequence . as it turns out, such a flner sequence always exists in an amenable graph, even when is its full automorphism group. the following lemma shows that every amenable graph admits a flner sequence consisting of sets which are invariant under the stabilizer of some vertex. the lemma was obtained jointly with omer angel. lemma . let g be an amenable graph, let be the full automorphism group of g and let v. there exists a flner sequence n in g such that fn = ,fn for all n. lemma easily follows by applying the following lemma to each set of some flner sequence, taking to be the stabilizer , of lemma . let g be any graph and let be a group of automorphisms of g under which all orbits of v are nite. for any nite non-empty set f v, there exists a nite non-empty set e v such that |e| |e| |f| |f| and e = e. proof. we show the existence of the desired e via a probabilistic method that is, we choose a random subset e of v and show that it satises the desired properties with positive probability. finitely dependent processes are finitary let {vi}i be the orbits of v under the action of thus, {vi}i is a partition of v such that each vi is nite and satises vi = vi. let u be a uniform random variable in and dene e := [ i:upi vi, where pi := |f vi| |vi| . thus, each orbit vi is included in e with probability pi, where the choices for dierent i are positively correlated through the use of the common variable u. then e|e| = x i p |vi| = x i pi|vi| = x i |f vi| = |f|, and e|e| = x i,j p || = x i,j max{pi pj, } ||, where denotes the set of edges between two disjoint sets u and v . let us show that each term in the second sum is at most ||, i.e., pi pj || || whenever pi > pj and = . to see this, note that the right-hand side is the probability that an edge e = {u, v} that is uniformly chosen from belongs to , or equivalently, with the convention that u vi and v vj, that u f and v / f. since this probability is at least pp, it suces to show that u and v are uniformly distributed in vi and vj, respectively. this follows from the observation that the bipartite graph ) is biregular each vertex in vi is adjacent to the same number of vertices in vj, and similarly, each vertex in vj is adjacent to the same number of vertices in vi. indeed, for any u, v vi and such that u = v, the mapping w w denes a bijection between n vj and n vj. we thus conclude that e|e| x i,j || = |f| = e|e|, where := |f|/|f|. thus, |e||e| is a random variable with non-negative expectation. since |e| |e| is zero when e is empty, conditioned on e = , we still have that |e| |e| has non-negative expectation. in particular, there is positive probability that e = and |e| |e|. finally, since e is nite and satises e = e almost surely, we see that e satises the desired properties with positive probability. suppose that n is a flner sequence guaranteed by lemma , i.e., fn = ,fn for all n, and further suppose that f f and f f = v (there is clearly no loss in generality in doing so). recall the denition of u,v from section . for u, v v, dene := min  n : v ,ufn . using that ,u = ,u for any , one easily checks that is diagonally -invariant, i.e., = for all . we stress that is not necessarily symmetric in that may not equal . in particular, we do not claim that is a metric. nevertheless, we still think of as a measure of distance from v to u. one nice property of that is easily veriable and which will be important is that, for any sequence of pairs of vertices i=, we have as i if and only if dist as i . the fact that may not be symmetric presents a certain challenge in the proof, for which we require the following lemma to address. we note that is indeed symmetric when the flner sequence n consists of balls, and that it is nearly symmetric when g is a cayley graph of yinon spinka in which case switching the roles of u and v in has the same eect as replacing each fn with f n . accordingly, in these cases, it is immediate that the two -balls of radius n around , {v : n} and {u : n}, have the same size, namely |fn|. the following lemma shows that this is in fact always true in our setting. lemma . let g be a transitive amenable graph and let be the full automorphism group of g. let f v be invariant under the stabilizer of some vertex , i.e., ,f = f. then |{u v : ,uf}| = |f|. proof. dene f : v by f := {v,uf}. since ,u = ,u for any , it follows that f is diagonally -invariant. thus, by the mass-transport principle , |{u v : ,uf}| = x u f = x v f = |,f| = |f|. we are now ready to give the proof of proposition . proof of proposition . let be a sequence to be chosen later which satises that n n. we shall construct a cell process a as a nitary factor of the i.i.d. process y = vv in which yv = n are independent random variables with yv,n ber. the entropy of y can be made arbitrary small, since h = h = x n= h = x n=  n log n + log n  log . as explained, the idea of the construction is to use the points in un := {v v : yv,n = }, for any given n, to construct voronoi cells, which are then used to dene the cells of an. precisely, we dene the voronoi cells of a non-empty set u v by cu := n v v : < for all u u \ {u} o , u u. thus, the voronoi cell cu associated to u consists of all vertices v v which are closer (in the distance measured by ) to u than to any other u u. in particular, voronoi cells associated to dierent vertices in u are disjoint, but they are not necessarily separated (they could be adjacent to one another). we therefore dene modied voronoi cells by slightly shrinking the sets cu. precisely, we dene cu := ( cu) using that the voronoi cells are disjoint, it is straightforward to check that dist, cu) > for distinct u, u u. we note that cu (in fact, already cu) may be empty and need not be connected. before proceeding with the construction of the cell process, let us rst show that the voronoi cells of u are almost surely nite whenever u is the set of points of an i.i.d. bernoulli process. to this end, it suces to show that the probability that cu intersects fn \fn is summable over n. indeed, since a xed vertex v fn \ fn belongs to cu only if u \ {} contains no element of {u : n}, it follows from lemma that the probability of this is at most p|fn|, where p is the density of the bernoulli process. since |fn| n, we see that |fn| p|fn| is summable, and hence that the cu is almost surely nite. finitely dependent processes are finitary we now turn to the construction of the cell process a. the rst level set in the cell process is simply taken to be the vertices in a modied voronoi cell of u, i.e., a := [ uu cu since the voronio cells of u are almost surely nite, we see that cu is almost surely nite for all u u since dist, cu) > for distinct u, u u, it follows that all connected components of a are almost surely nite. suppose now that, for some n , an has been dened in such a way that all connected components of an are almost surely nite, and let us now dene an+ let a n+ be the union of the modied voronoi cells of un+, i.e., a n+ := [ uun+ cun+, and recall that all connected components of a n+ are almost surely nite. intuitively, we would like to obtain an+ from an by adding a n+ however, this might create innite clusters, and we must take care to avoid this by instead only adding a suitable subset of a n+ it will suce to slightly increase the forbidden region (a n+)c as follows: let dn+ denote the union of the connected components of an that intersect (a n+)c, and add d+ n+ to the forbidden region. precisely, we dene an+ := an (a n+ \ d+ n+). it is straightforward to check that all connected components of an+ are almost surely nite. assuming that a a = v almost surely, it is also easy to check using that a is a nitary aut-factor of y , which would complete the proof of the proposition. it remains only to show that a a = v almost surely. by aut-invariance, this is equivalent to the fact that p as n . for n , let bn denote the connected component of in an {}. for n , dene the event en := n bn cun for some u un o . note that { an \ an} = en { / an}. thus, it suces to show that p(en | / an) c for some c > and all n as we now show, this holds when n is suitably chosen. since bn is almost surely nite, there exists a suciently large rn so that p(bn rn | / an) , where r := r. since s is a flner sequence, there exists sn suciently large so that |fsn| n and |fsn| |fsn| |rn|. set n := |fsn| then, noting that an is independent of un, p(en | / an) p  bn rn and rn cun for some u un | / an  = p  bn rn | / an  p  rn cun for some u un  . since the rst term on the right-hand side is at least by the choice of rn, it remains to show that, for some constant c > which does not depend on n, we have p  rn cun for some u un  c. yinon spinka let us rst see how to show this when fsn is a ball, say . in this case, it is not hard to see that the event in question occurs when un has a unique point in rn and no other point in +rn, so that p  rn cun for some u un  p  |un rn| = |un +rn| =  = p  ber =  p  ber =  c. we now handle the general case in more detail. set u := un. our goal is to bound from below the probability that rn cu for some u u. to this end, we rst nd a simple condition that implies the occurrence of this event. for a set f v, denote m := {u v : ,uf}. set r := rn and s := sn. let us show that |u m(f r s )| = |u m(f +r s )| = = r cu for some u u. suppose that the left-hand side holds. let us show that r cu, where u is the unique element in u m(f r s ). by the denition of cu, this is equivalent to r cu. recalling the denition of cu, we see that we must show that < for all u u \ {u} and w r. let u u \ {u} and w r. it suces to show that s and > s. towards showing this, we rst note that + = and = for any and v v, due to the fact that acts by an automorphism of g. in particular, +r = ,u and r = ,u, and we may drop the parenthesis when writing such terms. let us now show that s. since u m(f r s ), we have that ,uf r s , or equivalently, r ,ufs. since w r, it follows that s. next, we show that > s. note that u / m(f +r s ) since u m(f r s ) m(f +r s ). thus, / ,uf +r s , or equivalently, r ,ufs = . thus, w / ,ufs and, using that f, f, . . . , fs fs, it follows that > s. using , we obtain p  rn cun for some u un  p  |un m(f rn sn )| = |un m(f +rn sn )| =  = p  ber(|m(f rn sn )|, n) =  p  ber(|m(f +rn sn )| |m(f rn sn )|, n) =  . by lemma , we have that |m(f rn sn )| = |f rn sn | and |m(f +rn sn )| = |f +rn sn |. finally, by , |f rn sn | |fsn| |fsn| |rn| |fsn| and |f +rn sn | |fsn| + |fsn| |rn| |fsn|, so that, by standard estimates for bernoulli random variables, both probabilities in question are bounded below by a positive constant. the above proposition established the existence of a nitary cell process a. in particular, an is an invariant set which has high density when n is large. the following proposition shows that the clusters of a dense invariant set typically have relatively small boundary. lemma . let g be a transitive graph of degree d and let be a transitive unimodular group of automorphisms of g. let b v be a random set with no innite clusters and whose distribution is -invariant. let cv denote the cluster of v in b. then, for any > , p  |c| |c|  ( d + ) p( / b). finitely dependent processes are finitary proof. the proof uses the mass-transport principle. dene : v by := cv| |cv| if u / b, v b otherwise . note that, almost surely, x u = b |c| x u/ b |n c| = |c| |c| b and x v = / b x vb |n cv| |cv| = |n b| / b d / b. the -invariance of b implies that f := e is diagonally -invariant. thus, the mass- transport principle yields that e h |c| |c| b i d p( / b). the proposition now follows from markovs inequality. remark a result of h aggstr om states that any automorphism-invariant edge percolation on a d-regular tree with edge-density at least /d has an innite cluster with positive probability. in particular, automorphism-invariant cell processes do not exist on such a tree. moreover, by lemma , we see that -invariant cell processes do not exist on any transitive unimodular non-amenable graph. in fact, it is shown in that a closed subgroup of aut is amenable if and only if there is a - invariant site percolation on g with with no innite clusters and density arbitrarily close to it follows that a -invariant cell process on g exists if and only if is amenable. the site percolation constructed in is a factor of an i.i.d. process, though it is not nitary.","A cell process is a random sequence of subsets satisfying specific conditions on connected components. In this section, it is shown that nitary cell processes with low entropy exist on any transitive amenable graph. The construction involves using Bernoulli processes to create Voronoi cells, which are then merged to form the cell process. The proof relies on properties of amenable graphs and automorphism groups. The cell process is constructed incrementally to ensure finite connected components. The text also discusses the construction of dense invariant sets and the behavior of clusters within them. Finally, it is highlighted that automorphism-invariant cell processes do not exist on certain graphs unless they are amenable."
"in this section, we construct a random total order on v that has the following properties: it is a nitary factor of an i.i.d. process with arbitrarily small entropy. it is supported on total orders having the same order type as z. the successor/predecessor of any vertex can be found in a nitary manner. let us explain these properties. a random total order on v, or more generally, a random binary relation on v, may be regarded as a random element in {, }v with this viewpoint, the notion of nitary factor easily applies to such relations. namely, such a relation is a -factor of y if it has the same distribution as for some measurable function : t v {, }v satisfying that = for all , u, v v and y t v. such a factor is nitary if for every u, v v there almost surely exists a nite set w v such that is determined by ww , in the sense that = for any y t v which coincides with y on w. a total order on v has the same order type as z if there is an order preserving bijection between the two ordered spaces, i.e., a bijection f : v z such that f f if and only if u v. this may be equivalently formulated as saying that has no minimum or maximum and that there are nitely many elements between any two elements, i.e., every interval of the form {w v : u w v} is nite. in particular, in such an order, every vertex v has a successor (an element w v such that u w for all u v) and a predecessor (an element w v such that u w for all u v). given a factor from y to a random total order on v, we say that successors can be found in a nitary manner if for every u, v v there almost surely exists a nite set w v such that the event that u is the -successor of v is determined by yinon spinka ww . we note that, in general, there is no direct relation to the notion of nitary factor: it may be that such a factor is nitary though successors/predecessors cannot be found in a nitary manner, or it may that successors/predecessors can be found in a nitary manner though the factor is not nitary. on the other hand, for a total order having the order type of z almost surely, the second implication is easily seen to hold if successors/predecessors can be found in a nitary manner, then the factor is necessarily nitary. a total order which is a nitary factor of an i.i.d. process with innite entropy is easily obtained from the order induced by uniform random variables in assigned to each vertex. it is easy to see that this order almost surely has the same order type as q. a total order (also with the order type of q) which is a nitary factor of an i.i.d. process with nite entropy was constructed in for any quasi-transitive graph satisfying a geometric condition similar to . the application in did not require the i.i.d. process to have arbitrarily small entropy and so this was not stated there, though it easily follows from the proof there that this is possible. since the proof is short, we give it here. the following is essentially a reformulation of for our situation. lemma . let g be a transitive non-empty graph satisfying . for any < there exists a total order on v which is a nitary aut-factor of an i.i.d. bernoulli process with density . proof. let = vv be an i.i.d. bernoulli process with density . for any v v, dene zv = n {, , . . . }{,, } by zv,n := x uv:dist=n u. dene a relation on v in which u v if and only if zu zv, where denotes the lexicographical order on {, , . . . }{,, }. then is clearly a aut-factor of . it remains to show that is almost surely a total order on v and that the factor is nitary. since is clearly a preorder, to show that it is a total order, it suces to show that p = for distinct u, v v. it then follows from the denition of the lexicographical order that the factor is nitary. fix u, v v distinct and consider the event en := n \ i= {zu,i = zv,i}. since p p as n , it suces to show that p for all n by and the assumption that the graph is non-empty, we have n \ n n, as otherwise n n, which in turn implies that n = n by transitivity. thus, there exists some wn n \ n). then p  en | v\{wn}  max kz p = max{, } = . since en is measurable with respect to v\{wn}, it follows that p . using lemma and the cell processes constructed in the previous section, we are able to construct a total order satisfying all three properties described above. lemma . let g be a transitive amenable non-empty graph satisfying and let > then there exists an i.i.d. process y with entropy at most , and a random total order on v which almost surely has the same order type as z, such that is a nitary aut-factor of y for which successors/predecessors can be found in a nitary manner. proof. by lemma , there exists a total order on v that is a nitary factor of an i.i.d. process y having entropy at most . by proposition , there exist a cell process a that is a nitary factor of finitely dependent processes are finitary an i.i.d. process y having entropy at most . we construct the required total order on v as a nitary factor of . the idea is to use to order the sites within the cells given by a. more precisely, we will dene an increasing sequence of partial orders . . . such that each n induces a total order on every cell of an. since s n an = v, this will produce a total order given by the union s n n, which we will show has the desired properties. precisely, we dene to be the relation in which u v whenever u and v belong to the same cell of a and satisfy that u v. then is clearly a partial order that induces a total order on any cell of a in fact, it is the union of these total orders on the cells of a (that is, it only compares vertices that belong to the same cell). next, suppose we have dened the partial order n so that it is a union of total orders on the cells on an, and let us dene n. consider a cell c of an and let d := c an = d dk be the union of the cells d, . . . , dn of an that are contained in c. we dene n in such a way that d n c \ d by requiring that u n v whenever u d and v c \ d. to obtain a total order on c, it remains to order the vertices in d and the vertices in c \ d. the latter is ordered by dening u n v whenever u, v c \ d and u v. the former is ordered by giving an order to the cells d, . . . , dk and using the n order within each cell that is, we require that n coincides with n on each cell di, and that either di n dj or dj n di for any two cells di and dj. finally, the order of the cells is determined by requiring that di n dj whenever mindi mindj. that is, the i-th cell precedes the j-cell in n if and only if the -minimal element in the i-th cell is -smaller than the -minimal element in the j-th cell. it is straightforward that n extends n and that n is a union of total orders on the cells of an. we have thus obtained partial orders , , . . . such that, for each n, n extends n and is a union of total orders on the cells of an. to show that has the order type of z, it remains to show that, almost surely, every -interval is nite and there is no -minimum and no -maximum. it follows from the construction that if u is the n-successor of v, then it is also its n+-successor. thus, to conclude that every -interval is nite, it suces to show that, for every u, v v having u v, there exists n such that u n v and the interval n is nite. indeed, since n only compares vertices within the same cell of an and since all such cells are nite, all n-intervals are nite. finally, no minimum or maximum can exist as this would contradict the invariance of . we have thus established that almost surely has the same order type as z. it is straightforward from the fact that the n-successor of a vertex u is also the n+-successor of u, that the constructed factor from to has the property that successors/predecessors can be found in a nitary manner. as mentioned in the beginning of the section, this implies that the factor is also nitary.","The section describes the construction of a random total order on a set v with specific properties. It mentions that the total order is a nitary factor of an i.i.d. process with arbitrarily small entropy and is supported on total orders with the same order type as z. It explains how the successor and predecessor of any vertex can be found in a nitary manner. The text discusses the concept of nitary factors and order types, as well as the process of finding successors in a nitary manner. It also provides a proof for the construction of a total order that fulfills the specified properties."
"in this section, we construct a nitary coding for nitely dependent processes. we present the details of the proof of theorem . theorem may be proved in a similar manner (see remark in section ). let x be a -invariant nitely dependent process taking values in a nite set s. recall from the proof outline in section that we shall construct a nitary factor from an i.i.d. process y = to x. recall also that a will be a cell process that is a nitary factor of y cell and that will be a random total order on v that is a nitary factor of y ord, has the order type of z, and for which successors/predecessors can be found in a nitary manner. given the cell process a and the total order , we will use the additional randomness in y bits to construct a realization of x. yinon spinka","A nitary coding is constructed for nitely dependent processes in this section. The proof of the theorem is explained, with the possibility of proving a similar theorem as well. A -invariant nitely dependent process x, with values in a finite set s, is considered. A nitary factor is constructed from an i.i.d. process y that is a random total order on v and has the order type of z. This factor allows for successors/predecessors to be found in a nitary manner. Additional randomness in y bits is utilized to construct a realization of x using the cell process and the total order."
"fix > we shall choose the i.i.d. processes y bits, y cell, y ord to satisfy h < h + , h and h so that y has entropy h < h + we let y bits be any i.i.d. process in which y bits v is a random number of random bits satisfying h(y bits v ) < h + and e|y bits v | > h + recall that, by a random number of random bits, we mean a random variable w taking values in {, }, the set of nite words over {, }, and having the property that, conditioned on the length |w| of the word, w is uniformly distributed on {, }|w|. the desired random word can be obtained by taking w to be the empty word with probability p or a uniformly chosen sequence in {, }m with probability p, for some suitably chosen m and p < indeed, in this case, h = p log p log and e|w| = pm so that h and e|w| h + as m when p = m + ). since the entropy and length of w are related via h = e|w| + h, we see that holds when m is suciently large. let > be as in . by decreasing , we may additionally assume that < . recall from section that we may assume without loss of generality that x is -dependent. by proposition , there exists a cell process a and an i.i.d. process y cell of entropy at most such that a is a nitary factor of y cell. since p as n , lemma implies that p| |an|) as n , where an is the cell of in an. thus, by replacing with for some large m, we may assume that p| |a|) is arbitrary small. specically, we require that p  |a| |a|  < log |s| + let y ord be an i.i.d. process with entropy at most and let be a total order on v as guaranteed by lemma (note that if the graph g contains no edges, then x is already an i.i.d. process so that there is nothing to prove). . the construction of the nitary coding. recall that the n-th -successor of v is denoted by v + n and its n-th -predecessor by v n. in particular, v n are random elements of v which are determined from y ord is a nitary manner. recall also that the cell process a is a nitary factor of y cell. it may be helpful from this point onward to think of a and as given, and that our goal is to use them, together with the random bits of y bits, to construct a realization of x. we shall dene, for every time t and every vertex u v, a random variable lt u {, }. we shall dene these inductively with the t = variables given by l u := {u is a level agent and |y bits u |>}. we think of lt u as indicating whether u read a bit at time t. in particular, if lt u = for some t and u, then necessarily u is an agent . since at time an agent looks for an available bit at its own location, says that any level agent reads a bit at time if such a bit is available. before giving the main denitions of the construction, we rst set up some auxiliary notation and denitions. as we have already mentioned, our construction has the property that if an agent u reads a bit at some time t, then the bit it read is located at u + t, i.e., it is one of the bits of the word y bits u+t . in particular, the total number of bits read from location v by time t is mt v := l v + l v + + lt vt. finitely dependent processes are finitary the bits at any location v are read sequentially the rst agent to read a bit at v will read y bits v , the second will read y bits v and so on. precisely, the bit read by u at time t is w t u := ( y bits u+t (mt u+t) if lt u = otherwise . for this to be well-dened, we must make sure that u does not try to read a non-existent bit we mention already here that this does not occur, i.e., our denitions will ensure that mt v |y bits v | for all v v and all t the word read by u by time t is then w t u := w u w u w t u, where denotes concatenation. that is, w t u is the word obtained by concatenating the bits read by u until time t in the order they were read. in particular, w t u is a word in {, }of length |w t u| = l u + l u + + lt u. we emphasize that (mt u, w t u)uv is well-dened once (li u, ni u)uv,it is dened, as the former are functions of the latter and of y bits. as explained in the proof outline, we use simulations to obtain samples of distributions from random bits. we rst equip ourselves with simulations of all the possible distributions we may require throughout the construction of the nitary coding. the basic distributions we need are those of xv for a nite set v v. as we aim to obtain a -equivariant factor, we must take care when dealing with random elements of sv , as these are indexed by subsets of vertices. it would be more proper to view xv as a random element of s|v | by using the order . precisely, we proceed as follows. recall the denition of a simulation from section . by theorem , for every ordered sequence v, . . . , vm v of distinct vertices, there exists a simulation s of sm satisfying that estime h + since the distribution of x is -invariant, we may suppose that s = s for all (e.g., by choosing a simulation for a single representative of each orbit, and then setting s to equal the simulation of its representative). now, for a nite set v v, we let v, . . . , vm be the vertices of v , ordered according to , and set sv to be the simulation s, where, for notational convenience, we interpret sout v as an element of sv through the identication sout v vi = sout i. we stress that sv implicitly depends on the order . as we will also encounter situations in which regions of x have already been sampled, we will also need simulations of the distribution of xv conditioned on xu for some nite set u v which is disjoint from v . thus, for every such v and u and every su, we similarly let sv,u, be a simulation of p satisfying that estime v,u, h + for ease of notation later on, we allow v and u to intersect and we allow to have any domain containing u, by interpreting sv,u, as sv \u,u,u in such a case. we also identify sv with sv,,. recall that every cell c in an that is not contained in an has an associated level n agent, and that this agent is responsible for generating the output on c \ an we denote by an the cell of v in an, where an := if v / an, by un the set of level n agents and, for v an \ an, by un the level n agent associated to the cell an. with the above notation and denitions, we may now proceed to construct the nitary coding. our goal is to dene a random eld zt = (zt v)vv, which represents the output at time t. this output will be a function of (li u, mi u, w i u)uv,it (and of course of the cell process a and the total order ). once zt is dened for some t, it will then only remain to inductively dene (lt+ u )uv, as this then also denes (mi u, w i u)uv,it+ through the denitions above. this will therefore dene yinon spinka zt+ as well. we will then take a limit as t in order to obtain the output z = vv, which is the desired realization of x. to facilitate the inductive denition of (lt+ u )uv, we require some more denitions. we now regard t as xed and suppose that (li u)uv,it, and hence also (mi u, w i u)uv,it, are already dened. we dene two notions for a level n agent: that of having reached level n at time t, and that of having completed level n of the simulation by time t. we dene these notions inductively on n. we thus begin with level agents. given a level agent u u, we say that u reached level by time t . u completed level by time t if the stopping time stime a has been reached on input w t u, once a level agent has completed level of the simulation, the output is known on the corre- sponding level cell of the agent. that is, if a level agent u completed level by time t, then we will have zt v = sout a(w t u,)v for all v a to be more precise, let us dene zt, = (zt, v )vv by zt, v := ( sout a(w t u)v if v a for some u u and u completed level by time t otherwise . we will soon also dene zt,n = (zt,n v )vv for n , with the idea that it represents the known output on all cells of level at most n for which the simulation has completed. in particular, if zt,n v = for some v and n, then it will be the case that zt,n+ v = zt,n v . now x n and suppose that we have dened zt,, . . . , zt,n and the two notions for levels less than n, in such a way that the above property holds namely, if a level m {, . . . , n } agent u completed level m by time t, then the output is known on am at time t in the sense that zt,n v = zt,m v = for all v am. then, for a level n agent u un, we say that u reached level n by time t if every level n agent u un an has completed level n by time t. the idea here is that if u reached level n by time t, then the output is known on an an at time t, and we may use this information to start generating the output on the remaining part of the cell, namely, on an \ an that is, we use the simulation sv,u, with v = an, u = an an and = zt,n we thus say that u completed level n by time t if it reached level n by time t and the stopping time stime an,anan,zt,n has been reached on input w t u. putting this together leads to dening zt,n = (zt,n v )vv by zt,n v := ( sout an,anan,zt,n(w t u)v if van\an and un=u for some u and u completed level n by time t otherwise . the output zt = (zt v)vv at time t is then dened by zt v := limnzt,n v . that is, to determine zt v, we rst look at the level n at which v enters the cell process, and then consider the level n agent u responsible for generating the output on the cell of v in an. if u has indeed completed level n by time t, then we read the value of zt v from the output of the corresponding simulation. finally, we are ready to dene (lt+ u )uv. as mentioned, these numbers are always zero for non-agents, i.e., we set lt+ u := for u / u u . suppose now that u un for some n we say that u is active at time t + if it has reached, but has not completed, level n by time t. thus, if u is active at time t + , then ideally it would like to read a bit at that time, and indeed it finitely dependent processes are finitary may do so as long as there is an available bit at u + t + (recall that u may only read a bit from location u + t + at time t + ). this leads us to dene lt+ u :=  u is active at time t + and mt u+t+ < |y bits u+t+|  . this completes the inductive denition of lt+ u for all t and u v. we note that, by construction, once the output at a vertex is determined at some time, it remains unchanged at future times that is, if zt v = for some v and t, then zt+ v = zt v. denote by tv := min{t : zt v = } the time at which the output at v is rst determined. the output z = vv is then given by zv := lim tzt v = ( ztv v if tv < if tv = . this completes the construction of the nitary factor. we record for later use the following simple property of the construction. lemma . for any t , we have that (w t u, zt u)uv and (lt+ u )uv are measurable with respect to y cell, y ord, (|y bits v |)vv and (y bits v )vv,imt v. proof. the proof by induction on t is straightforward from the denitions.","The text discusses the process of choosing parameters for constructing a nitary coding system. It involves selecting i.i.d. processes for bits, cells, and order to satisfy specific entropy conditions. The construction of the coding involves defining random variables and simulations to obtain samples and distributions. The goal is to create a realization of the output using these parameters and simulations, with a focus on level agents reaching and completing levels of the simulation. The final output is determined based on the completion of levels and active agents at different times. The construction ensures that the output at a vertex remains unchanged once determined, leading to the final realization of the nitary factor."
"to conclude the proof of theorem , we must establish two properties of the above construction: that the output it produces has the desired distribution, and that the output can be determined from in a nitary manner. the former is stated in the following proposition whose proof is postponed to section below. proposition . the output z has the same distribution as x. proof of theorem . the random eld z is clearly a deterministic and -equivariant function of . thus, in light of proposition , we must only show that is nitary. since tv is almost surely nite , it suces to show that zt is nitary for every t this follows rather easily from the construction. to see this, we explain how to determine the value of zt v in a nitary manner. we begin by nding the level n in which v enters the cell process, i.e., v an \ an, and then nding the cell an of v in an. since a is a nitary factor of y cell, this may be done in a nitary manner. next, we nd the level n agent un associated to the cell an. since this is just the -minimal element in an \ an, and since the order is a nitary factor of y ord, this may also be done in a nitary manner. let us suppose by induction that all steps of the construction up to time t are nitary. thus, recalling the denition of active, we see that, for any vertex w, we may determine in a nitary manner whether w is active at time t. since successors/predecessors in may be found in a nitary manner from y ord, it then also follows that lt w may be determined in a nitary manner. using again that successors/predecessors may be found in a nitary manner, we conclude that w t w may be found in a nitary manner. we would now like to check whether u completed level n by time t, and if so, nd the output value. to check this, we start at level and work our way up to level n. thus, we rst nd all level agents which are contained in an (since the cell process and total order are nitary, this can be done in a nitary manner). next, for each such agent u, we check whether u completed level by time t. recall that the simulation sa depends on the cell a and on the order induced by on a since the input word w t u, the cell process and the order are nitary, we see that we may determine whether u completed level by time t in a nitary manner, and if so, also determine the output zt, w for all w a in a nitary manner. yinon spinka we now proceed to the next levels. consider some level m n. we again begin by nding all level m agents which are contained in an. for each such agent u, we check whether u reached level m by time t. for this we must check whether the level m agents in am completed level m by time t, which, by induction, may be done in a nitary manner. if u reached level m by time t, we then check whether u completed level m by time t. similarly to before, the simulation sam,amam,zt,m depends on am, am\am, the order induced by on am, and on (zt,m w )wamam since the input word w t u, the cell process and the order are nitary, we see that we may determine whether u completed level m by time t in a nitary manner, and if so, also determine the output zt,m w for all w am in a nitary manner. continuing up to level m = n yields that zt,n v may be determined in a nitary manner. since n is the level in which v enters the cell process, we have by denition that zt v = zt,n v . thus, zt v may be determined in a nitary manner, as required.","The text discusses the concluding proof for Theorem 1.2, focusing on establishing two key properties of the construction: the desired output distribution and determinability in a nitary manner. The output z is shown to have the same distribution as x, and it is proven that zt can be determined in a nitary way for every t. The process involves finding the level n of entry into the cell process, determining associated agents, checking completion at different levels, and ultimately showing that zt v is nitary. The construction demonstrates that zt v can be determined in a nitary manner, meeting the requirements of the proof for Theorem 1.2."
"in this section, we prove proposition . the proof is split up into several steps. the rst step is the following lemma which formalizes the intuition that the simulations used in the construction are fed independent unbiased bits. lemma . let {, }n consist of a sequence of independent unbiased bits. let uv be a collection of i.i.d. copies of , independent of . then, for any t , conditioned on , the collection (w t u u)uv has the same distribution as uv. proof. we prove the statement by induction on t, taking t = as a trivial base case (where w u := for all u v). suppose now that we know it for some t and let us show it for t+ recall that w t+ u = w t u w t+ u . thus, we need to show that, conditioned on , the collection (w t u w t+ u u)uv has the same distribution as uv. to this end, it suces to show that, conditioned on , the collections (w t u)uv and ( w t+ u u)uv are independent and that the conditional distribution of the latter is that of uv. indeed, the induction hypothesis will then yield the desired result. we may restate our goal as showing that, conditioned on and (w t u)uv, the collection ( w t+ u u)uv has the same distribution as uv. let f be the -algebra generated by y cell, y ord, (|y bits v |)vv and (y bits v )vv,imt v. by lemma , (w t u)uv and q :=  u : w t+ u = =  u : lt+ u = are f-measurable. since uv is independent of and hence also of f, it suces to show that, conditioned on f, the random variables ( w t+ u )uq are independent unbiased bits. note that q is the set of vertices that read a bit at time t + , that q := {v : mt+ v > mt v} is the set of vertices from which a bit was read at time t + , and that u u + t + denes a f-measurable bijection from q to q. recall also that w t+ u = y bits u+t+(mt+ u+t+) for u q. thus, it suces to show that, conditioned on f, the random variables (y bits v (mt+ v ))vq are independent unbiased bits. indeed, since q and (mt+ v )vq are f-measurable by lemma , since mt+ v > mt v for all v q, and since y bits is an i.i.d. process that is independent of , we see that the random variables (y bits v (mt+ v ))vq are conditionally independent given f, and that, for any v q, the conditional distribution of y bits v (mt+ v ) is the same as the distribution of y bits v (mt+ v ) given |y bits v |. since y bits v is a random number of random bits, the latter is the distribution of an unbiased bit, and the proof is complete. finitely dependent processes are finitary we will use the above lemma for xed t and then let t tend to innity. in doing so, we will encounter the limiting word w u := limtw t u. since w t+ u extends w t u, this limit is well-dened and is a word in {, }or {, }n . the next step towards proving proposition is to show that the output at every vertex v is eventually determined, i.e., that zv = almost surely. for this, we rst show that every vertex is eventually inactive. lemma . every vertex is almost surely eventually inactive. that is, for any u v, there almost surely exists a nite t such that u is not active at any time t t proof. dene := {v=u+t and lt u= for some t}. note that indicates whether u read a bit located at v. since an agent may read at most one bit from any location, also represents the number of bits read by u from location v. thus, recalling that |w t u| = l u + l u + + lt u, we have x v = x t= lt u = |w u | and x u = x t= lt vt = lim tmt v =: m v . the left-hand side describes the number of bits read by a given site u, while the right-hand side describes the number of bits read from a given site v. the mass-transport principle tells us that these quantities are the same in expectation: e|w u | = em v . let eu be the event that u is active at innitely many times t. we wish to show that p = note that, by , the event eu is contained in the event that for all but nitely many t , all bits at location u + t have been read by time t, i.e., eu  mt u+t |y bits u+t | for all suciently large t =  m u+t = |y bits u+t | for all suciently large t , where the equality follows from the fact that mt v m v |y bits v | for all v v and t suppose now that p > then by ergodicity, almost surely, ew occurs for some w v, and in particular, there almost surely exists w v such that m w+i = |y bits w+i| for all i since {w + i}iz = v almost surely, it follows by -invariance that m v = |y bits v | for all v v almost surely. thus, by , e|w u | = e|y bits v |. that is, the expected number of bits read by each site is precisely the expected number of available bits per site. it remains to show that this is impossible. dene := ( |w u | |an\an| if v an \ an and un = u otherwise . recall that un is the level n agent associated to the cell an. since a level n agent u is responsible for simulating the output on an\an and does so via the input word w u , we may think of as follows: every level n agent u equally divides a total cost of |w u | among the vertices it serviced. observe that x v = |w u | and x u = |w unv | |anv \ anv|, yinon spinka where nv is the level at which v entered the cell process, i.e., v anv \ anv, and where we used that an = an whenever un = u. thus, by and the mass-transport principle , e|y bits v | = e "" |w unv | |anv \ anv| # . this relates the expected number of available bits per site to the length of the input words used by the simulations. we would like to reach a contradiction to the fact that there are many available bits and that the simulation is ecient. suppose that u is a level n agent. it is straightforward from the denitions that the stopping time stime an,anan,zt,n is not reached on any prex of w t u that is not w t u itself (it may or may not be reached on the entire word w t u). it therefore follows from lemma that, conditioned on , |w t u| is stochastically dominated by stime an,anan,zt,n , where {, }n consists of a sequence of independent unbiased bits, independent of y . note that, if u reached level n by time t, then zt,n coincides with z on an an thus, taking expectations and t , we obtain that e h |w u | | y cell, y ordi e h stime an,anan,z | y cell, y ordi hence, by and , on the event that u un, we have e h |w u | | y cell, y ordi + ( hx) if n = |an \ an| log |s| if n , where we denote hx := h for a nite set v v. therefore, by and the choice of , e "" |w unv | |anv \ anv| | y cell, y ord # + + ) e + ec, where e is the event that nv = and |a| |a|. thus, by , e|y bits v | + + ) p + p. using and , we see that e|y bits v | < h + , which contradicts . we therefore conclude that p = as required. we are now ready to show that the output at every vertex is eventually determined. lemma . for any v v, we have that tv < almost surely. proof. since an almost surely increases to v, it suces to show that p = for all n we prove this by induction on n, taking n = as a trivial base case by setting a := . let n and suppose that p = by -invariance, we actually have that p = we may thus assume that zw = for all w an suppose now that zv = and v an. let u be the level n agent un associated to the cell an. observe that, by the denition of zv and zt,n v , we have that, for all t , u did not complete level n by time t. on the other hand, since zw = for all w an, there exists a nite t such that u has reached level n by time t it follows that u is active at time t for every t > t by lemma , almost surely, no vertex is active at innitely many times, thus completing the proof that p = now that we have established that the output at every vertex is eventually determined, it remains to show that the distribution of the output is the correct one, namely, that of x. the following immediately implies proposition . finitely dependent processes are finitary proposition . conditioned on , z almost surely has the same distribution as x, where we regard x as independent of . proof. throughout the proof, we regard x as independent of y cell and y ord. we also condition on throughout the entire proof, without explicitly mentioning this. in particular, any statement about distributions or independence should be understood as conditional on . since every nite subset of v is almost surely contained in some cell of the cell process, it suces to show that, for any n and any cell c of an, zc has the same distribution as xc. we prove this by induction on n, taking n = as a trivial base case . suppose now that n let c be a cell of an and denote c := c an we will show that zc d = xc and p = p for any feasible sc. by feasible , we mean that p > the desired equality in distribution zc d = xc follows immediately from and . both parts require some type of independence, which we now establish. let {, }n consist of a sequence of independent unbiased bits. let uv be a collection of i.i.d. copies of , independent of y bits. by lemma , for any t , (w t u u)u has the same distribution as u. taking the limit as t , we see that (w u u)u also has the same distribution as u. observe that, by construction, if c is some cell of the cell process, then zt c is a function of (w t u)uc. taking the limit as t , it follows that zc is a function of (w u )uc. it also follows from the denition of z and the fact that tv < for all v, that z is unchanged by concatenating any word to any w u . in particular, zc is also a function of (w u u)uc. we now show . to this end, let c, . . . , cm be the cells in an that are contained in c, so that c = c cm. by the induction hypothesis, zcj d = xcj for every j m. since a is a cell process, we have that dist > for j < j m. hence, using that x is -dependent, we see that {xcj}jm are independent. thus, it remains to show that {zcj}jm are also independent. since zcj is a function of (w u u)ucj, this follows from the fact that {w u u}uv are independent. to complete the proof, it remains to show . let w be the agent associated to c and recall that w c \ c and that c = an. note that zc\c = sout c,c,z(w w ) = sout c,c,z(w w w). recall that sc,c,z is shorthand for sc\c,c,zc. since w w w is independent of (w u u)u=w, and hence also of zc, we conclude that the conditional distribution of zc\c given that zc = is equal to the distribution of sout c\c,c,(w w w). thus, using that w w w has the same distribution as , we see that the distribution in question is that of sout c\c,c,, which is by denition p, as required.","In this section, we prove a proposition regarding the correct distribution of the output in a technical simulation setting. The proof involves the use of several steps and lemmas that establish the independence and unbiased nature of the simulated bits. Key elements include showing that the distribution of the output at each vertex is eventually determined and demonstrating that the distribution of the output matches the desired distribution. The proof involves intricate arguments based on finitely dependent processes and their convergence properties to ensure the correctness of the output distribution."
"remark we have given the details of the proof of theorem . theorem follows the same lines of proof, with minor modications, all of which are in fact simplications. to obtain a proof of theorem with the least modications to the existing proof, we may replace the random total order constructed in lemma with the total order induced by an i.i.d. process consisting of uniform random variables. using this order in the proof of lemma yields a random total order with the same properties as in lemma (except for the bound on the entropy of the i.i.d. process). when x is nite-valued, the proof then goes through with no yinon spinka further modications. otherwise, we let y bits v consist of innitely many independent random bits, and then the proof goes through after an additional modication to the proof of lemma (which relied on the fact that the entropy of x is nite to deduce a bound on the expected number of bits used by the simulation; instead we only rely on the fact that, almost surely, the simulation uses only nitely many bits; see theorem ). it is instructive to note that a shorter and conceptually simpler proof exists when one does not need to worry about the entropy of the i.i.d. process. this is essentially what is described in constructing a nitary coding in section one way to implement the described coding would be to simply replace u t with u everywhere in the construction in section that is, instead of having an agent u try to read an unused bit from location u + t at time t, it always reads bits located at u. since we may place an innite sequence of bits at every vertex, it will never run out of available bits. in this way, there is no moving around of bits from one location to another. this could be made conceptually even simpler if instead of using simulations from random bits to obtain samples of distributions as they are needed, from the start, each y bits v is a collection v,u, of independent random variables having distribution p for all nite u, v v and su. either way, a nice feature of this construction is that the coding radius depends only on the cell process a constructed in section namely, the coding radius for determining x is at most the maximum of min{r : a r is the cell of in amin{n:an}, the coding radius for determining the cell a and the coding radius for determining the cell process on a, i.e., )n we elaborate on this in the next remark. remark our main theorems give no information about the coding radius beyond its almost-sure niteness. however, some information about the coding radius may be extracted from the proof given here. specically, theorem may be enhanced to give a universal bound on the tail of the coding radius for any xed graph and nite-dependence range. more precisely, for any transitive amenable graph g and any integer k , there exists a sequence n= tending to zero such that any k-dependent invariant random eld x on g is a nitary factor of an i.i.d. process with a coding radius r satisfying that p cn for all n. the sequence depends only on the graph g and on the parameter k, and not on the group nor on the random eld x. indeed, the sequence is governed by the properties of the cell process . in particular, for many concrete choices of g , an explicit sequence may be found. to illustrate this in a simple setting, let us show that for g = z and k = , one may take cn = /n. in this case, instead of using the construction given in section , it is simpler to consider the nitary cell process a given by an := b bn, where n are independent random subsets of z, each being an independent bernoulli percolation with parameter / then the level n := min{n : an} at which enters the cell process is a geometric random variable with parameter /, conditioned on which, the lengths l := min{m : m / an} of the cell of in an in the positive/negative directions are geometric random variables with parameter n, and the coding radius r for determining x is bounded by max{l+, l}. thus, p e  r x n= nern r x m= mem r, where we used the substitution n = log rm. we remark that if one would like to simultaneously control also the entropy of the i.i.d. process , then it is plausible that this can be done by allowing to depend on the entropy gap , but we did not pursue this. remark we do not know whether condition is necessary as stated in theorem , however, as we now explain, some condition of this form is needed (i.e., the condition cannot be completely dropped). let g be an innite transitive graph on vertex set v and let h be a nite transitive graph on m vertices. let g be the graph obtained by replacing each vertex of g with a copy finitely dependent processes are finitary of h that is, the vertex set of g is v {, . . . , m}, and and are adjacent in g if and only if u and v are adjacent in g, or u = v and i and j are adjacent in h. any graph g obtained in this manner is transitive, but fails to satisfy . indeed, the balls of radius (or even when h is a complete graph) around and coincide. a simple case to have in mind is when g = z and h consists of an edge on two vertices, so that the vertices of g are z {, } and there is an edge between and if and only if |u v| let g be any graph as above and let vv be independent uniform random variables on {, . . . , m}. consider the random eld x on g dened by x := {wv=i}. it is clear that x is - dependent and aut-invariant. we claim that x is not a aut-factor of any i.i.d. process y on g whose single-site distribution has at least one atom . indeed, for any such process y , the event y = = y has positive probability, and on this event there is no aut-equivariant way to distinguish between , . . . , . that is, any aut-equivariant function : t v{,,m} {, }v{,,m} must satisfy = = whenever y t v{,,m} is such that y = = y. in particular, the event = = has positive probability, and hence, cannot have the same distribution as x. we remark that there are transitive subgroups of aut for which the above obstruction does not exist. for example, let be the subgroup of aut generated by aut and aut, both of which are naturally embedded in aut. simple modications to the proofs of lemma and lemma yield a -invariant random total order with the desired properties. the rest of the proof then goes through unchanged showing that our main result holds in this case: any nitely dependent -invariant process on g is a nitary -factor of an i.i.d. process with slightly larger entropy. we believe that our main result holds in many similar situations, where condition is replaced by a suitable condition on . we did not pursue this direction. open problems. one may wonder about the situation on non-amenable graphs such as a regular tree (of degree at least three). namely, is every automorphism-invariant nitely dependent process on a tree a nitary factor of an i.i.d. process? in fact, even the more fundamental question of whether such a process is a factor of i.i.d. is still open; see . the same questions may be asked on any transitive non-amenable graph. does there exist a stationary nitely dependent process on z (or, more generally, on some transitive amenable graph) that cannot be expressed as a nitary factor of an i.i.d. pro- cess with nite expected coding radius? as mentioned, the -dependent -coloring and -dependent -coloring of are believed to be examples of such processes, but this is still unproved. a nitary isomorphism is a nitary factor that is invertible and whose inverse is also nitary. somorodinky showed that every stationary nitely dependent process on z is nitarily isomorphic to an i.i.d. process. is this true in higher dimensions? namely, is every stationary nitely dependent process on zd nitarily isomorphic to an i.i.d. process?","The section ""Remarks and open problems"" discusses the proof of the main theorems, focusing on modifications and simplifications to achieve a proof with fewer changes. It introduces a construction method using random total orders and a coding process based on independent random variables. Additionally, it addresses the coding radius and the coding radius tail for different graphs and dependency ranges, providing insights and enhancements to the main theorems. The section concludes with open problems related to nitary factors of i.i.d. processes, non-amenable graphs, and the existence of nitary isomorphisms in higher dimensions."
,
"random switching near bifurcations tobias hurthand christian kuehn january , abstract the interplay between bifurcations and random switching processes of vector elds is studied. more precisely, we provide a classication of piecewise deterministic markov pro- cesses arising from stochastic switching dynamics near fold, hopf, transcritical and pitchfork bifurcations. we prove the existence of invariant measures for dierent switching rates. we also study, when the invariant measures are unique, when multiple measures occur, when measures have smooth densities, and under which conditions nite-time blow-up occurs. we demonstrate the applicability of our results for three nonlinear models arising in appli- cations.","The text discusses the interplay between bifurcations and random switching processes of vector fields. It provides a classification of piecewise deterministic Markov processes resulting from stochastic switching dynamics near fold, Hopf, transcritical, and pitchfork bifurcations. The existence of invariant measures for different switching rates is proven, along with the study of uniqueness of invariant measures, occurrence of multiple measures, smooth densities of measures, and conditions for finite-time blow-up. The applicability of the results is demonstrated through analysis of three nonlinear models in various applications."
"in this work we study the dynamics of randomly switched ordinary dierential equations of the form dx dt = x = f, x = x rd, x =: x, near bifurcation points. more precisely, we select two parameters p = p r so that has non-equivalent dynamics , which are separated by a distinguished bifurcation point p . then we look at the piecewise-deterministic markov process generated by switching between the vector elds f and f. this idea is motivated by several observations. here we just name a few: in parametric families of vector elds, bifurcations occur generically. therefore, they are immediately relevant for the study of pdmps as well. in addition, the interplay between random switching and bifurcation points is not studied well enough yet. from the perspective of pdmps, this setting provides natural examples to test and extend the general theory of invariant measures. stochastic bifurcation theory is a very active area, where still many questions remain open. hence, studying a well-dened set of standard cases involving bifurcations and stochasticity is highly desirable. institut de math ematiques, universit e de neuch atel, neuch atel, switzerland technical university of munich, faculty of mathematics, garching bei m unchen, germany arxiv:v jan parameters in many models are usually only known via a possible distribution and not exactly. therefore, our work contributes to the uncertainty quantication for nonlinear systems arising in applications. before describing our main results, we briey review some of the background from pdmps and from nonlinear dynamics to provide a broader perspective. the study of randomly switched deterministic vector elds goes back at least to the works of goldstein and kac . the set-up can informally be described as follows: given a starting point x rd and an initial vector eld fi taken from a nite collection {fj} of smooth vector elds on rd, we follow the ow along fi starting at x for an exponentially distributed random time. then a switch occurs, meaning that fi is replaced with a new vector eld fj, j = i. we ow along fj for another exponential time and switch again. this yields a continuous and piecewise smooth trajectory in rd that is, however, not the trajectory of a markov process. to obtain a markov process, one needs to supplement the switching process on rd with a second stochastic process that keeps track of the driving vector eld. the resulting two-component process belongs to the class of piecewise deterministic markov processes . pdmps were rst introduced by davis in an even more general setting. for instance, pdmps may involve jumps not only on the collection of vector elds but also on rd . the class of pdmps considered in this article is also known under the names of hybrid systems and random evolutions , . randomly switched vector elds have applications to areas such as ecology , gene regulation , molecular motors , epidemiology , queueing theory , and climate science , to name just a few. aside from their uses in modeling, randomly switched vector elds have intriguing theoret- ical properties. for example, switching between stable vector elds can result in an unstable situation, and vice versa. recently, examples of randomly switched vector elds were found that exhibit such a reversal of stability for almost all realizations of switching times . another interesting phenomenon is the regularizing eect random switching can have on a dynamical system. for example, random switching between two lorenz vector elds with just slightly dif- ferent parameter values induces an invariant probability measure that is absolutely continuous with respect to lebesgue measure on r, whereas the dynamics associated to each individual vector eld concentrate on attractors of lebesgue measure zero . another recent topic is the ergodic theory for randomly switched vector elds. important contributions to the ques- tion whether a switching system on a noncompact state space admits an invariant probability measure were made in . in , it was shown that a h ormander-type hypoellipticity condition on the vector elds at an accessible point yields uniqueness and absolute continuity of the invariant probability measure. in this work we focus on the invariant probability measure aspect and relate it to bifurcation points. bifurcation theory has become one of the most widely used techniques to study nonlinear systems . informally, the main idea is to study vector elds under parameter variation and to determine at which points the dynamics changes fundamentally, i.e., to detect the points where the phase portraits of the vector elds are not topologically equivalent upon small parameter variation. almost full classication results exist for bifurcations with relatively few parameters, i.e., codimension one or two. these results provide suitable unfoldings, which are basically partitions of parameter space into non-equivalent phase portraits . recently, substantial interest has been focused on understanding the interplay between stochas- ticity and bifurcations. yet, the setting in almost all of these works is focused on either stochastic dierential equations involving time stochastic forcing processes , or less frequently on random dierential equations with a xed random parameter distribu- tion . particularly interesting dynamics seems to appear for sdes in oscillatory situa- tions . recently numerical and semi-analytical work shows that interesting eects also occur for switched systems near bifurcations . therefore, it is very natural that one should try to link pdmps with bifurcation theory. in this paper, we provide a full mathematical classication of the pdmps associated to switched near local bifurcations for codimension one bifurcations. we not only include the generic fold and hopf bifurcations but also study the frequently occurring one-parameter transcritical and pitchfork bifurcations. we prove under which conditions on the switching rates invariant measures occur, when they are unique, when their densities are smooth, and we also provide explicit formulas for these densities in certain cases. in addition, we prove nite-time blow-up results for certain parameter regimes. in summary, our theorems provide building blocks, which can be employed in various pdmps. in addition, we demonstrate that we may also derive insights from our results in three nonlinear models arising respectively in ecology, nonlinear oscillations, and collective motion. the paper is structured as follows: in section we provide more technical background from local bifurcation theory and pdmps. in section we focus on all cases where below and above the bifurcation point there are non-trivial trapping regions. in these cases we characterize the occurring invariant probability measures completely. in section we consider the cases with only one non-trivial trapping region. we again study the invariant measures in full detail but now also nite-time blow-up can appear. in section , we indicate how our results can be used in three models arising from applications.","The text discusses studying randomly switched ordinary differential equations near bifurcation points. By selecting parameters with non-equivalent dynamics separated by a distinguished bifurcation point, the text explores the piecewise-deterministic Markov process generated by switching between vector fields. This study is motivated by the generic occurrence of bifurcations in parametric families of vector fields and the need to understand the interplay between random switching and bifurcation points. The work contributes to uncertainty quantification for nonlinear systems and provides a classification of piecewise deterministic Markov processes associated with switched near local bifurcations. The paper examines cases with non-trivial trapping regions below and above the bifurcation point and also considers instances with only one non-trivial trapping region. The results offer insights for various piecewise deterministic Markov processes and can be applied to models in ecology, nonlinear oscillations, and collective motion."
"we briey recall the technical background needed from the two main areas we consider in this work. hence, this section mainly serves as a reference and to x the notation. readers familiar with local bifurcation theory and pdmps can skip ahead to section","The ""Background"" section briefly reviews key technical concepts from local bifurcation theory and Piecewise Deterministic Markov Processes (PDMPs) to establish a reference and ensure consistent notation for readers. It is intended for those who might need a quick refresher on these topics before delving into the subsequent sections of the work."
"consider an ordinary dierential equation given by dx dt = x = f, x = x rd, x =: x, where p r is the bifurcation parameter, and we assume that the vector eld f : rd r rd is suciently smooth; in particular, in what follows f c is going to suce. we also refer to rd as the phase space of . the phase space together with the foliation by trajectories x is called phase portrait. suppose xis an equilibrium point (or steady state) of for the parameter value pso that f = without loss of generality, upon translating coordinates in the phase space rd and the parameter space r, we may assume that = =: consider the linearized problem near the steady state x = dxfx = ax, x = x rd. then xis called hyperbolic if the matrix a rdd has no spectrum on the imaginary axis. in the hyperbolic case, the hartman-grobman theorem implies that the systems and are locally topologically equivalent, i.e., small parameter variations for p , p > , do not qualitatively alter the phase portrait as the hyperbolic structure of a is robust under small parameter perturbations. more precisely, for any two parameter values p, p , there exists a homeomorphism h : rd rd such that the phase portraits of f and f are mapped to each other by h preserving the direction of time on trajectories. suppose a is not hyperbolic so that spec ir = . a local bifurcation occurs at p= if for any p > and any open neighbourhood u = u of x= , there exist two locally (wrt u) non-homeomorphic phase portraits of for two values p, p . in particular, a bifurcation just corresponds to the appearance of a topologically non-equivalent phase portrait under parameter variation. the general strategy to analyze bifurcation problems proceeds as follows: the system is reduced to the dimension dc of ker using a center manifold w c loc, on w c loc one uses smoothness to taylor-expand the vector eld and then simplies it using coordinate changes, and one proves that a nite number of polynomial terms is locally sucient to determine the topological equivalence class so a truncation yields a nite-degree polynomial vector eld. the steps - lead to dierent classes/families of polynomial vector elds, also called normal forms, depending upon degeneracy of spec and depending upon a nite number of partial derivatives of f. in this work we shall focus on the four most common bifurcations used in practical applications for dc = and dc = , which just require a single bifurcation parameter p, and where the system has already been reduced to normal form. these cases will be the fold, hopf, transcritical, and pitchfork bifurcations. as a motivating example, consider the supercritical pitchfork normal form x = px x, x r, p r. clearly, the equilibrium x= undergoes a bifurcation at p= as the phase portrait for p < has one globally stable equilibrium, while the phase portrait for p > has three equilibria. for p > , we nd that x= is unstable while the equilibria x = p are both locally stable; see also figure p x sp sp> figure : sketch of the bifurcation diagram for the supercritical pitchfork bifurcation normal form . there are two classes of topologically non-equivalent phase portraits here denoted by sp and sp> however, note that from the viewpoint of applications, treating p just as a static parameter is not always realistic. this approach presumes p is changed innitely slowly to bring the system to and across the bifurcation point. one option is to consider the case when p is just switched across the bifurcation point (e.g., consider shot noise eects, control action, activation of external interfaces of the system, etc). as an example, consider the problem of switching between p < and p > in the context of the pitchfork normal form . this leads us naturally to consider piecewise deterministic markov processes as introduced in the next section.","The text discusses local bifurcation theory in the context of ordinary differential equations. It introduces the concept of equilibrium points and hyperbolicity, as well as the Hartman-Grobman theorem which shows systems are locally topologically equivalent under certain conditions. The text also covers the occurrence of local bifurcations and outlines a strategy to analyze bifurcation problems using center manifolds and normal forms. It focuses on four common bifurcations: fold, hopf, transcritical, and pitchfork, providing an example of the supercritical pitchfork bifurcation normal form. Different classes of topologically non-equivalent phase portraits are discussed, and the importance of considering parameter changes beyond static variations is highlighted. The text suggests considering switching scenarios across bifurcation points, leading to the consideration of piecewise deterministic Markov processes in the next section."
"in this subsection we introduce a class of pdmps characterized by poissonian random switching between a nite number of deterministic vector elds. let i be a nite index set, and let ii be a collection of vector elds on rd with some degree of smoothness. to introduce the basic framework, we just assume that ii are in c, but for some of the results stated below higher degrees of smoothness are required. to be able to associate ows to the vector elds, we assume in addition that ii are forward complete, i.e. for any x rd the initial-value problem x = fi, x = x has a unique solution t t i that is dened for all t given a starting point x rd and an initial vector eld fi, the random dynamical system we consider follows the ow associated to x and fi for a random time. then a switch occurs, which means that the driving vector eld fi is replaced by a new vector eld fj chosen at random from {fk : k i \ {i}}. again, the system ows along fj for a random time until another switch occurs, etc. the stochastic process x = t that records the position of the switching trajectory on rd is not markov because knowing st lets us infer the driving vector eld at time t. if the times between consecutive switches are exponentially distributed and independent conditioned on the sequence of driving vector elds, and if the vector elds are chosen according to a markov chain on i, then the two- component process is already markov, where et i gives the index of the driving vector eld at time t. for more general distributions of switching times, one needs to adjoin a third component that keeps track of the time elapsed since the latest switch. it is possible to consider the situation where the rate of switching depends continuously on the location of the switching trajectory on rd , . for simplicity we assume that the switching rates do not depend on the process x. we can then give the following rigorous description of . let e = t be an irreducible continuous-time markov chain on the state space i. let x = t be the solution to the control problem xt = x + z t fes ds. the markov process has innitesimal generator l acting on functions g : rd i r that are smooth in x according to lg = fi, xg+ x j=i i,j g), where i,j is the rate at which e transitions from state i to state j. we denote the markov semigroup of by t or just by . an invariant probability measure of is a probability measure on rd i such that = pt for all t below, we collect some results on existence, uniqueness and absolute continuity for ipm of that have been established in the literature. we call a set m rd positive invariant if m is positive invariant under the ows ii associated with the vector elds ii, i.e. if for any x m, i i and t , we have t i m. thus, trajectories of x starting in a positive invariant set m or entering m at some time remain in m for all future times. if there is a compact positive invariant set m, existence of an ipm is guaranteed by the krylovbogoliubov method , which applies because is feller . in the noncompact situation, an ipm is guaranteed to exist, for instance, if is on average contracting . by harriss ergodic theorem, existence also holds if the semigroup admits a lyapunov function as well as a minorizing measure k for every compact set k rd. recall that the support of a borel measure on rd i is the set of points rd i such that > for every open neighborhood u rd of x. if xrd is an equilibrium for each of the vector elds fi, then the product of the dirac measure at xand the unique ipm of the continuous-time markov chain e is a trivial ipm for . if m rd is a compact positive invariant set containing such a common equilibrium x, the krylovbogoliubov method is not sucient to decide whether there are any additional ipm whose support is contained in m i. this more subtle existence question can often be addressed using the theory of stochastic persistence as developed by bena m , and as applied to the case of a common equilibrium by bena m and strickler . we now outline an existence result from that will be needed later on. let m rd be a compact positive invariant set containing the point x= , which we assume to be an equilibrium for all vector elds fi. as a technical condition, we also require that there is > such that whenever x m and x, then the entire line segment from to x is contained in m. for i i, let ai = dfi be the jacobian matrix of fi at then, the cone cm = {tx : t , x m, x} rd is positive invariant with respect to the ows of the linear vector elds given by ii. on cm i, we dene the pdmp , which is obtained from by replacing each vector eld fi with its linearization ai. whenever yt = , we dene the angular process t = yt yt, which evolves on the compact set sd cm. by krylovbogoliubov, admits at least one ipm. for any ipm of , dene the average growth rate as = x ii z sdcm ai . since ytsatises d dtyt= t aettyt, birkhos ergodic theorem implies that for almost every realization of with initial distri- bution , we have lim t ln t = . recall that an ipm of a markov process with markov semigroup and state space x is called ergodic if {, } for every measurable a x such that for all t , pt x = for -almost every x a. let denote the inmum and + the supremum of over all ergodic ipm of . in many situations of interest, has exactly one ipm, so = +. denition . we call a point x rd reachable from y rd if there is a nite sequence of indices i, . . . , in i and a correspond- ing sequence of positive real numbers t, . . . , tn such that tn in . . . t i = x; accessible from y if for any neighborhood u of x there is z u such that z is reachable from y; accessible from s rd if it is accessible from any y s. if x is accessible from rd, we simply say that x is accessible. a point x rd is accessible if and only if for every neighborhood u of x, for every y rd and for every i, j i there is t > such that pt y,i > if x is accessible, then the points , i i, are contained in the support of any ipm for . theorem (bena m, strickler, ). let m+ = m \ {}. the following statements hold. if > , then there exists an ipm of such that = in addition, for any starting point x m+, xt almost surely does not converge to as t . if + < and if the point is accessible, then for any starting point x m, xt converges almost surely to as t . in particular, there is no ipm that assigns positive mass to m+ i. now, we review sucient conditions for uniqueness and absolute continuity of the ipm. recall that the lie bracket of c vector elds f and f on rd is dened as = dff dff, x rd. let l denote the lie algebra generated by ii, i.e. l is the smallest collection of cvector elds on rd that contains ii, and is closed under linear combinations and the lie bracket operation. denition . we say that the weak bracket condition is satised at a point x rd if {f : f l} = rd. the weak bracket condition is essentially h ormanders condition for smoothness of transition densities for a diusion process with the noise acting along ii, see . theorem (bena m, le borgne, malrieu, zitt, ; bakhtin, hurth, ). let u rd be an open positive invariant set. suppose admits an ipm such that = assume in addition that there exists x u such that x is accessible from u and the weak bracket condition holds at x. then, is the unique ipm assigning full measure to u i, and is absolutely continuous with respect to the product of lebesgue measure on rd and counting measure on i. if d = , the weak bracket condition holds at any point that is not an equilibrium of all ii. the interesting condition is then existence of an accessible point. suppose now that admits an absolutely continuous ipm with probability density function . we refer to the projections i = , i i, as invariant densities. for some simple pdmps on r i, we can give explicit formulas for invariant densities. besides, we have the following regularity result. theorem . assume that ii are cvector elds on r with locally nite sets of critical points each. let x r such that fi = for every i i. then, the invariant densities ii of an absolutely continuous ipm are csmooth at x.","A class of Piecewise Deterministic Markov Processes (PDMPs) is introduced characterized by Poissonian random switching between a finite number of deterministic vector fields. The framework involves random switching between vector fields, with each switch leading to a new vector field being chosen randomly. The resulting stochastic process is not Markov due to the knowledge of the driving vector field at a given time. However, under certain conditions such as exponentially distributed switching times and Markov chain selection of vector fields, the process can become Markov. Various results on the existence, uniqueness, and absolute continuity of invariant probability measures for PDMPs are discussed, along with conditions for ergodicity and reachability of points in the state space. Sufficient conditions for uniqueness and absolute continuity of invariant probability measures are also outlined, emphasizing the importance of the weak bracket condition and accessibility of points in the state space. The text also mentions specific examples of PDMPs and regularity results regarding invariant densities."
"given a vector eld f on rd with ow function and a set v rd, we call v a trapping region with respect to f if for any x v and any t > we have t v. we split our analysis into two cases, which can occur for our normal forms in dierent parameter regimes. either, there exists a trapping region v rd of nite positive lebesgue measure. or, trajectories leave any bounded set except for a set of measure zero, which is going to consist of unstable equilibria in our case. in this section, we cover the case when such a trapping region exists both below and above the bifurcation value. the case when a trapping region exists only below or only above the bifurcation value is covered in section","In this section, we analyze two cases regarding trapping regions in a vector field on rd. The first case involves the existence of a trapping region with finite positive Lebesgue measure. The second case considers trajectories leaving any bounded set except for a set of measure zero, which mainly comprises unstable equilibria. The discussion focuses on scenarios where a trapping region exists both below and above the bifurcation value. Cases where a trapping region exists only below or only above the bifurcation value are addressed in another section."
"consider the ode for d = and assume the existence of a trivial branch of equilibria f = for all p. assume that the following conditions hold at = : xf = , xxf = , xxxf < , xpf = then a bifurcation occurs at , which can be proven to be locally topologically equivalent to the supercritical pitchfork bifurcation normal form x = px x the dynamics of is easy to analyze. for p < , there is a unique globally stable equilibrium point x= for p > , x= is unstable while the equilibria x = p are locally stable. for any p r, all trajectories remain bounded so trapping regions of positive measure are easy to nd. we now analyze the normal form from the viewpoint of pdmps by switching the parameter p. for xed parameters p< and p+ > , we switch between the vector elds f = px x, f = p+x x we denote the rate of switching from f to f by and the rate of switching from f to f by +. since is an equilibrium for both vector elds, the semigroup associated with the pdmp admits at least one ipm, namely the product of the dirac measure at and the measure on i = {, } that assigns probability + ++to and ++to the latter is precisely the ipm of the continuous-time markov chain e on the state space i. for ease of reference, we call this trivial ipm . theorem . the following statements hold. if + p+ < p, the semigroup admits exactly three ergodic ipm: the trivial measure , a measure such that i) = , and a measure such that i) = if + p+ p, then is the unique ipm for . theorem . suppose that + p+ < p. then, the ergodic ipm and assigning measure to i and i, respectively, are absolutely continuous. moreover, the corresponding invariant densities and are given by i = i = cx p + p+  p+ x p  p+ x + p+ , i i. here, c is a normalizing constant. proof of theorem : let m = and m+ = (, p+]. then, m is a compact posi- tive invariant set containing the common equilibrium moreover, as is globally asymptotically stable for f, is accessible from m. if we linearize f and f at x = , we obtain a = d dxf|x= = p, a = d dxf|x= = p+. we have cm = [, ) and cm s = {}. the angular process has a unique ipm that assigns probability + ++to {} {} and probability ++to {} {}. thus, + = = = p+ + p+ + + , which is positive if + p+ < pand negative if + p+ > p. by theorem , if + p+ < p, there exists an ipm such that = ; and if + p+ > p, there is no ipm assigning positive mass to m+ i. suppose now that + p+ < p, and consider the open positive invariant set . let x and observe that x is accessible from . since f and f do not vanish at x, the weak bracket condition is satised. by theorem , there is exactly one ipm assigning full measure to i. this measure is ergodic: by the ergodic decomposition theorem , there exists an ergodic ipm assigning positive mass to i. since is positive invariant, we have i) = and hence = . this argument also shows that is the only ergodic ipm that assigns positive mass to i. a completely analogous reasoning applies to the positive invariant set . it remains to consider the critical case + p+ = p, where theorem does not apply. to obtain a contradiction, we assume that there is an ergodic ipm that, without loss of generality, assigns measure to i. by theorem , has a density , and by theorem and are smooth in . therefore, they satisfy the formula in theorem . as p+ + p+ = , and behave asymptotically as x as x since x is not integrable in a neighborhood of , we arrive at a contradiction. proof of theorem : absolute continuity of and follows from theorem . as shown in the proof of theorem , = , so the invariant densities ( i )ii vanish outside of . by theorem , ( i )ii are con and thus satisfy the fokkerplanck equations, see for instance . written in terms of probability uxes i = i fi, i i, the fokkerplanck equations read for x = f + + f, = + f + f then, + , so + is constant. we even have + . the ode in becomes =  f + + f  , which is solved by =c exp  z dx f + z dx f  =cx p + p+  p+ x p p+ x + p+ . we obtain the desired formula for with = /f and = /f the formula for follows from the fact that both f and f are odd.","The section discusses the supercritical pitchfork bifurcation in a dynamical system described by a specified set of conditions. It shows that for certain parameter values, the system undergoes a bifurcation equivalent to the supercritical pitchfork bifurcation normal form. The analysis includes identifying equilibrium points and stability properties depending on parameter values. The section further examines the system from the perspective of piecewise deterministic Markov processes (PDMPs) by switching parameters. The text presents theorems regarding ergodic invariant probability measures (IPMs) and invariant densities. The proofs involve establishing positive invariant sets and ergodic properties. Additionally, the critical case of parameter equality is considered, along with the implications for IPMs and invariant densities."
"consider the ode for d = assume that x= x is a family of equilibrium points for all p in a parameter-space neighbourhood of p. let a = dxf, p) and assume that spec = { i}, = , = , = furthermore, consider the rst lyapunov coecient l = l, which is computable from f using partial derivatives up to and including third order; see the formulas in . assume that l < then a bifurcation occurs at , which can be proven to be locally topologically equivalent to the supercritical hopf bifurcation normal form x = px x x(x + x ), x = x + px x(x + x ). the dynamics of can be analyzed a lot easier upon changing to polar coordinates = , which gives = , r = pr r analyzing the simple vector eld and returning to euclidean coordinates, one nds that for p < , there is a unique globally stable equilibrium point x= for p > , x= is unstable while there exists a family of stable periodic orbits {x = p}. for any p r, all trajectories remain bounded so trapping regions of positive measure always exist. we now analyze the normal form from the viewpoint of pdmps by switching the parameter p, again working in polar coordinates. for xed p< and p+ > , we switch between the vector elds g = , g = . in analogy to the case of the supercritical pitchfork bifurcation, we denote the rate of switching from g to g by , and the rate of switching from g to g by +. as before, the origin is an equilibrium for both vector elds, so , dened as the product of the dirac measure at the origin and the discrete measure assigning probability + ++to and ++to , is an ipm. let denote the unique ipm for the pdmp induced by switching between the one-dimensional vector elds f = pr r, f = p+r r, r > at rates and +, whose existence is guaranteed by theorem . theorem . the following statements hold. if + p+ < p, admits exactly two ergodic ipm: the measure and a measure that is the product of lebesgue measure on the unit circle s, normalized by the factor , and the ipm . if + p+ p, then is the unique ipm for . proof: suppose rst that + p+ < p. let denote the product of lebesgue measure on s, normalized by the factor , and the ipm . then, for t > , i, j i, s, r > and measurable sets a s, b , we have pt ,r,j = ab pt r,j, where + t should be understood modulo , and where b p denotes the semigroup associated with the pdmp induced by f and f this form of independence for and r holds because the evolution of the angular component is entirely deterministic and in particular not aected by the switching times. thus, pt = z s a d x j{,} z b pt r,j =leb b pt = leb = . hence, is an ipm for . next, we show that is the only ipm such that i) = first we show that any point in s is accessible from s . fix two points s and s . for i i, we denote the ow associated with the vector eld gi by i. as s , the radial component of s tends to let s > such that s has angular component and radial component u < p. as p , a short computation shows that t has radial component  ep+tp+c + ep+tc  , where c = p+p p +p+p > this shows that the vector eld g is both forward and backward complete on the punctured disk s , with limtt = hence, there is t < such that t has angular component and radial component v < u. for t , let h denote the dierence of the radial components of t and t . then, h = uv > and h u p < as h is continuous, there is such that h = as the points t and t have the same angular component for any t , we have = thus, we can reach the point from as follows: first, ow along the vector eld g for time s + , then make a switch and ow along g for time t . for s , the vectors g and g are clearly transversal, so the weak bracket condition holds as well. by theorem , is indeed the only ipm assigning mass to s . the fact that and are the only ergodic ipm follows along the same lines as in the proof of theorem . now, we consider the case + p+ p. to obtain a contradiction, suppose that there is an ipm for such that i) > by the ergodic decomposition theorem, we may assume without loss of generality that i) = consider the marginal b = , which is a probability measure on i. for t > and with b p dened as above, we have for measurable b and i i b b pt = x ji z b pt r,j b = x ji z s z b pt r,j = x ji z s z sb pt r,j = x ji z s z pt ,r,j = = b . this computation shows that b is an ipm for (b pt). but theorem implies that (b pt) has no ipm if + p+ p, a contradiction.","The section discusses the supercritical Hopf bifurcation in the context of ordinary differential equations. It introduces the concept of Lyapunov coefficients and demonstrates how the system undergoes a bifurcation at a critical point. By analyzing the dynamics using polar coordinates, the text shows the existence of stable and unstable equilibrium points and periodic orbits. The discussion further extends to analyzing the system using piecewise deterministic Markov processes (PDMPs) and switching between vector fields. The analysis leads to the identification of ergodic invariant probability measures (IPMs) and their unique properties under different conditions."
"consider the ode for d = and assume the existence of a trivial branch of equilibria f = for all p. assume that the following conditions hold at = : xf = , xxf = , xpf = then a bifurcation occurs at , which can be proven to be locally topologically equivalent to the transcritical bifurcation normal form x = px x the dynamics of works as follows. there are two families of equilibrium points x= and x= p. for p < , xis locally stable, while xis unstable. for p > , the stabilities switch. there are bounded trapping regions of positive measure given in the dierent parameter regimes by vp< = and vp> = with the special case vp= = for any k > for xed p< and p+ > , consider the vector elds f = px x, f = p+x x these vector elds are not forward complete: trajectories for f that start to the left of p and trajectories for f that start to the left of move oto in nite time. to obtain a well-dened pdmp, we therefore restrict ourselves to the positive invariant set [, ), where both f and f have bounded trajectories and are in particular forward complete. we switch from f to f at rate and from f to f at rate +, and we let denote the product of the dirac measure at and the ipm of e. remark: it is possible to dene a pdmp that involves switching between f and f on the larger interval . since is not a trapping region for f, one needs to ensure that we switch away from f before reaching the point p. this can be achieved by letting the switching rate + depend on the location x of the switching trajectory, with + blowing up as x pfrom the right. theorem . the following statements hold. if + p+ < p, there are exactly two ergodic ipm: the trivial measure and a measure such that i) = if + p+ p, then is the only ipm. this statement can be shown along the same lines as theorem . we therefore omit the proof. theorem . suppose that + p+ < p. then, the ergodic ipm assigning mass to i is absolutely continuous. moreover, the corresponding invariant density is given by i = cx p + p+ p + p+ , i i. proof: absolute continuity of follows from theorem . as = , the invariant densities ii vanish outside of . by theorem , ii are con and thus satisfy the fokker planck equations. for the probability ux , we have = c exp  z dx f + z dx f  = cx p + p+ p + p+ . as in the case of the supercritical pitchfork bifurcation, we obtain the desired formula with = /f and = /f if the switching rates + and do not depend on x, the pdmp starting at a point to the left of will tend to in nite time with positive probability. to make this statement more precise, we dene for a < pthe stopping time a = inf{t : xt a}. proposition . let be a probability measure on r i such that i) = , and let pa denote the law of with initial distribution and stopped at time a. there is a nonincreasing function g : such that r p g da < and pa  p <  > , pa  a a+ < g | p <  = , a p proposition essentially says that xt goes oto in nite time with positive probability if the initial distribution assigns full measure to i: there is a positive probability that x reaches the interval (, p] in nite time. and once x is in (, p], it blows up to with probability in time less than  p p  +  p p  + . . . x k= g < . proof of proposition : fix a p let us rst show that pa > let > be so small that > , and let r, s > such that r = , s ( ) = p if pa > , we also have pa > if pa = , we use the estimate pa pa. suppose that s + r a and x . then, we have xr if in addition et = for all t , it follows that p < . hence, the term on the right side of equals pa > now, we come to the second statement. we will specify the function g later in the proof. since there is c > such that f f c for all x (, p], we have a < for all a p whenever p < . by the strong markov property, pa | p < ) = pa), where is the distribution of a+ under pa, and thus satises = in light of f f c, we have under pa xt t , t as a result, if we let g be dened by the relation g = a, we have a < g under pa. since trajectories of f starting in (, p] tend to in nite time, we also have r p g da < . proposition and the ergodic decomposition theorem imply that there is no ipm assigning positive mass to i. looking at proposition , it is natural to ask under which conditions a blow-up of xt to in nite time happens almost surely. the answer follows from theorem below. theorem . let a p and let be a probability measure on ri such that i) = if + p+ < p, we have pa = if + p+ > pand if i) > , we have pa < and pa  {p < } n lim txt = o = as stated in the following lemma, with probability , xt either diverges to in nite time or converges to as t . lemma . if a p and if is a probability measure on ri such that i) = , we have pa  {p < } n lim txt = o = proof of lemma : under pa, the complement of {p < } {limtxt = } is {p = } [ n= \ k= [ tk  xt n  . for xed n n, consider the event \ k= [ tk  xt n  . on this event, there is t > such that xt n for every t t, or there is a sequence of times tj such that tj = ( n, ) for every j. in the former case, let s > such that s ( n) = p then, p < or, pa-almost surely, there is r > t such that et = for r r r + s, which also yields p < . in the latter case, observe that the rst return time to state ( n, ) is nite with probability strictly less than , so by the strong markov property the event {tj = ( n, ) j} has probability proof of theorem : assume rst that + p+ < p. by lemma it suces to show that pa  lim txt =  = let m = [ p , ] and m+ = [ p , ). let f be a smooth vector eld that coincides with f on the interval [p , ], is strictly negative on (p , ), and has p as an equilibrium point. in addition, we assume that f f for all x r. then, m is positive invariant for the vector elds f and f, and is accessible from m. let ( x, e) denote the pdmp with vector elds f, f and switching rates , +. following the proof of theorem and applying theorem , we see that for any starting point x m+, xt almost surely does not converge to as t . the markov property and the fact that any switching trajectory starting in and converging to has to visit points in m+ imply that this result extends to starting points x . since f f, we nally infer . now, we consider the case + p+ > p, assuming that i) > dening f and ( x, e) as above, we obtain with theorem that for any starting point x m = [ p , ], xt converges almost surely to as t . fix x [ p , ). then, x, p  lim t xt =  = , where x, p is the distribution of ( x, e) with initial distribution x, the rst return time for state must then be innite with positive probability. in other words, there is a positive probability that the pdmp ( x, e) starting in stays in [ p , ) i for all t and thus coincides with starting in . in particular, pa x, > for all x ( p , ). let > be so small that i) > , and let s > such that s = p . then, for any , (ps y,i)a(( p , ) {}) > it follows that pa  lim txt =  x ii z p+ z p pa x,  lim txt =  (ps y,i)a > the claim made in part of theorem then follows from lemma .","The section discusses transcritical bifurcation in a dynamical system described by an ordinary differential equation (ODE). Through the analysis of equilibrium points and the dynamics of the system, various bifurcation scenarios are explored based on specific conditions and parameter regimes. The text also introduces a probabilistic description of the system using Piecewise Deterministic Markov Processes (PDMPs) and discusses the behavior of trajectories under different conditions, such as reaching in finite time or blowing up to infinity. Theorems and propositions provide insights into the ergodic properties of the system, including the convergence or divergence of trajectories. The presentation culminates in outlining conditions under which trajectories tend to converge or diverge, supported by rigorous mathematical proofs and analysis."
"subcritical pitchfork bifurcation consider the ode for d = and assume the existence of a trivial branch of equilibria f = for all p. assume that the following conditions hold at = : xf = , xxf = , xxxf > , xpf = then a bifurcation occurs at , which can be proven to be locally topologically equivalent to the subcritical pitchfork bifurcation normal form x = px + x the dynamics of works as follows. for p < , there are three equilibrium points x= and x = p. xis locally stable, while x are unstable. for p > , x= is the only equilibrium point and it is unstable. for p there is no trapping region of positive measure. however, = v is a trapping region for the dynamics when p < for xed p< and p+ > , we switch between f = px + x, f = p+x + x at rates and +. the trivial measure is dened exactly as for the supercritical pitchfork bifurcation. as for the transcritical bifurcation, the vector elds f and f are not forward complete. e.g., any trajectory of f not starting at blows up in nite time. if one wishes to dene a pdmp outside of the common equilibrium point , one can either let the rate + of switching from f to f depend on the location x, with + diverging to as x approaches pfrom the right and pfrom the left. or one can stop the pdmp with constant switching rates once it reaches certain thresholds. for the latter model, is the unique ipm. besides, we have the following result that is reminiscent of proposition and theorem . since f and f are odd functions, we may restrict ourselves to the interval , with the understanding that there are completely analogous statements about . theorem . let be a probability measure on r i such that i) = , and let a = inf{t : xt a} for a > p. let pa denote the law of with initial distribution and stopped at time a. there is a nonincreasing function g : [p+, ) such that r p+ g da < and pa > , pa | p+ < ) = , a p+ if + p+ < p, we have pa = for a p+ if + p+ > pand i) > , we have pa < and pa  {p+ < } n lim txt = o = the proof is analogous to the ones of proposition and theorem , and we omit it.","A subcritical pitchfork bifurcation is discussed in which the dynamics of the system involve three equilibrium points for different values of a parameter. A trapping region is identified for the dynamics, and switching between different equations occurs at specific rates. The text also mentions the denition of a probability measure outside the equilibrium point and provides a theorem related to probability measures on a specific interval. The proof is not provided, as it is similar to previous propositions and theorems mentioned."
"consider the same setting as in section , except that we now assume that the rst lyapunov coecient satises l > this leads to a subcritical hopf bifurcation normal form x = px x + x(x + x ), x = x + px + x(x + x ). here the unstable bifurcating family of periodic orbits {x = p} exists for p < and in this case x= is locally stable. xis unstable for p for p < , there is a trapping region of positive measure vp< = {x r : x p}. after a change of variables to polar coordinates, the system in becomes =, r =pr + r for xed p< and p+ > , we then switch between g = , g = at rates and +. here, we encounter the same issue as for the subcritical pitchfork bifurcation. then, theorem applies to the pdmp induced by the vector elds f = pr + r, f = p+r + r, and thus to the evolution of the radial component of the pdmp induced by g and g","In a setting with certain conditions, a subcritical Hopf bifurcation occurs leading to a certain normal form. A family of periodic orbits exists for certain values, with local stability at certain points. By changing variables to polar coordinates and switching between certain rates, the system encounters similar issues as in a subcritical pitchfork bifurcation. Theorem applies to the induced vector field evolution, specifically the radial component in polar coordinates."
"consider the ode for d = assume that the following conditions hold at = : f = = xf = , xxf = , pf = then a bifurcation occurs at , which can be proven to be locally topologically equivalent to the fold bifurcation normal form x = p x for p > , there are two equilibrium points x = p. x+ is locally stable, while xis unstable. for p < , there are no equilibria. only for p > , there is a trapping region given by vp> = . for p< , p+ > , we switch between f = px, f = p+ x at rates and +. theorem . let be a probability measure on r i, and let a = inf{t : xt a} for a < p+. let pa be the law of with initial distribution and stopped at time a. then, there is a nonincreasing function g : such that r p+ g da < and pa = , pa | p+ < ) = , a p+ in words, xt diverges to in nite time almost surely. proof of theorem : let us show that pa = the rest is analogous to the proof of proposition . for xed x p+ , let s such that s = p+ , and let s > such that s = p+ set s = max{s, s} and let i i. with x,ipa- probability , we have p+ < or there is r such that et = for all t . but the latter case also implies p+ < because any switching trajectory starting from x cannot move to the right of max{x, p+}. as a result, pa  p+ <  = x ii z x,ipa  p+ <  =","In the context of a technical presentation, this section discusses the occurrence of a fold bifurcation in a system described by a specific ordinary differential equation. The conditions under which this bifurcation happens are outlined, along with details about the stability of equilibrium points. The presentation also covers particular trapping regions and switching rates related to the bifurcation. A theorem is presented regarding the behavior of the system over time, indicating that trajectories will ultimately diverge to infinity almost surely. The proof of the theorem involves a step-by-step argument involving probability measures and stopping times."
"in this section, we provide several very brief examples of systems where the switching viewpoint near bifurcations via pdmps can yield insight into concrete dynamical systems arising in ap- plications. in particular, the normal form results can be used suciently close to bifurcation points after a normal form transformation. furthermore, they can also be used directly to form conjectures about the dynamics of the applications.","The section highlights how utilizing piecewise deterministic Markov processes (PDMPs) near bifurcations can offer valuable insights into various dynamical systems found in practical applications. By leveraging normal form results in close proximity to bifurcation points, these approaches can help form conjectures about the dynamics of real-world systems."
"the paradox of enrichment is a classical topic in ecology. one simple variant can be found in classical predator-prey systems, such as the rosenzweig-macarthur model x = x  x p  xy +x, y = xy +x my, where x, y [, ) are population densities of prey and predator, > is a parameter repre- senting a conversion factor, m > is the mortality of the predator, while p > is the carrying capacity for the prey. the basic concept of the paradox of enrichment is that increasing the carrying capacity p > can actually lead to more likely extinction events triggered by additional stochastic eects, which can be supported by a classical bifurcation analysis of as follows: let us x m = and = while varying p as the main bifurcation parameter. besides the two boundary equilibrium points = and = , we nd the nontrivial co-existence equilibrium point = ) =  , p  which is in the relevant domain given by the non-negative quadrant for p > / linearizing around shows that the coexistence equilibrium is locally asymptotically stable for p . another direct calculation shows that a supercritical hopf bifurcation occurs at p= the resulting locally asymptotically stable limit cycle generated in the hopf bifurcation for p > grows in phase space. hence, solutions can get closer to the two coordinate axes {x = , y } and {y = , x }, which could make it more likely that a stochastic eect triggers an extinction event of a species. therefore, enrichment may lead to a potential increase in extinction events. of course, it is important to mention that there is still a debate in the literature on the mechanisms and possible variations of the paradox of enrichment . we do not provide here a full discussion of the various arguments made in favor or against the paradox but instead point towards the eect of randomness in the parameter p. from an ecological perspective, it can be plausible to view p as a parameter, which switches randomly between dierent carrying capacities since the environment might be driven by external random events such as droughts, oods, storms, earthquakes, sudden human intervention, or even just dierent seasonal climate conditions. suppose we switch p randomly in a range near p= with rates and values p as dened in section , where the parameters p are chosen so that the supercritical hopf normal form is a good local approximation of near p. then theorem suggests an interesting dichotomy of the ergodic ipm. either, we have only the invariant measure , which is concentrated on the equilibrium branch ) with probabilities determined by the switching rates, or we have two probability measures given by and , where is a non-trivial product measure also supported on the periodic orbit. one natural ecological interpretation of this eect is that we can actually avoid the paradox of enrichment from the viewpoint of measures if we restrict to those switching rates, which only lead to the ipm , i.e., that we switch frequently enough from the periodic stable regime above the bifurcation to the stationary stable regime below the bifurcation.","The paradox of enrichment in ecology explores how increasing the carrying capacity for prey in predator-prey systems can lead to a higher risk of extinction due to additional stochastic effects. A classical model illustrates this paradox, showing that a supercritical hopf bifurcation occurs at a certain parameter value, resulting in locally asymptotically stable limit cycles that bring species closer to extinction. The text suggests that randomness in the parameter representing carrying capacity may influence the likelihood of extinction events. By considering the effects of switching rates between different carrying capacities, it is possible to avoid the paradox of enrichment by transitioning between stable regimes effectively."
"consider the van der pol / fitzhugh-nagumo system x = p x + x, p = x, which is a classical model used for bistable systems with relaxation oscillations and excitability. the parameter is usually assumed to be small, so that in the singular limit , we obtain the fast subsystem ode x = p x + x, where p r becomes a parameter. we can also view p as a random parameter for the dynamics. observe that there are several branches of equilibrium points for given by solving p = x x. if p , then there are three equilibria, two locally asymptotically stable and one unstable. at p= / and p= /, there are non-degenerate fold bifurcations. while for |p| > /, there is always only one globally stable equilibrium point x. let us focus on the case of switching p near p= /; the case p= / simply follows by a symmetry argument. if we switch the dynamics randomly above and below the fold bifurcation, theorem suggests that with probability one, we are going to diverge away from the region of the fold, i.e., we are going to obtain a point measure eventually concentrated on single remaining equilibrium xexisting for p> / hence, if we have random switching across both folds, then it is possible to obtain the classical structure of a relaxation oscillations . this conrms similar observations made already numerically for a similar class of randomly switched van der pol oscillators in .","The van der Pol / Fitzhugh-Nagumo system is a classical model for bistable systems with relaxation oscillations and excitability. In the case where a parameter is small, a fast subsystem ordinary differential equation is derived, leading to multiple equilibrium points. Near certain parameter values, there are non-degenerate fold bifurcations, resulting in globally stable equilibrium points. By randomly switching the dynamics above and below the fold bifurcation, it is suggested that with high probability, the system will evolve towards a single remaining equilibrium point, leading to the classical structure of relaxation oscillations."
"the next ode model we are going to discuss is motivated by the swarming motion of locusts in a ring-shaped arena . an adaptive network model for this situation was proposed in . the network model views locusts in clockwise-moving and anti-clockwise-moving nodes and keeps track of interactions between dierent locusts/nodes via the links of the network. the model is reduced to a low-dimensional ode via moment closure and we focus here only on the essential features of the following low-dimensional ode model x = q + w(y / y /), x = q + w(y / y /), y = q + w(y + y /l yy/x), +w(y /x + y /(x ) y y/x ) + aex dey y = q + w(y + y /x yy/x), +w(y /x + y /(x ) y y/x ) + aex dey and the conservation equation = axx dy + ae(x + x ) de, where q, w, w, ae, de, a, d are positive parameters. basically, x and x correspond to propor- tions of clockwise r and anti-clockwise l moving nodes, while y,, capture the link densities rr, ll and rl between the two classes of nodes respectively. one checks that for ae = = de, one can solve the steady-state problem, which provides a branch of solutions given by x = = x this state corresponds to an equal number of left and right moving nodes. this disordered state is locally asymptotically stable up to a supercritical pitchfork bifurcation at a = d p q/w the parameter a controls the rate at which new connections between left and right moving nodes form. therefore, for a high connectivity between dierent groups, the system can move into an ordered phase given by the steady state := q qd /(wa ) and similarly for x with reversed signs. this corresponds to a classical symmetry-breaking and above the supercritical pitchfork, the two majority states are locally asymptotically stable. clearly, we can also view a =: p as our randomly switched parameter across the pitchfork bifurcation. then theorem provides us with the case of either one or three ipm if we switch near a the interpretation for swarming is that we eectively can allow for a certain percentage of disordered motion as long as the switching rate back into the ordered phase is large enough to get an eective ordered phase. furthermore, if we have the case of three ipm, then we are bound to observe not only a pure ordered state but intermittent phases of disordered motion as the non-trivial measures are supported also near the locally unstable state above the bifurcation point. references vladimir v. anisimov. switching processes in queueing models. iste ltd, john wiley & sons, inc., l. arnold. random dynamical systems. springer, berlin heidelberg, germany, yuri bakhtin and tobias hurth. invariant densities for dynamical systems with random switching. nonlinearity, :, yuri bakhtin, tobias hurth, and jonathan c. mattingly. regularity of invariant densities for d-systems with random switching. nonlinearity, :, m. bal azs, g. horv ath, s. kolumb an, p. kov acs, and m. telek. fluid level dependent markov uid models with continuous zero transition. performance evaluation, : , special issue: performance m. baudel and n. berglund. spectral theory for random poincar e maps. siam j. math. anal., :, m. bena m. stochastic persistence, part i. preprint. m. bena m and e. strickler. random switching between vector elds having a common zero. https://arxiv.org/abs/. michel bena m, st ephane le borgne, florent malrieu, and pierre-andr e zitt. qualitative properties of certain piecewise deterministic markov processes. annales de linstitut henri poincar e, : , michel bena m, st ephane le borgne, florent malrieu, and pierre-andr e zitt. quantitative ergodicity for some switched dynamical systems. electron. commun. probab., :no. , , michel bena m, st ephane le borgne, florent malrieu, and pierre-andr e zitt. on the sta- bility of planar randomly switched systems. ann. appl. probab., :, michel bena m and claude lobry. lotkavolterra with randomly uctuating environments or how switching between benecial environments can make survival harder. ann. appl. probab., :, n. berglund and b. gentz. noise-induced phenomena in slow-fast dynamical systems. springer, m. breden and c. kuehn. exploring invariant sets of random dynamical systems via poly- nomial chaos. preprint, pages , paul c bresslo. stochastic switching in biology: from genotype to phenotype. journal of physics a: mathematical and theoretical, :, j. buhl, d.j. sumpter, i.d. couzin, j.j. hale, e. despland, e.r. miller, and s.j. simpson. from disorder to order in marching locusts. science, :, g. da prato and j. zabczyk. ergodicity for innite-dimensional systems, volume of london mathematical society lecture note series. cambridge university press, cambridge, m. h. a. davis. piecewise-deterministic markov processes: a general class of nondiusion stochastic models. j. roy. statist. soc. ser. b, :, with discussion. m. h. a. davis. markov models and optimization, volume of monographs on statistics and applied probability. chapman & hall, london, m. engel, j.s. lamb, and m. rasmussen. bifurcation analysis of a stochastically driven limit cycle. arxiv:, pages , stewart n. ethier and thomas g. kurtz. markov processes. wiley series in probability and statistics. john wiley & sons, inc., hoboken, new jersey, characterization and convergence. a. faggionato, d. gabrielli, and m. ribezzi crivellari. non-equilibrium thermodynamics of piecewise deterministic markov processes. j. stat. phys., :, a. faggionato, d. gabrielli, and m. ribezzi crivellari. averaging and large deviation princi- ples for fully-coupled piecewise deterministic markov processes and applications to molecular motors. markov processes and related elds, :, s. goldstein. on diusion by discontinuous movements, and on the telegraph equation. quart. j. mech. appl. math., :, t. gross and h. sayama, editors. adaptive networks: theory, models and applications. springer, j. guckenheimer and p. holmes. nonlinear oscillations, dynamical systems, and bifurca- tions of vector fields. springer, new york, ny, martin hairer. ergodic properties of markov processes. lectures given at the university of warwick, http://www.hairer.org/notes/markov.pdf, reuben hersh. the birth of random evolutions. math. intelligencer, :, c. huepe, g. zschaler, a.-l. do, and t. gross. adaptive network models of swarm dynam- ics. new j. phys., page , mark kac. a stochastic model related to the telegraphers equation. rocky mountain j. math., :, k.l. kirk. enrichment can stabilize population dynamics: autotoxins and density depen- dence. ecol., :, c. kuehn. deterministic continuation of stochastic metastable equilibria via lyapunov equations and ellipsoids. siam j. sci. comp., :aa, c. kuehn. moment closure - a brief review. in e. sch oll, s. klapp, and p. h ovel, editors, control of self-organizing nonlinear systems, pages springer, c. kuehn. quenched noise and nonlinear oscillations in bistable multiscale systems. epl , :, yu.a. kuznetsov. elements of applied bifurcation theory. springer, new york, ny, rd edition, sean d. lawley, jonathan c. mattingly, and michael c. reed. sensitivity to switching rates in stochastically switched odes. commun. math. sci., :, dan li, shengqiang liu, and jingan cui. threshold dynamics and ergodicity of an sirs epidemic model with markovian switching. journal of dierential equations, : , andrew j. majda and xin t. tong. geometric ergodicity for piecewise contracting pro- cesses with applications for tropical stochastic lattice models. communications on pure and applied mathematics, :, florent malrieu. some simple but challenging markov processes. ann. fac. sci. toulouse math. , :, e. mccauley and w.w. murdoch. predator-prey dynamics in environments rich and poor in nutrients. nature, :, e.f. mishchenko and n.kh. rozov. dierential equations with small parameters and re- laxation oscillations . plenum press, v. nair, s. sarkar, and r.i. sujith. uncertainty quantication of subcritical bifurcations. probab. eng. mech., :, david nualart. the malliavin calculus and related topics. probability and its applications . springer-verlag, berlin, second edition, m.l. rosenzweig. paradox of enrichment: destabilization of exploitation ecosystems in ecological time. science, :, m.l. rosenzweig and r.h. macarthur. graphical representation and stability conditions of predator-prey interactions. american naturalist, :, s. sadhu and c. kuehn. stochastic mixed-mode oscillations in a three-species predator-prey model. chaos, :, edouard strickler. randomly switched vector elds sharing a zero on a common invariant face. available at https://arxiv.org/abs/, s.h. strogatz. nonlinear dynamics and chaos. westview press, g. teschl. ordinary dierential equations and dynamical systems. ams, g. george yin and chao zhu. hybrid switching diusions, volume of stochastic modelling and applied probability. springer, new york, properties and applications.","The text discusses an adaptive network model based on the swarming motion of locusts in a ring-shaped arena. The model represents locusts as nodes moving in clockwise and anti-clockwise directions, with interactions between nodes tracked through network links. The model is reduced to a low-dimensional ODE, focusing on parameters related to the proportions and link densities of nodes. The system exhibits a transition from a disordered state to an ordered phase, controlled by the rate of new connections forming between different node groups. The model demonstrates how the system can transition between ordered and disordered states, highlighting the impact of connectivity on swarming behavior."
"kinked entropy and discontinuous microcanonical spontaneous symmetry breaking hai-jun zhou, cas key laboratory for theoretical physics, institute of theoretical physics, chinese academy of sciences, beijing , china school of physical sciences, university of chinese academy of sciences, beijing , china spontaneous symmetry breaking in statistical physics is a macroscopic collective phe- nomenon. for the paradigmatic q-state potts model it means a transition from the disordered color-symmetric phase to an ordered phase in which one color dominates. existing mean eld theories imply that ssb in the microcanonical statistical ensemble (with energy being the control parameter) should be a continuous process. here we study microcanonical ssb on the random-graph potts model, and discover that the entropy is a kinked function of energy. this kink leads to a dis- continuous phase transition at certain energy density value, characterized by a jump in the density of the dominant color and a jump in the microcanonical temperature. this discontinuous ssb in random graphs is conrmed by microcanonical monte carlo simulations, and it is also observed in bond-diluted nite-size lattice systems. spontaneous symmetry breaking is a fundamen- tal concept of physics and is tightly linked to the origin of mass in particle physics, the emergence of superconduc- tivity in condensed-matter system, and the ferromagnetic phase transition in statistical mechanics, to name just a few eminent examples . in statistical physics a theoret- ical paradigm for ssb is the potts model, a simple two- body interaction graphical system in which each vertex has q discrete color states . the equilibrium ssb transition of the potts model in the canonical ensemble, where inverse temperature is the control parameter, has been extensively investigated (see refs. for some of the recent results). driven by energyentropy com- petitions, this transition is a discontinuous phenomenon when q is suciently large, with the density of the dominant color jumps from /q to a much larger value at the critical inverse temperature c. to compensate for the extensive loss of entropy, such a discontinuous tran- sition is always accompanied by a discontinuous decrease of the systems energy density u . when the system is isolated and cannot exchange en- ergy with the environment (the microcanonincal ensem- ble ), it is generally believed that the ssb transi- tion will occur gradually, with the dominant color density deviating from /q continuously at certain critical energy density umic. indeed if the microscopic entropy density s is a c-continuous function of energy den- sity u and its rst derivative are contin- uous), there is no reason to expect a discontinuity of the order parameter the c-continuity of s can be easily veried for the mean eld potts model on a com- plete graph . for nite-dimensional lattices the phase separation mechanism (the nucleation and expansion of droplets ) will guarantee a c-continuous entropy prole in the thermodynamic limit. for random graph systems one would na vely expect umic to be an inection point of s , which ensures c-continuity. in this letter we investigate the microcanonical potts model on random graphs using the bethe-peierls mean eld theory, and discover that the entropy density s is actually not c-continuous but is kinked at u = umic for any q . consequently, there is a discon- tinuous microcanonical phase transition at umic, with a jump in the dominant density and a drop in the mi- crocanonical inverse temperature. this ssb transition is driven completely by entropy competitions between the microcanonical polarized phase and the disor- dered symmetric phase, and at umic the mp phase is hotter than the ds phase. these theoretical predic- tions for random graphs are veried by microcanonical monte carlo simulations. the discontinuous ssb tran- sition is also observed in three- and higher-dimensional bond-diluted lattices, but only for system sizes not too large . the phenomenon of kinked entropy may per- sist in other multiple-state spin glass systems or com- binatorial optimization problems . our work also adds new insight on the debate about ensemble inequiv- alence . mean eld theory. consider a graph g formed by n vertices and m edges. each vertex i has a discrete color umic + umic umic s u fig. : schematic drawing of kinked entropy density s. as the energy density u of the q-state potts model decreases to umic, s changes from concave to convex and its slope drops from to the system is color-symmetric at umic+ with a lower microcanonical temperature /, but at umic it has a highly dominant color and a higher micro- canonical temperature / arxiv:v apr ci {, , . . . , q} and an edge between vertices i and j has a ferromagnetic interaction energy eij = cj ci , where cj ci = if ci = cj . the total energy of a color conguration c is the summed edge energies, e = p g eij, which is symmetric with respect to color permutations. the partition function z at a given inverse tempera- ture is z x c ee = x c y g h + cj ci i . we now review the bethe-peierls theory for this model . for simplicity we describe the theoreti- cal equations for random regular graphs, which are maximally random except that every vertex has exactly k attached edges. (the mean eld theory for general graphs can easily be derived following the cavity method of statistical physics or through loop expansion of the partition function .) this theory is exact for tree graphs, and because random graphs are locally tree- like and there is no intrinsic frustration in the edge interactions, we expect it to be exact for rr graphs as well. without loss of generality we assume c = to be the dominant color. to compute the marginal probability of this color state for a randomly chosen vertex i we rst delete i and all its attached edges from the graph. because short loops are rather rare in the graph, the k nearest neighbors of i will now be far sep- arated in the perturbed cavity graph and consequently their color states will be independent. we denote by q the probability of such a neighboring vertex j to be in state cj = in the perturbed graph, and assume that vertex j has equal probability / to be in any of the other color states. when vertex i and its k edges are added back to the graph, its probability of being in state ci = is then =  +  + q q + q k . this quantity is also the dominant color density of the rr graph. a similar expression for the cavity probability q of the neighboring vertex j can be written down (j has k edges in the cavity graph): q = b  +  + q q + q k . this self-consistent expression is referred to as a belief- propagation equation . the free energy density f ln z of the system can be computed by rst summarizing the indi- vidual contributions of all the vertices, and then sub- tracting the individual contributions of all the edges (be- cause each edge contributes to the free energies of two vertices) . at a bp xed point the explicit expression of f is f = ln n + q k +  + q q ko + k ln h +  q + q i . one can verify that f q = when q = b. the mean energy density u is obtained from eq. as u = k e q + q  +  q + q  . the entropy density s of the system is then determined by s = . the bp equation always has a trivial xed point q= /q which corresponds to the disordered symmetric phase with all the colors being equally abundant . this xed point becomes unstable with respect to the it- eration qt+ b when >ds ln   + q/  . for k and q, eq. has a stable xed point with q and strictly larger than /q at >cp, which cor- responds to the canonical polarized phase of broken color symmetry. here cp is the lowest inverse temperature at which the cp phase becomes possible. the cp and ds phases have equal free energy density at a critical inverse temperature c , so an equilibrium phase transition occurs at c, with a sudden drop in energy density u . microcanonical ssb. for the bp equation has another xed point which is unstable with respect to qt+ b . this xed point is usually neglected because its free energy is higher than those of the ds and cp phases ). but we nd that it reveals a discontinuous microcanonical phase tran- sition between the ds phase and a new microcanonical polarized phase of the conguration space. plotting the predicted thermodynamic values of the mp xed point ), we observe that while q and are monotonic functions of as anticipated, the energy density u and entropy density s both are non- monotonic. this surprising feature of u and s leads to the two-branched entropy prole shown in the upper-left inset of fig. these two entropy branches merge and stop at umax, which is the maximal achievable en- ergy density of the mp phase. the entropy of the lower mp branch is slightly lower than that of the ds phase so this branch has no physical signicance. on the other hand, the entropy of the upper mp branch exceeds that of the ds phase as u decreases below certain critical value umic which is strictly lower than umax, indicating the sys- tem will jump from the color-symmetric phase to a color- symmetry-broken mp phase which is stable only in the - - - f cp ds mp - - - - mp fixed point u s q, q - - - - s u cp ds mp - - - - - - - s - - - u cp ds mp rr d, n = d, n = d, n = - - - fig. : potts model on regular random graphs, k = and q= free-energy densities f for the disordered symmetric , the canonical polarized , and the microcanonical polarized xed points of the bp equation. the ds solution is stable at inverse temperature <ds =, the cp solution exists for cp =, and the dscp phase transition occurs at c = with the energy density u dropping from to . energy density u, entropy density s, xed-point value q and density of the dominant color , for the mp xed point. the maximal achievable mp energy density is umax = . and : entropy density s and dominant color density versus energy density u for the ds , mp , and cp xed points. upper-left and lower-right insets of show an enlarged view of the mp entropy prole and the dierence s between the mp and ds entropy density values . symbols in are microcanonical monte carlo simulation results obtained on a single rr graph and several bond-diluted eight-dimensional periodic hypercubic lattices of side length l = , , , degree k = and q= the inset of is an enlarged view of the transition region, and the phase transition point umic for rr graphs is marked by the vertical dashed line, at which jumps from / to . microcanonical ensemble. the dominant color density at umic is strictly higher than /q, so the spontaneous breaking of color symmetry is a discontinuous emerging phenomenon. notice that at u slightly below umic the entropy density of the mp phase is higher than that of the ds phase. because the entropy densities of the ds and mp phases are equal at u=umic but have dierent slopes ), the systems entropy density function s is not c- continuous but is kinked at umic . since the micro- canonical inverse temperature is equal to the rst deriva- tive of s, ds du , there will be a sudden drop of the microcanonical and an associated sudden drop of the free energy density f as the system changes from the ds to the mp phase at umic. in other words, at umic the partially ordered mp phase is hotter than the disordered symmetric phase and has a lower free energy density. this peculiar feature of s is qualita- tively dierent from the recently discussed entropy inec- tion phenomenon, which is associated with the vanishing of the second-order derivative of s . we have checked that as long as q, the discontinu- ous ssb phenomenon holds for all the rr graph ensem- bles of degree k as demonstrated in table i, at each xed value of q the and gaps at umic both decrease with degree k . the discontinuous microcanonical phase transition will also occur in an extended potts model with additional kinetic energies . monte carlo simulations. we carry out microcanon- ical monte carlo simulations to check the theoret- ical predictions. there are many discussions on micro- canonical mc methods , and here we employ the simple demon method to draw a set of independent congurations which are located slightly be- low a prescribed objective energy level eo. starting from table i: the critical energy density umic, the jump of the dominant color density and the drop of the micro- canonical inverse temperature at umic, for the q-state potts model on rr graphs of degree k. k q umic k q umic an initial conguration c of energy e eo, an elementary mc step unfolds as follows: pick a vertex i uniformly at random and change its color ci to a uniformly ran- dom new value c i ; accept this color change if the energy e of the resulting new conguration satises e eo, otherwise keep the old color ci; increase the evolution time t by a tiny amount /n (one unit time therefore corresponds to n single-ip trials). this mc dynamics obeys detailed balance, so the sampled color congurations all have the same statistical weight. the simulation results obtained on a large rr graph instance are shown in fig. . we indeed ob- serve a discontinuous transition at the predicted critical energy density umic. the numerical results on the dom- inant color density also agree perfectly with theory. the predicted inverse temperature gap is also quan- titatively conrmed by computer simulations . we also consider bond-diluted d-dimensional hyper- cubic lattices of side length l with periodic boundary conditions . by keeping only k bonds in a maximally random manner for every vertex (see for construction details), the shortest loops passing through the vertices rapidly increase their lengths as k decreases and d increases, and the diluted lattice is locally resem- bling a random graph . a discontinuous ssb tran- sition is observed in the mc dynamics for such bond- diluted lattice systems at high dimensions (e.g., d = , fig. ) and also at the physical dimension d= . however, unlike the case of truly random graphs, we ex- pect that the ssb transition in these lattice systems will become continuous in the thermodynamic limit , be- cause phase separation is deemed to occur as the system size l becomes suciently large . conclusion. in summary, we predicted and con- rmed a discontinuous microcanonical ssb phase transi- tion in the q-state potts model on random graphs. such a discontinuous transition was also observed in bond- diluted nite-size lattice systems (even down to three di- mensions ). in the future we need to investigate the geometric property of the congurations in the mp phase (e.g., the possibility of a percolating cluster of connected same-color vertices) , and possible latent structures prior to the microcanonical transition , and to study systematically the microcanonical ssb transition in nite-dimensional nite-size systems and the associ- ated inequivalence between the microcanonical and the canonical ensembles . the discovered property of kinked entropy may be a general feature of random graphical models with a canonical discontinuous phase transition, and it may have important computational consequences in optimization tasks . the following funding supports are acknowledged: na- tional natural science foundation of china grants no. and no. ; the chinese academy of sciences grant no. qyzdj-ssw-sys numeri- cal simulations were carried out at the hpc cluster of itp-cas and also at the tianhe- platform of the na- tional supercomputer center in guangzhou. the author thanks youjin deng, gaoke hu, hao hu, shaomeng qin, mutian shen, and jinhua zhao for helpful discussions and/or valuable comments on the manuscript. k. brading, e. castellani, and n. teh, symmetry and symmetry breaking, in the stanford encyclopedia of philosophy (metaphysics research lab, stanford univer- sity, winter edition, ), edited by e. n. zalta. r. b. potts, some generalized order-disorder transfor- mations, proc. cambridge phil. soc. , . f. y. wu, the potts model, rev. mod. phys. , . r. j. baxter, exactly solved models in statistical me- chanics . v. gorbenko, s. rychkov, and b. zan, walking, weak rst-order transitions, and complex cfts ii. two- dimensional potts model at q > , scipost phys. , . h. w. j. bl ote, w. guo, and m. p. nightingale, scaling in the vicinity of the four-state potts xed point, j. phys. a: math. theor. , . h. hu and y. deng, universal critical wrapping proba- bilities in the canonical ensemble, nuclear phys. b , . s. wang, z.-y. xie, j. chen, b. normand, and t. xi- ang, phase transitions of ferromagnetic potts model on the simple cubic lattice, chinese phys. lett. , . c. h. lee and a. lucas, simple model for multiple-choice collective decision making, phys. rev. e . l. tian, h. ma, w. guo, and l.-h. tang, phase tran- sitions of the q-state potts model on multiply-laced sier- pinski gaskets, eur. phys. j. b , . q. n. chen, m. p. qin, j. chen, z. c. wei, h. h. zhao, b. normand, and t. xiang, partial order and nite- temperature phase transitions in potts models on irreg- ular lattices, phys. rev. lett. , . y. deng, y. huang, j. l. jacobsen, j. salas, and a. d. sokal, finite-temperature phase transition in a class of four-state potts antiferromagnets, phys. rev. lett. , . d. h. e. gross, a. ecker, and x. z. zhang, microcanoni- cal thermodynamics of rst order phase transitions stud- ied in the potts model, ann. physik , . d. h. e. gross, microcanonical thermodynamics and sta- tistical fragmentation of dissipative systems: the topo- logical structure of the n-body phase space, phys. rep. , . v. martin-mayor, microcanonical approach to the sim- ulation of rst-order phase transitions, phys. rev. lett. , . f. moreno, s. davis, c. loyola, and j. peralta, ordered metastable states in the potts model and their connection with the superheated solid state, physica a , . see supplementary information for additional theoreti- cal and numerical results, some technical details on con- structing a bond-diluted lattice system, and a qualita- tive discussion of the nucleation phenomenon in nite- dimensional systems. m. biskup, l. chayes, and r. koteck y, on the forma- tion/dissolution of equilibrium droplets, europhys. lett. , . k. binder, theory of the evaporation/condensation tran- sition of equilibrium droplets in nite volumes, physica a , . l. g. macdowell, v. k. shen, and j. r. errington, nucle- ation and cavitation of spherical, cylindrical, and slablike droplets and bubbles in small systems, j. chem. phys. , . t. nogawa, n. ito, and h. watanabe, evaporation- condensation transition of the two-dimensional potts model in the microcanonical ensemble, phys. rev. e , . y.-z. xu, c. h. yeung, h.-j. zhou, and d. saad, en- tropy inection and invisible low-energy states: defensive alliance example, phys. rev. lett. , . m. m ezard and a. montanari, information, physics, and computation . d. mukamel, statistical mechanics of systems with long range interactions, aip conf. proc. , . a. campa, t. dauxois, and s. ruo, statistical me- chanics and dynamics of solvable models with long-range interactions, phys. rep. , . y. murata and h. nishimori, ensemble inequivalence in the spherical spin glass model with nonlinear interac- tions, j. phys. soc. jpn. , . h. touchette, equivalence and nonequivalence of ensem- bles: thermodynamic, macrostate, and measure levels, j. stat. phys. , . k. huang, statistical mechanics (john wiley, new york, second edition, ). m. m ezard, g. parisi, and m. a. virasoro, spin glass theory and beyond . j.-q. xiao and h.-j. zhou, partition function loop series for a general graphical model: free-energy corrections and message-passing equations, j. phys. a: math. theor. , . h.-j. zhou and c. wang, region graph partition func- tion expansion and approximate free energy landscapes: theory and some numerical results, j. stat. phys. , . j. pearl, probabilistic reasoning in intelligent systems: networks of plausible inference (morgan kaufmann, san franciso, ca, usa, ). m. creutz, microcanonical monte carlo simulation, phys. rev. lett. , . k.-c. lee, rejection-free monte carlo technique, j. phys. a: math. gen. , . p. schierz, j. zierenberg, and w. janke, first-order phase transitions in the real microcanonical ensemble, phys. rev. e , . l. a. fern andez, v. martin-mayor, g. parisi, and b. seoane, spin glasses on the hypercube, phys. rev. b , . h.-j. zhou and h. ma, communities of solutions in sin- gle solution clusters of a random k-satisability formula, phys. rev. e , . h.-j. zhou and c. wang, ground-state conguration space heterogeneity of random nite-connectivity spin glasses and random constraint satisfaction problems, j. stat. mech.: theor. exp. , p . kinked entropy and discontinuous microcanonical spontaneous symmetry breaking hai-jun zhou supplementary information","The text discusses kinked entropy and discontinuous microcanonical spontaneous symmetry breaking in the q-state Potts model on random graphs. The study reveals a discontinuous phase transition at a certain energy density value, characterized by jumps in the density of the dominant color and the microcanonical temperature. This phenomenon challenges the existing belief that spontaneous symmetry breaking in the microcanonical ensemble should be continuous. Theoretical predictions are supported by microcanonical Monte Carlo simulations. The discontinuous phase transition is also observed in bond-diluted lattice systems. The study sheds light on the debate about ensemble inequivalence and suggests that kinked entropy may have computational consequences in optimization tasks."
,
,
"consider a complete graph in which every vertex interacts with every other vertex. the energy of a color congu- ration c = is e = n n x i= n x j=i+ cj ci , where the rescaling factor n is introduced to make the total energy an extensive quantity. suppose there are nc vertices of color c in the conguration c, then the total energy can be rewritten as e = n q x c= nc n  = n q x c= c , where c nc n is the density of color c. in the thermodynamic limit of n the energy density u is simply u = q x c= c . the total number of microscopic congurations corresponding to the coarse-grained state is = n! n!n!nq!. in the thermodynamic limit then the entropy density s is s = n x c= c ln c . the task is now to nd the values of which lead to the maximum of s under the constraints of xed energy density u and xed number n of vertices. this can be achieved by introducing a function z with two lagrange multipliers and : z = n x c= c ln c + q x c= c + n x c= c . the rst derivative of this function with c is z c = ln c + c + therefore, from the condition z c = we obtain that c = ec pq c= ec . the color-symmetric xed-point solution of eq. is c = q for all colors c. the energy density of this disordered symmetric solution is the maximum value u = q, and its entropy density is ln q. if the energy density u decreases from the maximum value, then color symmetry has to be broken. therefore the critical energy density for spontaneous symmetry breaking is simply umic = q. the other xed-point solutions of eq. can be characterized by two parameters, and m. the real parameter [ q, ] is the density of a dominant color, and the integer m {, , . . . , q } is the number of dominant colors. without loss of generality we assume that i = for i = , , . . . , m and j = m qm for j = m + , m + , . . . , q. at a xed integer value of m, the order parameter is expressed as = q + r   m q  . - - - - - u q= - - - - - s fig. : q-state potts model on a complete graph. the maximum energy density is u = q and the minimum energy density is u= there is a continuous microcanonical ssb transition at energy density umic = q, which is identical to the maximum energy density . the density of the dominant color deviates continuously from q . the inset shows the non-concave relationship between the entropy density s and the energy density u. notice that is a continuous function of energy density u, so the ssb transition at umic must be a continuous phase transition . the entropy density s at the polarized xed point of eq. is s = m ln ln m q m , where is determined by eq. . the parameter m should be set to an integer value which maximizes s. it turns out that m = for all values of q. therefore, in the ssb phase there is only one dominant color, and all the other colors are equally abundant in the system. we nd that in the general case of q the entropy density function s is convex in the vicinity of umic . this non-concave property means that there is a discontinuous ssb phase transition in the canonical ensemble at certain critical value c of the inverse temperature. to summarize, for the complete-graph q-state potts model , the ssb transition is always a discontinuous phase transition in the canonical ensemble but it is always a continuous phase transition in the microcanonical ensemble.","The text discusses exact results on complete graphs for the q-state Potts model. It analyzes the energy and entropy densities as well as conditions for maximum entropy under fixed energy and vertex constraints. The critical energy density for spontaneous symmetry breaking is identified as umic = q, and different types of fixed-point solutions are characterized. The phase transition behavior is determined, showing that in the microcanonical ensemble, the symmetry breaking transition is continuous, while it is discontinuous in the canonical ensemble."
"in the canonical ensemble the inverse temperature of the environment is the control parameter, and the energy density u of the system is not xed. we now briey describe some of the results obtained by the bethe-peierls mean eld theory and by canonical monte carlo simulations. for concreteness we consider regular random graphs of vertex degree k = and set the number of colors to q=, as in the main text. first, depending on the value of , the bp equation q = b may have one, two, or three xed-point solutions, see fig. the trivial xed point q = q corresponds to the disordered symmetric phase and it is locally stable for < ds = . when cp = there is another stable xed point with q much larger than q, which corresponds to the canonical polarized phase. in this cp phase one color is much more abundant than each of all the other q colors , that is, the density of the dominant color is much higher than q. the free energy density of the cp phase becomes lower than that of the ds phase as exceeds the critical value c = , see fig. therefore there is a discontinuous equilibrium phase transition at c, at which jumps from q = to a much higher value and u drops from to . because of the high free energy barrier between the ds and cp phases, there is a strong hysteresis eect in the canonical mc simulation dynamics in the vicinity of c, see figs. and b q = = = cp ds mp mc mc - - - - u cp ds mp mc mc fig. : q-state potts model with q= in the canonical ensemble, on k = regular random graphs. the bp xed points are the intersection points of the curve b and the dashed diagonal line. depending on there might be one, two, or three xed points. and : the density of the dominant color and the mean energy density u for the disordered symmetric , the canonical polarized , and the microcanonical polarized xed points. the up- and down-triangles are canonical mc simulation results obtained on a single rr graph of size n = , with the initial color conguration being completely disordered and random or being completely ordered . the vertical dashed lines mark the canonical phase transition point c =. when the bp equation also has an unstable xed point, referred to as the microcanonical polarized one, whose free energy density is higher than those of the ds and cp xed points. this xed point therefore is physically irrelevant in the canonical ensemble, see fig.","In the canonical ensemble of the random-graph Potts model, the inverse temperature controls the system with variable energy density. Results from Bethe-Peierls mean field theory and canonical Monte Carlo simulations show different fixed-point solutions based on the parameter values. The model exhibits a phase transition at a critical value, leading to a jump in the dominant color density and a drop in energy density. Hysteresis effects are observed near the transition point. The system may have multiple fixed points, with only the disordered symmetric and canonical polarized phases being physically relevant in the canonical ensemble."
"for the potts model of q = on the rr graph of degree k = , the upper-left inset of fig. in the main text has shown how the entropy densities of the ds and mp xed-point solutions change with energy density u, but the entropy kink is not visually obvious in that gure. to clearly demonstrate the entropy kink, let us dene a modied entropy density function s as s s ucp . since ucp is linear in u, if s has a kink then the entropy density s will also have a kink. we redraw the theoretical data of fig. at the vicinity of the critical energy density umic = in fig. the kink of the - - - - s - u cp u ds mp - - fig. : this gure is complementary to fig. of the main text. the same theoretical data in the upper-left inset of fig. is redrawn here, but with the vertical axis being s s ucp, with cp = . the inset here is an enlarged view of the kink of s at umic =. entropy density is now quite evident. associated with the entropy kink at umic is the discontinuity of the microcanonical inverse temperature . to verify this discontinuity by the microcanonical monte carlo simulation dynamics, we notice that the microcanonical inverse temperature can be estimated by = log  + edemon  , where edemonis the mean energy of the demon . the non-negative demon energy edemon is simply the dierence between the objective energy eo and the actual energy e of the color conguration c, namely edemon = eo e. therefore edemonis easy to compute through the microcanonical mc evolution process. figure shows the good agreement between the theoretically predicted and actually measured microcanonical inverse temperatures for the rr graph of degree k = at q= this gure also shows the measured microcanonical inverse temperatures at dierent energy densities u for two of the eight-dimensional bond-diluted lattice systems used in fig. an interesting feature is that, given a xed value of energy density u at the ds phase , the microcanonical inverse temperature of d= diluted lattice systems is considerably lower than that of the rr graph - - - - - - u k=, q= ds mp rr d, n = d, n = fig. : the relationship between the microcanonical inverse temperature of the potts model and the energy density u. solid line and dotted line are theoretical predictions for rr graphs of degree k =, and the vertical dashed line marks the predicted microcanonical phase transition point umic. symbols are microcanonical monte carlo simulation results obtained on the graph instances of fig. , including the rr graph and the two bond-diluted eight-dimensional periodic hypercubic lattices of side length l = , , degree k = instance, and the dierence increases as u further increases. further research is needed to fully understand such dierences.","For the Potts model on the RR graph of degree k, the section discusses the entropy kink at umic. A modified entropy density function is defined to clearly demonstrate the entropy kink. The theoretical data is redrawn around the critical energy density umic to highlight the kink in the entropy density. The associated discontinuity of the microcanonical inverse temperature is also examined through Monte Carlo simulations, showing good agreement between predicted and measured values. Additionally, differences in the microcanonical inverse temperature between different lattice systems at fixed energy densities are observed, warranting further research to fully understand these variations."
"we investigate here the asymptotic property of the microcanonical ssb phase transition of the potts model as the degree k of the rr graphs approaches innity. for this purpose, it turns out to be convenient to dene a parameter, c, as c e + e q . let us denote the bp xed-point solution as q = q + . then the mean energy density u is expressed as u =   + c q  k q  + q q + qc q  . after some careful derivations, we nd that the free energy density dierence between the mp phase and the ds phase, at energy density value u, is expressed as sdi= k ln  + qc q  + k ln   + c  + ln h    )c + c ki +k ln  q  + c q    + qc q   + c q  k q  + q q + qc q  ln  q + q q  . if we assume to be small, then based on the bp equation the expression for is = q c  c q  . in the limit of k , we nd that, to fourth order of , sdi=  qkc kc  + kc + kq   + c q   c q  . in the limit of k , it turns out that c/q is very close to unity, so we write c = q k with being a small quantity. to the leading order of , we have = q . then we obtain from the condition sdi= that = kq , = q k . because = q + for k , we see that the asymptotic behavior of the jump at umic is kq . / k q= q= q= q= - - - / k q= q= q= q= - - - - - fig. : asymptotic behaviors of the gap of the dominant color density and the gap of the microcanonical inverse temperature , at the microcanonical ssb phase transition on rr graphs of degree k. the dierent sets of theoretical curves are for dierent values of q. in the main panel of is rescaled by = q k, while in the main panel of is rescaled by = q k the insets of and demonstrate the k asymptotic decay of and the k asymptotic decay of , with the k and k power laws marked by the two dashed lines. this asymptotic scaling behavior is in agreement with numerical computations, see fig. at a given energy density u, the dierence between the microcanonical inverse temperature of the mp and ds phases is = ln  + q q q  . at the microcanonical ssb phase transition point, the scaling behavior of is then q q k . this asymptotic scaling behavior of is also in agreement with numerical computations, see fig. these asymptotic results suggest that the microcanonical ssb phase transition on rr graphs will be discontinuous for any nite value of degree k, and it becomes continuous only at k = , i.e., when the graph becomes completely connected. s: the potts model with large q values on rr graphs of xed degree k it is also interesting to see how the microcanonical ssb phase transition behaves at the limit of large q. for rr graphs of xed degree k we can determine the gaps and at umic as a function of q, see fig. we observe that is not monotonic in q but it attains a maximum value at q and then decays slowly as a power law with exponent much smaller than unity. on the other hand, is monotonic in q and approaches a nal negative value as q . a nite value of at q is reasonable because the microcanonical inverse temperature is the slope of s. it would also be interesting to know the limiting behavor of the potts model as both q and k approach innity. the results in fig. and fig. indicate that will decay to zero no matter whether q approaches innity faster or slower than k. for , as it is expected to have a nite limiting value at q at each xed value of k and it decays as k at k for each xed value of q, we conjecture that will decay to zero as both q and k approach innity, and consequently the microcanonical ssb phase transition will become continuous at the limit of q and k both approach innity. - - - q k= k= k= k= k= k= - - q k= k= k= k= k= k= fig. : asymptotic behaviors of the gap of the dominant color density and the gap of the microcanonical inverse temperature , at the microcanonical ssb phase transition on rr graphs of degree k, as the number q of colors changes. the dierent sets of theoretical curves are for dierent values of k. the dashed line in marks the scaling behavior q, which is much steeper than the actual decaying behaviors of","The text explores the asymptotic properties of the microcanonical ssb phase transition of the Potts model on RR graphs as the degree k approaches infinity. By defining a parameter c as c = e + eq, the text derives expressions for mean energy density u and free energy density difference. It is observed that the microcanonical ssb phase transition on RR graphs remains discontinuous for any finite degree k, becoming continuous only when k = infinity. Additionally, the behavior of the transition at large q values and the limiting behavior as q and k approach infinity are also discussed, indicating a transition to continuity in the phase transition in those limits."
,
"the potts model discussed in the main text only considers the interaction energies between neighboring vertices in the graph. here we introduce local kinetic energies to the vertices to make the model more general . suppose there is a particle of mass m on top of each vertex i and this particle can move in a small conned space so it has a kinetic energy p i m, where pi is the momentum of this particle. the dimensionality of pi is denoted as d. the total energy of this extended system then depends on the color conguration c = and the momentum vectors {pi} of all the vertices: etotal = x g cj ci + n x i= p i m . when the total energy of this system is restricted to a tiny interval [e, e +e), where e is extensive and e , the partition function of the system is z = e+e z e detotal x c n y i= z dpi pd  etotal e x i p i m  , where p is certain characteristic momentum value needed to count the number of microscopic states in the momentum space, and e is simply the color energy. by integrating out the momentum degrees of freedom we obtain that z = mnd/   nd  pnd e+e z e detotal x c  m  nd . let us denote the total energy density of the system as utotal, that is, e = nutotal. by noticing that the total number of color congurations at energy e = nu is exp  s  , where s is the entropy density of color congurations at given interaction energy density u, we can re-write the above expression as z utotal z du exp  n  s + d ln utotal u  , - - - - - - u utotal - - - - fig. : the potts model with kinetic energies for regular random graphs . the discontinuous phase transition occurs at utotal =, at which the value of color interaction energy density u drops from u= to u=, the density of the dominant color jump from = / to = , and the microcanonical inverse temperature drops from = to =. where p m. from eq. we see that at a given value of total energy density utotal, the mean interaction energy density uof the system will be determined by u= arg max u  s + d ln utotal u  . therefore umust be a root of utotal u= d , where ds du is the microcanonical inverse temperature of the system. notice that this equilibrium interaction energy density udoes not depend on equation simply says that the mean kinetic energy of a vertex is equal to d . for intermediate values of utotal, eq. has a pair of solutions u, and the one which corresponds to higher total entropy density will be the physically relevant solution. as demonstrated in fig. for regular graphs (k =, q=, and momentum dimensionality setting to be d =), there is a discontinuous phase transition when the total energy density is decreased to the critical value utotal =. such a discontinuous phase transition will occur for other values of k and q as well.","The Potts model with kinetic energies extends the traditional model by incorporating local kinetic energies to vertices, resulting in a more general model. The total energy of this extended system depends on color configurations and momentum vectors of all vertices. The partition function is derived, integrating out momentum degrees of freedom. The model undergoes a discontinuous phase transition at a critical energy density, leading to changes in color interaction energy and dominant color density. Equilibrium interaction energy density is determined by maximizing total entropy density. The system exhibits a discontinuous phase transition for regular random graphs, with potential occurrence for other graph configurations as well."
"the vertices in a d-dimensional hypercubic lattice of side length l are located at positions where xd are integer values. periodic boundary conditions are imposed, so that and x , x , . . . , x d) are the same position if (xd mod l) = (x d mod l) for every d = , , . . . , d. the total number of vertices in the system is n = ld. each vertex has d bonds linking itself to its nearest neighboring vertices in space. the length of the shortest loops in such a hypercubic graph is equal to four and it does not increase with system size l. to make it more dicult for nucleation to occur (see the next section), we dilute this hypercubic graph by deleting a large fraction of bonds and keeping only k bonds for each vertex. a maximally random bond-diluted lattice graph is constructed according to the following procedure: construct an initial d-dimensional bond-diluted hypercubic lattice graph in which every vertex has exactly k active bonds . pick a vertex i uniformly at random from the lattice graph and pick uniformly at random an active bond from its k active bonds; then move to vertex j and pick uniformly at random an inactive bond from its d k inactive bonds; then move to vertex k and pick uniformly at random an active bond from its k active bonds; this chain of alternative active and inactive bonds is further extended until it visits a vertex that is already in the chain . if the length of this sampled loop is odd, nothing is changed. but if the length of this loop is even, then all the active bonds in this loop are deleted from the graph while all the originally inactive bonds of this loop are added to the graph . this switching action keeps the active degree of every involved vertex unchanged. repeat steps and a large number of times to make the bond-diluted lattice graph as random as possible. this loop-switching algorithm is similar to the algorithm used in ref. . it is easy to prove that this algorithm is ergodic and it leads to a uniform distribution among all the valid k-regular lattice graphs. because all the bonds in the original hypercubic lattice are between spatial nearest neighbors, the constructed bond-diluted graphs naturally contain only bonds between nearest neighbors. we also consider diluted lattice graphs with longer interaction ranges to explore the eect of interaction range to the microcanonical ssb transition. for this purpose, we consider a lattice system in which each vertex i at position of the periodic hypercubic lattice has d bonds to all the other vertices located in or at the surface of the hypercubic box of side length centered on vertex i. that is, there is a bond between position and the positions where xd {l, l + , . . . , , , , . . . , l , l} . this lattice graph is much more densely connected than the simplest hypercubic graph of degree d, since each vertex has d attached edges. to make it sparse we only retain k edges for each vertex and delete all the other edges. such a maximally random diluted graph can be sampled by the same loop-switching algorithm as described above. we have performed some preliminary computer simulations for the physically relevant dimension d = the side length of the periodic cubic lattice is xed to l = , the vertex degree is xed to k = , while four dierent values are tried for the interaction range parameter l, namely l = , , , the microcanonical mc simulation results shown in fig. clearly demonstrate that the interaction range has a dramatic eect on the ssb transition. when the interaction range l the simulation results on these nite-size lattice graphs are very similar to the predicted results for random rr graphs.","In a d-dimensional hypercubic lattice with side length l, periodic boundary conditions are imposed, and each vertex is connected to its nearest neighbors by d bonds. By diluting the lattice and keeping only k bonds for each vertex, a maximally random bond-diluted lattice graph is created using a loop-switching algorithm. The algorithm ensures a uniform distribution among valid k-regular lattice graphs. The text also discusses considering diluted lattice graphs with longer interaction ranges to study the effects on the microcanonical ssb transition. Preliminary computer simulations for a lattice with d = 3, l = 4, and k = 4, varying interaction range parameters l, show significant impacts on the ssb transition, with closer interaction ranges resulting in behavior similar to random graphs."
"here we review some of the key ideas of the droplet nucleation theory and discuss when droplet formation will be severely suppressed. to be concrete, we consider the d-dimensional hypercubic lattice of side length l with periodic boundary conditions. the total number of vertices in the lattice system is n = ld, and the total number of bonds is m = dn if every vertex only interacts with its d nearest neighbors. in the canonical statistical ensemble and at the thermodynamic limit l , the q-state potts model dened on such a lattice will experience an equilibrium phase transition at certain critical inverse temperature c, between the disordered symmetric phase and the canonical polarized phase. in the ds phase all the q colors are equally abundant, while in the cp phase one randomly picked color is favored over all the other colors. we consider the case of this canonical ssb phase transition being discontinuous. let us denote the energy densities of these two phases at c as uc ds and uc cp, respectively. similarly, the entropy densities of these two phases at c are denoted as sc ds and sc cp. because the dscp phase transition is an equilibrium one, we have sc ds sc cp uc ds uc cp = c . now we consider the microcanonical ensemble of xed energy density u and assume u takes an intermediate value between uc cp and uc ds. because of the existence of the equilibrium canonical dscp phase transition, the entropy density s at this intermediate energy density must satisfy the following inequality: s rsc cp + sc ds , - - - - - - u ds mp l= l= l= l= fig. : the potts model on four graph instances of three-dimensional short-range interaction lattice systems. symbols are microcanonical mc simulation results. the side length of the diluted lattice graphs is l = so the total number of vertices is n = each vertex has k = attached edges in these graphs, with the interaction range being l = , l = , l = , and l = . the theoretical predictions for rr graphs of degree k = concerning the density of the dominant color are the solid line and the dotted line . the vertical dashed line marks the predicted microcanonical ssb phase transition point umic = for rr graphs of degree k = where the parameter r is dened by r uc ds u uc ds uc cp . at the thermodynamic limit l it turns out that s achieves the upper-limit of the inequality by phase separation. the argument goes as follows. suppose the ds and cp phases coexist in the system and r is the relative size of the cp phase. in the case of r close to zero, to minimize the surface area between these two phases, we may assume that the cp phase is conned within a hyperspherical droplet of radius r. the total number nv of vertices in this droplet is then nv = crd while the total number ns of vertices on its surface is ns = crd, where c, c are two constants. when the droplet becomes large in size, we see that lim r ns nv = . a consequence of eq. is that, when r becomes suciently large the surface interaction energy between the cp and ds phases will be negligible in comparison with the volume interaction energy of the droplet. then the cp droplet and the ds subsystem can be treated as two independent systems and the entropy density of the combined system is then s = rsc cp + sc ds . because of eq. , the rst derivative of s at u=uc ds is equal to c. consequently s is c-continuous at uc ds. according to this droplet picture, in the thermodynamic limit l, when the energy density u is decreased to uc ds, phase separation starts to occur and the relative size r of the cp droplet increases gradually from r= at u=uc ds to r = as u is gradually decreased to u = uc cp. the inverse temperature of the system keeps the value c during this cp phase expansion process. since r continuously increases from zero, the density of the dominant color must also deviate from /q in a continuously manner. for a stable droplet to form in the system, however, the size r of the droplet must exceed certain threshold length rth, which depends on the energy density u of the whole system. if r is too small, the energy gain of forming a partially ordered droplet will not be enough to compensate for the penalty of interfacial energy, and then the droplet will be suppressed. if the side length l of the system is comparable or even smaller than rth, then it is likely the system as whole will change directly from the ds phase to a partially ordered phase, without experiencing the intermediate phase coexistence stage. this partially ordered phase for such nite-size systems may contain a percolating cluster of connected vertices which are all in the dominant color state (see, for example, ref. for related simulation results obtained on the supercooled liquid/gas system). the required minimum radius rth for droplet formation and phase separation might be greatly increased in a bond-diluted lattice system as compared to an intact lattice system. when bonds are deleted in a maximally random manner and the active degree k of every vertex is quite small, the graph will be locally quite similar to a random graph, and the typical length loop of the shortest path of unbroken bonds linking two spatial neighbors will be relatively large. indeed, as the spatial dimensionality d of the lattice increases while the active degree k of the vertices is xed, short loops will be more and more dicult to form and the graph will be more and more like a completely random graph. if the radius r of a hyperspherical region in such a bond-diluted spatial graph is of the same order as loop, the vertices of this region will be well approximated by a tree and its surface interaction energy will be comparable to the volume interaction energy of this region. therefore, for phase separation to occur the side length l of the lattice system must be much larger than loop. now we oer a rough estimate of loop. consider a rooted tree in the bond-diluted hypercubic lattice and assume the path length between two leaf vertices is ltree . since each internal vertex of this tree has k attached edges, the total number of vertices in the tree is approximately kltree/ because the directions of the edges in this tree are quite random, the length of the spatial region which contains this tree is approximately ltree, and the total number of vertices of this region is then approximately  ltree d. to guarantee the absence of loops the total number of vertices in the tree must not exceed the allowed number of vertices in the region. therefore, we estimate loop to be the largest integer value ltree which satises the following condition: kltree/  ltree d/ . in the case of k = and d = as in fig. , the above condition suggests that loop = , which indicates a threshold number of vertices exceeding nth = d = (which is much larger than the accessible graph sizes in our computer simulations). if the interaction range l in the lattice system increases while the vertex degree k keeps xed , because the euclidean length of an edge of this system is approximately l, the above condition probably needs to be modied as kltree/ ld ltree d/ , and the characteristic length loop will increase with l. for the d = graph instances of fig. , the estimated lengths are loop = for l = d = ), loop = for l = , and loop = for l = . these quantitative estimates may help explain why at l the simulation results of fig. for the d = spatial graphs are quite close to the theoretical results predicted for rr graphs.","The text discusses droplet nucleation theory in the context of a d-dimensional lattice system. It explores the equilibrium phase transition between disordered and polarized phases in a q-state Potts model on the lattice. The text introduces the concept of phase separation and explains how phase separation occurs as the energy density of the system changes. It also discusses the conditions for stable droplet formation and phase separation, highlighting the importance of system size and energy density. Additionally, the text mentions the impact of bond dilution on droplet formation and phase separation, emphasizing the relationship between lattice size, vertex degree, and network structure."
,
,
"mad: mapping and debugging framework for implementing deep neural network onto a neuromorphic chip with crossbar array of synapses roshan gopalakrishnan institute for infocomm research astar singapore roshan@ir.a-star.edu.sg ashish jith sreejith kumar school of electrical and electronic engineering nanyang technological university singapore ashishji@e.ntu.edu.sg yansong chua institute for infocomm research astar singapore chuays@ir.a-star.edu.sg abstractneuromorphic systems or dedicated hardware for neuromorphic computing is getting popular with the advance- ment in research on different device materials for synapses, especially in crossbar architecture and also algorithms specic or compatible to neuromorphic hardware. hence, an automated mapping of any deep neural network onto the neuromorphic chip with crossbar array of synapses and an efcient debugging framework is very essential. here, mapping is dened as the deployment of a section of deep neural network layer onto a neuromorphic core and the generation of connection lists among population of neurons to specify the connectivity between various neuromorphic cores on the neuromorphic chip. debugging is the verication of computations performed on the neuromorphic chip during inferencing. together the framework becomes mapping and debugging framework. mad framework is quite general in usage as it is a python wrapper which can be integrated with almost every simulator tools for neuromorphic chips. this paper illustrates the mad framework in detail, considering some optimizations while mapping onto a single neuromorphic core. a classication task on mnist and cifar- datasets are considered for test case implementation of mad framework. index termsmapping, debugging, neuromorphic computing, neuromorphic chip, spiking neuron, synapse, crossbar array, deep neural network, mnist, cifar-","The abstract discusses the MAD framework, designed for implementing deep neural networks on neuromorphic chips with crossbar arrays of synapses. The framework focuses on automated mapping of neural networks onto the chip and provides an efficient debugging system for verification during inferencing. MAD is a Python wrapper that can be integrated with various simulator tools, making it versatile for different neuromorphic devices. The paper details the framework, including optimizations for mapping onto a single core, and demonstrates its use with classification tasks on MNIST and CIFAR datasets. Key terms include mapping, debugging, neuromorphic computing, spiking neuron, synapse, crossbar array, and deep neural network."
"edge computing is one of the recent developments in the eld of articial intelligence. the amount of data being pro- cessed with the ever increasing inter-connectivity of devices and internet of things, ranging from sensors to autonomous ve- hicles, demand for high real time data processing at the edge. the edge devices are usually selected from neuromorphic chips, embedded devices, fpga, gpu/cpu etc depending on the application. among these devices, neuromorphic chip this research is supported by programmatic grant no. ab from the singapore governments research, innovation and enterprise plan . a version of this paper is submitted to ijcnn has proven to be the efcient or potential candidate in terms of computational power and latency. neuromorphic chips are developed in digital , analog or mixed signal integrated circuit designs. usual design trend is that mostly the computation and memory section is done in analog domain whereas, the communication between cores are maintained in digital domain. the neuromorphic chip discussed in this paper is based on crossbar architecture of non volatile memory synapses. however, one of the main challenges is to efciently map the neurons on to the neuromorphic chip with hardware constraints such as core size, number of cores and fan-in/fan- out . the existing neuromorphic chips have a mapping framework which is more hardware specic. ibms truenorth chip uses corelet language based on matlab, a programming language specic to their hardware. within this matlab framework, a mapping technique is integrated as a minimization problem . spinnaker and brainscales uses a simulator-independent language, pynn based on python. sequential mapping is used in spinnaker. neural engineering framework is developed for neurogrid . neu- trams addresses an optimized mapping technique based on graph partition problem: kernighan-lin partitioning strategy for network on chips. even though, every neuro- morphic chip simulator tools are addressing certain mapping techniques, optimized mapping onto a single neuromorphic core is often neglected and left unexplored by default. most of these mapping techniques are hidden within a neuromorphic hardware specic simulators, which mitigate the requirement of an algorithm developer to understand the details of a neuromorphic chip. but, for an optimized co-development of a neural network model for a specic neuromorphic chip, the knowledge of hardware constraints is a must. over the years, convolutional neural networks evolved to arxiv:v jan become more deep and wide with respect to the evolution of different classication tasks i.e. from simple mnist hand- written digit classication to much more complex imagenet image classication. for mnist classication task, as the neural network is small, the neurons can be mapped manually onto a neuromorphic core. but, for large networks in the case of imagenet classication, it is near impossible to manually mention how the neurons in every layers are mapped to each core in a neuromorphic chip. hence, an automated procedure is necessary for identifying the neuron addresses with corresponding synaptic weights and input values. in this paper, aforementioned issues are mitigated with the help of mad framework and its optimizations. mad frame- work is a generic python wrapper which has an optimized algorithm for mapping any feed forward neural network such as mlp, cnn, snn onto a crossbar array of synapses with corresponding synaptic weights, thereby tting the neurons in minimum possible number of neuromorphic cores. python wrapper is also suitable as a debugging tool for verication of the inferencing of neural network architectures on the neuromorphic chip. thus together the framework is called as mapping and debugging framework. this python wrapper is developed in connection with the simulator in , where most of the techniques are quite similar to neutrams . the paper is organized as follows. section ii briey describe about the crossbar array of synapses and the spiking neuron in a neuromorphic chip. section iii illustrates the details of mad framework. section iv shows the implementation of mnist and cifar- classication task on mad framework. finally the paper is concluded with discussion in section v.","Recent developments in artificial intelligence have led to the emergence of edge computing, driven by the vast amounts of data generated by interconnected devices like sensors and autonomous vehicles. This demand for real-time processing at the edge has led to the use of various devices such as neuromorphic chips, FPGA, and GPU/CPU. Neuromorphic chips, particularly those based on crossbar architecture with non-volatile memory synapses, are highlighted for their computational power and efficiency. However, a key challenge lies in effectively mapping neurons onto these chips due to hardware constraints. Existing mapping techniques are often hardware-specific, but a new Python-based mapping and debugging framework called MAD offers optimized solutions for neural network development on neuromorphic cores. The framework enables efficient mapping of neural networks onto crossbar arrays of synapses, reducing the need for manual mapping and enhancing the inference process. This paper discusses the MAD framework, its implementation on tasks like MNIST and CIFAR classification, and its potential to streamline neural network model development for specific neuromorphic chips."
"a. spiking neuron integrator comparator reset spikes input current fig. block diagram of a spiking neuron. biological neuron computes the signal received through multiple dendrites and transmits the output signal through axons to other neurons connected in the network . fig shows a block diagram representation of the biological neuron. neuron has mainly two blocks, an integrator and a comparator. the integrator sums up all the input currents (excitatory post- synaptic current, epsc) and build up the membrane potential. this membrane potential is being monitored by the comparator to cross certain threshold. if the membrane potential crosses the set threshold, neuron emits an output spike and then resets the membrane potential back to its initial value. the communication between neurons in the biological network or in a spiking neural network is with the help of these output spikes. the entire mechanism of a spiking neuron can be modelled with the leaky integrate and re neuron model and its mathematical expression is given below: m du dt = + ri where, m = rc, is the membrane time constant of leaky integrator. u = membrane potential i = synaptic current urest = membrane resting potential b. crossbar array of synapses input axons output neurons word line memory device synapse fig. crossbar array of synapses in a neuromorphic core. fig. shows a crossbar array of synapses. the crossbar structure is very suitable for performing matrix dot vector multiplication along each column in a crossbar architecture. for instance, a neuromorphic core with a core size of , input voltages from respective axons out of are given through word line. bit line collects all the weighted current at each synaptic nodes and delivers to respective output neurons for integration. the weighted current depends on the memory element used in the intersection of word line and bit line as synapse. the synaptic weights, which draws analogy to conductances, are represented in the form of blue dots at the cross points. from kirchoffs current law, the total current owing into each neuron from respective bit lines is the sum of currents owing through each intersection in every column. in fact, in conventional neural networks, total current of a particular column is the value of a single neuron activation in a particular layer, formed by summation of products of input voltages and corresponding synaptic weights taking part in convolution operation.","A spiking neuron functions through an integrator and comparator blocks, where it sums input currents to build up membrane potential, which is monitored by the comparator to emit output spikes. This neuron model can be represented by the leaky integrate and fire neuron model. In a neuromorphic core, a crossbar array of synapses facilitates matrix dot vector multiplication, with input voltages from axons and weighted currents collected at output neurons. This structure allows for efficient data processing in spiking neural networks."
"this section illustrates the details of the construction of mad framework. the complete usage of the framework is explained with a owchart as shown in g. a particular neural network is chosen for a classication or a detection task. the parameters like lter size, strides and padding among each layers are xed. the chosen network is trained using deep learning tool for obtaining the weight les to be given as input to the mapping function. core utilization is dened as the number of axons and neurons utilized in a single neuromorphic core. core utilization, as shown in the owchart, is an output from another function which calculates the number of axons and number of neurons used for mapping a section of particular layer onto a single core. core utilization is represented as . the details of the mapping function, core utilization and padding techniques are given in the subsequent subsections. this section is ended by including optimizations to be considered while mapping. fix parameter values select a neural network for a classification/detection task padding? python wrapper train using deep learning framework weights yes no connectivity matrix in dictionary format virtual padding technique core utilization total number of utilized cores connections between cores for simulator fig. flowchart of python wrapper: the details of the python wrapper is shown with a owchart. the input and output of the mapping function that is used in the python wrapper is illustrated in the owchart. the core utilization and weight les are marked in different color to show that these inputs are the results from other functions. a. mapping function the mapping function is the core of the python wrapper as shown in g. fig. shows the input and output of the mapping function. the inputs to mapping function are input size, lter size, stride, padding, core utilization and weight les. the input size is the size of the input datasets, for eg. in the case of mnist or in the case of cifar- filter size is the size of lters used for convolution in each layers, here it is selected as throughout the layers of the chosen neural networks in section iv. stride and padding depends on the layers of the convolutional neural network. the detailed calculation of the core utilization is mentioned in subsection iii-b. weight les are the weights obtained after training the chosen neural network using deep learning tool. the output section in g. shows the necessary outputs that is obtained from the mapping function. there are mainly three outputs, a connectivity matrix for verifying the interconnectivity between the cores and within the core, to verify the cores utilized and an automated generation of connection list for simulator. the steps for mapping are as follows: all the neurons are rst named to follow a regular pattern eg. l-f-n this implies layer:, feature map:, and neuron in row: and column: prepare a connectivity list of population of neurons in a particular layer connected to the previous layer. choose a population of neurons from a particular layer, based on the core utilization, to be mapped on to a particular core. repeat this process until entire neurons in every layers are completely mapped onto the core. since the naming and connectivity list are xed at the beginning, the neurons and axons will be automatically duplicated among the cores for mapping. b. core utilization x x x x neuron_col neuron_row layer n layer n- fig. two layers of convolution layer to illustrate the optimization of core utilization. layer n- neurons are in green, whereas layer n neurons are in red. synaptic connections are shown for two neurons in layer n. consider two layers of a convolutional neural network shown in g the neurons in layer n is marked as red and neurons in layer n- is marked as green. first two neurons in layer n is connected to layer n- and the synaptic connections are shown with straight lines. the convolution lter size used is , hence you can see connections from each red neurons in layer n to green neurons in layer n- likewise, the synaptic connections can be imagined throughout the layer with respect to the kernel size and strides used for convolution. while mapping these two layers in g. onto a core with crossbar array, the green neurons in layer n- will be the axons and the red neurons in layer n will be the neurons as in g notice the overlap of lter window when it strides across the layer. in fact these overlapped green neurons can be mapped onto the crossbar array connections without any duplication. duplicating the axons, while one to one mapping of neurons connected to axons onto a core, is not a good design with respect to core utilization as input needs to be duplicated into many axons and also mapping requires bigger core sizes and ends up utilizing many cores . hence, the toeplitz matrix method is utilized for efcient mapping of these layers onto a neuromorphic core without input duplication. toeplitz method for convolution is illustrated in . inorder to calculate the core utilization, the number of neurons and axons connected together has to be chosen which could be entirely mapped onto a single core. the number of axons can be evaluated as an algorithmic condition in the mapping function as there are overlapping axons whereas neurons selection become bit straight forward. the overlapping axons are dened as the axons which share connections with more than a single neuron, the term overlapping is because of the overlapping nature of the axons with the neighbourhood of the kernel lter with respect to strides (see layer n- in g. , the overlapping axons among the green and yellow synaptic connections are ). depending on this overlap, kernel lter size and strides, the total number of axons to be selected follows the formula as given below: n axons = kxk + kxsx+ sxsxx+ kxsx where, n axons = total number of axons to be selected k = convolution lter size s = stride neuron row = number of neurons across row neuron col = number of neurons across column the selection of neurons, neuron row and neuron col, in a layer depends on the condition: number of axons, n axons <= number of physical axons (eg. or or ) in the neuromorphic core. eq. is considering only a single feature map, this can be easily extended to multiple feature maps by multiplying with respective channel size. c. mad framework optimizations ) core utilization: referring to g. , consider a case for calculating core utilization, suppose neurons has to be chosen from layer n for mapping onto a core. this can be done by choosing rows and columns of neurons or rows and columns of neurons. here, rows and columns of neurons correspond to neuron row and neuron col in eq. if the convolution kernel size, k used is and stride, s is , then for rows and columns of neurons the axons required are rows and columns, similarly for rows and columns of neurons the axons required are rows and columns. this can be easily estimated from the formula to calculate the output size of convolutions as given below: o width = i width f width stride width + o height = i height f height stride height + where, o width and o height = width and height of the convolution output respectively i width and i height = input width and height respectively f width and f height = width and height of lter kernel s width and s height = width and height of strides the above case suggest that choosing neurons from rows and columns are much better for core utilization than from rows and columns as input number of axons in former case is only whereas, in the later case it is that means the core utilization is in the former case and in the later case. the intuition from this example case is that the neurons to be selected for mapping onto the core is better to be in square shape than in rectangular shape. the section below provides a mathematical proof for choosing square shape rather than rectangular shape while mapping: - - - - - - - - - - - - - - - - - - yi xi min ) fig. graphical illustration of the theorem. theorem: given a, nd xi and yi such that: xi yi = a and minimum of p. proof: the graphical illustration of the theorem is shown in g. consider xy = a and x + y = z where z is a real number x + a x = z dz dx = a x at minima dz dx = , a x = x = + a if x > , z is minimum x = a also y = a and x + y is the minimum. ) padding: padding is a common technique used in deep learning for maintaining the shape of the convolution layers throughout the network. padding simply adds extra zeros around the input activations in a convolution layer during con- volution operation. in fact such added zeros doesnt provide any computational signicance as mathematically zeros are multiplied and added. while mapping, these padded zeros are in fact physical neurons, but need not be participating in computation. if these neurons are considered as physical neurons during mapping, then there will be a lot of wastage on axon usage. this will reduce the optimized utilization of core. hence, as shown in g. (mentioned as virtual padding technique), when padding is used in a particular convolution layer, a virtually padded neuron address is created and is assigned in the connectivity list. later, while mapping onto the core these virtually padded neurons are removed from the connectivity list, reducing the fan in connection of those particular neurons in the periphery of a layer connected to those padded neurons in the previous layer.","The MaD Framework construction details are explained, including core utilization calculations and mapping functions. A neural network is chosen for classification/detection tasks with fixed parameters. The flowchart shows the process using a deep learning tool to train the network and obtain weight files for the mapping function. Core utilization is determined by the number of axons and neurons used in a single neuromorphic core. The mapping function involves naming neurons, creating connectivity lists, and selecting neurons based on core utilization. The section also covers core utilization optimization strategies, such as selecting neurons in a square shape for better utilization. Additionally, the text discusses the use of padding techniques, highlighting a virtual padding technique to optimize core utilization by removing unnecessary neurons during mapping."
"this section mainly provides an instance of the utilization of neuromorphic chip using a classication task on mnist and cifar- datasets through the parameters, core utilization and number of cores utilized. here, the focus is not on improving the accuracies, but to show the neuromorphic core utilization while mapping, with a much simpler handcrafted neural network. all the accuracies mentioned in this section is iterated for ten times and then averaged it out. two sets of experiments are done for that purpose, one is to choose a particular neural network architecture for classication task on mnist and cifar- datasets and keep that architecture constant among different core sizes. different core sizes cho- sen here are , and (core sizes need not be in square shape but any other shapes are also possible). here, the accuracy will be same, as architecture is constant, while the core utilization and number of cores utilized will be different among different core sizes. second set of experiment is to change the neural network architecture for different core sizes. this will change the accuracy of neural network architectures for different core sizes, but the number of cores utilized will remain same. table i neural network architecture for mnist dataset nn core size architecture input layer layer layer table ii neural network architecture for cifar- dataset nn core size architecture input layer layer layer ) keeping architecture constant: consider the architecture shown in table i and ii. the softmax classier output layer is not shown in the tables. for this set of experiment, the architecture is maintained same irrespective of different core sizes neural network architecture chosen for mnist and cifar- datasets are given in the rst column under the core size, respectively in both tables i and ii. the neural network architecture is kept constant while mapping onto other core sizes as well. the convolutional lter size used is throughout the layers. in table i, between input layer and layer , stride used is and with padding in the input activations. between layer and layer , stride used is and with padding in the input. between layer and layer , stride used is again but without padding in the input. in table ii, between input layer and layer , stride used is and without any padding in the input. between layer and layer , stride used is and without padding in the input. between layer and layer , stride used is again but with padding in the input. from table iii and iv, the results for mnist and cifar- classication accuracy is constant among all the core sizes as the architecture remains same, while the core utilization and number of cores utilized changes with core sizes. table iii keeping architecture constant: mnist dataset core size acc core no of core no of core no of utilization cores utilization cores utilization cores total cores table iv keeping architecture constant: cifar- dataset core size acc core no of core no of core no of utilization cores utilization cores utilization cores total cores ) keeping architecture different: the different neural net- work architectures chosen for mnist and cifar- datasets for different core sizes are shown in table i and ii. for this set of experiment, the architecture is changed slightly to t onto the respective core sizes. the modication of the network is only done on the number of feature maps or channels in different layers. this modication will not really affect the mapping much. but, rather better accuracies are obtained with same number of cores utilized. the convolutional lter size used is throughtout the layers. the strides and padding used between all the layers are exactly same as mentioned in the previous subsection. from table v and vi, the results for mnist and cifar- classication accuracy is shown for different core sizes and can be seen that the accuracy improves with increase in core sizes. this is obvious that bigger network can be mapped on to neuromorphic chips with bigger core sizes, bigger the network, better the accuracy. the core utilization varies with mapping but the number of cores utilized remains same with core sizes. table v keeping architecture different: mnist dataset core utilization no: of acc % table vi keeping architecture different: cifar- dataset core utilization no: of acc %","The IV Results section presents an application of neuromorphic chip utilization for a classification task on mnist and cifar- datasets. Two sets of experiments were conducted: one maintaining a constant neural network architecture and varying core sizes, and the other changing the architecture for different core sizes. Results show that accuracy remains constant when architecture is constant, with core utilization and number of cores varying. When the architecture is altered, better accuracies are achieved with larger core sizes. Core utilization fluctuates with mapping, but the number of cores utilized remains constant."
"random access memories are popular in terms of in- memory computation. resistive random access memory core fig. division of a convolutional neural network layer into different neuromorphic cores. became more popular in the eld of neuromorphic computing chips with the capability of doing both computation and memory at the same time. these two terminal rram devices are very much compatible with the crossbar array of synapses architecture, which enhanced its acceptance in the eld of neuromorphic chips. apart from rram, there are other devices like oating-gate mosfet , memristors , thin-lm devices and spin devices to be the contender of synaptic devices in a neuromorphic chip. the mapping of different portions of a convolutional layer onto different cores is shown in the g. different colors within the layer shows that those neurons are mapped onto particular core. for example, neurons in yellow are mapped onto core and neurons in brown are mapped onto core etc. the challenges in mapping onto a single neuromorphic core are mainly explained in the section for optimizations. one of the major priority while mapping is to choose the shape of the neurons in a layer that the chosen neurons and its correspond- ing axons could map completely onto a neuromorphic core without splitting the matrix between cores. another concern is to avoid the padded neurons while inferencing or mapping as these padded neurons during training is necessary to keep the size of the input activations but during inference these padded neurons become hardware overhead. in this paper, these two challenges are mitigated using simple techniques in the mapping function. from the results, it can be seen that bigger the core size, easier to map a bigger network and better the accuracy. similarly, if the accuracy is xed, then the lesser number of cores are utilized in a neuromorphic chip with bigger core size. this is infact better compared to usage of more number of cores in a neuromorphic chip with smaller core sizes because the communication between neuromorphic cores will consume more power than the computations. eventhough the neuromorphic chip with bigger core size is preferable, the bottleneck is the design possibility of such bigger crossbar array of synapses with the latest cmos technology. number of cores in a neuromorphic chip depends on the core size and the available silicon area for the chip. hence, number of cores and core size become a neuromorphic hardware constraint other than the major hardware constraints like synaptic noise, precision of weight and outputs. this paper gives an overview of mapping in neuromorphic chip with respect to the utilization of number of cores. the python wrapper for mad framework can output a visual representation of each core in a format easily veriable by the users . the verication of network activations and inferencing becomes quite simple as well. the code for python wrapper can be shared upon request. acknowledgment","Random access memories like resistive random access memory (RRAM) are popular for in-memory computation in neuromorphic computing chips, allowing for simultaneous computation and memory functions. RRAM devices are compatible with the crossbar array of synapses architecture, enhancing their acceptance in the field. Other devices such as floating-gate MOSFET, memristors, thin-film devices, and spin devices are also contenders for synaptic devices in neuromorphic chips. Mapping convolutional neural network layers onto different neuromorphic cores presents challenges, including optimizing neuron shape for complete core mapping and avoiding padded neurons during inference to reduce hardware overhead. Larger core sizes facilitate mapping bigger networks with higher accuracy using fewer cores. However, designing larger crossbar arrays poses a bottleneck due to CMOS technology limitations. The number of cores in a neuromorphic chip depends on core size and available silicon area, introducing hardware constraints alongside synaptic noise and output precision. Optimal core size and quantity are essential considerations for efficient neuromorphic hardware design. A Python wrapper for MAD framework facilitates visualization of neuromorphic cores and simplifies network verification and inference. This paper provides insights into core utilization and offers a Python wrapper for visualization, which can be shared upon request."
,
"wild globally hyperbolic maximal anti-de sitter structures andrea tamburelli abstract. let be a connected, oriented surface with punctures and negative euler characteristic. we introduce wild globally hyperbolic anti-de sitter struc- tures on r and provide two parameterisations of their deformation space: as a quotient of the product of two copies of the teichmller space of crowned hyper- bolic surfaces and as the bundle over the teichmller space of of meromorphic quadratic dierentials with poles of order at least at the punctures. contents","The abstract discusses wild globally hyperbolic maximal anti-de Sitter structures on a connected, oriented surface with punctures and negative Euler characteristic. Two parameterizations of the deformation space are provided: as a quotient of the product of two copies of the Teichmüller space of crowned hyperbolic surfaces and as a bundle over the Teichmüller space of meromorphic quadratic differentials with poles of order at least at the punctures."
,
,
"description of the boundary at innity parameterisation of wild anti-de sitter structures references introduction globally hyperbolic maximal anti-de sitter three-manifolds are a special class of lorentzian manifolds that share many similarities with hyperbolic quasi- fuchsian manifolds. mess initiated the study of the deformation space gh of such structures showing that if s is a closed, oriented surface of genus at least , then gh is parameterised by two copies of the teichmller space of s. after that, many progress has been made in the understanding of the geometry of these manifolds : in particular, krasnov and schlenker noticed that they behave more like almost-fuchsian hyperbolic manifolds in the sense that they always contain a unique embedded maximal sur- face with principal curvatures in . they exploited this fact in order to construct a new parameterisation of gh by the cotangent bundle of the teichmller space of s by associating to a ghm anti-de sitter manifold m the conformal class of the induced metric and the holomorphic quadratic dierential that determines the second fundamental form of the maximal surface embedded in m. arxiv:v jan wild ghm ads structures this construction has been later generalised by the author to include non-compact surfaces . in particular, if is a connected, oriented surface with punctures and negative euler characteristic, we introduced a special class of globally hyperbolic maximal anti-de sitter structures on r that we called regular and are parameterised by the bundle over teichmller space of of meromorphic quadratic dierentials with poles of order at most at the punctures. these man- ifolds play a role also in the theory of ghm anti-de sitter structures with closed cauchy-surfaces, as they can be seen as the geometric limits of such structures along pinching sequences in the cotangent bundle parameterisation . in this paper we extend our previous results in order to include higher order poles. we expect this theory to be relevant for the study of degeneration of ghm anti-de sitter structures along more general diverging sequences . we rst show existence and uniqueness of the maximal surface with given embed- ding data: theorem a. given a complete hyperbolic metric h of nite area on and a meromorphic quadratic dierential q with poles of order at least at the punctures, there exists a unique complete, conformal equivariant max- imal embedding : ads into anti-de sitter space whose second fundamental form is the real part of q. the embedding comes together with a representation : isom that, by identifying isom with psl psl, is equivalent to a pair of representations l,r : psl. by the recent work of gupta , we will deduce that l,r are faithful and discrete and send peripheral curves to hyper- bolic elements. the main part of the paper is devoted to the study of the boundary at innity of the maximal surface, that, unlike the closed case, is only partially de- termined by the representation . recall that the boundary at innity of anti-de sitter space can be identied with rp rp and the action of = extends naturally on each factor. theorem b. the boundary at innity of ( ) is a locally achronal curve that contains the closure of the set of pairs of attracting xed points of . this set is completed to a topological circle by inserting, in a -equivariant way, a light-like polygonal curve at each end. we will dene precisely in section what we mean by light-like polygonal curve. here it suces to mention that it consists of an innite family of light-like segments on the boundary at innity of ads belonging to the right-foliation and the left- foliation in an alternate way, which is equivariant by the action of the cyclic group wild ghm ads structures generated by the hyperbolic translation along the corresponding peripheral curve. the boundary at innity of ( ) determines then a domain of dependence on which ) acts properly discontinuously and the quotient gives the desired wild globally hyperbolic anti-de sitter manifold dieomorphic to r. moreover, using the relation between maximal surfaces and minimal lagrangian maps, we are able to give an analogue of mess parameterisation for wild anti-de sitter structures. recall that an orientation preserving dieomorphism m : between hyperbolic surfaces is minimal lagrangian if there exists a riemann sur- face x and harmonic maps f : x and f : x with opposite hopf dierentials such that m = f f these are in one-to-one correspondence with -equivariant maximal surfaces in anti-de sitter space via the gauss map : the riemann surface x is determined by the conformal structure of the maximal surface, h and h are hyperbolic metrics on with holonomy l and r respectively, and the harmonic maps f and f are the projections of the equivari- ant gauss map . as a consequence of the work of gupta , we deduce that in our case the image of the gauss map is a pair of crowned hyperbolic surfaces and we prove the following: theorem c. the deformation space of wild globally hyperbolic maximal anti-de sitter structures on r is parameterised by the quotient of two copies of the te- ichmller space of crowned hyperbolic surfaces by the innite cyclic group generated by the diagonal action of dehn twists along the boundary curves and relabelling of the boundary cusps. outline of the paper. in section we recall well-known facts about anti-de sitter geometry, meromorphic quadratic dierentials and crowned hyperbolic surfaces. in section we prove existence and uniqueness of the equivariant maximal embedding starting from the data of a complete hyperbolic metric of nite area on and a meromorphic quadratic dierential with poles of order at least the boundary at innity of this surface is described in section we prove theorem c in section acknowledgement. the author would like to thank subhojoy gupta for answering specic questions about crowned hyperbolic surfaces. background material we recall here some well-known facts about anti-de sitter geometry, (meromor- phic) quadratic dierentials on riemann surfaces, and crowned hyperbolic surfaces that will be used in the sequel. throughout the paper, we will denote with a closed, connected, oriented surface and with = \ {p, . . . , pn} a surface with a nite number of punctures. we will always assume that < moreover, we wild ghm ads structures will denote with t the teichmller space of , i.e. the space of marked complete hyperbolic structures of nite area on up to isotopy. . anti-de sitter geometry. consider the vector space r endowed with a bi- linear form of signature x, y= xy + xy xy xy . we denote d ads = {x r | x, x= } . it can be easily veried that d ads is dieomorphic to a solid torus and the restriction of the bilinear form to the tangent space at each point endows d ads with a lorentzian metric of constant sectional curvature anti-de sitter space is then dened as ads = p rp . the natural map : d ads ads is a two-sheeted covering and we endow ads with the induced lorentzian structure. the isometry group of [ ads that preserves the orientation and the time-orientation is so, the connected component of the identity of the group of linear transformations that preserve the bilinear form of signature . the boundary at innity of anti-de sitter space is naturally identied with ads = p . it coincides with the image of the segre embedding s : rp rp rp, and thus, it is foliated by two families of projective lines, which we distinguish by calling s the right-foliation and s the left-foliation. the action of an isometry extends continuously to the boundary, and preserves the two foliations. moreover, it acts on each line by a projective transformation, thus giving an identi- cation between pso and psl psl. the lorentzian metric on ads induces on ads a conformally at lorentzian structure. to see this, notice that the map f : d s d ads  z z, + z z w  is a dieomorphism, hence ds is a model for anti-de sitter space if endowed with the pull-back metric f gads = |dz|  + z z  d . wild ghm ads structures therefore, by composing with the projection : d ads ads, we deduce that f continuously extends to a homeomorphism f : s s ads and in these coordinates the conformally at lorentzian structure is induced by the conformal class c = . notice, in particular, that the light-cone at each point p ads is generated by the two lines in the left- and right- foliation passing through p. . complete maximal surfaces in ads let u c be a simply connected domain. we say that : u ads is a space-like embedding if is an embedding and the induced metric i = gads is riemannian. the fundamental theorem of surfaces embedded in anti-de sitter space ensures that such a space-like embedding is uniquely determined, up to post-composition by a global isometry of ads, by its induced metric i and its shape operator b : tu tu, which satisfy ( db = ki = det where is the levi-civita connection and ki is the curvature of the induced metric on . we say that is a maximal embedding if b is traceless. in this case, the codazzi equation implies that the second fundamental form ii = i is the real part of a quadratic dierential q, which is holomorphic for the complex structure compatible with the induced metric i, in the following sense. for every pair of vector elds x and y on , we have re = i . in a local conformal coordinate z, we can write q = fdz with f holomorphic and i = eu|dz| thus, re is the bilinear form that in the frame {x, y} is represented by re =  re im im re  , and the shape operator can be recovered as b = ire. if the induced metric is complete, the space-like condition implies that, identifying d ads with d s via f, the surface is the graph of a -lipschitz map ([tamb, proposition ]) and its boundary at innity is a locally achronal topological circle in ads such that if two points are causally related, then a light-like segment joining them is entirely contained in ([tamb, lemma ]). wild ghm ads structures . ghmc anti-de sitter manifolds. this paper deals with the moduli space of a special class of manifolds locally isometric to ads we say that an anti-de sitter three-manifold m is globally hyperbolic maximal if it contains an embedded, oriented, space-like surface s that intersects every inextensible causal curve in exactly one point, and if m is maximal by iso- metric embeddings. it turns out that m is necessarily dieomorphic to a product s r . moreover, we say that m is cauchy compact if s is closed. we denote by gh the deformation space of ghmc anti-de sitter structures on sr. the theory is well-developed when s is closed of genus at least : theorem . gh is parameterised by t t. the homeomorphism is constructed as follows. given a ghmc anti-de sitter structure, its holonomy representation : isom = psl psl induces a pair of representations by projecting onto each factor. mess proved that both are faithful and discrete and thus dene two points in t. on the other hand, given a pair of fuchsian representations , there exists a unique homeomorphism : rp rp such that r = l for every the graph of denes a curve on the boundary at innity of ads and mess constructed a maximal domain of discontinuity d for the action of ), called domain of dependence, by considering the set of points whose dual space-like plane is disjoint from . the quotient m = d/) is the desired ghmc anti-de sitter manifold. later krasnov and schlenker introduced another parameterisation of gh by the cotangent bundle over t, which is what inspired our construction. let us recall it briey here. let m be a ghmc anti-de sitter manifold. it is well- known that m contains a unique embedded maximal surface s . lifting s to ads, we obtain an equivariant maximal embedding of h into ads, which is completely determined by its induced metric and a holomorphic quadratic dierential. by equivariance, these dene a riemannian metric i and a holomorphic quadratic dierential q on s. we can thus dene a map : gh t t m associating to a ghmc anti-de sitter structure the unique hyperbolic metric in the conformal class of i and the holomorphic quadratic dierential q. in order to prove that is a homeomorphism, krasnov and schlenker found an explicit inverse. they showed that, given a hyperbolic metric h and a quadratic dierential q that is holomorphic for the complex structure compatible wild ghm ads structures with h, it is always possible to nd a smooth map v : s r such that i = evh and b = ire are the induced metric and the shape operator of a maximal surface embedded in a ghmc anti-de sitter manifold. this is accomplished by noticing that the codazzi equation for b is trivially satised since q is holomorphic, and thus it is sucient to nd v so that the gauss equation holds. now, det = detre) = ev detre) = evq h and ki = ev = ev hence the gauss equation translates into the quasi-linear pde hv = ev evq h + kh . they proved existence and uniqueness of the solution to equation on closed surfaces and on surfaces with punctures, when q has simple pole sigularities at the punctures. in an analogous result was obtained for meromorphic quadratic dierentials with poles of order at most at the punctures. in section , we will extend this result to include higher order poles and describe the geometry of the associated maximal surface. . meromorphic quadratic dierentials. suppose that is endowed with a complex structure. a meromorphic quadratic dierential q on is a -tensor, locally of the form qdz, where q is a meromorphic function with poles at the punctures {p, . . . , pn}. in this paper, we are interested in meromorphic quadratic dierentials with poles of order n at the punctures. in this case, we can always nd a local coordinate chart around the puncture such that qdz = an zn + an zn + + a z  dz for some coecients aj c. meromorphic quadratic dierentials with poles at points p, p, . . . , pn of orders bounded above by n, n, . . . , nn n form a vector space over c of real dimension d = || + p i ni, by the riemann-roch theorem. in particular, the space of meromorphic quadratic dierentials with poles of order exactly n, . . . , nn is parameterised by rdn n. a meromorphic quadratic dierential q induces a singular at metrics |q| on that in local coordinates is written as |q| = |q||dz| the metric has cone singularities of angle at a zero of order m of q. when poles have order at least , the metric is complete of innite area, and poles are at innite distance from any point on the surface. moreover, strebel described the local picture of the singular at metric around a pole p of order n as a cyclic arrangement of half- planes glued along half-lines in their boundaries. these half-planes are constructed as follows . first, we choose a local coordinate w adapted to the wild ghm ads structures quadratic dierential in the sense that wq = wn dw if n is odd  wn/ + a w  dw if n is even . then, for every k = , . . . , n , the half-planes are the images of the natural charts k : h v = { < |w| < r} dened by the property that kq = d if p is a pole of odd order, these natural coordinates can be written explicitly as k = n  n exp  n log + ki n  where b > is big enough to ensure that k v . for even order poles the above construction needs to be slightly modied: for > small, we consider h = { c | < arg < + } . notice that there is a constant > depending on such that any pair of points , h is connected by a path in h with length bounded above by ||. the natural coordinates are then dened as a composition k = k : h h v where and k are dened as follows. the map k is given by extended to h for a suitable choice of b that guarantees that k(h ) v . an easy computation shows that kq =  + c + ib  d for a constant c c depending only on a. up to increase b further we can assume that c + ib < for every h . with this choice, the map f = + c log sends h injectively into a domain in the complex plane containing h + id fo some constant d > large enough, thus the function = f is well-dened as map : h h and the composition k = k is the desired natural coordinate. moreover, by construction, the union of the images k for k = , . . . , n is a punctured neighbourhood of p, and two consecutive charts only intersect along a half-ray in their boundary. wild ghm ads structures . crowned hyperbolic surfaces. a crown c with m boundary cusps is an incomplete hyperbolic surface bounded by a closed geodesic boundary c and a crown end consisting of bi-innite geodesic {i}m i= arranged in cyclic order, such that the right half-line of the geodesic i is asymptotic to the left half-line of the geodesic i+, where indices are intended modulo m. a crown comes equipped with a labelling of the boundary cusps compatible with the cyclic order. a polygonal end p of a crown is the z-invariant bi-innite chain of geodesic lines in h obtained by lifting the cyclically ordered collection of geodesics {i}m i= in c to its universal cover, where z is the group generated by the hyperbolic translation corresponding to the geodesic boundary c. notice that the ideal points of the chain of geodesics of the polygonal end limit to the end point of the axis of the lift of c. the hyperbolic crowns we will consider come with an additional real parameter, the boundary twist, that we associate with the geodesic boundary. in the corre- sponding polygonal end in the universal cover, this can be thought of as the choice of a marked point on the axis and the parameter is the signed distance of this point from the foot of the orthogonal arc from the cusp labelled with to . let s denote a compact, oriented surface of genus and b boundary com- ponents. a crowned hyperbolic surface is obtained by attaching crowns to a compact hyperbolic surface with geodesic boundaries by isometries along their closed bound- aries. this results in an incomplete hyperbolic metric of nite area on the surface. we denote with t the teichmller space of crowned hyperbolic sur- faces such that the i-th crown has mi boundary cusps, for every i = , . . . , b. in this context the marking is a homeomorphism f : s x sending a neighbourhood of the boundary to the crown end. two marked hyperbolic surfaces with crowns and are equivalent if there is an isometry i : x y that is homotopic to g f via a homotopy that keeps each boundary component xed, and g f does not dehn-twist around any crown end. proposition . the teichmller space of crowned hyper- bolic surfaces is homeomorphic to rn, where n = + pb i= here is a possible way to give coordinates to t: the rst +b parameters are the familiar fenchel-nielsen coordinates on the teichmller space of surfaces of genus and b geodesic boundaries. then, after xing an identica- tion of the universal cover of the surface with boundary with a domain in h, the marked crowned hyperbolic surface is determined by the end points of the lifts of the boundary cusps in a fundamental domain. in order to keep track also of the twist parameters, we x, in an equivariant way, a point on the lifts of each geodesic boundary so that the remaining pb i= mi parameters can be dened as follows: b real parameters are given by the signed distance between the base point xed above and the foot of the geodesic arc exiting from the boundary cusp labelled with """" intersecting the geodesic boundary orthogonally, and the other pb i= are wild ghm ads structures positive real numbers determined by the relative distance between the intersection points of the geodesic rays emanating from two consecutive cusps and orthogonal to the geodesic boundary. construction of the maximal surface in the next sections we are going to construct globally hyperbolic maximal anti-de sitter structures on r starting from the data of a complete hyperbolic metric h on of nite area and a meromorphic quadratic dierential q with poles of order at least at the punctures. we rst nd a complete equivariant maximal embedding into ads with induced metric i = evh and second fundamental form ii = re. we will then describe its boundary at innity and prove that acts by isometries and properly discontinuously on its domain of dependence, thus inducing a globally hyperbolic anti-de sitter structure on the quotient. let h t be a complete hyperbolic metric of nite area on and let q be a meromorphic quadratic dierential with poles of order at least at the punctures. recall that nding an equivariant maximal conformal embedding of into ads is equivalent to nding a solution to the quasi-linear pde hv = ev evq h + kh . this is an example of vortex equation, recently studied in the context of riemann surfaces with punctures in . we recall here for the convenience of the reader the main steps for the construction of the unique solution and the asymptotic esti- mates that will be used in the sequel. the main idea consists in choosing another complete background metric g in the same conformal class as h such that q g + kg at pi . in this way, the function u : r that satises evh = eug is the solution of the dierential equation gu = eu euq g + kg and the assumptions on g guarantees that u = is an approximate solution in a neighbourhood of the punctures. this metric g is dened as a smooth interpolation between the metric h of constant curvature and the at metric induced by the quadratic dierential. more precisely, we introduce a local coordinate zi in a neighbourhood ui of the puncture pi disjoint from the zeros of q and dene g = |q| for |zi| < ci ei|dzi| for ci |zi| ci h for |zi| > ci and on \ ui wild ghm ads structures for smooth interpolating functions i. moreover, we can assume that there exists i > such that q g i on ui because q g when zi proposition . there exists a bounded smooth function u : r satisfying gu = eu euq g + kg . proof. let f = eu euq g + kg. since f is an increasing function of u, the solution to equation is guaranteed by the existence of two continuous functions u : r such that u+ f, uf and uu+ . let us start with the supersolution u+. let f : r be a positive smooth function such that f = |zi|i on the neighbourhood {|zi| < ci} of the puncture pi for some i > to be chosen later. for any r we consider u+ = f. we claim that it is possible to nd > large enough and i > suciently small so that u+ is a supersolution. in fact, on vi = {|zi| < ci} ui, we can nd a constant di > such that |q| di|z| and we have g e|zi|i + e|zi|iq g kg i di|zi|i e|zi|i + e|zi|i  i di  u+ + + eu+ , which can be made negative, because the term in the middle is always non-positive and we can choose i small enough and large enough so that the sum of the rst and last term is negative. therefore, u+ is a supersolution on vi for every i > and > outside vi, we do not have control on the curvature of g and on the laplacian of f, but knowing that they are bounded, we can increase so that gf ef + efq g kg because ef grows the fastest when +. this proves that u+ is a supersolution everywhere on . as for the subsolution, let w : \q r be half of the logarithmic density of the at metric |q| with respect to g, that is ewg = |q|. we claim that w is a solution outside the zeros of q: in fact, gw ew + ewq g kg = ew + ewq |q| = because q|q| = and the rst term vanishes because the metric |q| is at outside the zeros of q. notice that w tends to at a zero of q and, by our denition wild ghm ads structures of the open sets ui and of the metric g, the background metric on has constant curvature in a small neighbourhood of the zeros of q. since any negative constant is a subsolution where the metric g has constant curvature , the function u= ( w on ui max on \ ui for a suciently large b > is a continuous subsolution, being it the maximum of two subsolutions. we remark that the resulting metric i = eug is complete because g is com- plete and u is bounded. moreover, the subsolution we found implies that i |q|. uniqueness follows then from a general result about vortex equations: proposition . for every non-zero holomorphic quadratic dierential on there exists a unique complete solution to equation . combining the above results we obtain: theorem . for any complete hyperbolic metric h on of nite area and for any meromorphic quadratic dierential q on with poles of order at least at the punctures there exists a unique complete equivariant maximal embedding : ads with induced metric i conformal to h and second fundamental form ii = re. moreover, the principal curvatures are in . proof. existence and uniqueness of such embedding follows from the above discus- sion. let be the positive principal curvature of the maximal surface. by denition of q, we have = det = euq g at the punctures. therefore, is bounded and a classical fact about maximal surfaces in anti-de sitter space implies that [, ). . asymptotic estimates. in order to describe the geometry of the maximal surface, we will also need the following precise estimate for the solution v in a neigh- bourhood of a puncture. recall that such a neighbourhood is covered by a collection of half-planes, in which the quadratic dierential pulls back to d by an abuse of notation, we will still indicate with v the function such that ev|d| equals the induced metric on ( ) in the -coordinates. proposition . let be a natural coordinate for q dened on a standard half-plane in a neighbourhood of a puncture p. then v = o e || p || ! as || +. proof. in a natural coordinate the function v satises the pde v = ev ev wild ghm ads structures because |q| = and the background metric is at. the subsolution and su- persolution in proposition also show that v is non-negative and innitesimal. in particular, we can assume that v on every half-plane and we have that ev ev v . the asymptotic estimate will then follow from [dw, lemma ] provided we show that the restriction of v to the boundary of a half-plane is integrable. in order to show this, we prove that v is exponentially decaying. let be a point in the boundary of the half-plane. if || is suciently large, this point is actually contained also in the precedent or the following standard half-plane. in both cases, we can nd a constant c > depending only on the gluing map between the half-planes and a ball of radius r = || c centered at , which is entirely contained in these two coordinate charts. using a coordinate in this ball br, the function v satises the same equation on this ball. therefore, the solution of the dirichlet problem ( h = h h|br = is a supersolution and as such is greater than v. it is then well-known that the solution of the above dirichlet problem is the function h = i( ||) i( |r) where i is the modied bessel function of the rst kind . hence, v h = o(|| e ||) as || +.","The text discusses the construction of maximal anti-de Sitter surfaces and their parameterization in the context of Lorentzian manifolds. It introduces globally hyperbolic maximal anti-de Sitter three-manifolds and their deformation space, showing that they share similarities with hyperbolic quasi-Fuchsian manifolds. Progress has been made in understanding the geometry of these manifolds, particularly in relation to embedded maximal surfaces with principal curvatures. The text also delves into the study of the boundary at infinity of maximal surfaces and presents a parameterization for wild anti-de Sitter structures. Furthermore, it extends the results to include non-compact surfaces and discusses the existence and uniqueness of maximal surfaces with specific embedding data. The relevance of these structures in the theory of anti-de Sitter manifolds with closed Cauchy-surfaces is highlighted. Additionally, the text touches upon meromorphic quadratic differentials, crowned hyperbolic surfaces, and the detailed construction of globally hyperbolic maximal anti-de Sitter structures."
,
,
,
"arxiv:v jan realizing data features by deep nets zheng-chu guo, lei shi, and shao-bo lin abstractthis paper considers the power of deep neural networks in realizing data features. based on rened covering number estimates, we nd that, to realize some complex data features, deep nets can improve the performances of shallow neural networks without requiring additional capacity costs. this veries the advantage of deep nets in realizing complex features. on the other hand, to realize some simple data feature like the smoothness, we prove that, up to a logarithmic factor, the approximation rate of deep nets is asymptotically identical to that of shallow nets, provided that the depth is xed. this exhibits a limitation of deep nets in realizing simple features. index termsneural networks, approximation rates, deep nets, covering numbers, data feature.","The text discusses the power of deep neural networks in realizing data features. It finds that deep nets can improve the performance of shallow neural networks for complex data features without requiring additional capacity costs. However, for simple data features like smoothness, the approximation rate of deep nets is asymptotically identical to that of shallow nets, up to a logarithmic factor, when the depth is fixed. This highlights the advantage of deep nets in realizing complex features but also points out a limitation in realizing simple features. Key terms include neural networks, approximation rates, deep nets, covering numbers, and data features."
"deep learning is recognized to be a state-of-the-art scheme in articial intelligence and machine learning and has recently triggered enormous research activities. deep neural networks is believed to be capable of discovering deep features of data which are important but are impossible to be found by shallow neural networks (shallow nets for short). it, however, simultaneously produces a series of challenges such as the efcient computation, algorithmic solvability, robustness, interpretability and so on. a direct consequence of these challenges is that users hesitate to utilize deep learning in learning tasks with high risk such as the clinical diagnosis and nancial investment, since it is not clear whether deep nets perform essentially better than the scheme in hand. thus, it is urgent and crucial to provide the theoretical guidance on when do deep nets perform better than shallow nets? generally speaking, there are three steps to study the above problem. the rst step is to correspond specic real-world applications to some data features. for example, gures are assumed to be local similarity ; earthquake forecasting is related to rotation-invariant features ; and computer vision requires the spareness of activated neurons on the receptive eld . the second step is to connect these data fea- tures with a-priori information which can be mathematically reected by specic properties of functions. in particular, local similarity usually corresponds to piece-wise smooth functions ; rotation-invariance generally corresponds to radial functions and sparseness on the receptive eld frequently corresponds to sparseness in the spacial domain . the last step is to pursue the outperformance of deep nets in approximating or learning these application-related z. c. guo is with school of mathematical sciences, zhejiang university, hangzhou, china. l. shi is with shanghai key laboratory for contemporary applied mathematics, school of mathematical sciences, fudan university, shanghai, china. s. b. lin is with department of mathematics, wenzhou university, wenzhou, china. z. c. guo and l. shi are the co-rst author. the corresponding author is s. b. lin . functions. in fact, the outperformance of deep nets has been rigorously veried in approximating piece-wise smooth func- tions , rotation-invariant functions and sparse functions , which coincides with the empirical evidences on image classication , earthquake prediction and computer vision . with the rapid development in deep nets approximation theory, there are numerous features that are proved to be realizable by deep nets , , , , , with much less neurons than shallow nets. different from these encouraging results, studies in learning theory showed that, however, to realize these features, capacities of deep nets are much larger than those of shallow nets with comparable number of free parameters. in particular, under some specied capacity measurements such as the number of linear regions , betti numbers , number of monomials , it was proved that the capacity of deep nets increases exponentially with respect to the depth but polynomially with respect to the width. an extreme case is that there exist deep nets with two hidden layers whose capacity measured by the pseudo- dimension is innite , . the large capacity of deep nets inevitably makes the deep nets learner sensitive to noise and requires a large amount of computations to nd a good estimator. in a nutshell, previous studies on advantages of deep nets showed that deep nets are capable of realizing various application-related data features, but it requires additional capacity costs. the rst purpose of our study is to gure out whether the large capacity of deep nets to realize data features is necessary. our study is based on two interesting observations from the literature , , , , , , . one is that the number of layers of deep nets to realize various data features is small, the order of which is at most the logarithm of the number of free parameters. the other is that the magnitude of free parameters is relatively small, which is at most a polynomial with respect to the number of free parameters. with these two ndings, we adopt the well known covering number , to measure the capacity of deep nets with controllable number of layers and magnitude of weights and present a rened estimate of the covering number of deep nets. in particular, we prove that the covering number of deep nets with controllable depth and magnitude of weights is similar as that of shallow nets with comparable free parameters. this nding together with existing results in approximation theory shows that, to realize various features such as sparseness, hierarchy, rotation-invariance and manifold structures, deep nets improve the performance of shallow nets without bringing additional capacity costs. as is well known, advantages of deep nets in realizing some special features do not mean that deep nets are always better than shallow nets. our second purpose is to demon- strate the necessity of deepening networks in realizing some simple data features. after building a close relation between approximation rates and covering number estimates, we prove that if only the smoothness feature is explored, then up to a logarithmic factor, approximation rates of shallow nets and deep nets with controllable depth and magnitude of weights are asymptotically identical. combining the above two statements, we indeed present rigorous theoretical verications to support that deep nets are necessary in a large number of applications corresponding to complex data features, in the sense that deep nets realize data features without any additional capacity costs, but not all. the rest of paper is organized as follows. in the next section, after reviewing some advantages of deep nets in approximation, we present a rened covering number estimate for deep nets. in section iii, we give a lower bound for deep nets approximation to show the limitation for deep nets in realizing simple features. in the last section, we draw a simple conclusion of this paper.","Deep learning is a cutting-edge approach in artificial intelligence and machine learning, sparking significant research interest. It is capable of uncovering deep data features inaccessible to shallow neural networks, yet faces challenges like efficient computation and interpretability. Research aims to determine when deep nets outperform shallow nets in specific applications by connecting data features with mathematical properties. While deep nets excel in approximating certain functions, their large capacity requirements pose sensitivity to noise and computation challenges. To address this, a study investigates the necessity of deep nets in realizing data features without additional costs, showing that deep nets improve performance without extra capacity demands. The study concludes that deep nets are essential for many applications with complex data features but not all."
"in this section, we study advantages of deep nets in ap- proximating classes of functions with complex features. after introducing some mathematical concepts associated with deep nets, we review some important results in approximation theory which show that deep nets can realize some application- related features that cannot be approximated by shallow nets with comparable free parameters. then, we present a rened covering number estimate for deep nets to show that deepening networks in some special way does not enlarge the capacity of shallow nets. a. deep nets with xed structures great progress of deep learning is built on deepening neural networks with structures. deep nets with different structures have been proved to be universal, i.e., , for deep convolutional nets, for deep nets with tree structures and for deep fully-connected neural networks. let i := and x = , . . . , x) id = d. let l n and d, d, . . . , dl n with d = d. assume k : r r, k = , . . . , l, be univariate nonlinear func- tions. for h = , . . . , h)t rdk, dene k( h) = ), . . . , k))t . deep nets with depth l and width dj in the j-th hidden layer can be mathematically represented as h{d,,dl,} = a hl, where hk = k(wk hk + bk), k = , , . . ., l, h = x, a rdl, bk rdk, and wk = (w i,j k )dk,dk i=,j= be a dk dk matrix. denote by h{d,,dl,} the set of all these deep nets. when l = , the function dened by is the classical shallow net. the structure of deep nets can be reected by structures of the weight matrices wk and parameter vectors bk and a, k = , , . . ., l. for examples, deep convolutional neural net- works corresponds to toeplitz-type weight matrices and deep fully-connected nets deep nets with tree structure fig. structures for deep nets deep nets with tree structures usually correspond extremely sparse weight matrices . throughout this paper, a deep net with specic structures refers to a deep nets with specic structures of all wk, bk, k = , . . . , l and a. figure shows two structures for deep nets. although deep fully-connected neural networks possess better approximation ability than other networks, the number of free parameters of this type networks is al = dl + l x k= , which is huge when the width and depth are large. a recent focus in deep nets approximation is to pursue the approx- imation ability of deep nets with xed structures. up till now, numerous theoretical results , , , showed that the approximation ability of deep fully-connected neural networks can be maintained by deep nets with some special structures with much less free parameters. in this paper, we are interested in deep nets with structures. for k = , . . . , l, we assume that the structure of deep nets is xed and there are fk,w free parameters in wk, fk,b free thresholds in bk and fl,a free parameters in a. then, there are totally n := l x k= + fl,a free parameters in the deep nets. we assume further n al. throughout the paper, we say there are fk,w free parameters in wk, if the weight matrix wk is generated through the following three ways. the rst way is that the matrix has fk,w entries that can be determined freely, while the reminder dkdkfk,w entries are xed, e.g., the weight matrix in deep nets with tree structures. the second way is that the weight matrix wk is exactly generated by fk,w free parameters, e.g., the toeplitz-type weight matrix in deep convolutional neural networks. the third way is that the weight matrix is generated jointly by both way above, that is, part of the weight matrix is xed, while the remaining part are totally generated by fk,w free parameters. denote by h{n,l,} the set of all these deep nets with l hidden layers, xed structure and n free parameters. denote further h{n,l,,r} = {hn,l, h{n,l,} : |wi,j k |, |bi k|, |ai| r, i dk, j dk, k l} the set of deep nets whose weights and thresholds are uni- formly bounded by r, where r is some positive number which may depend on n, dk, k = , . . . , l and l. we aim at studying the approximation ability and capacity of h{n,l,,r}. it should be mentioned that the boundedness assumption in is necessary. in fact, without such an assumption, , proved that for arbitrary > and arbitrary continuous function f, a deep net with two hidden layers and nitely many free parameters is fully able to generate an approximation hf, such that f hflp . this implies that the capacity of deep nets with two hidden layers and nitely many free parameters is comparable with that of lp, showing its extremely large capacity. therefore, to further control the capacity of deep nets, the boundedness assumption has been employed in large literature , , . b. a fast review for realizing data features by deep nets in approximation and learning theory, data features are usually formulated by a-priori information for corresponding functions, like the target function for approximation, re- gression function for regression and bayes decision func- tion for classication. studying advantages of deep nets in approximating functions with different a-priori information is a classical topic. it can date back to , when deduced the localized approximation property of deep nets which is far beyond the capability of shallow nets. the localized approximation of a neural network shows that if the target function is modied only on a small subset of the euclidean space, then only a few neurons, rather than the entire network, need to be retrained. we refer to [, def.] for a formal denition of localized approximation. since the localized approximation is an important step-stone in approximating piecewise smooth functions and sparse functions in spacial domains , deep nets perform much better than shallow nets in related applications such as image processing and computer vision . the following proposition, which can be found in , shows the localized approximation property of deep nets. proposition suppose that : r r is a bounded measurable function with the sigmoidal property lim t = , lim t+ = then there exists a deep net with two hidden layers, d + neurons and activation function provides localized approx- imation. rotation-invariance, is another popular data feature, which abounds in statistical physics , earthquake early warn- ing and image rendering . mathematically, rotation- invariant property corresponds to a radial function which is by denition a function whose value at each point depends only on the distance between that point and the origin. in the nice papers , , shallow nets were proved to be incapable of embodying rotation-invariance features. to show the power of depth in approximating radial functions, we present the denition of smooth radial function as follows. denition let a r, c > and r = s+v with s n := {}n and < v we say a univariate function g : a r is -lipschitz continuous if g is s-times differentiable and its s-th derivative satises the lipschitz condition |g g| c|t t|v, t, t a. denote by lip a the set of all -lipschitz continuous functions dened on a. denote also by lip the set of radial functions f = g(x ) with g lip . the following proposition, which can be found in , shows that deep nets can realize rotation-invariance and smoothness features of target functions, simultaneously. proposition let d and p . if is the logistic function, i.e. = +et , then for arbitrary f lip, there is an h h{n,,,r} such that f hlp cnr. furthermore, for arbitrary h h{n,,,r}, there always exists a function f lip satisfying f hl cnr/, where c, c are constants independent of d, d, . . . , dl or n. numerous learning problems in computer vision, gene analysis and speech processing involve high dimensional data. these data are often governed by many fewer variables, producing manifold-structure features in a high dimensional ambient space. a large number of theoretical studies , , have revealed that shallow nets are difcult to realize smooth and manifold-structure features simultaneously. conversely, deep nets, as studied in , , is capable of reecting these features, which is shown by the following proposition . proposition let id be a smooth d-dimensional compact manifold with d d. if is the relu activation function, i.e. = max{t, }, and f is dened on and twice differentiable, then there exists a g hn,,,r such that f gl cn d . where c is a constant independent of d, d, . . . , dl or n. the previous studies showed that, compared with shallow nets, deep nets equipped with fewer parameters are enough to approximate functions with complex features to the same accuracy. in the following table i, we list some literature on studying the advantages of realizing data futures. c. covering number estimates in the above subsection, we have reviewed some results on the advantages of deep nets in realizing data features. however, it does not mean that deep nets are better than shallow nets, since we do not know what price is paid for references features l , localized approximation sigmoidal sparse+smooth sigmoidal smooth+manifold relu , hierarchical+smooth sigmoidal hierarchical piecewise smooth relu finite radial+smooth relu log , sparse analytic log table i powers of deep nets in approximation such advantages in approximation. in this subsection, we use the covering number, which is widely used in learning theory , , , , , to measure the capacity of hn,l,,r and then unify the comparison within the same framework to show the outperformance of deep nets. let b be a banach space and v be a subset of b. denote by n the -covering number of v under the metric of b, which is the minimal number of elements in an -net of v . if b = l, we denote n := n) for brevity. our purpose is a tight bound for covering numbers of hn,l,, r. to this end, we need the following assumption. assumption for arbitrary t r and every k {, . . ., l}, assume |k k| c|t t| and |k| c for some c, c to be detailed, shows the lipchitz continuous prop- erty of k and exhibits the linear increasing condition of k. these assumptions have been utilized in , , to quantify covering numbers of neural networks with different structures. we can see that almost all widely used activation functions such as the logistic function, hyperbolic tangent sigmoidal function = + ) with tanh = /, arctan sigmoidal function = arctan + , gompertz function = eaebt with a, b > , relu = max{t, }, and gaussian function = et satisfy assumption with this assumption, we present our rst main result in the following theorem, whose proof can be found in appendix a. theorem let hn,l,,r be dened by . under assump- tion , there holds n  , hn,l,,r}  n n, where dmax := maxl dand c is a constant depending only on c, c and d. for satisfying assumption , it was deduced in , that log n = o  n log cr  , where c is a constant independent of or n. from theorem , we can derive log n = o  ln log cr  for some c independent of , l, d, d, . . . , dl or n. compar- ing with , we nd that, up to a logarithmic factor, deep nets do not essentially enlarge the capacity of shallow nets, provided that they have same number of free parameters and the depth of deep nets is at most log n. noting that the depths of deep nets in table i all satisfy this constraint, theorem shows that to realize various data features presented in table i, deep nets can improve the performance of shallow nets without imposing additional capacity costs. therefore, theorem together with table i yields the reason why deep nets perform much better than shallow nets in some complex learning tasks such as image processing and computer vision. recently, presented a tight vc-dimension bounds for piecewise linear neural networks. in particular, they proved that v cdim) = o, where v cdim denotes the vc-dimension of the set v and sgn := {x sgn) : f v }, where sgn) = if f and sgn) = otherwise. using the standard approach in , we can derive log n = o  ln log cr  provided that = = = l are piecewise linear, where c is a constant independent of , l, d, . . . , dl or n. comparing , there is an additional l in our analysis. the reason is that we focus on all activation functions satisfying rather than piecewise activation functions. it should be also mentioned that similar covering number estimates for deep nets with tree structures has been studied in , , . we highlight that different structures yield essentially non-trivial approaches. in fact, due to tree structures, the approach in , , is just to decouple layers by using the boundedness and liptchiz property of activation functions. however, in estimating covering number of deep nets with arbitrarily xed structure, we need a novel matrix- vector transformation technique, as presented in appendix a.","Deep neural networks have advantages in approximating functions with complex features compared to shallow networks. They can realize application-related features that shallow networks cannot approximate, even with comparable parameters. Deep nets with fixed structures, such as deep convolutional nets and deep fully-connected neural networks, have shown to be universal. Approaches to maintain approximation ability with fewer parameters have been studied. Deep nets excel in realizing data features like localized approximation, rotation-invariance, smooth radial functions, and manifold-structure features. By using covering number estimates, it has been shown that deep nets do not significantly increase capacity compared to shallow nets when having the same number of free parameters. This explains why deep nets outperform shallow nets in complex learning tasks like image processing and computer vision."
"previous studies showed that, to realize some complex data features, deep nets can improve the performance of shallow nets without additional capacity costs. in this section, we study in a different direction to prove that, to realize some simple data features, deep nets are not essentially better than shallow nets. a. limitations of deep nets approximation smoothness or regularity is a widely used feature that has been adopted in a vast literature , , , , , , . to present the approximation result, we at rst introduce the following denition. denition let c > and r = s+v with s n := {}n and < v we say a function f : id r is - smooth if f is s-times differentiable and for every j n, j = , . . . , d with + +d = s, its s-th partial derivative satises the lipschitz condition sf x . . . xd d sf x . . . xd d cx xv, where x, x id and xdenotes the euclidean norm of x rd. denote by lip the set of all -smooth functions dened on id. approximating smooth functions is a classical topic in neural networks approximation. it is well known that the approximation rate can be as fast as o for neural networks with n free parameters. in particular, the jackson- type error estimate dist, h{n,,,r, lp) c nr d has been established for shallow nets with analytic activation functions, where dist) := sup fu dist) := sup fu inf gv f glp denotes the deviations of u from v in lp for u, v lp. similar results has been derived in with deep nets with two hidden layers and a sigmoidal activation function. recently, derived an error estimate taking the form of dist, hn,l,,r, lp) c nr d log n for deep nets with l = log n and relu activation functions. we would like to point out that, for shallow nets with relu activation functions, estimates holds only for < r , which is also considered as the approximation bottleneck of shallow nets. the paper showed that deepening the networks can overcome this bottleneck for shallow nets. however, it should be mentioned from that for other activation functions except the relu activation functions, such a bottleneck does not exist. thus, the paper indeed conduct a nice analysis on the necessity of deepening relu nets. however, their established results can not illustrate the necessity of depth. in the following theorem that will be proved in appendix c, we show that deep nets cannot be essentially better than shallow nets in realizing the smoothness feature. theorem let p , l n. then dist, hn,l,,r, l) cr d , where c is a constant depending only on c, c, c, d and r. combining the estimates and , and noting fl cd,pflp with cd,p a constant depending only on d and p, we see that, when l is not too large, deep nets cannot essentially improve approximation by shallow nets approximation by dfcns fig. comparison between deep and shallow nets the approximation rate if one only considers the smoothness feature. when l is too large, it follows from theorem that we will need additional capacity cost for deep nets to improve the approximation ability of shallow nets. in other words, the smoothness feature is not sufcient to judge whether the depth of neural networks is necessary. b. remarks and discussions limitations of the approximation capabilities of shallow nets were rstly studied in in terms of providing lower bounds of approximation of smooth functions in the minimax sense. recently, highlighted that there exists a probabilistic measure, under which, all smooth functions cannot be ap- proximated by shallow nets very well with high condence. in another two interesting papers , , limitations of shallow nets were presented in terms of establishing lower bound of approximating functions with some variation restrictions. however, due to these results, it is still not clear whether the depth of neural networks is necessary, if only the smoothness information is given. theorem goes further along this direction and presents a negative answer. in theorem , to realize smoothness features, deep nets perform almost the same as shallow nets. this result veries the common consensus that deep learning outperforms shallow learning in some difcult learning tasks , but not always. moreover, our result also implies that whether deep nets can help to improve the performance of the existing learning schemes depends on what features for data we are exploring. combing our work with , , , , , , , , we can illustrate the comparison between shallow and deep nets in figure we declare that theorem only presents limitations of deep nets in realizing smooth features. as shown in figure , if more features are explored, we believe that the approximation rate of deep nets can break through the lower bound presented in .","Deep neural networks have been shown to enhance performance for complex data features compared to shallow nets. However, a study in this section challenges the notion that deep nets are always superior. The text discusses the limitations of deep nets in approximating smoothness and regularity features, highlighting that deep nets may not be significantly better than shallow nets for simple data features. The section presents theorems and error estimates, indicating that the necessity of depth in neural networks depends on the specific data features being explored. It concludes that while deep learning excels in challenging tasks, the performance improvement of deep nets over shallow nets varies based on the nature of the data features."
,
,
"arxiv:v jun martingale approximations and anisotropic banach spaces with an application to the time-one map of a lorentz gas mark demers ian melbourne matthew nicol december revised february abstract in this paper, we show how the gordin martingale approximation method ts into the anisotropic banach space framework. in particular, for the time- one map of a nite horizon planar periodic lorentz gas, we prove that h older observables satisfy statistical limit laws such as the central limit theorem and associated invariance principles. previously, these properties were known only for a restricted class of observables, excluding for instance velocity.","The paper discusses how the Gordin martingale approximation method fits into the framework of anisotropic Banach spaces. Specifically, it shows that for the time-one map of a finite horizon planar periodic Lorentz gas, Holder observables satisfy statistical limit laws like the central limit theorem and associated invariance principles. These properties were previously only known for a limited class of observables, excluding velocity."
"the traditional approach to proving decay of correlations and statistical limit laws for deterministic dynamical systems, following and continuing with young , involves symbolic coding. in particular, by quotienting along sta- ble leaves one passes from an invertible dynamical system to a one-sided shift. decay of correlations is then a consequence of the contracting properties of the associated transfer operator. in addition, nagaev perturbation arguments and the mar- tingale approximation method of gordin are available in this setting, leading to numerous statistical limit laws. these results on decay of correlations and statistical limit laws are then readily passed back to the original dynamical system. a downside to this approach is that geometric and smooth structures associated to the underlying dynamical system are typically destroyed by symbolic coding. in recent department of mathematics, faireld university, faireld, ct , usa. email: mde- mers@faireld.edu mathematics institute, university of warwick, coventry, cv al, uk. email: i.melbourne@warwick.ac.uk department of mathematics, university of houston, houston tx -, usa. email: nicol@math.uh.edu years, a method proposed by and developed extensively by numerous authors (for recent articles with up-to-date references see ) uses anisotropic banach spaces of distributions to study the underlying dynamical system directly. in particular, the method does not involve quotienting along stable manifolds. this leads to results on rates of decay of correlations and also to various statistical limit laws via nagaev perturbation arguments, see especially gou ezel . however, so far gordins martingale approximation argument has been absent from the anisotropic banach space framework. this is the topic of the current paper. the utility of such an approach is illustrated by the following example. example the landmark result of young established exponential decay of correlations for the collision map corresponding to planar periodic dispersing billiards with nite horizon. the method, which involves symbolic coding, also yields the central limit theorem for h older observables, recovering results of . turning to the corresponding ow, known as the nite horizon planar periodic lorentz gas, the clt follows straightforwardly from the result for billiards . however, decay of correlations for the lorentz gas and the clt for the time-one map of the lorentz gas are much harder. superpolynomial decay of correlations was established for suciently regular observables in using symbolic coding and dolgopyat-type estimates . this method also yields the clt for the time-one map , but again only for suciently regular observables. here, regular means smooth along the ow direction, so this excludes many physically relevant observables such as velocity. the rate of decay of correlations was improved to subexponential decay and nally in a recent major breakthrough to exponential decay . both references handle h older observables, suggesting that statistical limit laws such as the clt for the time-one map should hold for general h older observables. currently the nagaev method is unavailable for lorentz gases, and as a conse- quence the clt for the time-one map was previously unavailable except for a re- stricted class of observables. we show that the gordin approach is applicable and hence the clt and related limit laws are indeed satised by h older observables for these examples. in particular, observables such as velocity are covered for the rst time. in the remainder of the introduction, we describe some of the limit laws that follow from the methods in this paper. for deniteness, we focus on example . let x be the three-dimensional phase space corresponding to a nite horizon planar periodic lorentz gas, with invariant volume measure , and let t : x x be the time-one map of the lorentz ow. let : x r be a h older observable with mean zero and dene the birkhosum n = pn j= t j. it follows from that we can dene = lim nn z x n d = x n= z x t n d. by , typically > (the case = is of innite codimension). we obtain the following results. clt: n/n d n as n . that is lim n c) = / z c ey/ dy for all c r. weak invariance principle : dene wn = n/nt for t = , n, n, . . . and linearly interpolate to obtain wn c. then wn w w where w denotes brownian motion with variance moment estimates: for every p there exists cp > such that |n|p cpn/ consequently, limnnp/np p = e|y |p where y =d n. homogenization: now suppose that : x rk. we continue to suppose that is c for some (, ] and that r x d = consider the fast-slow system x = x + a) + b)), y = ty, where x = rd and y is drawn randomly from . we suppose that a : rd rd lies in c+ and b : rd rdk lies in c+. solve to obtain x = + n x j= a) + n x j= b)), y = t ny, and let x = x. this denes a random process on the probability space depending on y x. then x w z as , where z satises an it o stochastic dierential equation dz = adt + b dw, z = , where w is a k-dimensional brownian motion with covariance matrix and a = a + d x = k x ,= e b x b. here, b is the th column of b and the matrices , e rkk are given by = x n= z x t n d, e = x n= z x t n d. the remainder of this paper is organized as follows. in section , we recall back- ground material on martingale-coboundary decompositions and statistical limit laws. in section , we state an abstract theorem on obtaining martingale-coboundary de- compositions for invertible systems with stable directions. in section , we apply our results to the time-one map of the lorentz gas. in what follows, d denotes convergence in distribution while w denotes weak convergence.","The text discusses different approaches for proving decay of correlations and statistical limit laws for deterministic dynamical systems. The traditional method involves symbolic coding and quotienting along stable leaves, leading to various statistical limit laws. However, this approach destroys geometric and smooth structures. An alternative method using anisotropic Banach spaces directly studies the underlying dynamical system without symbolic coding, resulting in rates of decay of correlations and statistical limit laws. The text focuses on applying the gordin approach to the anisotropic Banach space framework, particularly for Lorentz gases. The utility of this approach is illustrated with examples like the nite horizon planar periodic Lorentz gas, where results such as the Central Limit Theorem are obtained for Hölder observables. The text sets up the framework for martingale-coboundary decompositions and statistical limit laws, providing a detailed organization for the subsequent sections."
"in this section, we review the approach going back to gordin . this method yields martingale approximations for observables of dynamical systems leading to various limit theorems. related references include . let be a probability space, and let t : x x be an invertible ergodic measure-preserving transformation. let f be a sub--algebra of the underlying -algebra on x such that t f f consider an observable l with r x d = denition we say that admits a martingale-coboundary decomposition if = m + t , where m, l, m is f-measurable, and e = the conditions on m in denition mean that {m t n : n z} is a sequence of martingale dierences with respect to the ltration {t nf : n z}. proposition let lp for some p suppose that p n |e|p < , p n e t n p < . then admits a martingale-coboundary decomposition with m, lp. proof this is a standard argument . we give the details for completeness. by , = p n + p n e converges in lp. dene m = + t lp. then m = p n= = p n=, where gn = e. clearly, gn = e is f-measurable. also, gn t is measurable with respect to t f f hence m is f-measurable. next, note that gn t = e t = e. hence e = e = e|t f] = e, where we used that t f f substituting into , we obtain e = as required. most observables in this paper are real-valued, but occasionally in this section we consider observables with values in rk. we write l to denote vector-valued observables and write l instead of l central limit theorem and invariance principles corollary assume that l and conditions hold with p = then the clt and wip hold with = r x m d = limnn|n| proof this is a standard application of martingale limit theorems . somewhat surprisingly, by the results of , if l and conditions hold for p = , then automatically m l even though proposition only gives m, l this suces for the clt. related references for this phenomenon whereby m has extra regularity include . in particular, the following result holds: theorem assume that l and conditions hold with p = then the clt and wip hold. proof the clt and wip in reverse time is an immediate consequence of . passing from reverse time to forward time is standard (see for example ). now let be vector-valued with values in rk. dene c` adl` ag processes wn in rk and wn rkk: wn = n/ x j<nt t j, w n = n x i<j<nt t i t j. proposition suppose that t is mixing. assume that l and conditions hold with p = then the series = p n= r x t n d, e = p n= r x t n d, con- verge. w , where w is a k-dimensional brownian motion with covariance matrix and w = r t w dw + et. proof by proposition , admits a martingale-coboundary decomposition with m, l, so the result holds by . moments for optimal moment estimates, the following projective version of conditions are better suited: p n n/|e|p < , p n n/ et n p < . proposition assume lp and conditions hold for some p > then maxkn |k| p = o. if in addition n/n d y for some lp random variable y , then limnnq/|n|q q = e|y |q for all q < p. proof let an = pn j= t j. then for r , r x k= k/|e|p r x k= k/ k x j= |e|p = r x j= r x k=log j k/|e|p r x j= j/|e|p. by condition , p k= k/|e|p < . similarly, p k= k/|ak e|p < . recalling that t f f, it follows from that maxkr |ak| p r/ for general n choose r so that r < n r. then max kn |ak| p max kr |ak| p r/ / finally, k = t n so max kn |k| p = max kn |an ank| p max kn |ak| p n/, proving the rst statement. the second statement is an immediate consequence of the rst, see for example [, lemma ]. now let be vector-valued with values in rk and dene s n = p i<j<n t i t j. proposition assume that lp and conditions hold for some p then maxkn |s k | p/ = o. proof by proposition , we have a martingale-coboundary decomposition = m + t with m, lp. write s n = x i<j<n m t i t j + x j<n t j = in + jn we use the notation a b to denote a const.b, where the constant is independent of the other parameters present. where in = p i<j<n m t i m t j and jn = x i<n m t i + x j<n t j. now, max kn |jk| x i<n |m| t i + x j<n || t j. hence maxkn |jk| p/ n  |m|p||p + ||p||p  . next, we recall the identity ik = in ink t k (m n m nk t k)(m nk t k), k n, where m n = pn i= m t i. set m, n = x in m t i, i n = x j<in m t im t j. then m nk t k = m, nk t n and ink t k = i nk t n for all k n. hence ik =  i n i nk (m, n m, nk)m, nk  t n and so max kn |ik| p/ max kn |i k | p/ + max kn |m, k | p max kn |m, k | p. now i k = k x i= xi where xi = m t i i x j= m t j = m t im, i since {mt n; n } is a sequence of lp martingale dierences, {xi; i } is a se- quence of lp/ martingale dierences. by the inequalities of doob and burkholder , max kn |i k | p/ |(pn i=x i )/| p/ = |pn i=x i |p/ hence, using that p , max kn |i k | p/ pn i=|x i |p/ = pn i=|xi| p/ |m| p pn i=|m, i| p. applying burkholder once more, maxkn |m, k | p n/|m|p ; in particular maxkn |i k | p/ n|m|p|m|p. substituting these estimates into yields maxkn |ik| p/ n|m|p|m|p and the result follows. remark there is an error in due to an inaccurate appli- cation of a result of . (the argument in is ne for nonuniformly expanding maps but false for nonuniformly hyperbolic maps since the observable is not adapted to the ltration for the martingale.) this error was repeated in the rst version of the current paper and was spotted by the referee. as pointed out to us by the referee, the reference can be used for the ordinary moments n and this argument is now employed in the proof of proposition . (indeed, proposition is an improvement on the previous result [, eq. ] since it is no longer required that l.) however, it remains an interesting open problem to obtain optimal control of the iterated moments sn. homogenization as shown in , rough path theory yields homogenization of fast-slow systems provided the iterated wip and suitable iterated moment estimates hold. the iterated moment estimates have been relaxed in . we now apply these results to the fast-slow system . dene the c` adl` ag process x and the stochastic process z as in the introduction. we continue to assume that a c+ and b c+ for some > theorem suppose that t is mixing. assume that lp and condi- tions hold with p = then x w z as proof the iterated wip holds by proposition . by , it now suces to show that maxkn |k| q = o and maxkn |sk| q = o for some q > this and more follows from propositions and . remark the standard wip and moments are insucient to determine the limiting stochastic process z. by rough path theory the iterated process wn provides the extra information required to determine limiting stochastic integrals, and thereby the modied drift term . the iterated moment estimate s n provides the required tightness. note that wn and s n involve summation over i < j. the behaviour of their symmetrized versions follows immediately from the ordinary wip and moment estimate, and hence provides no extra information. (indeed the symmetrized version of w n is w n w n which converges weakly to w w .)","The section discusses martingale approximations for observables of dynamical systems, leading to various limit theorems. It introduces the concept of martingale-coboundary decomposition for observables and presents propositions, corollaries, and theorems related to this decomposition. The section also covers optimal moment estimates and projective versions of conditions for better estimations. Additionally, it discusses homogenization of fast-slow systems using rough path theory, emphasizing the importance of iterated moments and their relaxation. The text concludes by highlighting the significance of iterated processes in determining limiting stochastic integrals and modified drift terms."
"let t : x x be an invertible ergodic measure-preserving transformation on a probability space . we suppose that x is covered by a collection ws of disjoint measurable subsets, called local stable leaves, such that tw s w s for all x x, where w s is the partition element containing x. let f denote the -algebra generated by ws. note that w s t w s for all y t w s, so t w s is a union of elements of ws. hence t f f we denote by l the set of functions in l that are f-measurable. theorem let l be a mean zero observable. assume that there exists > and c > such that for all n , | r x t n d| c||n for all l. r x diam) d cn. then the conditions in are satised for all p < , and the conditions in are satised for all p < proof this is a standard argument. we again give the details for completeness. let = |e|p sgn e = t n, where = |e|p sgn e l, and ||||p . then |e|p p = |e|p p = z x e d = z x e d = z x d = z x t n d. by assumption , |e|p p = z x t n d c||n c||p n, and the rst part of conditions and follows by taking pth roots and using the restriction on p. next, using the pointwise estimate |e | diam) and as- sumption , |e t n|p p = |e |p p | diam)|p p p| diam)| pc||p n. the second part of conditions and follows. in the remainder of this section, we show that the conditions in theorem are satised in many standard situations. (the verications below are not needed for our main example in section ) verifying condition in theorem suppose that t : x x and ws are as above. let y x be a positive measure subset that is a union of local stable leaves in ws. dene the rst return time r : y z+ and rst return map f : y y , r = inf{n : t ny y }, f = t ry. let hn be the random variable on x given by hn = #{ j n : t jx y }. lemma let : x r be measurable. suppose that > n) = o) for some > and that there are constants c , such that | diam)| chn for all w s ws, n then condition in theorem holds. proof we have z x diam) d c n+ x k= k z x {hn=k}d c n+ x k= k z y {hn=k}r d. if y y {hn = k}, then pk j= rf j > n, and so rf j > n k for some j = , . . . , k hence z y {hn=k}r d k x j= z y {rf jn k }r d. it follows from the tail assumption on r that there is a constant c > such that > n) cn and r y {r>n}r d cn. write r = {rn}r + {r>n}r. then z y {rf jn k }r d z y n{rf jn k }d + z y {r>n}r d = n(r n k) + z y {r>n}r d ck+n + cn ck+n. therefore, r y {hn=k}r d ck+n, and z x diam) d ccn x k= kk+ = o, as required. verifying condition in theorem for completeness, we show that theorem includes examples that t within the chernov-markarian-zhang setup (in the summable decay of correlations regime, so > ) for h older mean zero observables : x r. in particular, we recover limit theorems that have been obtained previously for such invertible examples . since there are no new results here, we only sketch the construction from . remark when treating examples falling within the chernov-markarian-zhang setup, a signicant simplication is to suppose that there is exponential (or rapid) contraction of stable leaves under the underlying dynamics. for billiards with subexponential decay of correlations, such a condition fails since on average stable directions contract as slowly as unstable directions expand. in general, one should assume that there is an inducing set such that expansion and con- traction occurs only on visits to y . this general point of view is the one adopted here, as codied by the random variable hn in lemma . it is part of the setup that x is a metric space and t : x x is the canonical billiard map corresponding to the rst collision with the boundary of the billiard table. it is assumed that there is a set y x and a rst return map f = t r : y y such that f is uniformly hyperbolic and the return time has tail bounds satisfying = o), where we assume that > . moreover, y is modelled by a young tower with exponential tails . a standard argument (see for example [, theorem ]) shows that t : x x is modelled by a young tower f : with polynomial tails , with tail rate o) for all < in particular, there is a measure-preserving semiconjugacy : x, so we can work with f : instead of t : x x and observables = : r where : x r is h older. the nal part of the set up that we require is that is covered by stable leaves ws satisfying t) w, for all x , where w is the element of ws containing x. due to the uniform hyperbolicity of f = t r, the contraction condition in lemma holds . hence f : satises condition of theorem and it remains to verify condition . let f : denote the quotient young tower obtained by quoti- enting along stable leaves. consider observables : r that are lipschitz with respect to a symbolic metric on , with lipschitz norm . by , there is a constant c > such that z f n d z d z d c | |n, for all : r lipschitz, l( ), n (the dependence on and | | is not stated explicitly in but follows by a standard argument using the uniform boundedness principle. alternatively, see for a direct argument.) returning to the two-sided tower f : and the lifted observable = : r, it follows for instance from that there exists a choice (depending only on the h older exponent of ) of symbolic metric on and a sequence of observables l, , such that is f-measurable and hence projects down to an observable : r. sup l < . lim| f | = here, f is the -algebra generated by ws and l is the transfer operator correspond- ing to f : . let l with projection l( ). following [, proof of corol- lary ], r f n d= r f f +n d= i + i + i, where i = z ( f ) f +n d, i = z d z d, i = z f +n d z d z d. now |i| | f |||. also, i = r ( f ) d r d, so |i| | f ||| by , limij = for j = , by , i = z f +n d z d z d = z l f n d z l d z d , so by and , |i| cl | |n ||n. together, these estimates establish condition in theorem .","The section presents an abstract theorem applicable to systems with invertible, ergodic measure-preserving transformations on a probability space. The theorem outlines conditions for mean zero observables, involving a measurable function on a subset covered by stable leaves, and establishes the satisfaction of these conditions. Various verifications and examples within the Chernov-Markarian-Zhang setup are discussed, involving concepts such as the contraction of stable leaves, return maps, and tower models with exponential tails. The presentation concludes by demonstrating how the outlined conditions of the theorem are met in specific scenarios, providing insights into the application of the theorem in practical analysis settings."
"in this section, we use the results of to show that the hypotheses of theorem are satised for the time-one map corresponding to a nite horizon planar periodic lorentz gas for all h older observables . hence the results of section hold for all p , establishing the results listed in the introduction.","The text demonstrates that the hypotheses of a theorem are satisfied for the time-one map of a finite horizon planar periodic Lorentz gas with Hölder observables. Consequently, the results presented in the section apply to all p values, confirming the findings outlined in the introduction."
"let t = r/z denote the two-torus, and let bi t, i = , . . . , d, denote open convex sets such that their closures are pairwise disjoint and their boundaries are c curves with strictly positive curvature. we refer to the sets bi as scatterers. the billiard ow t is dened by the motion of a point particle in q = t \ sd i= bi undergoing elastic collisions at the boundaries of the scatterers and moving at con- stant velocity with unit speed between collisions. hence t is dened on the three dimensional phase space x = q s, s = / , where indicates that and are identied. between collisions, t = , while at collisions the point becomes where and + are the pre- and post-collisions angles, respectively. dening x = x/ , where we identify at collisions, we obtain a continuous ow t : x x let m = sd i= bi . the billiard map f : m m is the discrete- time map which maps one collision to the next. parametrizing each bi by an ar- clength coordinate r and letting denote the angle that the post-collision velocity vector makes with the normal to the scatterer (directed in- wards in q), we obtain the standard coordinates on m. for x x, dene the collision time to be the rst time t > that t m. since the closures of the scatterers are disjoint, there exists min > such that min for all x m. in addition, we assume that the billiard has nite horizon so that there exists max < such that max for all x x. it is well known that the ow preserves the contact form = cos dx + sin dx, so that the contact volume is d = dx ddx we denote by the normalized lebesgue measure on x, which by the preceding calculation is preserved by the ow. the main result of this section is the following. theorem let t be the time-one map corresponding to a nite horizon lorentz gas as described above, and let : x r be a mean zero h older observable. then conditions and of theorem hold with n replaced by ecn for some c > as a consequence, conditions and hold for all p , and all the results described in section apply in this setting. we remark that the observable is assumed to be h older continuous only on x, not x thus is allowed to be discontinuous at the boundary of x, i.e. at collisions. in particular, theorem applies to the velocity.","In the context of Lorentz gases, the section discusses the dynamics of point particles moving in a two-torus under elastic collisions with scatterers. The main result presented is a theorem stating that certain conditions hold for mean zero Hölder observables in this system, with these conditions being extended to all Hölder exponents. This result applies to the Lorentz gas system with a finite horizon and can be generalized to various properties analyzed in the section. The observables are required to be Hölder continuous on the phase space x but are allowed to be discontinuous at collisions."
"the remainder of this section is devoted to the proof of theorem , which consists of verifying the conditions of theorem . first we recall some of the essential properties and main constructions used in . hyperbolicity and singularities the singularities for both the collision map and the ow are created by tangential collisions with the scatterers. let s = { m : = }. away from the set s = s f s the map f is uniformly hyperbolic: letting = + minkmin, where kmin denotes the minimum curvature of the scatterers, there exist stable cs and unstable cu cones in the tangent space of m such that stable and unstable vectors in these cones undergo uniform expansion and contraction at an exponential rate given by . flowing cs backward and cu forward between collisions allows us to dene two families of stable cs and unstable cu cones for the ow that lie in the kernel of the contact form. (hence they are at two-dimensional cones in the tangent space of the ow; see for an explicit denition of these cones.) let p denote the projections from x onto m under the forward and backward ow. then cu is continuous on x away from the surface s = {x x : p + s}, and cs is continuous on x away from the surface s+ = {x x : p s}. to maintain control of distortion, we dene the standard homogeneity strips hk =  m : k , k k, for some k which is determined to ensure a one-step expansion condition. a similar set of homogeneity strips hk, k k, is dened for near following , we dene a set of admissible stable curves as for the ow. a c curve w belongs to as if the tangent vector at each point of w belongs to cs, and w has curvature bounded by b and length |w| bounded by here, > is chosen to satisfy a complexity bound and b is chosen large enough that the family as is invariant under t, t (once long pieces are subdivided according to the length ). we call w as homogeneous if p + lies in a single homogeneity strip. we dene ws to be the family of maximal c connected homogeneous stable manifolds for the ow. note that ws forms a partition of x . moreover, each element of ws belongs to as. when we dene a homogeneous stable manifold w ws, we take into account cuts introduced at the boundary of the extended singularity set, which includes the boundaries of the homogeneity strips. thus p + lies in a single homogeneity strip for all t let f denote the sigma algebra generated by elements of ws. since ws forms a partition of x, it follows that f comprises countable unions of elements of ws. due to our denition of cs, if w as, then p + is a stable curve for the map; and if w ws, then p + is a local homogeneous stable manifold for the map. norms and banach spaces with the class of admissible stable curves dened, we can now describe the banach spaces used to prove decay of correlations in . let (, ]. for w as, let c denote the closure of c functions in the holder norm dened by ||c = sup xw || + sup x,xw x=x | | dw, where dw is arclength distance along w. dene the weak norm of c by ||w = sup w as sup c ||c z w dmw, where mw denotes arclength measure on w. the weak space bw is dened as the completion of the set { c : ||w < }. the strong norm b is dened as in . the space b is similarly dened as the completion of a class of smooth functions on x in the b norm. since we do not need the precise denition of b here, we omit its denition; however, the following lemma summarizes some of the important properties of these spaces. lemma we have the inclusions c c b bw ), where the rst two inclusions are injective. moreover, | |w b c| |c and the unit ball of b is compactly embedded in bw. when we refer to functions c as elements of b or bw, we identify with the measure d. with this identication, the two denitions of lt given in the next section are reconciled. the following lemma is central to our verication of condition in theorem , and is a strengthening of . let c denote those functions which are in c for all w ws with ||c = supw ws ||c nite. lemma there exists c > such that for bw and c, || c||w||c. again, due to our identication, when c, we intend = r x d. lemma is proved at the end of this section. transfer operator we dene the transfer operator lt, for t , by lt = t, for c this can be extended to any element of bw, and more generally a distribution of order by lt = , for all c, ). by , the map lt from [, ) b to b is jointly continuous, so {lt}t is a semi-group of bounded operators on b. dene the generator of the semi-group by z = limt lt t for c while z is not a bounded operator on b, the strong continuity of lt implies that z is closed with domain dense in b. indeed, by the domain of z contains all c c such that c where denotes the ow direction, and there is a constant c > such that zb c||c for all such . condition of theorem recall that t and f denote the time-one map for the ow and the collision map, respectively. by the nite horizon condition, any w ws must undergo k n/maxcollisions after n iterates by t. by [, lemmas and ], diam = |nw| c|f k)| c|p +| cn/max|w|, where > is the hyperbolicity constant dened in . we have used here that the lengths of p + and w are bounded multiples of one another (indeed the jacobian of this map is c , see ). let : x r be c. then diam) || diam cn/max. hence condition holds with n replaced by n/max. condition of theorem by , z has a spectral gap on b and, using results of , lt admits the following decomposition: there exists > , a nite rank projector : b b and a family of bounded operators pt on b satisfying pt = pt = , and a matrix b z : with eigenvalues , z, . . . , zn c satisfying re zj < for j = , . . . , n, such that lt = pt + et b z for all t moreover, there exists c > such that for all in dom b, |pt|w cetzb for all t now suppose c c is of mean zero and c. by , r x t d = r x lt d = r x pt d + r x et b z d. hence by lemma , z x t d c n |pt|w + |et b z|w o ||c. letting denote the projector corresponding to the simple eigenvalue , we see that = r d = since is the conformal probability measure with respect to lt. hence by lemma , |et b z|w = |et b z|w cet||w cet||c by and , |pt|w cetzb cet||c substituting these estimates in , | r x t n d| = | r x n d| cen||c||c for all n the result extends to c as in by a standard mollication argument. in particular, there are constants c, c > such that | r x t n d| cecn||c||c for all n , for all c, c. let k denote the set of bounded functions on x that are constant on el- ements of ws, and let | |c = supw ws | |c note that these functions are f-measurable. moreover, k c and ||c = ||c for k. hence | r x t n d| cecn||c||c for all n , for all c, k. finally, let c, l. recall that l is the set of functions in l which are f-measurable, so there exists a pointwise representative in the equivalence class of in l that is constant on local stable manifolds and such that sup || = ||. in particular, k with ||c = ||and r x | | d = hence | r x t n d| = | r x t n d| cecn||c||. hence condition holds with n replaced by ecn. as promised, we end this section by proving lemma . proof of lemma by density of c in bw, it suces to prove the lemma for c and c. the normalized lebesgue measure on x projects to the measure = cos drd on m; this is the unique smooth invariant probability measure for the billiard map f. let w s denote the set of maximal connected homogeneous stable manifolds for f. note that p + = w s. indexing elements of w s, we write w s = {v}, which denes a partition of m. we disintegrate into conditional measures on v, , and a factor measure on . indeed, the conditional measures are smooth on each v, and we can write d = d m d, where m is arclength measure along v , and | log |c c, | |c c|v|, for some c > depending only on the table q . the exponent comes from the denition of the homogeneity strips. this is the standard decom- position of into a proper standard family . we further subdivide = sd i= i, where i is the index set corresponding to each component mi = bi [ , ] of m. write x = sd i= xi where xi = {x x : p + mi}. on each xi, we represent lebesgue measure as d = c cos dr d ds, where c is a normalizing constant, range over mi, and s ranges from to the maximum free ight time under the backwards ow, which we denote by ti max. next, for each i, the ow surface v = {x xi : p + v} is smoothly foliated by elements of ws, which are simply ow translates of one another. for each s and v, let w,s = tv, where t is dened for z v so that w,s lies in the kernel of , i.e. it is an element of ws. note that for s < , some points in v may not have lifted oof m. for such small times, w,s denotes only those points that have lifted oof m. similarly, for s > min, some part of tv may have collided with a scatterer. for such times, w,s only denotes those points which have not yet undergone a collision. thus s s w,s = v . using this decomposition, we may represent lebesgue measure on each xi by d = dmw,s d ds, where is smooth along each w,s, satisfying analogous bounds to , since the contact form is con xi and the projection p + is suciently smooth (see [, lemma ]), so that the arclength of w,s varies smoothly with that of v. standard families in are standard pairs dened on local unstable manifolds, while here we use local stable manifolds. the decompositions of have equivalent properties due to the symmetry of the map f under time reversal. using the fact that each w,s ws can be subdivided into at most c elements of as, we are ready to estimate z x d d x i= z xi d x i z ti z i z w,s dmw,s d ds x i z ti z i c ||w||c||c d ds c max||w||c z |v|d . this last integral is nite by since our decomposition of consti- tutes a proper standard family, yielding the desired estimate for . for completeness, we nish by proving . for x v, let rs denote the distance measured along v from x to the nearest endpoint of v. by [, theorem ], there exists c > such that sup > < ) c . we claim this quantity provides an upper bound on the relevant integral. to see this, we use the decomposition to write, c = sup > < ) = sup > z < ) d sup > z c |v| |v {rs < }| d sup > c z {:|v|>} |v| d = c z |v| d , where we have used the fact that |v| > for -a.e. , and the bound |v {rs < }| = if |v| > (one can also prove a reverse inequality, but we do not need this here.) acknowledgements we are very grateful to the referees for several helpful sug- gestions, especially with regard to the issue mentioned in remark . this research resulted from a research in pairs on techniques des martingales et espaces de banach anisotropes at cirm, luminy, during august md was supported in part by nsf grant dms im was supported in part by european advanced grant stochexthomog . mn was supported in part by nsf grant dms references v. ara ujo, i. melbourne and p. varandas. rapid mixing for the lorenz attractor and statistical limit laws for their time- maps. comm. math. phys. v. baladi. the quest for the ultimate anisotropic banach space. j. stat. phys. v. baladi, m. f. demers and c. liverani. exponential decay of correlations for nite horizon sinai billiard ows. invent. math. p. b alint and s. gou ezel. limit theorems in the stadium billiard. comm. math. phys. p. b alint and i. melbourne. statistical properties for ows with unbounded roof function, including the lorenz attractor. j. stat. phys. m. blank, g. keller and c. liverani. ruelle-perron-frobenius spectrum for anosov maps. nonlinearity r. bowen. equilibrium states and the ergodic theory of anosov dieomor- phisms. lecture notes in math. , springer, berlin, l. a. bunimovich, y. g. sina and n. i. chernov. statistical properties of two- dimensional hyperbolic billiards. uspekhi mat. nauk d. l. burkholder. distribution function inequalities for martingales. ann. prob- ability o. butterley. a note on operator semigroups associated to chaotic ows. ergodic theory dynam. systems n. chernov. a stretched exponential bound on time correlations for billiard ows. j. stat. phys. n. chernov and r. markarian. chaotic billiards. mathematical surveys and monographs , american mathematical society, providence, ri, n. chernov and h.-k. zhang. billiards with polynomial mixing rates. nonlin- earity n. chernov and h.-k. zhang. a family of chaotic billiards with variable mixing rates. stochastics and dynamics i. chevyrev, p. k. friz, a. korepanov, i. melbourne and h. zhang. multiscale systems, homogenization, and rough paths. probability and analysis in inter- acting physical systems: in honor of s.r.s. varadhan, berlin, august, , springer proceedings in mathematics & statistics , , p. i. chevyrev, p. k. friz, a. korepanov, i. melbourne and h. zhang. determin- istic homogenization under optimal moment assumptions for fast-slow systems. part preprint, j. dedecker, f. merlev` ede and f. p` ene. empirical central limit theorems for ergodic automorphisms of the torus. alea lat. am. j. probab. math. stat. j. dedecker and e. rio. on the functional central limit theorem for stationary processes. ann. inst. h. poincar e probab. statist. m. f. demers. a gentle introduction to anisotropic banach spaces. chaos soli- tons fractals d. dolgopyat. prevalence of rapid mixing in hyperbolic ows. ergodic theory dynam. systems p. k. friz and m. hairer. a course on rough paths. universitext, springer, cham, m. i. gordin. the central limit theorem for stationary processes. soviet math. dokl. s. gou ezel. almost sure invariance principle for dynamical systems by spectral methods. ann. probab. s. gou ezel. limit theorems in dynamical systems using the spectral method. hy- perbolic dynamics, uctuations and large deviations. proc. sympos. pure math. , amer. math. soc., providence, ri, , pp. h. hennion and l. herv e. limit theorems for markov chains and stochastic properties of dynamical systems by quasi-compactness. lecture notes in math. , springer, berlin, c. c. heyde. on the central limit theorem and iterated logarithm law for sta- tionary processes. bull. austral. math. soc. d. kelly and i. melbourne. smooth approximation of stochastic dierential equa- tions. ann. probab. d. kelly and i. melbourne. homogenization for deterministic fast-slow systems with multidimensional multiplicative noise. j. funct. anal. c. kipnis and s. r. s. varadhan. central limit theorem for additive functionals of reversible markov processes and applications to simple exclusions. comm. math. phys. a. korepanov, z. kosloand i. melbourne. explicit coupling argument for nonuniformly hyperbolic transformations. proc. roy. soc. edinburgh a c. liverani. central limit theorem for deterministic systems. international con- ference on dynamical systems (f. ledrappier, j. lewowicz and s. newhouse, eds.), pitman research notes in math. , longman group ltd, harlow, , pp. t. j. lyons. dierential equations driven by rough signals. rev. mat. iberoamer- icana r. markarian. billiards with polynomial decay of correlations. ergodic theory dynam. systems m. maxwell and m. woodroofe. central limit theorems for additive functionals of markov chains. ann. probab. i. melbourne. rapid decay of correlations for nonuniformly hyperbolic ows. trans. amer. math. soc. i. melbourne. superpolynomial and polynomial mixing for semiows and ows. nonlinearity rr i. melbourne and m. nicol. almost sure invariance principle for nonuniformly hyperbolic systems. comm. math. phys. i. melbourne and m. nicol. large deviations for nonuniformly hyperbolic sys- tems. trans. amer. math. soc. i. melbourne and a. t or ok. central limit theorems and invariance principles for time-one maps of hyperbolic ows. comm. math. phys. i. melbourne and a. t or ok. statistical limit theorems for suspension ows. israel j. math. i. melbourne and a. t or ok. convergence of moments for axiom a and nonuni- formly hyperbolic ows. ergodic theory dynam. systems i. melbourne and p. varandas. a note on statistical properties for nonuniformly hyperbolic systems with slow contraction and expansion. stoch. dyn. , pages. m. peligrad and s. utev. a new maximal inequality and invariance principle for stationary sequences. ann. probab. d. ruelle. thermodynamic formalism. encyclopedia of math. and its applica- tions , addison wesley, massachusetts, y. g. sina . gibbs measures in ergodic theory. russ. math. surv. m. tyran-kami nska. an invariance principle for maps with polynomial decay of correlations. comm. math. phys. m. viana. stochastic dynamics of deterministic systems. col. bras. de matem atica, d. voln y. approximating martingales and the central limit theorem for strictly stationary processes. stochastic process. appl. d. voln y. a nonadapted version of the invariance principle of peligrad and utev. c. r. math. acad. sci. paris l.-s. young. statistical properties of dynamical systems with some hyperbolicity. ann. of math. l.-s. young. recurrence times and rates of mixing. israel j. math.","The section provides a detailed proof of a theorem involving hyperbolicity and singularities in a mathematical setting. It covers concepts such as stable and unstable cones in tangent spaces, homogeneity strips, admissible stable curves, Banach spaces, and decay of correlations. The proof involves the transfer operator, spectral decomposition, and conditions related to the time-one map for the system. Lemmas and technical details are provided to support the main theorem. Acknowledgements and references to related works are also included. The proof concludes with a demonstration of the lemma through a decomposition and estimation approach."
"g trafc forecasting: if verticals and mobile operators cooperate francesco malandrino cnr-ieiit, politecnico di torino, cnit email: francesco.malandrino@ieiit.cnr.it carla-fabiana chiasserini politecnico di torino, cnr-ieiit, cnit email: chiasserini@polito.it abstractin g research, it is traditionally assumed that ver- tical industries set the performance requirements for the services they want to offer to mobile users, and the mobile operators alone are in charge of orchestrating their resources so as to meet such requirements. motivated by the observation that successful orchestration requires reliable trafc predictions, in this paper we investigate the effects of having the verticals, instead of the mobile operators, performing such predictions. leveraging a real-world, large-scale, crowd-sourced trace, we nd that involving the verticals in the prediction process reduces the prediction errors and improves the quality of the resulting orchestration decisions.","Vertical industries and mobile operators collaborating for traffic forecasting can lead to more accurate predictions and improved resource orchestration. Traditionally, mobile operators set service performance requirements for mobile users, but involving vertical industries in making traffic predictions can reduce errors and enhance decision-making quality. Research based on real-world crowd-sourced data highlights the benefits of this collaborative approach."
"unlike their fourth-generation counterparts, g networks will not only transport data, but also process them. network, computing, and memory resources controlled by mobile net- work operators , will concurrently support multiple services, under the network slicing paradigm , . it is universally expected that vertical industries (e.g., auto- motive or media companies) specify the requirements of their services, i.e., which computation must be performed and the associated target key performance indicators . mnos, on the other hand, have to manage their network so as to ensure that all target kpis are met at the lowest cost for themselves, a problem known as service orchestration , . our purpose in this paper is to study a different model of interaction between vertical industries and mnos, whereby verticals provide not only the target kpis but also an estimation of their expected trafc patterns. the reason for this change is that service orchestration is greatly simplied if the evolution of the demand to serve is known , or it can be reliably predicted , and verticals are in a better position than mnos to make such a prediction. indeed, unlike verticals, mnos cannot access, for technical and legal reasons, detailed, application-layer information on the trafc owing through their network. it follows that, since network slices are tailored around a single type of service, the service-specic predictions that verticals can make may be more useful than predictions made by mnos. our rst task is therefore to compare the accuracy of the predictions that mnos and verticals can make based on the information they can access. to this end, we leverage a real- world, large-scale, crowd-sourced trace, containing mobility i.e., services with the same kpis. and trafc information about over , users in the los angeles area. thanks to its crowd-sourced nature, the trace contains a superset of the information available to verticals and mnos; therefore, we can consider a state-of-the-art prediction technique, feed it the data available to mnos and verticals, and check which of them yields the most accurate result. beyond the accuracy of predictions, we are interested in the effect of prediction errors on the resulting orchestra- tion decisions. specically, we are interested in two adverse consequences of inaccurate predictions, namely, unused capacity, when the demand is overestimated and the network slice is over-provisioned, and scale-up events, when the capabilities of an under-provisioned slice must be swiftly improved to face a higher-than-predicted demand. our task is to establish which of these events is more common and how the predictions obtained by verticals and mnos affect them. finally, we compare both alternatives against a scenario where verticals and mnos share not only the trafc prediction but the input information they use to make them. this serves as a useful benchmark,a lthough it would be realistic only in very specic scenarios, e.g., when the mno also acts as a vertical and provides value-added services such as video calls or streaming. the rest of this paper is organized as follows. sec. ii presents the real-world dataset we use for the forecasts, while sec. iii describes the techniques we adopt. after presenting the metrics of interest and numerical results in sec. iv, we conclude the paper in sec. v.","Next-generation 5G networks are expected to not only transport data but also process it through network slicing. Mobile network operators (MNOs) are tasked with managing resources to meet service requirements efficiently. This study explores a new model where vertical industries provide traffic patterns along with key performance indicators, as they are better positioned to predict demand compared to MNOs. Using a large-scale dataset from Los Angeles, predictions made by verticals and MNOs are compared for accuracy and impact on orchestration decisions. The study evaluates the consequences of prediction errors, such as unused capacity and scale-up events. Additionally, scenarios where verticals and MNOs share information are examined. The paper details the dataset, techniques, results, and concludes with implications for future network orchestration."
"for our analysis, we use a real-world, crowd-sourced dataset collected from the wefi app . wefi provides its users with location-specic information on the available wi-fi networks, and such information is crowd-sourced from the users them- selves. specically, the app tracks: the current time and loca- tion; the mobile operator and cell the user is associated with ; the ssid and bssid of the wi-fi network she is con- nected to ; arxiv:v jan table i the wefi trace. metric value covered area km collection time march number of records million unique users , unique cells , unique bssids , unique apps , total trafc tbyte coverage % fig. the area covered by the wefi dataset. colors reect the average download rate considering all applications; warmer colors correspond to a higher trafc demand. the amount of data used by the currently-active application, and the identity of the appli- cation itself. new records in the trace are created every time any of the above pieces of information changes, e.g., the user switches between apps. the features of the trace are summarized in tab. i. fig. depicts the area covered by the trace greater los angeles and the trafc density therein. we can observe a higher trafc demand in the most densely-populated zones, e.g., downtown los angeles, and a lower demand in rural or wilderness areas. also notice, in the far east and north of the map, the edwards and twentynine palms military bases, with a much higher trafc than the surrounding rural areas. importantly, unlike similar datasets collected by mobile operators , , the wefi trace includes information on different mobile operators and technologies (including wi- fi), as well as different applications an aspect that makes the trace especially well-suited to study g networks. indeed, the network slicing paradigm is predicated on tailoring slices to individual applications; in this context, knowledge on application-specic trafc patterns is much more useful than information on global demand uctuations.","A real-world dataset gathered from the crowd-sourced wefi app is used for analysis. The dataset includes information on wi-fi networks, time, location, mobile operators, cells, and applications. It covers a large area, with millions of records from unique users, cells, BSSIDs, and apps. The dataset tracks trafc demand with varying levels in different areas, such as high demand in densely populated zones and military bases. Unlike datasets from mobile operators, the wefi trace includes information on multiple operators, technologies, and applications, making it ideal for studying 5G networks and network slicing for tailored application-specific slices."
"here we briey describe the data available to mnos and to verticals, the prediction technique that we apply, and the output that mnos and verticals can obtain. input and output data. the information available to mnos, i.e., in machine learning jargon, the features their forecast is based upon, include, for each cell and time period: the total demand ; the number of users covered by the cell (regardless their activity). in the case of the wefi trace, time periods correspond to one- hour time intervals. notice that, due to technical and legal reasons, mnos have no knowledge of what individual users do, i.e., which app they use. verticals, on the other hand, only have information about their own service on the positive side, they know the identity of their users and their ne-grained location in the case of the wefi trace, a m tile. for each time period and tile, verticals can thus keep track of: the trafc demand for their service; the number of users of their service; their trafc history, e.g., the amount of data they down- loaded in the past. for both mnos and verticals, the quantity to predict is the total demand of a specic app, i.e., the trafc the network slice will process. as it is commonplace in machine learning, we split our dataset into a training set, including the rst three weeks of the trace, and a testing set, containing the last one. prediction technique. the prediction task under study belongs to the class of time-series forecasting problems. input data are multi-variate, i.e., the forecast must be based on the evolution of multiple quantities in the case of mnos, for example, the trafc demand of each individual cell. this rules out the use of traditional approaches like the holt-winters method adopted in , which requires uni-variate time series. we therefore turn to neural networks, namely, long short- term memory networks, introduced in and, since then, successfully applied in a variety of elds, from computer vision to voice recognition. specically, we use the implementation from googles tensorflow library, accessed through the keras high-level front-end.","This section discusses the data available to mobile network operators (MNOs) and verticals, the prediction techniques applied, and the outputs obtained. MNOs rely on features like total demand and the number of users per cell for forecasting, while verticals focus on their specific service's traffic demand and user data. Both MNOs and verticals aim to predict the total demand of a specific app, utilizing a training set for the initial weeks and a testing set for the final period. To handle multivariate input data for time-series forecasting, neural networks, particularly long short-term memory networks, are used instead of traditional methods like the holt-winters approach. This implementation leverages Google's TensorFlow library through the Keras high-level front-end."
"this section shows the predicted behavior of the traf- c demand, describes the performance metrics we consider , and discusses the results we obtain . fig. shows the actual trafc , as well as the trafc predicted: using the data available to mnos ; using the data available to verticals ; using both . a. performance metrics as mentioned in the introduction, we are interested in two main aspects, namely, the prediction accuracy and the quality of orchestration decisions. quantifying the prediction accuracy for simplicity, we assume that a service corresponds to an app, and that each vertical only provides one service. fig. actual and predicted trafc for youtube , facebook , and netix . fig. prediction error for different apps and scenarios. fig. unused capabilities and scale-up events for different apps and scenarios. is fairly straightforward; specically, we resort to the well- known rmse metric, dened as: rmse = v u u t n n x j= (yj yj), where n is the number of time periods the forecast extends across , while yj and yj are, respectively, the actual and predicted trafc at the jth time period. note that all metrics are computed separately for each app. with regard to orchestration decisions, there are two adverse effects we seek to minimize. the rst is unused capabilities, i.e., network slices that are over-provisioned with respect to the actual trafc demand. we can quantify unused capabilities as: w = n x j= max (, yj yj) . that is, for each time period, we consider the difference between the predicted demand (according to which the slice is dimensioned) and the actual one; if positive, such a difference is representative of the amount of unused capabilities. the second adverse effect is represented by scale-up events, i.e., under-provisioned slices whose capabilities have to be swiftly extended to cope with unforeseen increases in trafc. as discussed in , such events have the potential to decrease the qos/qoe of all services supported by the mno. the quantity u expresses the number of time periods in which such events happen: u = n x j= [yj> yj]. b. results each of the plots in fig. refers to one of the three most used apps in the trace: youtube, facebook, and netix. as one might expect, the trafc demand exhibits clear weekly and daily patterns, e.g., morning and evening peaks. however, the magnitude of trafc peaks is not consistent throughout all days, e.g., see the peaks around periods and in fig. this feature of trafc patterns makes prediction harder; indeed, we can observe that these higher-than-usual peaks are never properly predicted, regardless of the scenario. fig. shows the prediction error (rmse, as dened in ) associated with different services and scenarios. a rst fact to notice is that the prediction error is, in general, fairly small: a testament to the effectiveness of the lstm prediction technique, as well as to the overall regularity of the trafc demand. it is perhaps even more interesting to observe that the rmse changes signicantly across apps; comparing fig. to fig. , we can conclude that more numerous and irregular peaks are associated with a higher prediction error. the effect of shifting the task of trafc prediction from the mno to the vertical (second group of bars) is also inconsistent across services. the vertical seems to be better than the mno at predicting youtube, but the opposite is true for facebook; as for netix, no signicant difference can be observed. the third group of bars refers to the benchmark scenario where mnos and verticals share their information and jointly predict the demand: in this case, the resulting error is close to the lowest of the errors yielded by the other two scenarios. fig. focusses on the impact of the trafc prediction on orchestration decisions. the x-axis therein shows the total unused capabilities, i.e., the w-metric dened in , while the y-axis shows the frequency of scale-up events, i.e., the u- metric dened in . each dot corresponds to a combination of app and scenario (identied by the marker used), e.g., the red square corresponds to the vertical making predictions about youtube trafc. a rst observation we can make is that scale-up events are much more common than unused capabilities. indeed, scale- up events happen in around % of time periods (i.e., roughly twice per day), while unused capabilities account for less than % of the total. this is unwelcome news, since scale-up events are, in most real-world cases, a more serious issue than unused capabilities, and far more likely to result in a violation of target kpis and/or higher costs for the mno . this seems to disagree with the low rmse values summarized in fig. ; however, it is worth recalling that any underestimation of the demand, no matter how slight, leads to a scale-up event. for the same reason, the netix app is associated with almost the same number of scale-up events as facebook and youtube, in spite of the much lower rmse. even more interestingly, there is a remarkably consistent relationship between the different scenarios. mno predictions yield the highest number of scale-up events and the lowest amount of unused capabilities; moving to vertical and joint predictions has the effect of reducing the scale-up events in exchange for a small increase in unused capabilities. it is important to remark how this happens for all apps, in spite of the different levels of prediction accuracy they are associated with. in other words, while involving the verticals in trafc prediction does not necessarily improve the predictions accuracy per se, it does yield better orchestration decisions, with a healthier balance between scale-up events and unused resources.","This section presents numerical results on predicting traffic demand, performance metrics, and orchestration decisions. The analysis includes actual and predicted traffic for various apps, prediction error, unused capabilities, and scale-up events. The prediction accuracy is measured using the Root Mean Square Error (RMSE) metric. Results show weekly and daily traffic patterns, with varying peak traffic levels. Prediction errors differ across apps, with more irregular peaks leading to higher errors. Involving verticals in traffic prediction improves orchestration decisions by reducing scale-up events at the cost of some unused capabilities. Overall, including verticals in prediction leads to a healthier balance between scale-up events and unused resources."
"we considered the orchestration problem in g networks based on network slicing. after remarking that good orches- tration decisions depend on accurate trafc predictions, we investigated whether verticals are in a better position than mnos to make such predictions. using a real-world, large- scale, crowd-sourced trace, we found that, while the prediction error is not consistent throughout different apps, involving the verticals in the prediction leads to a slightly higher amount of unused capacity and, more importantly, to a lower frequency of scale-up events. a rst direction to extend our work is considering additional prediction techniques, including generalizations of the holt- winters method. furthermore, we can couple the predictions with actual orchestration algorithms, including state-of-the-art approaches taken from the literature as well as purpose-built ones.",The section discusses the orchestration problem in 5G networks based on network slicing. It emphasizes the importance of accurate traffic predictions for good orchestration decisions. The study compares the ability of verticals and MNOS to make these predictions using real-world data. The results show that involving verticals in predictions leads to slightly more unused capacity but fewer scale-up events. Future work includes exploring additional prediction techniques and integrating predictions with orchestration algorithms from the literature or purpose-built ones.
,
"arxiv:v jul some sharp bounds for steklov eigenvalues sheela verma tata institute of fundamental research centre for applicable mathematics bangalore, india g. santhanam department of mathematics and statistics indian institute of technology kanpur kanpur, india abstract. this work is an extension of a result given by kuttler and sigillito (siam rev :, ) on a star-shaped bounded domain in r let be a star-shaped bounded do- main in a hypersurface of revolution, having smooth boundary. in this article, we obtain a sharp lower bound for all steklov eigenvalues on in terms of the steklov eigenvalues of the largest geo- desic ball contained in with the same center as . we also obtain similar bounds for all steklov eigenvalues on star-shaped bounded domain in paraboloid, p =  r : z = x + y .","The text presents new sharp bounds for Steklov eigenvalues in star-shaped bounded domains. The authors extend previous work by Kuttler and Sigillito on a hypersurface of revolution in R^n. They establish a precise lower bound for all Steklov eigenvalues in terms of the eigenvalues of the largest geodesic ball contained within the domain. Additionally, similar bounds are obtained for Steklov eigenvalues in star-shaped bounded domains in a paraboloid."
"let be a bounded domain in a compact connected riemannian manifold with smooth bound- ary . the steklov eigenvalue problem is to nd all real numbers for which there exists a nontrivial function c c such that = in , = on , where is the outward unit normal to the boundary . this problem was introduced by steklov for bounded domains in the plane in its importance lies in the fact that the set of eigenvalues of the steklov problem is same as the set of eigenvalues of the well-known dirichlet- neumann map. this map associates to each function dened on , the normal derivative of its harmonic extension on . the eigenvalues of the steklov problem are discrete and form an increasing sequence = < . the variational characterization of l, l < is given by l = sup e inf =e r dv r ds , where e is a set of l functions , , . . . , l such that i h, i l and e=  h : r ids = , i l . for background on this problem, see . there are several results which estimate rst nonzero eigenvalue of the steklov eigenvalue problem . the rst upper bound for was given by weinstock in he proved that among all simply connected planar domains with analytic boundary of xed perimeter, the circle maximizes later f. brock obtained a sharp upper bound for by xing the volume e-mail addresses: sheela.verma@gmail.com, santhana@iitk.ac.in. mathematics subject classication. primary c; secondary j key words and phrases. laplacian, steklov eigenvalue problem, star-shaped domain, rayleigh quotient. sharp bounds for steklov eigenvalues of the domain. he proved that for a bounded lipschitz domain rn, ) n n n , where n is the volume of the unit ball in rn and equality holds if and only if is a ball. in several recent papers, bounds for all eigenvalues of the steklov problem have been studied . in particular, sharp upper bounds for some specic functions of the steklov eigenvalues have been derived in . weyl-type bounds have also been obtained for steklov eigenvalues in . let rn be a star-shaped domain with smooth boundary . let p be a center of . let rm = min {d|x }, rm = max {d|x } and hm = min {x, |x }, where is the outward unit normal to . with these notations, bramble and payne proved that rm n rm n+ hm. equality holds when is a ball. kuttler and sigillito proved the following lower bound for a star-shaped bounded domain in r theorem . let be a star-shaped bounded domain in r with smooth boundary and centered at the origin. then, for k < , k+ k k   + q + min /r)  max q r + r , where r = max  |x| : x , x = |x|ei and equality holds for a disc. following the idea of kuttler and sigillito , garcia and montano and the rst author obtained a similar bound for the rst nonzero steklov eigenvalue on a star-shaped domain in rn and sn, respectively. let be a star-shaped bounded domain with smooth boundary centered at a point p and be the outward unit normal to . for any point q , let < , where cos) = , r. let a = tan . theorem . let rn. then with the above notations, the rst nonzero eigenvalue of the steklov problem satises n n  + a a + a a + . theorem . let be a star-shaped bounded domain in sn such that sn\ {p}. then the rst nonzero steklov eigenvalue satises  rm rm  a + a + a ! sinn sinn ) . here rm and rm are dened as above. in theorem , we obtain a lower bound similar to , for all steklov eigenvalues on a star- shaped domain in hypersurface of revolution centered at pole. in theorem , we prove a result for a star-shaped domain in a paraboloid in r analogous to the above. the main tool used to prove these results is the construction of suitable test function for the variational characterization of the corresponding eigenvalues.","The Steklov eigenvalue problem seeks real numbers for which a nontrivial function satisfies specific boundary conditions on a bounded domain within a connected Riemannian manifold with smooth boundary. This problem is significant as the eigenvalues of the Steklov problem are equivalent to those of the well-known Dirichlet-Neumann map. Various upper and lower bounds for the eigenvalues have been derived, including bounds for specific functions of the eigenvalues. Results have been obtained for star-shaped domains, with specific lower bounds established. The main approach involves constructing suitable test functions for the variational characterization of the eigenvalues."
"let m be a hypersurface of revolution with metric g = dr + hgsn, where gsn is the usual metric on sn and r for some l r+. moreover, we assume that h satises h = , h = let be a star-shaped bounded domain in m with respect to the pole p of m. let be the smooth boundary of with outward unit normal . since is star-shaped with respect to the point p and have smooth boundary, then for every point q , there exists sharp bounds for steklov eigenvalues a unique unit vector u tpm and ru > such that q = expp. observe that in geodesic polar coordinates, and can be written as = { : u tpsn, u= } and \ {p} = { : u tpsn, u= , < r < ru} . dene rm = min ru, rm = max ru. let r be the radial vector eld starting at p, the center of and be the unit outward normal to . since is a star-shaped bounded domain, for any point q , cos) = , r> therefore < for all q . by compactness of , there exists a constant such that < for all q . recall that for any point q , tan) = ru h . additionally, assume that h also satises the following conditions h r is a decreasing function of r on , h is an increasing function of r on . lemma . let h be a function dened on such that h r is a decreasing function. then h satises the following properties: if a , then h ah. if a , then h ah. proof. since h r is a decreasing function of r, for a , ar r and h r h ar for a , ar r and h r h ar . which gives the desired results. the following theorem gives a sharp lower bound for all steklov eigenvalues on a star-shaped domain in m. theorem . let m, , , rm and rm be as the above. let a = tan then l, l < satises the following inequality. l  rm rm  a + a + a ! hn hn l ) , where b m is the geodesic ball of radius rm centered at p. further, if is a geodesic ball, then equality occurs. conversely, if equality holds for some l, then is a geodesic ball of radius rm. proof. for a continuously dierential real valued function f dened on , we rst nd a lower bound for r f dv and then an upper bound for r f ds to nd a lower bound for r f dv r f ds . let f be a continuously dierential real valued function dened on . then for q , f =  f r  + hf therefore z f dv = z up z ru ""f r  + hf # hn dr du. let u = u, = r rm ru . then f = uf ru f uru. by abuse of notations, we denote u by u and u by . then the above integral can be written as z f dv = z up z rm rm ru  f  + h  ru rm  ( ru  ru f  +f ru f f, ru  hn  ru rm   ru rm  d du. sharp bounds for steklov eigenvalues next we estimate f, ru. for any function on , cauchy-schwarz inequality gives rm h  ru rm  f  f, ru ru ru rm h  ru rm  f  ru rm h  ru rm f thus z f dv z up z rm rm ru    ru r rm h  ru rm  f  + ru    rm h  ru rm f hn  ru rm  d du. note that rm ru rm and ru rm ru. hence rm h h  ru rm  ru rm h, hn hn  ru rm  . we assume < and by substituting above inequalities in , we get z f dv z up z rm ""(rm ru    ru ru rm  rm h ) f  +ru    rm  rm ru h  f # hn d du  rm rm  z up z rm ""   a  f  +    h f # hn d du. by solving the equation   a = for we see that   a = = a + a > from this it follows that z f dv  rm rm  a + a ! z up z rm ""f  + hf  hn d du =  rm rm  a + a ! z b f dv. now we nd an upper bound for r f ds. sharp bounds for steklov eigenvalues recall that the riemannian volume element on , denoted ds, is given by ds = sec hn du . then z f ds = z up f sec hn du. by using the fact that hn hn hn and substituting r = ru rm , this integral becomes z f ds sec hn hn z s f ds. by inequalities and , we have r f dv r f ds  rm rm  a + a ! hn sec hn r b f dv r s f ds . we now construct some specic test functions for the variational characterization of l. we choose the functions i, i < such that ihn q h + ru is the ith steklov eigenfunction of b. let be an arbitrary function which satises z b ihn q h + ru ds = note that z ids = z up i q h + ru h hn du. by substituting r = ru rm , the above integral becomes z ids = hn z b i q h + ru hn ds = fix e = {, , . . . , l} in . then it follows from that l inf = r ids=, il r dv r ds  rm rm  a + a + a ! hn hn inf = r b ihn h+ruds=, il r b dv r b ds . since ihn q h + ru is the ith steklov eigenfunction of b, we have inf = r b ihn h+ruds=, il r b dv r b ds = l ) . by substituting the above value in , we get . if is a geodesic ball, then rm = rm and a = , hence equality holds in . next if equality holds in for some l, then equality holds in and ru = rm. hence is a geodesic ball. sharp bounds for steklov eigenvalues remark . in and , authors obtained a lower bound for the rst nonzero steklov eigen- value on a star-shaped bounded domain in rn and sn, respectively. using the above idea, a similar bound can be obtained for all nonzero steklov eigenvalues on a star-shaped bounded domain in rn and sn.","The text discusses eigenvalues on a hypersurface of revolution with specific metrics and assumptions. It defines star-shaped bounded domains and provides bounds for Steklov eigenvalues on such domains. The conditions for h are specified, along with a lemma and theorem providing sharp lower bounds for Steklov eigenvalues. The text also presents a proof and detailed mathematical reasoning for deriving these bounds. Additionally, it discusses the construction of specific test functions for the variational characterization of eigenvalues and mentions lower bounds obtained in previous works for star-shaped bounded domains in different spaces."
"in this section, we state and prove the result for a star-shaped bounded domain in a paraboloid p =  r : z = x + y . we rst x some notations which will be used to state the main result of this section. we use the parametrization  r cos , r sin , r for paraboloid p, where [, ) and r then the line element ds and the area element da on p is given by ds =   + r dr + r d and da = r + r dr d, respectively. let p be a star-shaped bounded domain with respect to the origin and have smooth boundary . then there exists a function r : [, ) r+ such that = {, ) : [, )} and \ {} = { : [, ), < r < r} . hereafter, we denote r by r. let rm = min {r : [, )} and rm = max {r : [, )}. dene b = { : [, )}. let be the outward unit normal to . let a = max   + r   r r  : [, )  . with these notations, we prove the following theorem. theorem . let , , a, rm and rm be as the above. then l, l < satises l  rm rm  a + a + a ! l ) . furthermore, if equality holds for some l then is a geodesic ball of radius rm and if is a geodesic ball then equality holds in . proof. let f be a continuously dierentiable real valued function dened on . we rst obtain a lower bound for r f da. z f da = z z r "" + r f r  + r f # r p + r dr d = z z r "" r + r f r  + + r r f # dr d sharp bounds for steklov eigenvalues let = , = r rm r . since = r rm r r, we have + r p + and r +r + . thus the above integral can be written as z f da z z rm "" p + rm r f  + rm p + r f r r f # r rm d d = z z rm "" p + f  + r p + rm f r r f # rm r d d z z rm "" p + f  + p + (f  + r r f  r r f f  rm r d d. for any function on , cauchy-schwarz inequality gives r r f f r r  f  f  . as a consequence, we have z f da z z rm "" p + f  + p + (    f    r r f )# rm r d d = z z rm ""(   +    r r ) p + f  +    p + f # rm r d d. note that   +   r r   + r   r r  a and rm r rm rm . lets assume < , then the above integral becomes z f da  rm rm  z z rm ""   a  p + f  +    p + f # d d. solving the equation   a = for , we obtain   a = = a + a > sharp bounds for steklov eigenvalues by substituting these values, we have z f da  rm rm  a + a z z rm "" p + f  + p + f # d d =  rm rm  a + a z z rm "" + f  + f # p + d d =  rm rm  a + a z b f da. now we give a lower bound for r f ds. z f ds = z f s + ( + r ) r r  r d + a z f r d. by substituting = , = r rm r and using the fact that r rm, we get z f ds rm + a rm z f rm d = rm + a rm z b f ds. hence for a continuously dierentiable real valued function f dened on , it follows from and that r f da r f ds  rm rm  a + a + a r b f da r b f ds . now using the same argument as in theorem , we get the desired result.","The text discusses eigenvalues on a paraboloid in R3. A star-shaped bounded domain on the paraboloid is considered, where specific notations and parametrizations are used. The text defines various mathematical elements such as line element and area element, and establishes a theorem related to Steklov eigenvalues. The theorem provides conditions and relationships involving certain parameters, geodesic balls, and integrals. The text also presents calculations and inequalities to derive lower bounds for certain functions defined on the domain. Overall, the section focuses on proving results related to eigenvalues within the specified geometric context."
the authors would like to thank dr. prosenjit roy for various useful dis- cussions. some part of this work was done when the rst author was working under project pda/iitk/math/ at iit kanpur.,The authors express gratitude to Dr. Prosenjit Roy for valuable discussions. A portion of the work was completed while the first author was involved in Project PDA/IITK/Math at IIT Kanpur.
,
