[
    {
        "Section_Num": "Abstract",
        "Section": "Abstract",
        "Text": "arxiv:1901.00230v2 4 dec 2019 the weil algebra of a double lie algebroid eckhard meinrenken and jeffrey pike abstract. given a double vector , we define a bigraded bundle of algebras called the weil algebra bundle'. the space w of sections of this algebra bundle 'realizes' the algebra of functions on the supermanifold d. we describe in detail the relations between the weil algebra bundles of d and those of the double vector bundles obtained from d by duality operations. we show that vb-algebroid structures on d are equivalent to horizontal or vertical di {latin small ligature ff}erentials on two of the weil algebras and a gerstenhaber bracket on the third. furthermore, mackenzie's definition of a double lie algebroid is equivalent to compatibilities between two such structures on any one of the three weil algebras. in particular, we obtain a 'classical' version of voronov's result characterizing double lie algebroid structures. we find that w is the weil algebra of the lie algebroid, as defined by mehta and abad-crainic. we show that the deformation complex of lie algebroids, the theory of im forms and im multivector fields, and 2-term representations up to homotopy, all have natural interpretations in terms of our weil algebras. contents",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "1",
        "Section": "1. Introduction",
        "Text": "an important class of finite-dimensional representations for a ffine lie algebras are the kirillov-reshetikhin modules, which are characterized by their drinfel'd polynomials . moreover, graded characters of tensor products of single-column kr modules are macdonald polynomials at t = 0 for untwisted a ffine types .an open problem is to determine a uniform model for kr crystals. this has been achieved by using kashiwara's construction of projecting an extremal level-zero module/crystal . this was done explicitly by naito and sagaki using lakshmibai-seshadri paths . the construction of kashiwara was also shown to partially extend to general br,s in nonexceptional a ffine types . in contrast, the models in are all type-dependent, crystals are connected with mathematical physics. furthermore, tensor products of kr crystals describe the dynamics of soliton cellular automata, a generalization of the takehashi-satsuma box-ball system equation). we refer the reader to for more details. another important property of kr crystals is that 2010 mathematics subject classification. key words and phrases. kirillov-reshetikhin crystal, crystal, crystal basis, a ffne lie algebra. r.b. was partially supported by the nserc discovery grant of her postdoc supervisor michael lau at universit e laval. t.s. was partially supported by the australian research council grant dp170102648. 1 they are perfect , a technical condition that allows highest weight crystals to be modeled using a semi-infinite tensor product known as the kyoto path model . we achieve this by considering the decomposition of the classical highest weight crystal b into a6 highest weight crystals, which is multiplicity free. the novelty of our approach is doing a further levi decomposition and reconstructing the a fine action to a type a7 crystal rather than through the classical decomposition. we then given an explicit description of the combinatorial r-matrix. we note that the local energy function is given. as a potential application of our results, the combinatorial r-matrix allows us to study soliton cellular automata. this paper is organized as follows. in section 2, we give the necessary background. in section 3, we give our main results. in section 4, we give a conjecture about the decomposition in an effort to prove . acknowledgments. the authors thank the referee for useful comments on our manuscript.",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "Abstract",
        "Section": "Abstract",
        "Text": "complementary reinforcement learning towards explainable agents a preprint jung hoon lee allen institute for brain science seattle, wa 98109 jungl@alleninstitute.org january 25, 2019 abstract reinforcement learning algorithms allow agents to learn skills and strategies to perform complex tasks without detailed instructions or expensive labelled training examples. that is, rl agents can learn, as we learn. given the importance of learning in our intelligence, rl has been thought to be one of the key components to general artificial intelligence, and recent breakthroughs in deep reinforcement learning suggest that neural networks are natural platforms for rl agents. however, despite the efficiency and versatility of nn-based rl agents, their decision-making remains incomprehensible, reducing their utilities. to deploy rl into a wider range of applications, it is imperative to develop explainable nn-based rl agents. here, we propose a method to derive a secondary comprehensible agent from a nn-based rl agent, whose decision-makings are based on simple rules. our empirical evaluation of this secondary agent's performance supports the possibility of building a comprehensible and transparent agent using a nn-based rl agent.",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "1",
        "Section": "1 Introduction",
        "Text": "reinforcement learning , inspired by our brain's reward-based learning, allows artificial agents to learn a wide range of tasks without detailed instructions or labeled training sets which are necessary for supervised learning . given that rl agents' learning resembles our process of learning and that learning is essential to our intelligence, it seems natural to assume that rl is one of the key components to brain-like intelligent agents or general artificial intelligence. recent breakthroughs from deepmind team showed that rl could train neural network based agents to outperform humans in video-games and even 'go', reigniting interests in rl and its applications in nn-based agents, and noticeable developments in nn-based rl agents have been reported since then; see for examples. however, despite rapid improvements in rl, nn-based rl agents' decision-making process remains incomprehensible to us. for instance, the alphago's strategies employed during the match with sedol lee exhibited efficiency leading to victories, but the exact reasons behind its moves are unknown. alphago demonstrated that incomprehensible decision-making can still be effective, but its effectiveness does not mean that it cannot be faulty. in 'high stake problems' , any mistakes can be critical and need to be avoided. a loss in one go match out of 100 matches is insignificant, but crashing a car once out of 100 drives is perilous and unacceptable. if we comprehend the exact internal mechanisms of rl agents, we can correct their mistakes without negative impact on their performance. that is, 'transparent' agents with comprehensible internal decision-making processes are necessary to safely deploy rl agents into high stake problems. two earlier studies showed that the decision-making processes of rl agents can be translated into humanreadable descriptions. their proposed algorithms, which fall into the 'post-hoc' interpretation approach , do provide some insights into rl agents' decision-making process, but they do not address how we correct rl agents' actions. then, how do we build transparent rl agents? we propose a secondary agent as a potential solution, which utilizes simple rules to choose the best action. this proposal is based on two ideas. first, the secondary agent's a preprint - january 25, 2019 action can be analyzed because it utilizes simple rules. second, the secondary agent can perform general tasks, if it takes advantage of trained rl agents. in this study, we propose a quasi-symbolic agent as a secondary agent and compare its performance to rl agents' performance. specifically, qs agents learn, from rl agents' behaviors, the values of transitions of states. after learning the values of transitions, qs agents identify the most valuable state-transitions and search for a sequence of actions to reach one of the hub states. if qs agent cannot find a proper action plan to reach a hub state, they choose the best action by comparing the values of immediate transitions. in this study, we tested qs agents' performance using the 'lunar-lander' benchmark problem available in 'openai gym' environments . our results show that qs agents' performance is comparable to rl agents' performance. while our experiments are conducted in a simple environment, the results indicate that comprehensible agents with transparent decision-making process can be derived from rl agents.",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "2",
        "Section": "2 Related work",
        "Text": "while deep learning spreads its influence, it remains unclear whether dl can safely be applied to high stake decision problems . dl agents rely on cascades of nonlinear interactions among computing nodes , but the interactions are too complex to be analyzed, which limits our mechanistic understanding of dl agents' decision-making. this means that despite extensive testings we cannot fully predict their actions. thus, deploying dl agents to high stake problems may lead to critical failures; such failures have already been reported . to deploy dl agents into high stake decisions, it is imperative to understand the exact process of their decisions. if reasoning behind their decisions become clear, developers could improve dl agents' reliability, policy-makers could introduce regulations to ensure fairness of dl agents, and users could use dl agents more effectively . while we note the diverse aspects of dl explainability , lipton pointed out that explainable intelligent agents can be obtained by analyzing the internal mechanisms or the agents' behaviors, which were referred to as transparency and post-hoc interpretability, respectively. if we could understand the internal mechanisms, we will be able to fully understand dl agents' decision-making process; that is, their decisions will become 'transparent'. several methods have been proposed to analyze the internal mechanisms of dl agents , and olah et al. presented an intriguing way to synergistically use them to gain insights into dl agents' decision-making. also, multiple studies sought the post-hoc interpretability of dl agents, which we could use to predict and prevent potential failures. specifically, human interpretable descriptions can be automatically generated by secondary agents , and representative examples or image parts can be identified by algorithms such as sensitivity analysis . the majority of studies have focused on feedforward dl, but a few studies pursued the explainability of rl. specifically, hayes and shah showed that a secondary network could be trained to provide user-interpretable descriptions, and waa et al. further showed that user-interpretable descriptions could be contrasive; that is, their methods can explain why rl agents would prefer one option to another . although such post-hoc interpretability can be used to evaluate the quality/reliability of rl agents, it does not provide a way to fix rl agents' mistakes. however, with transparent agents, we can correct their mistakes selectively. in our study, we propose a potential approach, which can help us obtain transparent rl agents.",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "3",
        "Section": "3 Network Structure",
        "Text": "qs agents interact with rl agents and make plans for future actions by utilizing the model of environment . all these three networks/agents are constructed using the 'pytorch', an open-source machine learning toolkit . below, we discuss qs, rl and env network in details.",
        "Subsections": [
            {
                "Section_Num": "3_3",
                "Section": "3.3 Env network",
                "Text": "the env network models the environment. thus, it receives the state vector s and action a as inputs and returns the next state . the inputs layer includes 12 nodes which represent 8 state variables and 4 possible actions in the lunar lander environment, and the output layers include 8 nodes . in our experiments, we set the hidden layer of the env network to have 300 nodes. the env network is trained using the mean squared error . in each episode of rl training, the error is accumulated. after each episode, the env network is updated using the accumulated error; that is, 1 episode is a single batch of backpropagation. the initial learning rate is 0.05 and is decreased to 10 percent at every 1000 episode.",
                "Subsections": [],
                "Groundtruth": ""
            }
        ],
        "Groundtruth": ""
    },
    {
        "Section_Num": "4",
        "Section": "4 Results",
        "Text": "in this section, we discuss the operating principles of qs agents including the interplay between qs and rl agents. then, we present our experiments which were conducted to evaluate qs agents' performance in solving lunar-lander problem compared to rl agents. training and operating rules of quasi-symbolic agents unlike rl agents, qs agents do not work alone. instead, their operations depend on rl agents in both learning and inference modes; that is, rl and qs agents are complementary with each other. the matching and value networks in qs agents are updated by using rl agents' behaviors during training. specifically, in each time step in the training period, the previous state s and the current state s are used to generate the state transition vectors, and this transition vector is fed into the matching network as inputs. the matching network first determines the novelty of the current transition vector; this novelty detection is done by inspecting synaptic inputs to matching nodes . when a novel transition vector is introduced, a new output node is added to the matching and value networks , and the connection between matching and value networks is established . the strength of the connection wk is determined by the reward given to the rl agent after the transition. when the earlier input is introduced again, the maximally activated matching node is identified, and the connection between the identified matching node and the value node, which is exclusive and one-to-one , is updated by adding the reward obtained to the previous strength . it should be noted that only one matching node is allowed to be active and used to assess the value of the transition. in brief, the matching network memorizes transition vectors observed during training, and the value network stores the amount of rewards induced by the observed transitions. in the inference mode, in which qs agents choose the best action, qs agents utilize both the trained rl agents and env networks to make action plans, whose lengths are variable. in doing so, qs agents identify the most valuable transition vectors {increment}sk by inspecting connections' strength between matching and value networks. due to the one-to-one connections between matching and value networks, the synaptic weights allow us to identify the most valuable transitions observed during the training period, which are referred to as hub states hereafter. after identifying the hub states, qs agents search for an action plan to reach one of the hub states. in doing so, qs agents utilize the env network and the trained rl agent to predict future states to make an action plan. at each state, rl agent provides a possible action, and env network returns the next state in response to the suggested action. employing them recursively , qs agents can predict the future states. during this planning, at each time step, qs agents examine whether a transition vector is one of the hub states or not. if qs agents do expect to reach one of the hub states, they stop planning and execute the current plan. the maximum length of the action plan is 10 time-step, unless stated otherwise. if no hub state cannot be reached within the predefined maximal time-step, qs agents start over and make a new plan. for each state, qs agents are allowed to make a total of 5 different plans. if they cannot find a path to the hub states in all five plans, they inspect the values of the first transition in the five plans and choose the best immediate action according to the reward estimated by the value network. in this case, rather than taking a sequence of actions, qs agents execute a single action only. qs agents' performance compared to rl agents to evaluate qs agents' performance, we compared their performance to that of rl agents by using the lunar-lander benchmark task included in the openai gym environment . in this study, we constructed rl agents , qs agents and env network using pytorch, an open-source machine learning library ; their schematics are illustrated. we trained rl, qs agents and env network during 5000 episodes; see section 3 for training details. show the total amount of rewards given to the rl agent and the error function of the env network during 5000 episodes. as shown in the figure, while the reward in each trial fluctuates from one trial to another, the amount of rewards, on average, increases rapidly until 1000 episodes. after 1000 episodes, the speed of improvement is reduced. similarly, the error of env network is reduced most rapidly in the first 1000 episodes, and then the speed of error-reduction slows down. at every 1000 episode, we froze the learning and tested both qs and rl agents using the same 100 environments of lunar lander; that is, the environments are instantiated with the same random seeds. the rl agent in this study uses stochastic policy to choose actions , and thus its behaviors depend on the random seed forwarded to the pytorch. moreover, qs agents' behaviors are also stochastic, as they rely on rl agents for their decisions. to avoid potential biases based on their stochastic behaviors, we constructed 10 independent qs and rl agents by forwarding distinct random seeds to pytorch and calculated the average reward for both agents. figures 3a and b show the average amount of qs and rl agents for all 100 instantiations of environment. x-axis represents the identity of environment, and yaxis represents the reward averaged over 10 independently constructed agents. rewards are estimated after training the rl agent in 1000 episodes and 5000 episodes . as shown in the figure, the performance of qs agents with 10 time-step action plan are comparable to that of rl agents . figure 3c shows the average reward calculated using 10 independent agents in 100 environments, after training the rl agent in 1000, 2000, 3000, 4000 and 5000 episodes, respectively; that is, the mean values and standard errors are estimated from 1000 individual experiments . then, we varied the parameters to examine how they affect qs agents' performance. first, we tested the effects of action plans' lengths. qs agents' performance improves, as the maximal length of action plans increases. we also increased theta hub to further examine the effects of the number of hub states on qs agents' performance and found that qs performance is negatively correlated with theta hub . finally, we perturbed the threshold value theta for the novelty detection to see how it affects qs agents' performance. as shown in fig. 5b, qs agents obtained less rewards. this may be explained by the fact that the accuracy of predictions on qs agents' future states decreases, as the threshold theta becomes lower,",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "5",
        "Section": "5 Discussion",
        "Text": "in this study, we propose qs agents to develop transparent rl agents. the newly proposed qs agents have two operating units, matching and value networks. with these two units, qs agents evaluate actions suggested by rl agents and choose the most probable choice. to select the most probable action, qs agents search for a path to reach one of the hub states by utilizing the env network which models the environment. our results suggest that this future plan ensures qs agents' good performance. then, do qs agents have transparent decision processes? qs agents' decisions are transparent for two reasons. first, the two operating units of qs agents have simple structures, which can be analyzed easily. the value network simply accumulates rewards given after the transition of states into synaptic weights, and it returns these stored values depending on inputs . that is, the value network is, in principle, equivalent to conventional memory units. the matching network identifies the old input , which is the closest to the current input by using the cosine similarity . in addition, as only a single matching node is allowed to be active, it is clear that the matching network's operation can be easily analyzed. second, qs agents rely on a simple set of rules to select the best actions based on suggestions made by rl. with both simple inner mechanisms and operating rules, qs agents have transparent decision-making process. moreover, it should be noted that the output nodes of the matching network are working independently from each other. that is, the matching nodes can be removed and added without interfering with other matching nodes' operations. this property makes manual modification of qs agents' actions possible. if a new piece of evidence finds a particular statetransition to be unacceptable, it can be removed. on the other hand, if some state-transitions {increment}s, which have not been previously observed, are considered valuable, they can be added to qs agents. similarly, the synaptic weights of value networks and hub states can also be changed, if necessary. therefore, qs agents can be continuously and incrementally improved to avoid mistakes.",
        "Subsections": [
            {
                "Section_Num": "5_1",
                "Section": "5.1 Potential variations of QS agents",
                "Text": "to address the possibility of deriving a comprehensible secondary agent from rl agents, we sought generic algorithms to be applied to general rl problems, but qs agents and their operations can be customized in domain-specific ways. below, we list a few potential variations of our generic qs agents. first, current qs agents evaluate individual transitions {increment}s using the rewards rl agents obtain immediately after completing the transition. however, the actual values of transitions can be estimated differently. for instance, instead of using the immediately obtained reward, we can use the total rewards from the transition to the end of an episode for evaluating state transitions. 5 a preprint - january 25, 2019 second, in this study, qs agents treat all state variables equally and rely on a single-state vector {increment}s. however, state variables do not have equivalent values in agents' behaviors. third, the state transition vectors {increment}s can be coupled with state vectors s to estimate the values of actions more precisely. for instance, an agent's horizontal move can be either bad or good depending on the current state. if both state and transition vectors are used, qs agent may have better estimation of agents' actions. 5.2 implications for the brain' complementary system prefrontal cortex has been thought to be a hub for high-level cognitive functions such as decision-making, learning and working memory . however, pfc does not work alone and is known to be connected to other brain areas. the two main areas that have strong interactions with pfc are hippocampus and anterior cingulate cortex . notably, complementary learning system theory suggests central roles of the interplay between pfc and hippocampus in our ability to learn continuously . then, why does pfc need to interact with acc which has been postulated to be associated with multiple functions such as error likelihood and prediction of expected outcomes . we note that qs agents can predict the future outcomes, consistent with one of acc's hypothetical functions. based on our results that qs agents can evaluate rl agents' decisions, we propose that one of acc functions is to evaluate possible actions suggested by pfc and choose the best one depending on context.",
                "Subsections": [],
                "Groundtruth": ""
            }
        ],
        "Groundtruth": ""
    },
    {
        "Section_Num": "Abstract",
        "Section": "Abstract",
        "Text": "transition densities of reflecting brownian motions on lipschitz domains kouhei matsuura abstract. in this paper, we study the continuity of the transition density of the reflecting brownian motion on a general lipschitz domain. we also provide local estimates for the density. applying the estimates, we prove that the surface measure on the domain is in the local kato class of the reflecting brownian motion.",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "Abstract",
        "Section": "Abstract",
        "Text": "ieee transactions on communications 1 end-to-end performance optimization in hybrid molecular and electromagnetic communications .telemedicine refers to the use of information and communication technology to assist with medical information and services. in health care applications, high reliable communication links between the health care provider and the desired destination in the human body play a central role in designing end-toend telemedicine system. in the advanced health care applications, e.g. drug delivery, molecular communication becomes a major building block in bio-nano-medical applications. in this paper, an e2e communication link consisting of the electromagnetic and the molecular link is investigated. this paradigm is crucial when the body is a part of the communication system. based on the quality of service metrics, we present a closed-form expression for the e2e ber of the combination of molecular and wireless electromagnetic communications. next, we formulate an optimization problem with the aim of minimizing the e2e ber of the system to achieve the optimal symbol duration for ec and dmc regarding the imposing delivery time from telemedicine services. the proposed problem is solved by an iterative algorithm based on the bisection method. also, we study the impact of the system . transactions on communications parameters, including drift velocity, detection threshold at the receiver in molecular communication, on the performance of the system. numerical results show that the proposed method obtains the minimum e2e bit error probability by selecting an appropriate symbol duration of electromagnetic and molecular communications. index terms molecular communication, on-body communication, off-body communication, end-to-end telemedicine.",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "I",
        "Section": "I Introduction",
        "Text": "in our modern era, the amount of the required health care services is rapidly increasing. these high expectancies are going to overtake a proportional increase in health system infrastructures and professional personnel . telemedicine, which can be defined as the implementation of telecommunication technologies to provide medical services, has been introduced as a promising solution for the increasing shortages of the traditional medicine. telemedicine allows the measurement of biological and vital signals regardless of the borders and distances. in the telemedicine paradigm, the biological signals are obtained through variety of the biological sensing technologies, from the simple pulse oxiometer and temperature sensors to more sophisticated electrophysiological and electrocardiology devices . the gathered signals must be post-processed, and eventually transmitted to the associated health care provider. the exterior communication hardwares are placed in order to form the pathway from the on-body devices to at a distant hospital or physicians station. one of the advanced applications of the telemedicine is the drug delivery which can be controlled via end-to-end communication links. the drug delivery is an engineered method to deliver drugs to their targeted locations while minimizing the undesired side effects. one of the most important drug delivery applications is gene therapy. the utilization of the drug delivery in the gene therapy allows to convey of the desired genetic information to the patient's organism . the main challenge in the gene therapy is minimizing the risk of in vivo toxicity and prolonging the lifespan of the payload. also, the gene expression is short-lived due to the degradation of the plasmid in the nucleus . draft june 18, 2019 submitted paper 3 consequently, the high reliable with minimum error probability e2e-telemedicine communication links are crucial in gene therapy. the e2e communication scenario plays a central and fundamental role in designing the telemedicine system between the health care provider and the human body. the applicability of the telemedicine crucially depends on the reliability of e2e communication links . therefore, several quality of service metrics are defined as the measurement of the performance criteria for e2e communication, in which several reliable communication links must work together in various circumstances and media including inner, in-to-on, on, and off body areas. inspired by nature, one of the best solutions for inner body communication is to use chemical signals for carrying the information inside the human body to nanomachines through nanonetworks . this communication paradigm, which is called molecular communication , has several advantages in comparison to electromagnetic based and acoustic wave based communication. the advantages includes but not limited to low energy consumption, the biocompatible characteristics, and the existence of the biological receptors to serve as antenna . similar to electromagnetic communication , different aspects of the communication have been studied in mc, such as channel modeling , noise and interference , and modulation and coding . recently, diffusion-based molecular communication has been received a significant attention among various mc propagation scenarios such as walk-way or flow-based . in dmc, without any additional and external energy, the molecules carrying the information propagate via brownian motion in all available directions in a fluidic medium . however, the major challenge of dmc is its high limited range of the communication . the intermediate nanomachines, which are serving as relays, are deployed to overcome this issue. the performance of relay-assisted dmc is investigated in . in-to-on body wireless communication is the ec between nanomachines inside the body and the wearable device on the surface of the body skin . the main propagation environments of electromagnetic waves are inside and around the human body. on-body wireless communication is the interconnection and networking between wearable devices and the gateway on communications transceivers . and ultimately, off-body wireless communication carries information from gateway transceivers to health care providers. the evaluation of the performance of the special scenario involving both the in-body to on-body and on-body to off-body electromagnetic wireless propagation links, including bit error rate , energy consumption, and data rate are considered in . in this study, we assume that inner nanomachines send information to a distant health care provider via dmc, inner-body, in2on-body, on-body, and off-body links as e2e-telemedicine communication. the time is divided into multiple slots with equal durations. at each time slot, a message is transmitted from inner nanomachine to the distant health care provider. in addition, the time slots are divided into several symbol durations for conveying the information in each link. the inner nanomachine sends its massage to the relay then the relay sends the received massage to the receiver nanomachine and ultimately through this sequence the massage is received by the health care system. due to the fact that no buffer is considered in the nanomachines, ec and dmc communications should be in serial1. therefore, ber and the delay of ec and dmc communication affects the e2e-ber and the total delay. it implies that the combination of ec and dmc must be considered as an integrated communication link. in addition, some telemedicine services are imposing the limited delivery time of the command from the health care provider to the end nanomachine. it results in a compromise between the performance of ec and dmc. the main question we aim to answer is: how to choose the physical layer parameters, such as the symbol durations, in order to minimize the e2e ber? for answering this question, we firstly derive the analytical closed-form expression of e2e-ber when binary pulse shift keying modulation for ec and on-off keying modulation for dmc is employed. then, we formulate the optimization problem to determine the optimal symbol durations in ec and dmc and solve it by using the bisection algorithm. it is important to emphasis that the main contribution of this paper is that what happen if we combine the ec and dmc as an integrated e2e-telemedicine communication and illustrate the trade off between the physical layer parameters such as symbol durations in ec and dmc which based 1 it means as soon as the packet arrives at an intermediate node, it is relayed over the next link towards the destination. draft june 18, 2019 submitted paper 5 on the best of authors' knowledge has not been addressed, yet. we also study the effect of the system parameters including the drift velocity and the detection threshold of dmc receiver on the performance of the e2e system. the rest of paper is organized as follows: in section ii, the system model is described. in section iii, the channel model of each communication is formulated, and the optimization problem is formulated and solved in section iv. the numerical results are presented in section v, and finally, the paper is concluded",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "II",
        "Section": "II system model",
        "Text": "we assume the e2e e-health communication includes molecular and electromagnetic wireless communication. in this e2e e-health system, the information is exchanged between the health care provider and nanomachines receptors in the body environment. the considered e2e ehealth system is shown in fig.1. one should note that all inner-body communications are dmc and in2on-body, on-body, and off-body communications are ec. according to the imposed e2e delivery time, the total time duration of each time slot denoted by ts, is divided into two time durations: one for dmc denoted by ts,dmc, and the other one for ec denoted by ts,ec . fig. 2 illustrates the symbol time duration and the dedicated time intervals of each communication type in the proposed system model. a. inner-body communication we use a generic nanotransmitter as the transmitter node, which can be reused as often as necessary. these types of transmitters are not natural and produced artificially. the transmitter which resides in a liquid communication medium , constitutes the transmitted signal by encoding the information onto the special type of the messenger molecules, and releases them to environment. due to the very small sizes of messenger molecules , their propagation in this medium is governed by brownian motion . we use a relay-assisted dmc system, because such relay node can potentially improve the reliability and performance of a communication link -. the relay node resides in the environment can be an artificial nanomachine or biological one . in nature, many june 18, 2019 draft 6 ieee transactions on communications fig. 1: overview of e2e e-health communication. fig. 2: symbol durations in the considered dmc and ec systems. biological systems have the both of molecule emission and reception capabilities  , for example, biochemical positive feedback loops process is common in protein channel of human body . these are a key for designing a nanomachine, which acts as relay node. this mc system consists of a point source nanomachine, denoted by node t, which only transmits draft june 18, 2019 submitted paper 7 information signals, a destination nanomachine, denoted by node d, which receives moleculartype signals as information particles, and finally a relay nanomachine, denoted by node r, which is relaying molecular-type signals. fig. 3 demonstrates the relay-assisted diffusion-based molecular communication system employed in this article. as could be seen, the relay-assisted molecular communication occurs at the beginning of each time slot, i.e, ts,dmc. the assisting relay node is transmitting and receiving in the fullduplex fashion , and also we assume ts,dmc is divided into two intervals of equal duration. in the first interval, denoted by ts,mol, the transmitter node transmits the information to the relay. next, the relay receives this information and retransmits the received information towards the destination in the second interval, denoted by ts,mol. we adopt ook modulation due to the fact that it is the most efficient binary modulation scheme in terms of molecular reception in dmc . by using ook modulation, the transmitter nodes release molecules to send information bit 1 and no molecule to send information bit 0 at the beginning of the time slot2. also, node t exploits type-a molecules which can be detected by node r, and node r uses type-b molecules which can be detected by node d. the use of two different types of molecules guarantees nearly interference free communication. b. wireless communications links ec between nanomachines inside the body and the wearable device on the body skin or at most 20 mm away from it, is called in2on-body communication . the wearable device communicates with node t electromagnetically in its own time interval. the communication between the wearable device on the body skin and the gateway on the body or at most 20 cm away from it, is called on-body communication . the gateway connects the on-body part to the off-body part via ec, in its own time interval. finally, the off-body wireless part carries information from the gateway to the health care provider in the last time interval of 2 in the case that another modulation scheme is employed in the molecular communication, just pe mol in and should be adjusted accordingly. june 18, 2019 draft 8 ieee transactions on communications fig. 3: relay-assisted diffusion-based molecular communication system. the time slot. further details of ec channel model and the corresponding ber are described in the following section. the main parameters and notification used throughout this paper is listed in table i.",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "III",
        "Section": "III E2E bit error performance ",
        "Text": "to derive the closed-form e2e-ber, each type of communication described in the previous section is modeled and ultimately, e2e-ber is calculated by concatenating them. a. molecular communication channel model and ber we assume a diffusive environment, e.g, blood, inside the human body. there exists three nodes namely the transmitter node, relay node, and the receiver node in the introduced scheme. the transceivers are placed in the 3-dimensional diffusive environment where the drift velocity is also attended. therefore, the diffusion of the molecules are coalesced with the drift velocity of the environment to propagate them toward the receiver. the receiver considered in this paper is an spherical passive receiver which counts the number of molecules . escription of terms and symbols used throughout this paper. parameter variable drift velocity diffusion coefficient d molecular noise mean  molecular noise variance molecular symbol duration ts,dmc number of molecules for sending bit-1 qa, qb location of the relay node location of the destination node detection threshold distance between nanomachine and wearable device din2on distance between wearable device and gateway of on-body communication on snr of off-body communication {macron} off total symbol duration time ts,total volume of it, without absorbing them . the vector of the drift velocity of the medium, the location of the receiver, and the exponential function, respectively. to attain the cumulative distribution function of the probability of arriving the molecules into the volume of the receiver, we should integrate over the volume of the receiver. as stated in , it could be approximated by using simpson's rule. therefore, the probability of arriving the molecules on communications the spherical passive receiver in the receiver of node r, maximum-a-posterior probability rule is employed for detection of the transmitted molecule . it is clear that, the above parameters corresponding to in2on body communication depend on location of nanomachines inside the blood vessel in deep tissue or near-surface of skin which details of the model derivation can be found in and are summarized in table iv draft june 18, 2019 submitted paper 13 by considering binary phase-shift keying modulation. ",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "VI",
        "Section": "VI Conclusion",
        "Text": "in this paper, we investigated the e2e communication link consisting of the electromagnetic and molecular communication. first, we derived a closed-form expression for the e2e bit error probability of concatenation of molecular and wireless electromagnetic communications. then, we formulated the optimization problem that aims at minimizing the e2e bit error probability of the system to determine the optimal symbol durations for both molecular and wireless electromagnetic communications. in addition, we studied the impact of the parameters consisting of the detection threshold at the receiver, the location of the relay node, drift velocity, and symbol duration in mc on the performance. the results reveal that an adaptive system must be considered to achieve the minimum bit error rate and optimal performance for the e2e system.",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "Abstract",
        "Section": "Abstract",
        "Text": "twist-2 operators induced dark matter interactions hrishabh bharadwaja,1,2, sukanta duttab,2 1department of physics & astrophysics, university of delhi, new delhi, india. 2sgtb khalsa college, university of delhi, new delhi, india. abstract we study the e {latin small ligature ff}ective interactions of the fermionic, scalar and vector dark matter with leptons and neutral electroweak gauge bosons induced by the higher dimensional e {latin small ligature ff}ective twist-2 tensor operators. we constrain these lepto-philic, and gauge boson b-philic e {latin small ligature ff}ective interactions of dm with the visible world from the wmap and planck data. the thermally averaged indirect dm pair annihilation cross-section and the spin-independent bound electron scatterinng cross-section are observed to be consistent with the respective experimental data. constraining coe fficients of the effective operators from the low energy lep data for the dm {less-than or equal to}80 gev, we further study their sensitivities in the pair production . ",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "Abstract",
        "Section": "Abstract",
        "Text": "on the cohomology of surfaces with pg = q = 2 and maximal albanese dimension johan commelin matteo penegini january 3, 2019 abstract in this paper we study the cohomology of smooth projective complex surfaces s of general type with invariants pg = q = 2 and surjective albanese morphism. we show that on a hodge-theoretic level, the cohomology is described by the cohomology of the albanese variety and a k3 surface x that we call the k3 partner of s. furthermore, we show that in suitable cases we can geometrically construct the k3 partner x and an algebraic correspondence in s *x that relates the cohomology of s and x. finally, we prove the tate and mumford-tate conjectures for those surfaces s that lie in connected components of the gieseker moduli space that contain a product-quotient surface.",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "1",
        "Section": "1 Introduction",
        "Text": "a smooth projective complex surface with invariants pg = q = 2, and assume that the albanese morphism {greek small letter alpha}: s {rightwards arrow}a is surjective. the results of this paper are inspired by the following two observations: 1. the induced map on cohomology is injective. such a hodge structure is said to be of k3 type. 2. let s  be a smooth projective complex surface , then morrison showed that there exists a k3 surface together with an isomorphism : h2tra that preserves the hodge structure, the integral structure, and the intersection pairing. tra denotes the transcendental part of a hodge structure, that is, the orthogonal complement of the hodge classes.) these observations lead to the following questions. in general we are not able to answer this question. however, an intesting class of examples of the surfaces that we consider is formed by so-called product-quotients: these are surfaces birational to a surface . since we are not able to settle question b in general, we may aim for something weaker, sitting in between question a. and question b. we use the notion of motivated cycles  . we very briefly recall what is known for the surfaces under consideration. it is important to stress that the classification of these surfaces is not yet complete. hence we present the state of the art up to now. we shall pay particular attention to those surfaces which are product-quotients recalling definitions, important properties and its associated group theoretical data. furthermore, we recall what is known about their moduli space. following morrison's theory. indeed, for those surface which are product-quotients we are able to find an algebraic k3 partner. finally in the last section we see how the results obtained can be used to prove that the tate and mumford-tate conjectures hold for these surfaces. as already mentioned here we use the notion of motivated cycles ",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "Abstract",
        "Section": "Abstract",
        "Text": "we study the jordan frame formulation of generalizations of scalar-tensor theories conceived by replacing the scalar with other fields such as vectors. the generic theory in this family contains higher order time derivative terms in the jordan frame action which is indicative of ill-posedness. however, we show that equations of motion can always be reduced to a second-order-in-time form as long as the original einstein frame formulation is well posed. the inverse transformation from the jordan frame back to the einstein frame is not possible for all field values in all theories, but we obtain a fully invertible transformation for vector-tensor theories by a redefinition of the vector field. our main motivation is a better understanding of spontaneous scalarization and its generalizations, however our conclusions are applicable to a wide class of theories. jordan frame has been traditionally used for certain calculations in scalar-tensor theories of gravitation, and our results will help researchers generalize these results, enabling comparison to observational data.",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "I",
        "Section": "I Introduction",
        "Text": "scalar-tensor theories have been among the most popular alternatives to general relativity , and also had a large impact on cosmology . these theories commonly posit that gravitation is governed by scalar degrees of freedom in addition to the usual metric of general relativity, but their phenomenology can be very diverse otherwise due to various different coupling terms in their actions. an important feature of stts is the freedom to choose the fundamental field variables while formulating them, possibilities in such redefinitions are infinite, but two specific cases, called frames, have been of special importance. the first is the jordan frame where the fundamental metric field of the theory couples minimally to matter degrees of freedom, and the second is the einstein frame where the metric is such that the metric action is in the einstein-hilbert form, hence identical to that of gr . einstein and jordan frames have been investigated in great detail in the literature which has shown their equivalence in many cases , revealed that one frame can be more useful for analyzing specific problems such as approximation schemes , and even led to the discovery of previously overlooked stts . the aim of this work is generalizing the analysis of the relationship between these two frames to theories that contain higher spin fields such as vectors instead of scalars, or less common conformal scaling functions a such as those that depend on field derivatives. our main motivation is the recently investigated phenomenon of spontaneous tensorization which is a generalization of spontaneous scalarization in the scalar tensor theories introduced by damour and esposito-far` ese . in def theories, the scalar fields spontaneously grow to large values from arbitrarily small perturbations near neutron stars due to a tachyonic instability. such a theory, with some minor caveats, confirms to all known weak-field tests while providing large deviations from gr in the strong field, hence provides an especially good target to be tested using gravitational wave observations . the desirable controlled spontaneous growth in def theories is not a direct results of the scalar nature of the coupling, or the tachyonic nature of the instability. any field that carries an instability, such as a ghost on a vector field, in principle can lead to similar spontaneous growth which is called spontaneous tensorization . overall, the theory of def is but one member of a large family of theories with similar observational signatures, all of which can be potentially tested with gravitational waves in the near future . all spontaneous tensorization theories have been formulated in the einstein frame for reasons we will discuss, and he main theme of this study is their properties in the jordan frame. despite our motivation, we will not specify our coupling terms to those that incite spontaneous growth, hence our results are general. we will use the terminology of einstein and jordan frames in a generalized sense, the former is always the one where the gravitational action is in the einstein-hilbert form, and the latter is always the one where matter fields couple to the metric minimally. in sec. ii we present the tranformation between einstein and jordan frames in the quintessential stt of brans and dicke whose conformal matter coupling structure is kept in all other theories we are interested. in sec. iii we obtain the jordan frame for a vectortensor theory. in sec. iv we go back to scalar fields, but this time study derivative couplings. we demonstrate the existence of higher derivative terms in the jordan frame, commonly indicative of ill-posedness, and present the results from the existing literature which resolve this problem. we also discuss the invertibility of the frame transformations. in sec. v we analyze the most general spontaneous tensorization theory which also has potentially dangerous higher derivative terms in the jordan frame. we address this problem by showing that the equations of motion have at most second order time derivatives. in arxiv:1901.00194v1 1 jan 2019 2 the last section, we summarize and discuss our results. ii. changing the frame in scalar-tensor theories the most elementary case to compare the einstein and jordan frames is the brans dicke theory . non-invertibility of the frame transformation is not a new problem, and is present in the simplest stts such as bd as well. nevertheless, we would still want to have an invertible transformation between frames for as much of the configuration space as possible. note that we encountered the non-invertibility problem in vector-tensor theories as well . a proper analysis requires tools from the theory of partial diffential equations, and we hope mathematical physics can provide some insight for this problem which has not been addressed in the gravitational physics literature to the best of our knowledge. lastly, the fact that the equations of motion are ultimately second order in time derivatives in the jordan frame may suggests that there is also a field redefinition that would allow the action to contain only first order derivatives, but we could not identify a simple example of this aside from the trivial transformation of going back to the einstein frame. v. jordan frame for generic spontaneous tensorization we have seen that spontaneous growth can be generalized from scalars to vectors, or from a tachyonbased mechanism to a ghost-based mechanism. this approach can be continued to various other fields such as spinors , other mechanisms such as spontaneous growth through the higgs mechanism , or any combination of them. all these form the family of spontaneous tensorization theories. how generic are the issues of higher time derivatives and invertibility of frame transformations that we encountered in sec. iv? if they appear in a generic spontaneous tensorization theory, can they be resolved similarly to the case of ghost-based spontaneous scalarization? consider the following general action for which all the theories we have investigated so far are special cases . hence, changing from the einstein frame , to the jordan frame does not introduce ill-posedness. these cover all examples of spontaneous tensorization in the literature . all these theories, aside from spontaneous vectorization in sec. iii, also contain conformal scaling functions which contain derivatives. hence, the resolution of the higher time derivative problem we outlined is central to their viability in the jordan frame as physical theories. invertibility of the frame transformation in relation to the existence of a solution is not resolved in general. all possible values provide too big a configuration space in the jordan frame, and the questions we posed in sec. iv are open in this generic case as well. however, we remind that such problems are present even in the def theory. ",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "VI",
        "Section": "VI Conclusions",
        "Text": "we studied the jordan frame formulation of generalizations of stts, where the scalar is replaced with other fields, and couplings can depend on derivatives. our motivation came from the specific class of theories that feature spontaneous tensorization. these are most naturally defined in the einstein frame where the action for the additional field to the metric is in the canonical form. however, our results can be applied to any generalization of stts that is based on a conformal scaling of the metric in the matter action by some function of a dynamical field and its first derivatives. the first case we examined is the vector-tensor theory obtained by replacing the scalar in the def theory by a vector where the conformal scaling function ax depends on the norm of the vector field. a completely invertible transformation can be obtained if the vector field is redefined in the jordan frame,  moreover, interesting connections can be observed to the recently discovered spontaneous growth in einstein-maxwell-scalar theories . jordan frame of ghost-based spontaneous scalarization where the conformal scaling depends on the derivatives of the scalar field presents challenges. first, the jordan frame contains higher derivative terms indicative of illposedness due to ostrogradsky's theorem, which is odd since this is not an issue in the einstein frame, and we would not expect the nature of the theory to change in such radical fashion due to a frame change. the equations of motion indeed contain up to fourth order time derivatives, but it can be shown that such terms cancel each other to render the equations second order in time. we could not find a formulation where this problem is resolved, but noted that this is the case even in the def theory where the scalar field is restricted to be positive in the jordan frame. it is important to understand the meaning of the field values in the jordan frame where the transformation back to the einstein frame is not defined, which we leave to future studies. we finally showed that a generic spontaneous tensorization theory contains higher time derivative terms in its formulation, much like ghost-based spontaneous scalarization. the equations of motion are again ultimately rendered second order in time, even though they naively contain fourth time derivatives, demonstrating that the jordan frame formulation does not introduce illposedness. we should add at this point that the fact that there are only first derivative terms in the einstein frame action does not guarantee well-posedness. a theory can be rendered unphysical by other factors such as indefinitely growing fields such as ghosts, even if the equations of motion have no more than two time derivatives. the einstein frame formulation of spontaneous tensorization theories are not known to be completely free of such undesirable features , but our work here shows that transferring to the jordan frame at least does not add new sources of ill-posedness. certain calculations on stts have been performed using the jordan frame such as the gravitational wave memory for the def theory . consequently, we believe this study will enable researchers to extend similar work to spontaneous tensorization in general. possibility of nearfuture testing is a basic appeal of spontaneous scalarization and tensorization. calculations of specific observational signs will enable the gravity community to compare the predictions of these theories to actual observations, and understand the differences between individual theories in the spontaneous tensorization family. acknowledgments this project started with a question posed by leonardo gualtieri to whom we are grateful. the author is supported by grant no. 117f295 of the scientific and technological research council of turkey . ",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "1",
        "Section": "1. Introduction",
        "Text": "sparse bounds for di {latin small ligature ff}erent operators have been a recent and active topic in harmonic analysis. localization and sparseness are two main ingredients which make sparse bounds especially e {latin small ligature ff}ective in quantitative weighted norm inequalities. the literature about sparse bounds is too extensive to be given here in more or less adequate form. we mention only that sparse bounds for calder on-zygmund operators can be found in . also, there are several general sparse domination principles . key words and phrases. sparse bounds, singular integrals, theorem 1.1 provides a more convenient tool compared to theorem a. indeed, there is no need now to work with the grand maximal truncated operator mt, which typically requires some additional effort. the fact that we do not require the weak type of t in theorem 1.1 allows us to obtain a sparse domination result for a singular pointwise sparse domination 3 integral operator t with minimal set of assumptions close in the spirit to the t1 theorem. the paper is organized as follows. in section 2 we present a proof of theorem 1.1. we also show separately how this proof looks in the model case of calder on-zygmund operators. in section 3 we discuss some variations and extensions of theorem 1.1. a sparse t1-type result is presented in section 4. finally, in section 5 we collect di {latin small ligature ff}erent examples of operators admitting the pointwise sparse domination. we show how theorem 1.1 simplifies sparse bounds for these operators.",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "Abstract",
        "Section": "Abstract",
        "Text": "we investigate the role that planet detection order plays in the kepler planet detection pipeline. the kepler pipeline typically detects planets in order of descending signal strength . we find that the detectability of transits experiences an additional 5.5% and 15.9% e {latin small ligature ffi}ciency loss, for periods < 200 days and > 200 days respectively, when detected after the strongest signal transit in a multiple-planet system. we provide a method for determining the transit probability for multiple-planet systems by marginalizing over the empirical kepler dataset. furthermore, because detection e {latin small ligature ffi}ciency appears to be a function of detection order, we discuss the sorting statistics that affect the radius and period distributions of each detection order. our occurrence rate dataset includes radius measurement updates from the california kepler survey , gaia dr2, and asteroseismology. our population model is consistent with the results of burke et al. , but now includes an improved estimate of the multiplicity distribution. from our obtained model parameters, we find that only 4.0 +/- 4.6% of solar-like gk dwarfs harbor one planet. this excess is smaller than prior studies and can be well modeled with a modified poisson distribution, suggesting that the kepler dichotomy can be accounted for by including the e {latin small ligature ff}ects of multiplicity on detection e {latin small ligature ffi}ciency. using our modified poisson model we expect the average number of planets is 5.86+/-0.18 planets per gk dwarf within the radius and period parameter space of kepler. key words: methods: data analysis - planets and satellites: fundamental parameters",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "1",
        "Section": "1 Introduction",
        "Text": "the kepler mission has revolutionized our understanding of the frequencies and properties of planets around sun-like stars. with the final data release dr25, providing all of the data up until the failure of two reaction wheels , the primary phase of the project has o {latin small ligature ffi}cially concluded. within this span, kepler has provided evidence for {almost equal to}4, 500 transiting exoplanets.1 nearly 50% of these candidates have been confirmed or validated , demonstrating that planets are common and widespread in the milky way. there have been many attempts to quantify the frequency of planetary systems and the properties of the planets themselves , with a special attention given to attempting to characterize the frequency of planets with earth-like properties. ",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "2",
        "Section": "2  Stellar Selection",
        "Text": "using the final release of kepler data which includes q1q17, we select a stellar sample for use in creating a detection efficiency map that accounts for kepler completeness. we use the stellar parameters provided by mathur et al. with improved radius values derived from gaia dr2 . the updates from gaia dr2 have yet to provide updated corresponding mass values. thus we must still utilize the kepler dr25 stellar mass parameters .we also place requirements on the duty cycle and the time length of the light curve . the fduty limit requires that 60% of dataspan has been collected. this ensures that a significant portion of the light curve is filled, while still including stars lost in the q4 ccd loss . time-varying noise measurements have been provided in the dr25 dataset through a value known as cdpp . this parameter has been calculated for every field star over 14 different time periods . these values correspond to the amount of noise a planet signal will need to exceed, given a transit duration, to generate detection. we minimize the inclusion of stellar and instrumental fluctuations . from this we produce a stellar sample of 86,605 solar-like stars.",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "3",
        "Section": "3 Planet Selection",
        "Text": "when available we utilize the updated planetary parameters provided by the california kepler survey and the asteroseismic updates provided by van eylen et al. . one of the main advantages for the inclusion of these updates is the improved planet radius measurements. since our study, like others, does not account for parameter uncertainty, such improvements are essential for accurate occurrence rates. where cks and asteroseismic data are unavailable, the measurements provided by the kepler dr25 catalog , in conjunction with the gaia dr2 radius updates , are implemented. through private communication, it was indicated that this early release of gaia data may contain some planet radius outliers. to combat this issue, we test the radius values against the kepler dr25 catalog. we utilize the kepler dr25 radius measurements. overall, 19 planets exceed this outlier limit . we use the periods provided in the kepler dr25 catalogs. both the cks and kepler dr25 provide flags for false positives. we include data from both confirmed and candidate planets in dr25 and cksf p = false in the cks update. to further avoid contamination from false positives, we only include planets with periods. periods beyond 500 days have been noted to be highly contaminated by false positives because they barely meet the three transit limit of the pipeline . our period and radii range exceeds the conservative cutoffs adopted by many previous studies, but is necessary when exploring the effects of multiplicity. often planetary systems span the entire range of the kepler parameter space, thus the inclusion of nearly all the planets is needed for an accurate calculation. there exist 3 multi-planet systems where one planet within the system fall beyond the range of this study. we only select the planets from these system that lie within our radius and period cuts.  the smoothed recovery fraction at each mes bin. the vertical lines represent the uncertainty in each bin under the assumption of a binary distribution. the bin values are plotted at the center of each bin.  in providing a stronger statistical argument. although some of the known planets, in these 3 systems, extend beyond the bounds of this study, we expect many other systems within the dataset to contain planets beyond the range of our selection bounds. furthermore, if we include the planets that lay beyond our radius and period cuts, our analysis we will artificially inflate the number of inferred planets within this range. the accuracy of the kepler detection order can be affeected by systems with existing false positives. when removing these data points, we manually ensure that the detection order only reflects the order in which valid kois are detected. for example, a system with 5 real kois and 1 false positive would have detection orders ranging from 15 regardless of order at which the false positive was detected. it should be noted that these false positives do create cuts in the data, similar to that of a planet and therefore affect the detection order. however, without reordering these systems we artificially inflate our multiplicity calculation in section 7. higher multiplicities are especially sensitive to mild increases as their detection probabilities are very low. further discussion in section 4 shows that we use the same detection e {latin small ligature ffi}ciency for all planets found after the first detected planet, thus only planets artificially being re-assigned to 1 are of concern. since most false positives provide relatively weak signals, only 14 systems experience this artificial re-ordering. after making the discussed cuts we find that the highest detection order existing in the parameter space is 7. this means that the highest system multiplicity we consider in this study is a 7 planet system. we find 3062 kois meet the indicated period and radius requirements. it has been suggested that gas giants eject companion planets while migrating inward . their large hill radius forces the planets to become unstable as the hill radius ratio falls below 10. these hot jupiters create an independent population of single planet systems . if it forms via a distinct channel, this population has the ability to skew the inferred distribution of the model for the generic underlying population. further evidence of this independent population was discussed . it showed that multi-planet systems with one planet of mass > 0.1 jupiter mass are dynamically unstable on short timescales. this 0.1 jupiter mass limit roughly corresponds to the r = 6.7r limit used here. we find that 120 of these single hot jupiters exist in the dataset, leaving us with 2942 kois that fit all the parameter requirements described. our final catalog of planets and their corresponding parameters can be found online.",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "4",
        "Section": "4 Injection Recovery",
        "Text": "here we shall discuss how we can account for the detection effciency as a function of detection order. christiansen injected artificial planet signals into the calibrated pixels of each of the kepler field stars and processed the altered light curves with the standard detection pipeline. this allows the recovery fraction to be assessed, producing a probability function was fit to the empirical probability of recovery, of the form: therefore planet detection order was not considered. however, many of the target stars are known to host real kois, and these signals will remain in the christiansen analysis. this provides an opportunity to consider the effects of detection order on recovery. here we define detection order by the variable m, where m=1 indicates the first planet discovered in the system . likewise, planet m=2 and m=3 corresponds to the second and third planets found by the kepler pipeline. the highest detection order existing in the parameter space is 7the break at 200 days was selected by testing different values. beyond 200 days, we find that the distributions begin to change significantly. to focus on the 2 systems.",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "5",
        "Section": "5 Effects of mutual inclination",
        "Text": "here, we shall discuss how the effects of mutual inclination are handled within our model. the initial recovery study was performed without consideration of higher multiplicity planets. thus, there was no accounting for mutual inclination. the artificial planets were injected with a random impact parameter from 0 to 1. to understand the effects of mutual inclination on detection effciency we look at the difference of impact parameters for recovered planet systems. b is calculated by taking the difference of the artificial planet and the largest mes koi impact parameter in each system. since an existing koi is required for this test, we only look at systems with known planets. we find that the detected planets do not significantly differ in b than the difference of two randomly drawn populations of b values. because the artificial planets were injected with uniformly drawn impact parameters, we conclude that the b, and therefore mutual inclination, plays an insignificant role in detection efficiency. however, larger mutual inclinations can cause certain planets to geometrically avoid transit completely.",
        "Subsections": [
            {
                "Section_Num": "5_1",
                "Section": "5.1 Transit Probability",
                "Text": "analytic models of transit probability have been found for double transit systems as a function of mutual inclination . however, larger multiplicity systems are more difficult and require semi-analytic models . in order to simplify our calculation, we simulate various semi-major axis to stellar radius ratios and look at 106 lines of sight to predict the probability of transit. to determine the period population we need a function for m transit probability at some semi-major axis value . in order to create a function for probability of transit in addition to m -1 other transits, it is essential that we know the distributions of exoplanet periods. clearly, this argument is circular in nature. we deal with this issue by using a non-uniform method of sampling from the empirical period population. this is performed for detection order m = 2 : 7, since the analytic probability is suffcient for m=1. to establish the desired detection order, the required number of planets are drawn from the empirical kepler period data. for example, when looking at the case of m=3, is selected and then the two additional planets are drawn from the known kepler mnras 000, 1-17 transit multiplicity in planet occurrence rates 5 period sample. the periods of the additional two planets are redrawn at each line of sight. this is the same as saying we marginalized the additional two planets over the kepler period population. in order to properly account for the transit probability of higher detection orders, we need to know the unbiased underlying populations of periods. to approximate this, we sample the empirical distribution of kepler planet periods, but weighted with a probability . 3. this is done to account for the geometric bias against the detection of longer period planets. to account for the mutual inclination between orbits,this mild distribution was found by looking at the impact parameter ratios within kepler systems. once all orbits have been selected, the number of lines of sight where all planets transit is divided by 106 to establish the transit probability.  to avoid the creation of unstable systems, we check the planet separations . if any separation is < 10% the semi-major axis of the outer planet we resample the entire system. this process is repeated until no separations fall below the 10% threshold. although mutual hill radius would provide a better measure of stability, our metric requires no assumptions about the mass of the planets. furthermore, we find that changing this threshold makes little difference to the probabilities calculated, indicating that stability accounting has little effect statistically. it is worth noting that equation 2 does not account for grazing transits. to properly account for this, we find that grazing transits provide an increase of 0.2% to the overall transit probability. however, this uniform distribution is weighted far more heavily towards large planets than the underlying planet radius distribution, thus we expect the true correction to be much smaller. to properly account for grazing transit one must have some understanding of the underlying radius population. any attempt to do so here would add more uncertainty to the calculation and provide a very minimal correction. thus, we ignore such complications here.",
                "Subsections": [],
                "Groundtruth": ""
            }
        ],
        "Groundtruth": ""
    },
    {
        "Section_Num": "6",
        "Section": "6 Detection Efficiency Grid",
        "Text": "to represent the kepler survey detection efficiency a grid is created in period and radius space. both log10p and log10r are divided into 100 bins, creating 10,000 regions of the parameter space. for every region we uniformly sampled in log space for period and radius, all 86,605 stars are assigned m planets based on the detection order of interest. for example, in the detection grid for the first transiting planet , the probability of detecting at least one planet is calculated at each bin. similarly for m=2, the probability of detecting at least one planet at each bin in addition to finding another planet in some other arbitrary bin. the probability for transit of high multiplicity systems using the fang & margot mutual inclination model. the solid black line represents the probability function used for an m = 1 planet transit . a machine-readable version of this data is available online. each region is calculated using these planetary assignments and the procedures provided in the next sections . this process is then repeated for each of the 10,000 regions. we calculated 7 detection effciency grids: first planet probability , second planet probability , and the seventh planet probability . this procedure is similar to that of burke et al. and traub , but now with 7 different detection order grids. 6.1 probability of detection for m = 1 we shall begin with the formula for the detection of the first planet and then discuss the modifications made for the detection of higher order systems. in our base model we assume all planets have perfectly circular orbits and consider the effects of eccentricity in section 8.5. this assumption of little or no eccentricity is reasonable for the typical multiple systems sampled by kepler, where non-circular orbits would result in unstable system architecture. the color map is representative of log10. the fading of color across detection order shows the decreasing detection probability. a machine-readable version of this data is available online. where p is the orbital period of the planet. the expected number of transits can be found with ntr = dataspan p where dataspan is the span of the data within the kepler survey. because of various shut downs and data downloads throughout the kepler mission, it is possible that some of the transits may have been missed. to account for the probability of the transit occurring in the window of the kepler mission we adopt the window function provided by burke et al. where duty is the duty fraction of the targeted stellar source. the kepler pipeline requires at minimum 3 transits for candidate consideration; pwin is the probability that at least 3 transits will be detected by the available kepler data. since most targets have a duty = .95, short period transits produce a pwin nearly 1 and approach 0 as j < 3. almost all of our sample have data throughout the full data set span of 1458.931 days. the mean dataspan for this study is 1427.445 days. other studies have used various way to account for the effects of limb darkening such as that of claret & bloemen . we attempt to mimic the pipeline by looking at the empirical limb darkening values chosen for existing kois . we find that the two limb darkening parameters used to fit planet transits within the pipeline are strongly correlated to stellar temperature . finally, we account for the systematic detection effciency using the gamma distribution cdf described in section 4. this probability is dependent on detection order and we shall now discuss in the next section how higher multiplicity planets can be accounted for. we change the transit probability to reflect the probability of m planets transiting, accounting for the probability of finding this planet with at minimum m -1 other planets. to best capture the probabilities of our simulation in section 5.1, we interpolate between simulated data points for the transit probability. pm tr = linear interpolate mnras 000, 1-17 transit multiplicity in planet occurrence rates 7 for example, if we are looking at a planet with m=3 with ap/r {star operator}= 32, we would expect a transit probability of 0.008. this can be clearly seen in the data provided by figure 2. since no such simulated value exist at this exact point, we interpolate between the the two neighboring estimations to establish this value. this now produced 7 distinct detection grids . the first four grids can be seen in figure 3. the detection order of the exoplanet in question will dictate which grid is most appropriate for application. to summarize, we have described how the recovery probabilities are a function of detection order . we use this to create 7 different detection efficiency maps . in order to create a map for m=1 planets, we sample across planet period and radius space. doing so, we calculate the probability of detection and averaged over all stars within the kepler stellar sample. we expand upon this idea, creating a map for m=2 planets. here the new recovery cdf is implemented to account for the additional loss of planets at higher detection orders. furthermore, we account for the probability of two planets within the system transiting using a mild mutual inclination model . jumping from m=1 to m=2 we lose an additional 5.5% and 15.9% of the planets for periods < 200 days and periods > 200 days respectively. this is due to properties of the pipeline when fitting multiple transit systems. this procedure is repeated for m=3:7 each accounting for the appropriate number of transiting planets according to the data in figure 2 . there is an additional loss of nearly 70% at each respective discovery order due to the unlikely event of multiple orbital alignment with our line of sight. it is clear that these two factors, geometric transit likelihood and pipeline recovery, have a significant e {latin small ligature ff}ect on the multiplicity extracted from the kepler data set.",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "7",
        "Section": "7 The Likelihood Function",
        "Text": "using the efficiency grids derived in the previous section, we can infer properties of the underlying planetary population. here we will discuss the likelihood function required to implement bayes theorem and extract these population parameters. we adopt the approach of previous studies , modeling the underlying population as characterized by independent power-law distributions in period and radius. we also make explicit the assumption that there is a single planetary population - assuming that systems which show only one transit are drawn from the same underlying distribution as those which show multiple transits. we will examine the validity of this assumption in section 8.1. he sorting simulation for m=1 and m=2. the solid blue line represents the beta distribution fit to the respective data set. the boxes are a histogram of the simulated data after being sorted. it is apparent that sorting has a more dramatic effect on radius than period. a mild deviations from the model is noted in the radius skew. this discrepancy dissolves as we move into higher detection orders. furthermore, the effects of these deviations are insignificant, given the cuts on duty cycle, data span, and stellar type already made. means that we include more of the kepler parameter space than was used in most previous papers. we require continuity at rbr and pbr through the normalization constants for q and g. our method expands on the poisson process likelihood used by youdin . the main difference is the separation of planets by detection order . in doing so, we require different occurrence factors for each m, increasing the required number of parameters. previous studies such as burke et al. have used a single occurrence value, providing an average occurrence factor. by separating the occurrence factor as a function of detection order, we can allow for differences in detection effciency while simultaneously fitting for the occurrence of planet multiplicity. this value provides information on the occurrence of each m multiplicity. however, to find meaningful information from these values, they must be disentangled from each other as discussed in section 7.0.3. the 86,605 accounts for the number of stars in our test sample and {greek small letter eta}m is the detection probability at the given detection order. the function om is the sorting order correction for the probability distribution function. this function is necessary to account for the bias in the pdf introduced by our sorting in terms of detection order . it is often more useful to consider the natural log of the likelihood, which can be simplified . ",
        "Groundtruth": ""
    },
    {
        "Section_Num": "8",
        "Section": "8 Discussion",
        "Text": "in this section, we now apply the formalism we have developed to infer the revised occurrence rate parameters for planets orbiting gk dwarfs. this sample includes data from the final kepler release dr25 and updated planet radius measurements from the cks and gaia dr2. beyond these recent data improvements, we now include a corrected detection effciency for multiple-planet systems. given that many multiple-planet systems span much of the kepler parameter space, in implementing two detection efficiencies, this study expands on the poisson process likelihood function used by other authors, allowing for the treatment of planet multiplicity. this bayesian framework is fit using an mcmc, where 20,000 steps are used to model the posterior of each parameter. ",
        "Subsections": [
            {
                "Section_Num": "8_1",
                "Section": "8.1 Forward Modeling the Results",
                "Text": "thus far, we have accounted for various parameter and population dependencies. to ensure that this process yields meaningful results, we choose to sample the extracted population and subject it to the detection constraints described in section 6. here we present the exomult forward modeling software. this code, developed in r, simulates these detection e {latin small ligature ff}ects and produces a population of detected planets. using this program, we can make far fewer assumptions and directly recover the expected population. for example, the probability of transit for all 7 planets can be directly accounted for by sampling system inclination, mutual inclination and the argument of periapsis directly. furthermore, the detection probability will not be marginalized over all stars, but rather reviewed for each system independently. the first step in our forward model is drawing each system of planets according to the population parameters given in figure 8. each system is randomly oriented with mutual inclinations drawn from a rayleigh distribution. for planets with detectable impact parameters , the planets within each system are sorted in decreasing mes. the probability of recovery is assigned to each planet according to the procedure laid out in sections 6.1 and 6.2. based on the calculated probability of detection, the planet is either detected or lost by drawing from a random number generator. we explore the effects here. when we remove the single planet systems from the data set, this gap is no longer apparent. one plausible explanation for this gap is a unique population of single planet systems shows that a weak gap can be seen in the multi-planet systems when aggregated). to explore this theory, we isolate the multi-planet systems and run our fitting procedure again. this indicates that if a separate population does exist, the population parameters are weakly affected by their inclusion in our dataset. the resulting forward model of this fit is presented in figure 5. furthermore, the increase in uncertainty seen in these parameters is due to the reduced samples used for fitting . it is notable that the empirical kepler data set is sharply peaked, while the model does not provide a similar sharpness for the m = 1 radius population .  a plot of the forward modeled population derived by our bayesian analysis. the red x marks symbolize the model values with their corresponding 68.3% confidence intervals. to find this interval the model is sampled 50 times using the posterior parameter distributions, the uncertainty reflects the fluctuations we find from these trials. the black points show the kepler data with poisson uncertainty. for m=5:7 many of the bins have 1 or 0 planets, where small number statistics cause significant variations. in order to minimize this variations we present the resulting combination of m >= 4. however, it should be noted that our forward model does differentiate between these detection orders. left: forward model of multiple and single planet systems. right: forward model of only multiple-planet systems. this model was produced by only fitting to the data of multiple-planet systems. the mentioned radius gap. furthermore, it is possible that a true accounting for planet period and radius covariance could produce such a peak. millholland et al. and weiss et al. show that the planets within multiple systems tend to have similar mass and radius components. although these features are not properly accounted for here, figure 5 shows that these mild population characteristics remains small and do not deviate greatly from a simple broken power-law model. we hope to include such features in the next iteration of this software. it is possible that future studies may use this forward modeling technique to directly determine the population parameters. unfortunately, it remains computationally expensive to properly account for all detection features. traub overcame this cost by ignoring multiplicity.",
                "Subsections": [],
                "Groundtruth": ""
            },
            {
                "Section_Num": "8_3",
                "Section": "8.3 Survival Function",
                "Text": "within this study, we only use planets provided by the kepler pipeline. the highest multiplicity seen is m=7 for a gk type star. this is certainly not the actual highest multiplicity within this parameter space. shallue & vanderburg use a deep convolutional neural network to extract an 8th planet from the kepler-90 light curve, proving this assertion to be true. using a poisson survival function we can extrapolate the probability of existence for these higher multiplicity systems. the fm values found by this study represent the fraction of stars with at minimum m planets. this lends itself well to a survival function, in this case we use a modified poisson distribution to model multiplicity. poisson distributions are ideal for planet multiplicity as these distributions are used for counting statistics. the modification is that the distribution is not truly normalized .this modification allows for an excess or scarcity of zero planet systems. we are only interested in stars that do harbor planets, thus this modification is necessary. since these stars are currently assumed to have zero planets by this paper, inclusion of these additional planets would increase the value. however, we would expect our lamba parameter to slightly decrease, with the inclusion of these additional singles, as this value only considers systems that do harbor planets. with this function in hand, we can extrapolate to higher multiplicity. for example, our model suggests that 32.3+/-2.7% of gk stars will harbor at least 8 planets within the kepler parameter space. in the parameter space of the kepler survey, our solar system has two mnras 000, 1-17 12 zink, christiansen, and hansen table 4. a representation of the expected empirical multiplicity as a function of selection e {latin small ligature ff}ects. each column shows the expected population using the best fit model from this study . starting from the left, moving right, each effect is adding in addition to all previous effects. the multiple detection effciency is broken into two columns. the data column directly used the multiplicity values shown in figure 6. in contrast, the model column uses the modified poisson distribution inferred from the multiplicity data . geometric mutual inclination single detection multiple detection multiple detection real kepler effciency effciency . this lack of multiplicity in our solar system could be important for habitability, but such claims still lack strong evidence.",
                "Subsections": [],
                "Groundtruth": ""
            },
            {
                "Section_Num": "8_4",
                "Section": "8.4 Kepler Dichotomy",
                "Text": "analysis of the statistics of the kepler multiple planet systems suggest that the underlying planetary population requires a two component model. one component is composed of systems with high planet multiplicity and a low inclination dispersion, while the other requires either low intrinsic multiplicity or a large inclination dispersion to reduce the frequency of transits by multiple planets. this has been termed the kepler dichotomy. lissauer et al. inferred that the two populations had roughly equal frequencies and subsequent analyses confirmed this. there have been several models proposed to explain this on dynamical grounds . the simplest solution is to consider a single population of planets in which some fraction have experienced excitation of their mutual inclinations. however, to meet the requirements of the transit statistics, the excitation is su fficiently large that dynamical stability is hard to maintain . thus, the kepler results seem to imply the existence of a low multiplicity population of planetary systems, whether due to formation or later dynamical instability. however, this finding rests on the relative frequencies of systems with single transiting planets versus multiple transiting planets. if the completeness is a function of the detection order, this may weaken the claim for a kepler dichotomy. in figure 6 we show that a single poisson distribution can account for the multiplicity probabilities extracted from our analysis. we find a much smaller fraction of intrinsically single systems than fang & margot and find a distribution broadly similar to the model for a single, dynamically motivated population described in hansen & murray . however, we still find 6% of stars harbor intrinsically single or double planet systems. to test the robustness of this low multiplicity contribution we forward model the inferred population using the poisson multiplicity model. in table 4 under the label multiple detection e {latin small ligature ffi}ciency we present the multiplicity results of this model. we can see that almost all of the empirical population fall within the multiplicity model. this indicated that that apparent deviations in our infer fm values can be described by statistical fluctuations in population. additionally, our fm are very dependent on the choice of mixture values displayed in table 3. a proper accounting of these values would require distribution dependence. averaging over these parameters, as done here, can cause mild deviations in the inferred fm values. in extracting the population fm values, we have only employed a mild rayleigh distribution to account for mutual inclination of each system as directed by fang & margot and have no larger inclination component. it appears that accounting for systematic loss of planets at higher multiplicity substantially reduces the low multiplicity population inferred as per the kepler dichotomy. we shall now discuss how this works. using the forward model presented in section 8.1, we look at how the inclusion of detection efficiency affected the gap seen between systems with one transiting planet and those with two transiting planets. the population provided by the parameters in figure 8 is modeled 20 times and the median from each group is recorded in table 4. using our population parameters and a mild mutual inclination model show that this anomaly is largely due to kepler detection efficiency. table 4 shows how the frequency of detected systems of different transit multiplicity changes as we include di fferent systematic effects. in the first column, we include only the correction of the probability of transit due to geometric alignment. for a simple numerical comparison, this results in a ratio of double transit to single transit systems of 0.37, to be compared to the observed value of 0.21 . the inclusion of a small mutual inclination dispersion, comparable to that of fang & margot , does not improve the ratio . in the third column, we show the model in which we include the completeness corrections from christiansen without the multiplicity treatment discussed here. this results in a partial improvement of the ratio to 0.25. it is also notable that the number of expected high transit multiplicity systems also drops significantly with the inclusion of this effect. finally, in the fourth and fifth column, we show the expected numbers including the full, multiplicity-dependent completeness correction discussed here . we find that the expected number of different transit multiplicities are now very well matched to the observed numbers, substantially weakening the need for an additional population to explain the observations. the ultimate reason for this is that high transit multiplicity systems usually contain several planets that lie in the low mes region of parameter space, so that the incompleteness knocks planets down the multiplicity scale, resulting in many single transit systems that, in an ideal world, would show two or three transiting planets. furthermore, the improved stellar radius measurements from gaia suggests mnras 000, 1-17 transit multiplicity in planet occurrence rates 13 that many stars have larger radii than previously believed . increasing the stellar radius of system will decreases the probability of detection for an exoplanet. this correction will overall increase the inferred occurrence measurements. it is important to remember that our dataset does not include single hot jupiter planets as discussed in section 3. this observed population of 120 planets does not follow our power-law trend and appears to be uniquely single . while these outliers do provide some type of population dichotomy, their presence is not the most prominent cause of the excess of singles. our extracted population parameters f1 and f2 indicate that 4.0 +/- 4.6% of the underlying population does have only one planet, and that this contribution can be described by the modified poisson distribution used to fit the higher multiplicity systems. there is dynamical evidence that single transiting systems are more dynamically excited than multiple systems and this is consistent with the notion that some fraction of compact planetary systems are dynamically perturbed by the existence of giant planets on larger scales. previously, hansen found that explaining the original excess of single transits required a frequency of giant planets on large scales that was roughly double that found by radial velocity surveys. the reduction found here substantially alleviates that discrepancy. other recent work also supports the notion that single transiting systems are drawn from the same underlying planetary population as multiple harbor systems. weiss et al. find that both populations share essentially the same stellar and planetary properties, while zhu et al. use transit timing variations to infer that there is a strong correlation between multiplicity and dynamical excitation. they reject the notion that this is driven by giant planet excitation because they see no correlation with the metallicity of the host star, but such a correlation would be di {latin small ligature ffi}cult to see at the level of 4% as found here. this is further supported by munoz romero & kempton , who find no metallicity di {latin small ligature ff}erence between hosts of single and multiple transiting systems, but could easily accommodate mixtures at the 50% level.",
                "Subsections": [],
                "Groundtruth": ""
            },
            {
                "Section_Num": "8_5",
                "Section": "8.5 Considering Eccentricity",
                "Text": "for consistency. this model was inferred by simulating in situ gravitational assembly of planetary embryos and observing the resulting eccentricity population of the fully formed planets. although derived within a specific scenario, this distribution matches well with a model in which planets explore the full range of available phase space subject to the constraint of dynamical stability . as such, it represents a plausible description of the level of eccentricity to be expected in such systems. comparing these values to those of our base model, we find that eccentricity flattens the cdf of planet multiplicity, the red line illustrates the eccentricities used to draw the underlying the beta distribution . the black line represents the empirical cdf of the detected single planet systems and the blue line represents the eccentricities of the detected multi-planet systems. we test the strength of this hypothesis. implementing only one true underlying eccentricity model, we inspect the detected eccentricity populations from both the single and multi-planet populations. when tested with the hansen & murray model , we find no significant difference between the the observed eccentricities of multi-planet and single planet systems. this indicates that the differences noted by van eylen et al. may be real. however, van eylen et al. suggests a beta distribution for single planet systems with <e> = 0.26. this is a significantly larger average eccentricity than expected by the hansen & murray model. when larger eccentricities are tested we do find observable differences between the single and multi-planet systems. the kipping model was calculated using radial velocity discoveries and contains a significant fraction of massive planets. this distribution is probably too eccentric for the tightly packed model discussed here, but illustrates the effects of detection bias on the eccentricity population. we present the results of our test on the kipping model. we find that multi-planet systems tend to produce more low eccentricity detections than single planet detections despite being drawn from the same underlying population. analyzing the statistical difference with an anderson-darling test produces a p-value of 10-7, suggesting these di {latin small ligature ff}erences would appear statistically significant. furthermore, we can see that neither of the detected populations closely mimic the true beta distribution, highlighting the importance of detection efficiency consideration when performing eccentricity occurrence measurements. this effiect is caused by the increased transit duration for higher eccentricity transits. increasing the transit duration improves the planet mes, making the signal easier to detect. since the highest mes planets are the most likely to be detected, this biases the empirical population toward higher eccentricity. the sorting order in combination with the multiplicity detection efficiency of the kepler pipeline will further exaggerate this bias in the single planet systems. it is clear that low eccentricity distributions are less affected by this bias. manually tuning the beta distribution we find that models with <e> >= 0.18 will produce statistically significant differences between the empirical eccentricity population of singles and multiple planet systems. since van eylen et al. suggests a <e> >= 0.26 model for the singles and a <e> = 0.05 model for the multi-planet systems, it is di {latin small ligature ffi}cult to determine the effect of detection bias on their eccentricity model. at this point we cannot rule out that two distinct populations of eccentricity exist between the single and multi-planet systems, but propose that such claims require further evidence.",
                "Subsections": [],
                "Groundtruth": ""
            },
            {
                "Section_Num": "8_6",
                "Section": "8.6 Extrapolation to Longer Periods",
                "Text": "as mentioned above, our general populations parameters do not differ greatly from those of previous studies.  the lack of long period planets provided a weaker power-law, furthermore, we find tension with foreman-mackey et al. . foreman-mackey et al. avoid the assumption of a particular functional form for the extrapolation to longer periods, by using a gaussian process regression to determine the shape of the distribution. however, they use the results of the terra pipeline in it's original form, in which it only reported the highest signal to noise candidate around each star. although they back out an estimate of the detection effciency from the results of petigura et al. , we have shown in section 7 that detection order can bias the results. in particular, we expect foreman-mackey et al. to undercount small planets and long period periods. both of these biases will lower the value and we should regard the foreman-mackey et al. result as a lower limit. for the occurrence of habitable planets we follow the procedure provided by burke et al.",
                "Subsections": [],
                "Groundtruth": ""
            }
        ],
        "Groundtruth": ""
    },
    {
        "Section_Num": "9",
        "Section": "9 Conclusion",
        "Text": "we present a new method for determining the frequency of exoplanet multiplicity within the kepler dataset. in doing so we provide the following new fitting features and conclusions: 1. previous studies have discussed and provided methods for calculating high multiplicity transit probabilities. for occurrence calculations these procedures are often too complex and computationally expensive to carry out. we provide a new method which marginalizes over mutual inclination and the empirical kepler period set to determine the transit probabilities for kepler multi-planet systems. using this, we provide the transit probabilities for multiple systems containing up to 7 planets. this simplification is important and useful when trying to fit multiplicity parameters via mcmc or some other fitting method that requires 104 calculations. our method does make some simplification assumptions in the interests of speed. we assume the measurements of planet radius and period are perfect. the uncertainty in period is negligible, however the radius measurements retain significant uncertainty and the present dispersion may yet mask finer features in the distribution. in accounting for mutual inclination, we adopt the model provided by fang & margot . this is derived using a di {latin small ligature ff}erent multiplicity model than that found here. all orbits are assumed to be circular in our base model. because many of the systems are very compact, circular orbits are required for any type of stability. tidal circularization will also force many of these planets into circular orbits. however, it is possible that some portion of the population, investigated here, contains varying amounts of eccentricity. we show that any amount of eccentricity will increases our the overall multiplicity values, but decreases the fraction of systems with planets. we have assumed the appropriate model for exoplanet occurrence is a broken power-law. furthermore, we assume period and radius and uncorrelated. it has been shown by owen & wu and weiss et al. that a mild correlation exist between period and radius at short periods where photoevaporation can take e {latin small ligature ff}ect. nevertheless, the fact that our forward modeling matches the data inspires confidence that the model provides a coherent description of the data. 2. in systems with more than one detected planet, we find that detection e {latin small ligature ffi}ciency decreases for higher detection order planets. this conclusion was achieved by re-visiting the christiansen injections and looking at systems with pre-existing planets. multiple planets systems experience an additional loss, for lower mes planets within each system, of at least 5.5% and 15.9% for periods < 200 days and > 200 days respectively. this type of increased selection e {latin small ligature ff}ects indicates that a larger fraction of the population is being missed. being able to infer a larger population of multiple exoplanet systems significantly decreases the gap between single and double planet systems. the initial motivation for additional detection e {latin small ligature ffi}ciencies for multi-planet systems, was the 61 known kois lost during the christiansen injections. when testing our additional selection e {latin small ligature ff}ects, for multiples, we expect 41 +/- 7 planets should be lost due to a similar type of injection test. because we find that 61 kois are lost we suspect higher order detection efficiencies may be necessary for an accurate accounting of the true underlying populations. 3. using bayesian statistics, we expand the poisson process likelihood to account for variations in detection order. furthermore, we are able to infer population multiplicity from this fitting process. the results from this fit match that of burke et al. , but provide an improved measurement with reduced uncertainty from gaia, cks, and asteroseismology . furthermore, by looking at the occurrence of single and double-planet systems, we only find a 0.9 {greek small letter sigma} di {latin small ligature ff}erence between these two populations . this disparity can be explained by a modified poisson distribution indicating that the kepler dichotomy ; may largely be an artifact of detection efficiency and statistical fluctuation. using a poisson process likelihood requires that each planet is drawn independently, which is clearly not the case for planets mnras 000, 1-17 transit multiplicity in planet occurrence rates 15 in multiple systems. much of the work in this study is accounting for these dependencies. ignoring the independence requirement of poisson process could be suspect, but is again justified by the success of our forward model, where this assumption is not necessary. the independence of radius between planets within a system has also not been accounted for within this study. 4. given our inferred multiplicity model we can extrapolate to higher multi-planet systems. we find that 32.3 +/- 2.7% of solar-like stars should contain at least 8 planets within 500 days. the existence of a single 7 planet system and a single 8 planet system indicates these systems should be rare but still detectable. we would expect to find < 1 eight planet systems within the constraints of this study. 5. we introduce and demonstrate that forward modeling a broken power-law distribution can still provide a reasonable model for the exoplanet population, despite growing evidence for a gap . we find that our fitting model also produces similar populations of multiplicity to that of the empirical kepler data set, indicating the success of this method. 6. using the the eccentricity model of hansen & murray , we show that eccentricity can affect the multiplicity occurrence by slightly decreasing the expected number of planets around each star. we also find that for eccentricity models with <e> >0.18 the kepler pipeline will significantly skew the empirical population of eccentricity for single transiting systems, suggesting that differences seen between the single and multiple planet systems may be artificial.",
        "Subsections": [
            {
                "Section_Num": "9_1",
                "Section": "9.1 Future Goals",
                "Text": "as mentioned previously, the uncertainties in the radius measurement are still quite large. using a bayesian hierarchical model, this uncertainty can be incorporated when fitting for population parameters . we hope to include this feature into our next generation of occurrence fitting. the multiplicity parameters derived here can be use in determining an eta earth measurement. the importance of neighboring planets could be essential for the long term stability of an earth analog , thus it is important to understand the likelihood of this earth analog within a multiple system. the new detection efficiency is limited to m >= 2. ideally, we would want the detection efficiency for each detection order. to do so one would need to perform an alternative injection experiment, where numerous planets are injected into each system and the recovery of each order can be better sampled. it would also be useful to understand the effects of resonance on detection efficiency. looking at a select group of stars and injecting many planets at various period ranges could provide an understanding of these features . with the loss of kepler and the upcoming release of tess it will be essential to combine data across missions to calculate a more robust occurrence measurement. doing so will require accounting for differing detection efficiencies across each mission. the method described here may provide a unique way of incorporating these different selection effects while producing a uniform population distribution. acknowledgement we would like to thank the anonymous referee for useful feedback. the simulations described here were performed on the ucla ho ffman2 shared computing cluster and using the resources provided by the bhaumik institute. this research has made use of the nasa exoplanet archive, which is operated by the california institute of technology, under contract with the national aeronautics and space administration under the exoplanet exploration program.",
                "Subsections": [],
                "Groundtruth": ""
            }
        ],
        "Groundtruth": ""
    },
    {
        "Section_Num": "Abstract",
        "Section": "Abstract",
        "Text": "s-dual amplitude and d3-brane couplings komeil babaei velni1 and h. babaei-aghbolagh2 1department of physics, university of guilan, p.o. box 41335-1914, rasht, iran 2department of physics, university of mohaghegh ardabili, p.o. box 179, ardabil, iran abstract recently, it has been observed that the iib scattering amplitudes are compatible with the standard rules of s-duality. inspired by this observation, we will find the treelevel s-matrix elements of one ramond-ramond and three open strings by imposing this symmetry on the tree-level s-matrix elements of one kalb-ramond and three open strings. we also find a sl invariant form of the d3-brane effective action containing four gauge fields with derivative corrections that was derived from one-loop level four-point amplitude. using the expansion of the nonlinear sl invariant structures, we find the action with derivative corrections at the level of more gauge fields. ",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "Abstract",
        "Section": "Abstract",
        "Text": "abstract in many recent applications when new materials and technologies are developed it is important to describe and simulate new nonlinear and nonlocal diffusion transport processes. a general class of such models deals with nonlocal fractional power elliptic operators. in order to solve these problems numerically it is proposed to consider equivalent local nonstationary initial value pseudo-parabolic problems. previously such problems were solved by using the standard implicit backward and symmetrical euler methods. in this paper we use the one-parameter family of three-level finite difference schemes for solving the initial value problem for the first order nonstationary pseudo-parabolic problem. the fourth-order approximation scheme is developed by selecting the optimal value of the weight parameter. the results of the theoretical analysis are supplemented by results of extensive computational experiments. keywords: elliptic operator, fractional power of an operator, finite element approximation, three-level schemes, stability of difference schemes ",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "1",
        "Section": "1 Introduction",
        "Text": "in many recent applications the new mathematical models are proposed, which are based on fractional derivative equations in time and space coordinates . very different applied mathematical models of physics, biology or finance describe a subdiffusion or superdiffusion models. the latter problems are often simulated by using fractional power elliptic operators. di {latin small ligature ff}erent numerical techniques, such as finite difference, finite volume methods, can be used to approximate problems with fractional power elliptic operators. in this paper we will use the method of finite elements, since this method is well-suited to solve problems in non-regular domains and to use non-uniform adaptive grids . a comparison of different approaches to solve fractional-in-space reaction-diffusion equations is done . in particular the integral and adaptively preconditioned lanczos method are analyzed. the most straightforward algorithm to solve such systems is to construct explicitly eigenvectors and eigenvalues of the given discrete elliptic operator and to diagonalize the matrix a . but we should note that the direct implementation of this approach is very expensive for general elliptic operators in multidimensional domains. it requires the computation of all eigenvectors and eigenvalues of very large matrices. a general approach to solve fractional power elliptic problems is based on some approximation of the nonlocal operator. one can adopt a general approach to solve numerically equations involving fractional power of operators by a popular method is to split the task to solve numerically equations involving fractional power into two steps. first the original elliptic operator is approximated and then the fractional power of its discrete variant is taken. using dunford-cauchy formula the elliptic operator is represented as a contour integral in the complex plane. then applying appropriate quadratures with integration nodes in the complex plane we get a method that involves only inversion of the original elliptic operator. the approximate operator is treated as a sum of resolvents , ensuring the exponential convergence of quadrature approximations. in paper a more promising quadratures algorithm is proposed, when the integration nodes are selected in the real axis. the new method is based on the integral representation of the power operator . in this case the inverse operator of the fractional power elliptic problem is treated as a sum of inverse operators of elliptic operators. such a rational approximation is obtained when the fractional power of the operator is approximated by using the gauss-jacobi quadrature formulas for the corresponding integral representation. in this case, we have a pade-type approximation of the power function with a fractional exponent. the optimal rational approximations are investigated in . a separate class of methods approximates the solution of fractional power elliptic problem by some auxiliary problem of high dimension. in it is shown that the solution of the fractional laplacian problem can be obtained 2 as a solution of the elliptic problem on the semi-infinite cylinder domain. this idea is used to construct numerical algorithms for solving stationary and nonstationary problems with fractional power elliptic operators, for solving fractional power elliptic problems we have proposed a numerical algorithm on the basis of a transition to a pseudo-parabolic equation, so called cauchy problem method. the computational algorithm is simple for practical use, robust, and applicable to solving a wide class of problems. we have used this algorithm also for solving the nonstationary problem with fractional power elliptic operators . for the auxiliary cauchy problem, standard two-level schemes are applied. depending on the weight parameters the first and second order accuracy of the approximation is obtained. for many applied problems a small number of pseudo-time steps is su {latin small ligature ffi}cient to get a good approximation of the solution of the discrete fractional equation. the effciency of this algorithm is improved in , where a special graded grid in pseudo-time is used. another possibility to increase the accuracy of approximations is to use high order discrete schemes for solving the auxiliary pseudo-parabolic equation. in this paper we propose and investigate a fourth order three-level scheme. the paper is organized as follows. in section 2 a problem for a fractional power of elliptic operator is formulated. in section 3 the cauchy problem method is given. the main results are described in section 4, where unconditionally stable fourth-order three-level scheme is proposed and investigated. section 4 provides results of computational experiments, they illustrate the theoretical results on the approximation accuracy of fractional power problems. a model two dimensional problem is solved by using different numerical schemes. at the end of the work the main results of our study are summarized.",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "6",
        "Section": "6 Conclusions",
        "Text": "1. we have formulated the problem of finding the high order di {latin small ligature ff}erence schemes for solving the nonstationary cauchy type problem which is equivalent to the fractional power elliptic problem. the high order approximations are used to approximate the time dependence of the solution, while the elliptic operator is approximated by the standard finite element scheme. 2. the suffcient stability conditions are given for the two-level discrete schemes with weight parameters. the second order accuracy is proved for the symmetrical crank-nicolson type scheme. 3. the family of three-level symmetrical discrete schemes is constructed and investigated. it is proved that the second order approximation is valid . it is shown that for a special weight parameter  we get the fourthorder three-level scheme. the value of this optimal parameter depends on the fractional power {greek small letter alpha} of the elliptic operator. the initial condition on the first time level of the main grid is computed by using the symmetrical two-level scheme with a specially selected fine time grid. 5. the theoretical results are illustrated by results of numerical experiments. a two-dimensional problem is solved for the elliptic operator with the discontinuous sink term coe {latin small ligature ffi}cient. acknowledgements this work of second author was supported by the mega-grant of the russian federation government .",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "Abstract",
        "Section": "Abstract",
        "Text": "the paper deals with modelling of acoustic waves which propagate in inviscid fluids interacting with perforated elastic plates. the plate can be replaced by an interface on which transmission conditions are derived by homogenization of a problem describing vibroacoustic fluid-structure interactions in a transmission layer in which the plate is embedded. the reissner-mindlin theory of plates is adopted for periodic perforations designed by arbitrary cylindrical holes with axes orthogonal to the plate midplane. the homogenized model of the vibroacoustic transmission is obtained using the two-scale asymptotic analysis with respect to the layer thickness which is proportional to the plate thickness and to the perforation period. the nonlocal, implicit transmission conditions involve a jump in the acoustic potential and its normal one-side derivatives across the interface which represents the plate with a given thickness. the homogenized model was implemented using the finite element method and validated using direct numerical simulations of the non-homogenized problem. numerical illustrations of the vibroacoustic transmission are presented. keywords: vibro-acoustic transmission, perforated plate, thin layer, two scale homogenization, helmholtz equation, finite element method",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "3",
        "Section": "3 Homogenization of the transmission layer",
        "Text": "in this section, we introduce the convergence result which yields the limit acoustic pressure and the plate displacements and rotations. these are involved in the limit two-scale equations of the vibroacoustic problem imposed in the transmission layer. the asymptotic analysis is based on the unfolding method which was inaugurated in the seminal paper and elaborated further for thin structures ",
        "Groundtruth": ""
    },
    {
        "Section_Num": "6",
        "Section": "6 Coupled numerical simulation",
        "Text": "the purpose of this part is to illustrate, how the homogenized vibroacoustic transmission model derived in this paper can be used for numerical simulations. acoustic waves using the two-scale in this paper. to this aim we consider an analogous problem as the one specified in section 5.1, whereby the mathematical model given by the coupled equations -. the geometry of the waveguide {ohm sign}g is depicted in fig. 14. the boundary conditions at the inlet and outlet parts of the domain boundary, {greek capital letter gamma}in and {greek capital letter gamma}out, are defined as in the validation test reported in section 5.1. on the rest of the boundary {partial differential} represents the rigid wall, the computed macroscopic responses are shown in fig. 14 which depicts the acoustic pressure field and the distributions of the following quantities defined in the interface ",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "7",
        "Section": "7 Conclusion",
        "Text": "in this paper, we derive transmission conditions which serve for coupling acoustic fluid pressure fields on an interface which represents a compliant perforated elastic plate. for this, we consider a fictitious layer which embeds the elastic plate with periodic perforation, such that the perforation period is proportional to the layer and plate thicknesses. to derive the transmission conditions, the layer is decoupled form the outer acoustic field which is respected by introducing neumann fluxes . the layer is then treated by the asymptotic analysis based on the periodic unfolding homogenization method. as the result, the layer reduces to the 2d planar manifold {greek capital letter gamma}0 where the homogenized model presents a coupled system of pdes governing the in-layer variables: the mean pressure field and the plate deflection and rotations. further averaging procedure based on a weighted integration in the transversal direction w.r.t. the layer mid-plane yields additional relationships which enable us to couple the outer acoustic field with the in-layer variables. in this way, the dirichlet-to-neumann operator is constructed which couples traces of the outer acoustic pressure 38 figure 14: acoustic pressure fields and plate deformations induced by the incident wave prescribed at the inlet of the waveguide. with its normal-projected derivatives on both sides of the interface. the numerical examples reported here illustrate the validation tests which have been performed to explore the modelling errors associated with the homogenization and the 3d-to-2d dimension reduction of the layer which is replaced by the interface coupling conditions. we used the circular shape of holes, however, arbitrary shaped cylindrical holes can be considered. the validation tests were based on the comparison of responses computed using the homogenized models with the corresponding responses of reference model, here presented by direct numerical simulations of the nonhomogenized vibroacoustic problem. justifying the scale separation, numerical results obtained using the homogenized vibroacoustic model are quite close to the corresponding results of the dns. this observation underlines the main advantage of the homogenized model: 39 it provides very good approximation of the reference solution, but at a considerably lower computational cost than the dns solution. to illustrate the computational e {latin small ligature ff}ort reduction, in the presented examples, the piecewise linear fe approximation of the homogenized problem has only about 2 * 104 degrees of freedom at the macroscopic level, whereas the microscopic subproblems are solved each with about 1.5 * 104 dofs to get the homogenized coe {latin small ligature ffi}cients. to compute relevant results even for much simpler geometry employed in the validation test, the dns requires more than 2*105 dofs for the approximation of the acoustic field in the layer and about the similar number of dofs for the compliant elastic structure. among the topics of the future research, the homogenization-based modelling of the compliant plate with arbitrarily shaped periodic perforations presents one of the most interesting issues since such structures provide significantly bigger potential to modify the vibroacoustic transmission. first steps towards optimal design of perforated plates in the acoustic transmission problems were reported in . acknowledgment. this research was supported by project gacr 17-01618s of the scientific foundation of the czech republic and due to the european regional development fund-project application of modern technologies in medicine and industry , and in part by project lo 1506 of the czech ministry of education, youth and sports.",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "A",
        "Section": "A Appendix",
        "Text": "",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "Abstract",
        "Section": "Abstract",
        "Text": "we show that a new chiral, confining interaction can be used to break pecceiquinn symmetry dynamically and solve the domain wall problem, simultaneously. the resulting theory is an invisible qcd axion model without domain walls. no dangerous heavy relics appear. ",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "7",
        "Section": "7 Conclusions",
        "Text": "10 1 introduction the appearance of topological defects during spontaneous breaking of symmetries constitutes a clear and profound connection between particle physics and cosmology . as the universe cools down several phase transition take place and, depending on the homotopy groups of the manifold of degenerate vacua, stable topological defects may form . in particular, the cosmic domain wall problem is a well-known potential issue of axion models . recently, it has been pointed out that majoron models can also su {latin small ligature ff}er from domain walls . to solve such a long-standing problem, several mechanisms have been proposed. being a cosmological-particle physics issue, it is not surprising that one can tackle it from both, cosmology and particle physics sides. a couple of well known solutions are: cosmic inflation and the lazarides-shafimechanism . in the first one the dangerous walls are pushed beyond the horizon, being a clear example of a cosmological solution. in the second case, one associates the spontaneously broken discrete symmetry to a gauge symmetry. this removes the physical degeneracy among the di {latin small ligature ff}erent vacua, which become gauge equivalent. another interesting solution that has been recently suggested implements the witten e {latin small ligature ff}ect to solve the domain wall problem. more exotic postinflationary solutions involve primordial black holes to perforate the walls, change their topology and destroy them . - 1 as noted by holdom , the cosmic domain wall problem seems to be associated to the breaking of symmetries by scalars. one can imagine that the degeneracy of the associated vacua disappears for theories where the breaking of peccei-quinn symmetry is dynamically triggered by new confining forces. in addition, we show how the associated instantons implement the instanton interference effect , solving the domain wall problem. a similar construction has been explored by barr and kim . in this reference it was suggested that new confining interactions can solve the domain wall problem. however, despite it avoids the domain wall problem with a ndw = 1 scenario, it does illustrate the appearance of a phenomenological and cosmological problem, namely the overclosing of the universe by heavy stable relics. this issue seems almost unavoidable in the context of confining interactions, since they usually bring associated conserved quantum numbers. baryon number in the standard model is the most clear example. if the lightest of these unconfined bound states, which we will call hyperbaryons, is stable it might overclose the universe depending on its mass. this is a reasonable assumption, since it seems rather artificial to protect an anomalous symmetry from anomalies of another gauge group. the first question that arises is which kind of groups are appropriate for g. many possibilities emerge. another example of exotic matter stabilized by the conserved z2 symmetry of an so confining interaction are hyperbaryons in the context of comprehensive unification . since the product of representations decomposes into representations with the same class , only products of an even number of spinors can give us so singlets. therefore, since so supports a conserved z2 quantum number instead of u, no stable so singlet can appear. all the possible hyperbaryons, i.e. so singlet bound states, are z2 singlets and decay. we believe that this example does illustrate the principles of model building. 6 discussion before closing we comment on di {latin small ligature ff}erent aspects of the model that deserve mention: {bullet} in an hypothetical sohc * upq * so theory, the unbroken group by the instanton interference mechanism is a z2 symmetry that can be automatically associated to the center of both so groups. then, the lazarides-shafimechanism can be naturally implemented without adding extra fermions. {bullet} it is attractive to imagine that some sort of interaction with the condensate {mathematical left angle bracket} {greek small letter psi} {greek small letter psi} {mathematical right angle bracket} can generate small neutrino masses. a plausible possibility is to use {greek small letter psi} in a radiative mechanism, together with the appropriate scalars, in close analogy to the mechanism presented in . 7 conclusions new chiral confining interactions with fermions in the spinor representation can simultaneously make the axion invisible and solve the domain wall problem. this work is supported by the spanish grants fpa2017-85216p , sev-2014-0398 and prometeo/2018/165 .",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "Abstract",
        "Section": "Abstract",
        "Text": "one of the most important tasks in network management is identifying different types of traffic flows. as a result, a type of management service, called network traffic classifier , has been introduced. one type of ntcs that has gained huge attention in recent years applies deep learning on packets in order to classify flows. internet is an imbalanced environment i.e, some classes of applications are a lot more populated than others e.g, http. additionally, one of the challenges in deep learning methods is that they do not perform well in imbalanced environments in terms of evaluation metrics such as precision, recall, and f1 measure. in order to solve this problem, we recommend the use of augmentation methods to balance the dataset. in this paper, we propose a novel data augmentation approach based on the use of long short term memory networks for generating traffic flow patterns and kernel density estimation for replicating the numerical features of each class. first, we use the lstm network in order to learn and generate the sequence of packets in a flow for classes with less population. then, we complete the features of the sequence with generating random values based on the distribution of a certain feature, which will be estimated using kde. finally, we compare the training of a convolutional recurrent neural network in large-scale imbalanced, sampled, and augmented datasets. the contribution of our augmentation scheme is then evaluated on all of the datasets through measurements of precision, recall, and f1 measure for every class of application. the results demonstrate that our scheme is well suited for network traffic flow datasets and improves the performance of deep learning algorithms when it comes to above-mentioned metrics. index terms augmentation, deep learning, imbalanced data, kernel density estimation, large scale data, long short term memory networks, network management, traffic classification.",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "I",
        "Section": "I Introduction",
        "Text": "with the ever-increasing amount of traffic that goes through the network, network management has become a difficult task. one of the most important tasks in network management is identifying the types of traffic that are passing through the network. classifying the applications is a fairly simple task with high evaluation metrics. additionally, ntcs have been able to take care of this matter efficiently. two major purposes of ntcs are detecting anomalies in the network and classification of applications for quality of service purposes , . there have been several types of ntcs that use different methods for handling the task at hand, however, each one has its own drawbacks. these methods are generally divided into three categories as follows {bullet} port-based: this approach is not efficient since some applications do not use a specific port e.g, bittorrent. moreover, if the port is changed, this method is no longer reliable . {bullet} deep packet inspection : these applications use the patterns in the payloads of packets for classification. they generally have three major drawbacks. the first one is that they need to be updated with new patterns in the payloads of emerging applications . in addition, they are not able to identify all of the flows. furthermore, if we do not have access to the payload of packets for privacy reasons, their accuracy is very much affected. {bullet} machine learning based: the flaws of the two above methods has gained attention to the third type of classifiers, which use machine learning and specifically deep learning algorithms. this type of algorithms usually work with the features in the header of the packets, but some of them may also take into account the information in the payloads , . although they are still limited, they have shown great potential in terms of evaluation metrics and will be a great substitution in the future for the aforementioned methods. most of the traces that are gathered from real internet traffic are imbalanced i.e, some types of application flows are generally more populated than others e.g, http -. this matter is a lot bolder when it comes to large-scale traffic and will cause some serious problems in the way of algorithms' f1 measure. augmentation is an approach in machine learning that addresses the issue of small amount of data for training. this approach usually tries to increase the training data in a way that can be still classified in the same category. augmentation is a popular method used especially in image classification and can be done through methods like cropping, zooming, rotating, and filliping vertically or horizontally. another way of achieving augmentation is through generating artificial data for a class. in order to address the challenges of machine learning arxiv:1901.00204v1 1 jan 2019 algorithms in imbalanced network datasets, we introduce a novel augmentation method to improve the accuracy of deep learning algorithms on real-world traffic traces by using kde and lstm. the remainder of this paper is organized as follows: in section ii we review the related works in the area of ntcs. in section iii we describe our augmentation scheme. the dataset and deep learning model that was used in order to classify the traffic traces are mentioned in sections iv and v, respectively. finally, the evaluation of our method is demonstrated in section vi.",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "II",
        "Section": "II related works",
        "Text": "due to the high variety in the classes, datasets, and performance metrics that are used in this area, having a comparison between the works in this subject is a difficult task . considering this, there are several well-known pieces of research that are done up to this point. there are several works that have applied deep learning architectures or neural networks in order to solve the classification problem. in lopez-martin et al. have presented a deep convolutional recurrent neural network architecture in order to classify network flows and have found the best setting in that environment in terms of hyper-parameters and feature set. nevertheless, they have not taken any measures to handle the imbalance problem of their dataset. additionally, the scale of their dataset is approximately fifth of the one that we are using. in rahul et al. have also proposed using convolutional neural networks in order to classify network traffic but only consider three classes of applications in their work on a limited amount of data. in a comparison between cnn and stacked autoencoders in order to classify not only types of traffic but also applications in the network in a standard vpn/none-vpn dataset in packet level has been given. the scheme that was used, unlike ours, relies on the features from both header and payload of packets which may not be available in some privacy-preserving datasets. finally, in auld et al. have deploy a bayesian neural network in the form of a multi-layer perception and accordingly classify their dataset. in this work, the lowest performance metrics are from the classes with the lowest number of data. some works in this area have attempted to battle the imbalanced property through different measures. in rotsos et al. have introduced a method through probabilistic graphical models for semi-supervised learning in a naive bayes model. for their learning, they have assumed a dirichlet distribution prior for the classes with high {greek small letter alpha} value. this is based on the assumption that some classes have a higher probability than others. in addition, in , an augmentation method has been proposed by using an auxiliary classifier generative adversarial network , although only two classes of network is considered: ssh and none-ssh. furthermore, their method is table i: features of each flow feature type source port numerical destination port inter-arrival time payload length direction of packet sequential tcp window size only evaluated on traditional machine learning algorithms like support vector machines, random forest, and naive bayes. also has presented a new feature extraction method using a divide and conquer approach for an imbalanced dataset in the network. as an instance of lstm used for generating sequential data, has introduced a method to generate data using lstm and evaluated the method to show that it can capture the temporal features in the dataset. lstm has also been used as an augmentation tool in works such as and for generating handwriting and human movement data, respectively, and has proven to be efficient in both cases. iii. augmentation scheme for generating time series network data in this section, we describe our contribution of augmentation scheme for generating new data in network traffic traces. every flow in the network has the same 5-tuple attributes: {bullet} source ip address. {bullet} destination ip address. {bullet} source port number. {bullet} destination port number. link layer protocol e.g, tcp and udp. every application in the internet creates a flow of packets between communicating peers . in order to represent flows in our work, we have to choose a set of features for each one that can capture the nature of a flow. according to , the appropriate set of features that will give acceptable results for classifying flows are mentioned in table i. these features are gathered for the first 20 packets of each flow, which are more than enough for capturing the temporal and spatial features of a flow. as shown in table i, we can put the features in two categories: sequential and numerical. each group has its own way of augmentation which are described in the following. a. generating sequential features in this section, we demonstrate our approach to generating sequential features. as mentioned earlier, traffic flow comprises the sequence of packets that are transmitted between a source and a destination. some applications are uni-directional i.e, the packets are only transmitted in one direction e.g, uploading a data. however, in some type of applications, packets go in both directions such as when a client is communicating with the server and gets a response for its request. whether a packet is sent from source or destination depends on the sequence of packets that have already been sent up to this point in the flow. therefore, we can conclude that the sequence of directions of packets in a specific application is of time-series nature and can be generated through means of sequence generation like in . tcp window size is another feature of the flow that is dependent on the previous values in the flow. generally, this value is an indicator of the conditions of the connection and processing speed of data in the flow . thus, its amount at each step of the flow is affected by previous steps' values. one of the most common ways to generate a sequence is using recurrent neural networks , which try to learn the patterns in time-series data e.g, speech, music, text, etc . in our work, we use one type of rnns called lstm networks . each lstm block tries to learn the probability distribution in a step of a sequence whilst taking into consideration the information from previous steps. in order to train the network, we gathered the patterns of packet directions in a flow for up to 20 packets in a class of flow application. we encode every direction by 1 or 0 with the former being from source to destination and the latter is the other way around. at the end of each sequence, we put a unique character as an indicator of the ending of the flow. then every sequence is shifted by one character to the right and is used as labels in order to train each step of generation in lstm. in the generation phase, first, we choose a direction based on the distribution of that direction in the dataset for the first time step and give that as input to the lstm. afterwards, we use the output of each step as probability distribution of each character and generate a new direction. then, we feed that output direction to lstm in order to generate next step probabilities. the maximum number of steps are 19 in order to generate the pattern of flow up to 20 packets . let xt and ht denote the direction of the packet in the dataset and the generated direction by the lstm at time step t, respectively. therefore, the generation process is demonstrated in fig. 1. in order to generate window size values, we use the same scheme, although the characters in this case are the values of window sizes in our dataset instead of 0 and 1. b. generating numerical features in this section, we describe our method of generating numerical features of a flow. as shown in table i, we consider four numerical features for each packet of the flow. in order to generate new samples from these features, first, we need to learn their probability distribution. since these features are not sequential, we can use conventional probability density distribution estimation methods. one of these methods is kde that is in the category of kernel methods. kde, also known as the parzen-rosenblatt window, is one of the most famous methods used to estimate the probability density function of a dataset. kde, as a non-parametric density fig. 1. generating 19 time steps of packet sequence with lstm estimator, does not have any assumptions about the density function as opposed to the parametric family of algorithms.",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "IV",
        "Section": "IV dataset",
        "Text": "in this section, we describe our dataset and its labeling method. for this paper, we used real traces of traffic from the campus of amirkabir university of technology that includes more than 70 gigabytes of packets from udp and tcp link layer protocols. next, we label flows using ndpi, which is an open source dpi tool released by ntop for classifying the flows based on applications . the reason for our choice table ii: classes of applications class number of flows http 58774 dns 126960 ntp 4633 bittorrent 6146 http download 16326 ssl no cert 10603 steam 4460 rdp 1425 ssl 341846 ssh 9746 facebook 2772 twitter 2198 google 96072 windowsupdate 2343 telegram 186256 instagram 6683 microsoft 18196 playstore 5304 youtube 3747 fig. 2. the percentage of different classes of applications in our dataset. of labeling tool is that according to , ndpi is the most accurate open-source dpi tool among available dpi tools. nineteen classes of traffic from more than 50 gigabytes of packets were chosen which include 904490 flows. 85 percent of these flows were chosen for training and the rest are used for test dataset. the classes of applications are the ones with the most number of instances in the dataset and can be seen in table ii. as shown in table ii, there are different classes of applications in our dataset and the names of our labels are chosen based on the labels given by ndpi. the percentage of each class is shown in fig. 2. as demonstrated by the bar chart, the imbalance feature of the dataset is clear. the most populated class of appliaction algorithm 1 augmentation process input set c of classes with low population output set {modifier letter circumflex accent} c of generated flows 1: for each c belong to c do 2: pd {leftwards arrow}pattern of directions in c 3: ptcp {leftwards arrow}pattern of tcp windows in c 4: train lstms for each pattern in p and ptcp 5: {modifier letter circumflex accent} pd {leftwards arrow}generated direction patterns from lstm 6: {modifier letter circumflex accent} ptcp {leftwards arrow}generated tcp window patterns from lstm 7: {modifier letter circumflex accent} p {leftwards arrow} {modifier letter circumflex accent} pd {union} {modifier letter circumflex accent} ptcp 8: nf {leftwards arrow}sets of numerical features in c 9: for each set nf belong to nf do 10: pdf {almost equal to}kde 11: rs {leftwards arrow}generated random samples from pdf 12: end for 13: gen flows {leftwards arrow}new data from rs and {modifier letter circumflex accent} p based on 14: {modifier letter circumflex accent} c {leftwards arrow} {modifier letter circumflex accent} c {union}gen flows 15: end for is ssl with more than 37 percent of the population and the least populated class is rdp with less than 0.16 percent. furthermore, more than 83 percent of the whole dataset consists of only 4 classes. additionally, 10 classes have less than 1 percent, which are the less populated classes and therefore, some of them are expected to be susceptible to low evaluation metrics.",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "VI",
        "Section": "VI evaluation",
        "Text": "in this section, we present the evaluation results of the model on three different datasets. in order to fully discover the advantages of our method, three sets of datasets are prepared: {bullet} actual data: the exact dataset from section iv. sampled data: dataset of section iv over-sampled using . augmented data: dataset of section iv augmented using our method. the method of sampling in is a simple yet effective approach to handle the problem of imbalanced classification and is widely used in many works such as in . classes ntp, facebook, twitter, windowsupdate, instagram, playstore, and youtube are chosen for augmentation and over-sampling because the crnn network gets the worst results in these classes. furthermore, these are the classes that have low number of samples in the dataset. the evaluation metrics that are chosen to measure the performance of our approach are those that are mostly used for imbalanced datasets and give an appropriate analysis of the methods that are employed. these metrics are precision, recall, accuracy, and f1 measure, whose formulas are given in the following. the f1 measure shows the overall performance of algorithm on both precision and recall. in fig. 3 the precision metric for all three datasets is given. although in some classes with less instances that have been augmented like playstore and instagram, there has been a slight decrease in precision, others have mostly had an fig. 3. precision measure comparison of per class in the dataset fig. 4. recall measure comparison of per class in the dataset improvement in this matter. in some cases, the sampled dataset performed better than our method such as bittorrent and google, but due to the lack of generalization, we can see that in a class like playstore, which is sampled in large scales, this method has a huge decrease in the results. furthermore, the number of classes that are improved by our augmentation is more than those that performed better in sampled dataset. fig. 4 depicts the recall of each class in three separate datasets. in every augmented class, there is a clear upgrade in recall measure. this is due the fact that the number of fn predictions are less for these classes compared to the normal dataset. this might have some negative effect on the fig. 5. f1 measure comparison of per class in the dataset fig. 6. comparison of total precision, recall, and f1 measure resulted from both methods over-populated class of dns, but for others this metric is improved. due to higher generality in our augmentation, it is obvious that the amount of increase in recall in our approach is higher than sampling in augmented classes in every instance. moreover, sampling has caused a decrease in recall in 12 classes compared to actual dataset. fig. 5 illustrates the f1 measure in all the classes of the dataset. this figure verifies the fact that overall performance of our method is better than sampling in each and every one of the classes. fig. 6 shows the overall measures on the whole datasets. as fig. 7. confusion matrix resulted from the actual dataset fig. 8. confusion matrix resulted from the augmented dataset shown in this figure, although sampling improved the recall, it has also a slight decrease in precision due to the lack of generalization. however, the overall performance as shown by the increase, albeit a small one, on the f1 is better than the actual dataset. on the other hand in our method, in all three metrics, there is a noticeable improvement which is more than any that is caused by sampling method. fig. 7 and fig. 8 illustrate the confusion matrices of actual and augmented datasets, respectively. as shown in fig. 7 classes of http, dns, and ssl, which have high number of instances in the dataset, have noticeable negative effect on majority of classes' prediction. fig. 8 shows that our method is able to improve this matter and lessen the number of false predictions. additionally, the number of true positives in http and ssl is increased. although dns predictions have less true positives, the number of false negatives is diminished. moreover, the overall accuracy in our method is increased by 6.56 percent.",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "VII",
        "Section": "VII conclusion",
        "Text": "in this paper, we proposed an augmentation method for imbalanced network traffic classification on real traffic traces based on lstm and kde. in order to compare the performance of our scheme, we considered two sampled and augmented datasets. the results that are obtained from crnn show that our approach gets better results in overall measures of precision, recall, and f1.",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "Abstract",
        "Section": "Abstract",
        "Text": "1 nasal patches and curves for expression-robust 3d face recognition mehryar emambakhsh and adrian evans abstract the potential of the nasal region for expression robust 3d face recognition is thoroughly investigated by a novel five-step algorithm. first, the nose tip location is coarsely detected and the face is segmented, aligned and the nasal region cropped. then, a very accurate and consistent nasal landmarking algorithm detects seven keypoints on the nasal region. in the third step, a feature extraction algorithm based on the surface normals of gabor-wavelet filtered depth maps is utilised and, then, a set of spherical patches and curves are localised over the nasal region to provide the feature descriptors. the last step applies a genetic algorithm-based feature selector to detect the most stable patches and curves over different facial expressions. the algorithm provides the highest reported nasal region-based recognition ranks on the frgc, bosphorus and bu-3dfe datasets. the results are comparable with, and in many cases better than, many state-of-the-art 3d face recognition algorithms, which use the whole facial domain. the proposed method does not rely on sophisticated alignment or denoising steps, is very robust when only one sample per subject is used in the gallery, and does not require a training step for the landmarking algorithm. 2",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "I",
        "Section": "I Introduction",
        "Text": "while much previous research on expression invariant 3d face recognition has focused on modelling expressions and detecting expression insensitive facial parts, there have been relatively few studies evaluating the potential of the nasal region for addressing this issue. despite this, the nose has a number of salient features that make it suitable for expression robust recognition. it can be easily detected, due to its discriminant curvature and convexity , is difficult to hide without attracting suspicion , , is relatively stable over various facial expressions and is rarely affected by unintentional occlusions caused by hair and scarves. although it has been reported that the 2d image of the nose has too few discriminant features to be used as a reliable region for human identification , its 3d surface has much undiscovered potential. this paper further investigates the 3d nasal region for human identity authentication and verification purposes and presents a novel algorithm that provides very high discriminant strength, comparable with recent 3d face recognition algorithms, which use the whole facial domain. the proposed approach is based on a very consistent and accurate landmarking algorithm, which overcomes the issue of robust segmentation of the nasal region. the algorithm first finds an approximate location of the nose tip and then finely tunes its location, while accurately determining the position of the nasal root and detecting the symmetry plane of the face. next, the locations of three sets of landmarks are found: subnasale, eye corners and nasal alar groove. these landmarks are utilised on feature maps created by applying multi-resolution gabor wavelets to the surface normals of the depth map. two types of feature descriptors are used: spherical patches and nasal curves. feature selection is then performed using a heuristic genetic algorithm and, finally, the expression-robust feature descriptors are applied to the well-known and widely used 3d face recognition grand challenge , bosphorus and binghamton university 3d facial expression datasets. results show the algorithm's high potential to recognise nasal regions, and hence faces, over different expressions, with very few gallery samples per subject. the highest rank-one recognition rates achieved are: 1) a r1rr of 97.9% and equal error rate of 2.4% for frgc v2.0 and receiver operator characteristic iii experiments, respectively; 2) a r1rr of 98.45% and 98.5% for frgc's neutral vs. neutral and neutral vs. non-neutral samples, respectively; 3) a r1rr of 96.2% when one gallery sample per subject is used for the frgc dataset vs. 4330 probe samples); 4) a r1rr of 95.35% for the bosphorus dataset when 2797 scans of 105 subjects are used as probes and the set of 105 neutral scans is used as the galley. january 3, 2019 draft 3 the remainder of the paper is organized as follows. after the literature review provided in section ii, the alignment and nasal region cropping steps, followed by the nasal region landmarking, are detailed in section iii. the feature extraction algorithm is described in section iv and section v explains the feature descriptors used. the feature selection algorithm is detailed in section vi and experimental results, including a thorough comparison with previous work, is provided in section vii. finally, conclusions are given in section viii. a. scientific contribution and comparison with previous work the major contribution of this paper is a novel surface normal-based recognition algorithm that provides a thorough evaluation of the recognition potential of the 3d nasal region. the results achieved are not only better than previous 3d nose recognition algorithms but also higher than many recognition algorithms that employ the whole face. the algorithm employs a novel, training-free, highly consistent and accurate landmarking algorithm for the nasal region and a robust feature space, based on the response of gabor wavelets to surface normal vectors, is also introduced. to localise the expression robust regions on the nose a heuristic ga feature selection is applied to two different geometrical feature descriptors. because of the smoothing effects of the gabor wavelets, there is no need for sophisticated denoising algorithms. indeed, only simple median filtering is required for the surface normals, even with noisy datasets such as the frgc spring 2003 folder. an additional advantage of the proposed approach is that a fast principal component analysis -based self-dependent method can be employed for facial pose correction. this eliminates the need for sophisticated pose correction algorithms or reference faces for fine tuning the alignment. the proposed approach significantly extends our previous work in which the nasal landmarking and recognition was performed on the depth map. this paper increases the number of landmarks and their detection accuracy and presents new feature extraction and selection algorithms. the work is inspired by recent algorithms on utilising facial normal vectors in 3d and regional normal vectors . to compare the new algorithms with previous approaches which used similar methodologies, the application of normals, computed over the nasal surface, is used for identification as well as the verification scenario. by using multi-resolution gabor wavelets the ability of the algorithm to handle more noisy samples is enhanced, providing higher r1rr than the approach of li et al. , which excluded the noisy frgc spring 2003 samples. this work also extends the application of facial curves, introduced as feature descriptors by berretti et al. , to nasal spherical patches, producing a r1rr increase of > 2%, and showing a higher class separability for the spherical patches than for curves for 3d face january 3, 2019 draft 4 recognition.",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "II",
        "Section": "II Recent literature review ",
        "Text": "robustness against the deformations caused by facial expressions has been a popular research topic in 3d face recognition. the face is a non-rigid object and therefore 3d matching techniques for rigid objects, such as the iterative closest point algorithm , can become trapped in local minima and fail to provide accurate matching scores. an empirical approach to deal with the variations caused by expressions is to capture a range of facial expressions for each subject and store them in the gallery . then, the facial biometric features of each test subject can be compared with all the stored expressions and a decision made on the identity of the subject. this method has numerous disadvantages: capturing a range of facial expressions for each subject is not always straightforward and requires a high storage capacity per subject. in addition, facial expressions will not necessarily remain constant and may differ between the test and gallery captures . one approach to overcome this problem is to use computer graphics algorithms to artificially create different expressions for each facial capture. in , expressions are learned using pca eigenvectors and then used to re-generate the expressions on the probe samples. although this approach does not require multiple samples per subject in the gallery, it is still vulnerable to the number of training samples used to model the facial expressions. also, a universal definition of facial expression for all subjects still remains to be found and the need to classify the expression types prior to face recognition increases the computational complexity. another approach is to employ region-based methods, in which the least variant parts of the face over different expressions are detected using facial segmentation , or extracted using their expression invariant capabilities , . spreeuwers proposes a multiple regional approach based on a pca-linear discriminant analysis feature extraction method . in regional recognition, scores from a combination of different masks on the nose, cheek, forehead, chin and mouth are fused to finalise the decision making. use a regional registration algorithm in conjunction with lda classifiers, giving an expression robust 3d face recognition approach . they also demonstrate that the nasal region has a high discriminatory power. a focus on integrating multiple regions is provided by queirolo et al. in which four regions are segmented and stored for the gallery sessions before matching is performed using a novel matching criterion, called the january 3, 2019 draft 5 surface interpenetration measure, and simulated annealing. using facial curves is another popular approach to 3d face recognition that can be categorised as a subset of regional algorithms. drira et al. use the intersections of planes with the facial surface to define a set of radial curves which pass through the nose tip, and then perform a quality assessment in order to handle missing data and occlusions . another curve-based algorithm is proposed by berretti et al. first, keypoints are detected on the facial surface and then the least variant curves on the face are selected using a statistical model and matched with those in the gallery. as an extension to curves, isogeodesic stripes centralised on the nose tip are used in an expression invariant 3d face recognition method that employs a novel descriptor, termed the 3d weighted walkthroughs, to quantify the differences between corresponding stripes . in another curve-based approach, drira et al. find geodesic curves on the nasal region for a subset of the frgc dataset . to overcome the sensitivity of holistic face recognition algorithms to expression variations, mian et al. propose a landmark-based method, in conjunction with a localised feature descriptor that incorporates the 2d texture and 3d point clouds. in an alternative approach, wang et al. apply shape difference boosting to the bosphorus dataset to learn the expressions and identify those facial regions which remain constant over different expressions . instead of using depth or the point coordinates for 3d registration, mohammadzade et al. use the surface normals of the points in conjunction with a fisher's discriminant paradigm . this approach selects the normals which maximise the concentration of within-class scatter while simultaneously maximising the between-class distribution. recently, li et al. proposed local normals histograms, captured from multiple rectangular regions on the face, to set up an expression-robust feature space and use a novel sparse classifier to perform the matching . despite the robustness of these algorithms against facial expressions, they often rely on accurate and consistent facial segmentation, which is not a straightforward task in 3d. to address this issue, some researches have focused on the nasal region, which shows high consistency over different expressions. for example, in one of the first investigations on 3d nose recognition, chang et al. initially segment the face into different non-overlapping regions, using the curvature information . then, three overlapping nasal regions are detected and stored in the gallery. the same regions are segmented in the probe images and matched using the icp algorithm. wang et al. propose the use of local shape difference boosting for 3d face recognition and also apply the boosting algorithm to different nasal regions . the regions are cropped using the intersection of spheres of radius r, centred on the nose tip, with the face surface. when the value of r was increased, the recognition ranks reached a maximum and then plateaued. a combination of the nasal region, forehead and eyes are used for a 2d/3d face recognition by mian et january 3, 2019 draft 6 initial nasal root estimation fine tuning nasal root and tip update the nose region's point clouds eye corners detection alar groove detection subnasale detection initial nasal root initial nose tip finalised nasal root and tip initial nose point clouds new nose point clouds fig. 1: the landmarking algorithm steps in a block diagram; the naming convention for the nasal landmarks in our work. al. . a modified icp algorithm is used for matching, in conjunction with a pattern rejector based on spherical face representation and shift-invariant feature transform , producing high recognition ranks on the frgc dataset, in particular for the neutral probes. dibeklio used the dijkstra algorithm to segment the nose and evaluated the performance using a subset of the bosphorus dataset .",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "III",
        "Section": "III Preprocessing and nasal region landmarking",
        "Text": "the algorithm explained in is used to crop the face. next, median filtering with a 2.5 * 2.5 mm2 mask size is applied twice on the cropped face. the image is then resampled to a uniform grid with 0.5 mm/pixel horizontal and vertical resolutions using delaunay triangulation and aligned using the iterative pca algorithm . the aligned face is then intersected with three cylinders to crop the nasal region, according to . the depth map of the cropped nasal region is again median filtered with a january 3, 2019 draft 7 2.5 * 2.5 mm2 mask to further smooth its surface and decrease the spike noise effects. the block diagram in fig. 1-a shows how the landmarks in fig. 1-b are detected. a. local minima detector, nose tip re-localisation, nasal root and subnasale detection first, an initial position of the nasal root is detected by . then, the location of the nose tip , found in section iii, is more finely tuned. vthis process results in several curves on the nasal region, shown in fig. 3-a. the proposed landmarking algorithm relies on a minima detector, which finds a set of minima on rotated versions of the curves and then maps them to the original curve. the rotation is required because some of the original curves are strictly decreasing functions that do not have an actual minimum.",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "IV",
        "Section": "IV Feature extraction",
        "Text": "the proposed feature space is based on surface normals. for an aligned depth map of the nasal region, represented by its point clouds . in order to reduce the sensitivity of the normal vectors to noise and enable the extraction january 3, 2019 draft 13 of multi-resolution directional region-based information from the nasal region, instead of calculating the normal vectors directly from the nose surface, they are derived from the gabor wavelet filtered depth map. the algorithm proposed by manjunath et al. is used to minimise the wavelets overlap and redundancy in the filtered images . the discrete fourier transform of the resampled gabor wavelet gs,o for the sth scale and oth orientation level is computed and its zero frequency component is set to zero. the hadamard product of the resulting gf s,o and the fourier transform of nz is then calculated and the absolute value of its inverse fourier transform is computed for each scale and orientation. ",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "V",
        "Section": "V Localised feature descriptors using spherical patches and curves",
        "Text": "the feature descriptors are used to define a part of the nasal region, containing a set of normal vectors from the gabor wavelets filters. histograms of the resulting feature vectors for the x, y and z maps are concatenated to create the feature space. this procedure is illustrated in fig. 6 for sm = 3 and om = 4. the feature descriptors are used to reduce the dimensionality of the feature space, decrease the redundancy and enable the use of probabilistic feature selection to lower the sensitivity to facial expressions while maintaining the most discriminative parts. the basic landmarks previously identified, see fig. 1-b, are used to create the new keypoints shown in fig. 7-a. these new landmarks are easily obtained by dividing the horizontal and vertical lines that connect the landmarks. a sphere centralised on each point is then intersected with the nasal surface and its inner parts are cropped. then, the histogram of the normals of gabor-wavelet filtered depth images are computed, based on the procedure explained in section iv. the intersection process is depicted in fig. 7-b. a set of spheres of identical radii are intersected with the nose surface. these spherical feature descriptors provide the capability to evaluate the potential of overlapping spherical regions on the nasal surface, when used as feature vectors. january 3, 2019 draft 14 fig. 7: grid of landmarks used for the spherical patches in . the nasal curves are found using the combination of new landmarks, illustrated in . alternatively, using different pairs of landmarks, a set of orthogonal planes to the nasal region can be found. intersecting the planes with the nose surface results in a set of curves on the nasal region. for example, the normal vector of a plane passing through two nasal landmarks a1 and a2, and orthogonal to the xy plane can be defined by {modifier letter circumflex accent} pa1a2 = {modifier letter circumflex accent} az * {square root} , where {modifier letter circumflex accent} az = is the unit vector along the z-axis. when a1 and a2 are selected from the set of landmarks shown in fig. 7-c, they can be used to create the set of curves shown in fig. 7-d, which provide the feature descriptors. for each curve, the concatenated histograms of the x, y and z components of the normal vectors from the gabor wavelet filters outputs are computed, giving the feature vector.",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "VIII",
        "Section": "VIII Conclusions",
        "Text": "to address the problem of expression invariant face recognition, a novel algorithm is introduced, that utilises the 3d shape of nose. the algorithm is based on a highly consistent and accurate landmarking algorithm, a robust feature space, discriminative feature descriptors and feature selectors. the proposed method is applied over three well-known face datasets, frgc, bu-3dfe and bosphorus. the matching results show that the algorithm is very successful for both the identification and verification scenarios, producing a r1rr of 97.9% on frgc v2.0, an eer of 2.4% on roc iii, and r1rr of 98.45% and 98.5% for neutral and non-neutral probes, respectively. the proposed method does not rely on sophisticated preprocessing algorithms for its denoising and alignment. in addition, when there is only one sample per subject in the gallery, for all the merged folders of the frgc dataset, a r1rr of 96.2% is obtained. for the bosphorus dataset a r1rr of 95.35% is obtained when one neutral sample per subject is used for gallery and the remaining samples with various expression types as probes. the results of the proposed method reveal the high potential of the nasal region for 3d face recognition. the recognition ranks are not only significantly higher than previous nasal region-based algorithms, but also have a better performance than many 3d holistic and multi-modal approaches. there are several aspects of the algorithm which can be utilised in other applications. for example, january 3, 2019 draft 26 the feature extraction step, which is based on histograms of gabor wavelet normals, can be applied to other 3d object recognition methods. also, the feature selection paradigm described here can be easily applied to other pattern recognition algorithms, to maximise the within-class and between-class similarity and dissimilarity, respectively, enabling the extraction of a lower dimensional and less redundant feature space. the application of the proposed landmarking algorithm can be investigated for performing facial alignment, low dimensional face recognition and pattern rejection. finally, the application of the feature extraction step on the whole facial region, to make it robust against occlusions is an interesting area of future research.",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "Abstract",
        "Section": "Abstract",
        "Text": "normal forms for dirac-jacobi bundles and splitting theorems for jacobi structures jonas schnitzer* dipartimento di matematica universit {latin small letter a with grave} degli studi di salerno via giovanni paolo ii, 132 84084 fisciano italy abstract the aim of this paper is to prove a normal form theorem for dirac-jacobi bundles using the recent techniques from . as the most important consequence, we can prove the splitting theorems of jacobi pairs which was proposed by dazord, lichnerowicz and marle in . as an application we provide a alternative proof of the splitting theorem of homogeneous poisson structures. contents",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "Abstract",
        "Section": "Abstract",
        "Text": "the surface diffusion and the willmore flow for uniformly regular hypersurfaces jeremy lecrone, yuanzhen shao, and gieri simonett abstract. we consider the surface di {latin small ligature ff}usion and willmore flows acting on a general class of hypersurfaces parameterized over a uniformly regular reference manifold possessing a tubular neighborhood with uniform radius. the surface diffusion and willmore flows each give rise to a fourth-order quasilinear parabolic equation with nonlinear terms satisfying a specific singular structure. we establish well-posedness of both flows for initial surfaces for the willmore flow, we also show long-term existence for initial surfaces and we prove that these solutions become spherical as time goes to infinity.",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "1",
        "Section": "1. Introduction",
        "Text": "the surface diffusion and willmore flows are geometric evolution equations that describe the motion of hypersurfaces in euclidean space . the normal velocity of evolving surfaces is determined by purely geometric quantities. for both flows, the mean curvature is involved in the evolution equations, while the willmore flow additionally depends upon gauss curvature. these flows have been studied by several authors for compact hypersurfaces. in this setting, existence, regularity, and qualitative behavior of solutions have been analyzed in for the surface diffusion flow, and in for the willmore flow, to mention just a few publications. in this paper, we consider uniformly regular hypersurfaces. it should be emphasized that these surfaces may be non-compact. the concept of uniformly regular riemannian manifolds was introduced by amann and it contains the class of compact riemannian manifolds as a special case. the study of geometric flows on non-compact manifolds is an active research topic, both from the point of view of pde theory and in relation to its applications in geometry and topology. to the best of our knowledge, the current literature on the surface diffusion and willmore flows for non-compact manifolds all concern surfaces defined over an infinite cylinder or entire graphs over rm, or the willmore flow with small initial energy, cf. . our work generalizes the study of these two flows to a larger class of manifolds. 2010 mathematics subject classification. 35k55, 53c44, 54c35, 35b65, 35b35 . key words and phrases. surface diffusion flow, willmore flow, uniformly regular manifolds, geometric evolution equations, continuous maximal regularity, critical spaces, stability of spheres. this work was supported by a grant from the simons foundation . 1in our main result we establish well-posedness for initial surfaces that are parameterized over a uniformly regular hypersurface. moreover, we show that solutions instantaneously regularize and become smooth, and even analytic in case {greek capital letter sigma} is analytic. in order to obtain our results, we show that the pertinent underlying evolution equations can be formulated as parabolic quasilinear equations of fourth order over the reference surface {greek capital letter sigma}. our analysis relies on the theory of continuous maximal regularity and the results and techniques developed in . the results in theorem 4.3 and theorem 5.1 are new. however, an analogous result to theorem 4.3 was obtained in for the surface di {latin small ligature ff}usion flow. for the willmore flow, theorem 5.1 is also new even if {greek capital letter sigma} is a compact surface. the organization of the paper is as follows: in sections 2.1 and 2.2, we introduce the concept of uniformly regular manifolds and define the function spaces used in this paper. in sections 2.3 and 2.4, we review continuous maximal regularity theory and its applications to quasilinear parabolic equations with singular nonlinearity. these results form the theoretic basis for the study of the surface diffusion and willmore flows. in section 3, we introduce the concept of uniformly regular hypersurfaces with a uniform tubular neighborhood -hypersurfaces) and work out several examples. we utilize these concepts to parameterize the evolving hypersurfaces driven by surface di {latin small ligature ff}usion and willmore flows as normal graphs over a reference hypersurface. in section 4, we establish our main results regarding existence, uniqueness, regularity, and semiflow properties for solutions to the surface di {latin small ligature ff}usion flow over hypersurfaces in rm+1. in section 5, we likewise establish well-posedness properties for solutions to the willmore flow over -hypersurfaces in r3. additionally, we show stability of euclidean spheres under perturbations. we conclude the paper with an appendix where we state and prove some additional properties of normal graphs over -hypersurfaces. ",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "2",
        "Section": "2. Preliminaries",
        "Text": "2.1. uniformly regular manifolds. the concept of uniformly regular manifolds was introduced by h. amann in and . loosely speaking, an m-dimensional riemannian manifold is uniformly regular if its di {latin small ligature ff}erentiable structure is induced by an atlas such that all its local patches are of approximately the same size, all derivatives of the transition maps are bounded, and the pull-back metric of g in every local coordinate is comparable to the euclidean metric gm. we will now state some structural properties of uniformly regular manifolds which will be used in the analysis of the the surface diffusion flow and the willmore flow in subsequent sections. ",
        "Groundtruth": ""
    },
    {
        "Section_Num": "Abstract",
        "Section": "Abstract",
        "Text": "the goal of this paper is to implement a system, titled as drone map creator using computer vision techniques. dmc can process visual information from an hd camera in a drone and automatically create a map by stitching together visual information captured by a drone. the proposed approach employs the speeded up robust features method to detect the key points for each image frame; then the corresponding points between the frames are identified by maximizing the determinant of a hessian matrix. finally, two images are stitched together by using the identified points. our results show that despite some limitations from the external environment, we could have successfully stitched images together along video sequences. keywords image stitching, drone, opencv, boofcv, java",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "1",
        "Section": "1 Introduction",
        "Text": "drones are getting popular in a wide range of research in recent years. many companies have developed their own drone technology such as google, facebook, and amazon. in addition, drones are used in drone journalism so they can obtain videos of areas that are hard to reach. besides being used for commercial reasons, drones can be used for military, security, exploration, and surveillance. a quadcopter is one kind of drone that has four rotors. two spin clockwise, and the other two spin counter-clockwise. they work in tandem to balance the drone. additionally, a drone can fly autonomously based on a pre-entered program without piloting. since most of drones carry video cameras with considerably high resolution, the use of them for image processing and computer vision techniques has become a very interesting topic. applications using drones and cameras have been developed to solve many problems and make people's lives easier. one of challenges of such drone applications is that a drone cannot work properly when a gps signal or a pre-captured satellite map is not available. this research will address the limitation by proposing a drone map creator that utilizes computer vision techniques for video streams captured by a drone. 1 in our research project, our environment was the university parking lot, and we used image processing techniques to reach our goal. in this paper, we will show how we get the images by remotely controlling the ar-drone to feed the image stitching algorithm, which will generate a big image by blending these images together.",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "2",
        "Section": "2 Background",
        "Text": "the aim of our research project is to come up with an image processing algorithm that will let us cover the university of bridgeport parking lot, and this can be done by taking successive images and then combining these images into one large image. to have images with overlapped areas so that they can be combined, we needed a stable device with a good quality camera, which will capture the images in a certain order. that's why we chose the ar-drone 2.0. the ar-drone operator can directly set its yaw, pitch, roll, and vertical speed, and the control board adjusts the motor speeds to stabilize the drone at the required pose. the first camera with approximately 75 {degree sign} * 60 {degree sign} field of view is aimed forward and provides a 640 * 480 pixel color image. the second one is mounted on the bottom and provides color image with 176 * 144 pixels and its field of view is approximately 45 {degree sign} * 35 {degree sign}. therefore, the front camera of the drone will provide us with good quality stabilized overlapped images which can be combined into one image using the algorithm that we developed. along with the ar-drone 2.0, we also used existed free software libraries that we found helpful to develop our algorithm. the libraries are opencv and boofcv. opencv is an open source computer vision and machine learning software library. the library has many optimized algorithms, which can be used in many areas such as face recognition, identifying objects, tracking moving objects, and stitching images together to produce a high resolution image of an entire scene. opencv also leans mostly towards real-time vision applications and it has many available interfaces . according to all of these properties, we used the java interface of opencv, which is a primary component of our algorithm. in addition to opencv and all the image processing features it has, boofcv has also demonstrated very high-level image processing capabilities. boofcv is a java library for realtime computer vision and robotics applications. it includes low-level image processing routines, feature tracking, and geometric computer vision. boofcv is organized into several packages: image processing, geometric vision, calibration, recognition, and visualization. the library has also an example for an image stitching algorithm, which is the main goal of our research project. image stitching refers to combining two or more overlapping images together into a single large image. when stitching images together, the goal is to find a 2d geometric transform which minimizes the error in overlapping regions. there are many ways to do this. boofcv uses an example where point image features are found, associated, and then a 2d transform is found robustly using the associated features. 2 another way to achieve image stitching is by the scale invariant feature transform algorithm. according to andrea vedaldi, a sift feature is a selected image region with an associated descriptor. additionally, there's another image stitching technique on which our algorithm is based on. this technique is called speeded up robust features . the surf and sift techniques are very similar but they have a few minor di {latin small ligature ff}erences in some details. the main concept of our image stitching algorithm is based on the opencv library as well as the surf algorithm, which gave us the results that we almost were aiming for.",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "3",
        "Section": "3 Objective and Approach",
        "Text": "our goal in this project is to use the drone's camera to take successive pictures from the university parking lot and blend those images together to build a new image, which will cover the entire parking lot. to do that, we implemented two approaches. first, we used the boofcv library that has its own image stitching algorithm and features. second, we developed our own algorithm based on the opencv library and the surf algorithm. the images were taken during certain periods of time so that the drone captures overlapped images. this way, we can identify and adjust these areas to create one large image. to perform our code in the field test, we divided our environment like the image below. we had the parking lot divided in a 4x4 matrix so that we could organize and better control the drone's movement. figure 1: drone's movement grid the first step in our approach is to take an image at our starting point. then, the drone will move forward taking pictures to cover the first column of our matrix. after completing the first column, the drone will move to realign with the next column, but now taking pictures in the opposite direction. we repeated these steps until the matrix was fully covered. each column is composed by four smaller images that were stitched together, then we stitched the columns together after receiving appropriate modifications, such as rotating the images. to perform the 3 stitching algorithm, we had two cases in our environment: first, stitching images from bottom to top. second, stitching images from side to side.",
        "Subsections": [
            {
                "Section_Num": "3_1",
                "Section": "3.1 First algorithm: BoofCV",
                "Text": "the boofcv library has its own image stitching algorithm. this algorithm, finds some image points features and then a 2d transform is found robustly using the associated features. this image stitching algorithm can be summarized as follows: first, detect and describe point features, which is the discovery of the best key points of each image. then associate features together, only with the common key points between the images. after that, apply a robust fitting to find a transform that will make the necessary changes in the images, such as rotations. the final step is to render the combined images to generate the final image. figure 2: boofcv algorithm steps once boofcv renders two images together, it generates a single image with a black background to complete the square referring to the image file itself. after that, when we try to stitch this result with a third image, we will not get the desired result. this happens because the black background will be considered as a part of the image, so the algorithm will try to find a correspondence between the background and some features of the third image and will not find the correct correlation between the images. even if we use some techniques to cut o {latin small ligature ff}the background, the result will still present distortions. this will happen because of the translations that boofcv does. these translations change the rectangular shape of the image, so there will always be some black background. thus, the result of stitching more than two images using boofcv will be a very distorted image. to avoid this problem, we decided to implement our own algorithm, using opencv.",
                "Subsections": [],
                "Groundtruth": ""
            },
            {
                "Section_Num": "3_2",
                "Section": "3.2 Stitching algorithm using OpenCV",
                "Text": "our stitching algorithm was developed based on the opencv library and mostly prepared using the image processing technique surf. to perform the main feature of our algorithm, the key point detection using surf is our first and main step. according to herbert bay, the surf algorithm is a local feature detector and descriptor that can be used for tasks such as object recognition or registration or classification or 3d reconstruction. it is partly inspired by the scale-invariant feature transform descriptor. the surf algorithm is based on the same principles and steps as the sift, but details in each step are di {latin small ligature ff}erent. the algorithm has three main parts: interest point detection, local neighborhood 4 description and matching. surf uses a blob detector based on the hessian matrix to find points of interest. the determinant of the hessian matrix is used as a measure of local change around the point and points are chosen where this determinant is maximal. figure 3: key points and feature detection from two consecutive images after identifying the key points and matching them between the images, we must identify the common area between these images and crop it. intersection in the first input intersection in the second input figure 4: intersections in the input images figure 5: new first image without the intersection now, the first image has only the part that is not included in the second image. we always keep the last image for performance and level details. the next step is simply to blend those two images together; the first in the bottom and the second in the top. the result of this rendering will be the first input to the algorithm followed by the pictures taken when the drone moves forward. 5 figure 6: final image after the stitch algorithm",
                "Subsections": [],
                "Groundtruth": ""
            }
        ],
        "Groundtruth": ""
    },
    {
        "Section_Num": "4",
        "Section": "4 Results",
        "Text": "we took three pictures , 7 and 7) with the front camera of the drone and cut them to feed the algorithm. we were able to get two satisfactory results, even though they have very visible di {latin small ligature ff}erences, as shown in figure 7 and figure 7. image 1 image 2 image 3 figure 7: results the stitching algorithm provided by the boofcv library does not have a complete rectangular image because it rotates the images for a better binding, which means that the image will have gaps that will be filled by a black background, as shown in figure 8. on the other hand, the algorithm implemented by us simply binds the images together without any gaps and black backgrounds, as shown in figure 8. boofcv result stitched images figure 8: boofcv vs our results it is clear that the boofcv algorithm binds two images with more precision than our implementation, and we barely see the transition between these two images, but it will present a lot of distortions as soon as we attempt to stitch a higher number of images. this distortion 6 makes it difficult to bind more images because increasing the amount of images will increase the amount of distortions, which will decrease the quality the results. however, our algorithm will not present distortions, but it will have clear lines of transitions between the bound images. since some details of the images can get lost during this process, this will also affect the quality of the results. therefore, classifying which algorithm works better for this problem is a difficult decision.",
        "Subsections": [],
        "Groundtruth": ""
    },
    {
        "Section_Num": "5",
        "Section": "5 Conclusion",
        "Text": "after running some examples, we decided that, the best approach for creating a bigger image from other small images is the algorithm developed by us. even though boofcv can fit two images with better accuracy, the drone mapping will have a large number of images to stitch, and boofcv's results would be impracticable because of the amount of distortion it presents. thus, the algorithm developed by us is a good candidate for the proposed task. at the end of this project, we could not reach the goal of mapping the parking lot. working with image stitching algorithms requires very high quality images, which unfortunately the ar-drone 2.0 does not provide to us. one good possibility for completing this task is using the new version of it, or even using some different drone, with preferentially a 1080p camera. another problem while dealing with the use of drones for this project is the low stability of the drone. because of external environment, the pictures taken by the drone were often without a common area, so the algorithm did not perform as expected. thus, the best drone to be used in such project is one with not only a high quality camera, but also with a good stability. with the efficient resources, a parking lot mapping can be done, and it can be used to create a very reliable vigilance system.",
        "Subsections": [],
        "Groundtruth": ""
    }
]