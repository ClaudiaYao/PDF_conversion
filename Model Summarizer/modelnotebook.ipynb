{"cells":[{"cell_type":"markdown","metadata":{"id":"DMsrySAKzEq6"},"source":["\\# This notebook has steps for model initialisation and Training**\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33213,"status":"ok","timestamp":1712817450684,"user":{"displayName":"Sudha Ravi","userId":"04078799224499823775"},"user_tz":-480},"id":"DjYjTBehuyBg","outputId":"2458fdef-2af1-4fac-c567-04f5077ee5f8"},"outputs":[],"source":["!pip install transformers\n","!pip install tokenizer\n","!pip install datasets\n","!pip install rouge_score\n","!pip install sentencepiece\n","!pip install rouge"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2738,"status":"ok","timestamp":1712817453416,"user":{"displayName":"Sudha Ravi","userId":"04078799224499823775"},"user_tz":-480},"id":"jOMAm4fQ3pRj","outputId":"5b190ab6-fa2c-4b8b-ae29-596ed3274e98"},"outputs":[],"source":["from model import SummarizationModel\n","from model import load_data,generate_summary,process_section,summarize_pdf\n","import os\n","\n","\n","# path for the extracted pdf's json file\n","data_file_path = os.path.join('datasets', 'testimage16.json')\n","\n","\n","model_summarizer = SummarizationModel(\"allenai/led-large-16384-arxiv\")\n","\n","# Load  the json file for summarization\n","pdf_data = load_data(data_file_path)\n","\n","#Write the final summary to the summary jsonfile\n","output_file =os.path.join('results', 'summary_results_allenai.json')\n","summarize_pdf(pdf_data, output_file,model_summarizer)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\dprsudh\\Anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n","  from pandas.core.computation.check import NUMEXPR_INSTALLED\n","c:\\Users\\dprsudh\\Anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n","  from pandas.core import (\n"]},{"name":"stdout","output_type":"stream","text":["Section Name:  1 Introduction\n","Generated Summary:  Transformers have become the model of choice in natural language processing\n","(nlp) in computer vision, however, convolutional architectures remain dominant.\n","inspired by the transformer scaling successes in nlp, we experiment with\n","applying a standard transformer directly to images, with the fewest possible\n","modications. We find that large scale training trumps inductive bias.\n","Section Name:  2 Related Work\n","Generated Summary:  transformers were proposed by vaswani et al. for machine translation. They have\n","since be- come the state of the art method in many nlp tasks. We show that large\n","scale pre training makes vanilla transformers competitive with (or even better\n","than) state-of-the-art cnns.\n","Section Name:  3 Method\n","Generated Summary:  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery.\n","Please submit your best shots of the U.S. for next week. Visit CNN.com/Travel\n","next Friday for a new gallery of snapshots from around the world. Please share\n","your best photos of the world with CNN iReport.\n","Section Name:  NA\n","Generated Summary:  In model design we follow the original transformer (vaswani et al., ) as closely\n","as possible. We reshape the image x rhw c into a sequence of attened d patches\n","xp rn(p c), where (h, w) is the resolution of the original image, c is the\n","number of channels, and n is the resulting number of patches. The resulting\n","sequence of embedding vectors serves as input to the encoder. We use standard\n","learnable d position embeddings, since we have not observed signicant\n","performance gains from using more advanced d aware positions.\n","Section Name:  3.2 Fine-tuning and Higher Resolution\n","Generated Summary:  typically, we pre train vit on large datasets, and ne tune to (smaller)\n","downstream tasks. for this, we remove the pre trained prediction head and attach\n","a zero initialized d k feedforward layer. When feeding images of higher\n","resolution, we keep the patch size the same, which results in a larger effective\n","sequence length.\n","Section Name:  4 Experiments\n","Generated Summary:  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery.\n","Please submit your best shots of the U.S. for next week. Visit CNN.com/Travel\n","next Friday for a new gallery of snapshots from around the world. Please share\n","your best photos of the world with CNN iReport.\n","Section Name:  NA\n","Generated Summary:  we evaluate the representation learning capabilities of resnet, vision\n","transformer (vit), and the hybrid. to understand the data requirements of each\n","model, we pre train on datasets of varying size. when considering the\n","computational cost of pre training the model, vit performs very favourably,\n","attaining state of the art on most recognition benchmarks at a lower pre\n","training cost. lastly, we perform a small experiment using self supervision, and\n","show that self supervised vit holds promise for the future.\n","Section Name:  4.1 Setup\n","Generated Summary:  We train all models, including resnets, using adam (kingma & ba, ) with =, =, a\n","batch size of and apply a high weight decay of. We found that, in contrast to\n","common practices, adam works slightly better than sgd for resnets in our\n","setting. We use a linear learning rate warmup and decay, see appendix b. for\n","details. for ne tuning we use sgd with momentum, batch size, for all models. for\n","imagenet results in table, we ne tuned at higher resolution: for vit l/ and for\n","vit h/, and also used polyak & juditsky averaging with a factor of (ramachandran\n","et al. ; wang et al., b). metrics.\n","Section Name:  4.2 Comparison to State of the Art\n","Generated Summary:  We compare our largest models vit h/ and vit l/ to state of the art cnns from\n","the literature. all models were trained on tpuv hardware, and we report the\n","number of tpu v cores ( per chip) used for training multiplied by the training\n","time in days. The smaller model pre trained on jft m outperforms bit l on all\n","tasks, while requiring substantially less computational resources to train.\n","Section Name:  4.3 Pre-training Data Requirements\n","Generated Summary:  Vit transformers perform well when pre trained on a large jft m dataset. with\n","fewer inductive biases for vision than resnets, how crucial is the dataset size?\n","we perform two series of experiments. first, we pre train vit models on datasets\n","of increasing size. second, we train our models on random subsets of m, m, and m\n","as well as the full jft- m.\n","Section Name:  4.4 Scaling Study\n","Generated Summary:  we perform a controlled scaling study of different models by evaluating transfer\n","performance from jft m. in this setting data size does not bottleneck the models\n","performances, and we assess performance versus pre training cost of each model.\n","model set includes: resnets, rx, rX rx and rx rx; vision transformers, vit b/,\n","b/, l/, l/ and h/ pre trained for epochs; and hybrids, r+vit b/ b/ l/ l/.\n","Section Name:  4.5 Inspecting Vision Transformer\n","Generated Summary:  Self attention allows vit to integrate information across the entire image even\n","in the lowest layers. we investigate to what degree the network makes use of\n","this capability. we compute the average distance in image space across which\n","information is integrated, based on the attention weights (figure, right). this\n","attention distance is analogous to receptive eld size in cnns.\n","Section Name:  4.6 Self-supervision\n","Generated Summary:  transformers show impressive performance on nlp tasks. Much of their success\n","stems not only from their excellent scalability but also from large scale self\n","supervised pre training. We also perform a preliminary exploration on masked\n","patch prediction for self supervision, mimicking the masked language modeling\n","task used in bert.\n","Section Name:  5 Conclusion\n","Generated Summary:  We have explored the direct application of transformers to image recognition.\n","unlike prior works using self attention in computer vision, we do not introduce\n","image specic inductive biases into the architecture. instead, we interpret an\n","image as a sequence of patches and process it by a standard transformer encoder\n","as used in nlp. this simple, yet scalable, strategy works surprisingly well when\n","coupled with pre training on large datasets. thus, vision transformer matches or\n","exceeds the state of the art on many image classication datasets.\n","Section Name:  A Multihead Self-attention\n","Generated Summary:  standard qkv self attention is a popular building block for neural archi-\n","tectures. multihead self attention (msa) is an extension of sa in which we run k\n","self attention operations, called heads, in parallel. to keep compute and number\n","of parameters constant when changing k, dh is typically set to d/k.\n","Section Name:  B Experiment details\n","Generated Summary:  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery.\n","Please submit your best shots of the U.S. for next week. Visit CNN.com/Travel\n","next Friday for a new gallery of snapshots from around the world. Please share\n","your best photos of the world with CNN iReport.\n","Section Name:  C Additional Results\n","Generated Summary:  we report detailed results corresponding to the gures presented in the paper.\n","table corresponds to figure from the paper and shows transfer performance of\n","different vit models pre trained on datasets of increasing size. we show\n","transfer accuracy on several datasets, as well as the pre training compute (in\n","ex-aflops)\n","Section Name:  D Additional Analyses\n","Generated Summary:  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery.\n","Please submit your best shots of the U.S. for next week. Visit CNN.com/Travel\n","next Friday for a new gallery of snapshots from around the world. Please share\n","your best photos of the world with CNN iReport.\n","Section Name:  D.1 SGD vs. Adam for ResNets\n","Generated Summary:  resnets are typically trained with sgd and our use of adam as optimizer is quite\n","unconventional. We compare the ne tuning published as a conference paper at iclr\n","resnet resnetx dataset. adam pre training outperforms sgd pre training on most\n","datasets and on average. this justies the choice to use adam as the optimizer.\n","Section Name:  D.2 Transformer shape\n","Generated Summary:  we ran ablations on scaling different dimensions of the transformer architecture\n","to nd out which are best suited for scaling to very large models. decreasing the\n","patch size and thus increasing the effective sequence length shows surprisingly\n","robust improvements without introducing parameters. these ndings suggest that\n","compute might be a better predictor of performance than the number of\n","parameters, and that scaling should emphasize depth over width if any.\n","Section Name:  D.3 Head Type and class token\n","Generated Summary:  In order to stay as close as possible to the original transformer model, we made\n","use of an additional token, which is taken as image representation. This token\n","is then trans- formed into a class prediction via a small multi layer perceptron\n","(mlp) with tanh as non linearity in the single hidden layer. this design is\n","inherited from the transformer model for text, and we use it throughout the main\n","paper.\n","Section Name:  D.4 Positional Embedding\n","Generated Summary:  we ran ablations on different ways of encoding spatial information using\n","positional embedding. we tried the following cases: providing no positional\n","information: considering the inputs as a bag of patches. relative positional\n","embeddings: considering relative distance between patches to en- code the\n","spatial information as instead of their absolute position.\n","Section Name:  D.5 Empirical Computational Costs\n","Generated Summary:  Vit models have speed comparable to similar resnets. Large vit models have a\n","clear advantage in terms of memory efciency over resnet models. The theoretical\n","bi quadratic scaling of vit with image size only barely starts happening for the\n","largest models at the largest resolutions.\n","Section Name:  D.6 Axial Attention\n","Generated Summary:  axial attention (huang et al., ; ho et al. ) is a simple, yet effective\n","technique to run self- attention on large inputs that are organized as\n","multidimensional tensors. The general idea of axial attention is to perform\n","multiple attention operations, each along a single axis of the input tensor.\n","Instead of applying -dimensional attention to the attened version of the inputs,\n","each attention mixes information along a particular axis.\n","Section Name:  D.7 Attention Distance\n","Generated Summary:  to understand how vit uses self attention to integrate information across the\n","image, we analyzed the average distance spanned by attention weights at\n","different layers. this attention distance is analogous to receptive eld size in\n","cnns. as depth increases, attention distance increases for all heads. in the\n","second half of the network, most heads attend widely across tokens.\n","Section Name:  D.8 Attention Maps\n","Generated Summary:  to compute maps of the attention from the output token to the input space\n","(figures and ), we used attention rollout (abnar & zuidema, ). briey, we\n","averaged attention weights of vit- l/ across all heads and then recursively\n","multiplied the weight matrices of all layers.\n","Section Name:  D.9 ObjectNet Results\n","Generated Summary:  we also evaluate our agship vit h/ model on the objectnet benchmark following\n","the evaluation setup in kolesnikov et. al. (2008) resulting in % top- accuracy\n","and %Top accuracy.    We evaluated our agship vit h/ on theobjectnet benchmark\n","using the evaluation setup in kolesnikov  et.al. ( 2008) resulting in % Top\n","accuracy and% Top accuracy.\n","Section Name:  D.10 VTAB Breakdown\n","Generated Summary:  table shows the scores attained on each of the vtab k tasks. published as a\n","conference paper at iclr figure : further example attention maps as in figure\n","(random selection) table : breakdown of v tab k performance across tasks.\n","caltech cifar- dtd flowers pets sunsvhn camelyon eurosat resisc retinopathy\n","clevr count clevr dist dmlab dspr loc dspr ori kitti dist snorb azim snorb elev\n","mean vit h/ (jft) vit l/ ( jft)\n","Section Name:  NA\n","Generated Summary:  Transformer architecture has become the de facto standard for natural language\n","processing tasks. Its applications to computer vision remain limited. We show\n","that a pure transformer applied directly to sequences of image patches can\n","perform very well on image classication tasks. when pre trained on large amounts\n","of data and transferred to multiple mid sized or small image recognition\n","benchmarks.\n"]}],"source":["\n","\n","from model import SummarizationModel\n","from model import load_data,generate_summary,process_section,summarize_pdf\n","import os\n","\n","# path for the extracted pdf's json file\n","data_file_path = os.path.join('datasets', 'testimage16.json')\n","\n","#Summarize using facebook/bart-large-cnn\n","\n","model_summarizer = SummarizationModel(\"facebook/bart-large-cnn\")\n","\n","\n","# Load  the json file for summarization\n","pdf_data = load_data(data_file_path)\n","\n","#Write the final summary to the summary jsonfile\n","output_file =os.path.join('results', 'summary_results_facebookbart.json')\n","summarize_pdf(pdf_data, output_file,model_summarizer)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3.9.13 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"2e392266d6a5626032387dc1b7e3fce21e875c52abef25ac7b7f00de119355c6"}}},"nbformat":4,"nbformat_minor":0}
