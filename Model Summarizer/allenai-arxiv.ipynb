{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["\\# This notebook has steps for model initialisation and Training**\n","\n"],"metadata":{"id":"DMsrySAKzEq6"}},{"cell_type":"code","source":["!pip install transformers\n","!pip install tokenizer\n","!pip install datasets\n","!pip install rouge_score\n","!pip install sentencepiece\n","!pip install rouge"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DjYjTBehuyBg","executionInfo":{"status":"ok","timestamp":1712817450684,"user_tz":-480,"elapsed":33213,"user":{"displayName":"Sudha Ravi","userId":"04078799224499823775"}},"outputId":"2458fdef-2af1-4fac-c567-04f5077ee5f8"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.3)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n","Requirement already satisfied: tokenizer in /usr/local/lib/python3.10/dist-packages (3.4.3)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.18.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n","Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.25.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.2)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n","Requirement already satisfied: rouge in /usr/local/lib/python3.10/dist-packages (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","import pandas as pd\n","import textwrap\n","import json\n","import os\n","import torch.optim as optim\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","import logging\n","transformers_logger = logging.getLogger(\"transformers\")\n","logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n","from transformers import LEDForConditionalGeneration, LEDTokenizer\n","from datasets import load_dataset, load_metric\n","import torch\n","from rouge import Rouge\n","\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jOMAm4fQ3pRj","executionInfo":{"status":"ok","timestamp":1712817453416,"user_tz":-480,"elapsed":2738,"user":{"displayName":"Sudha Ravi","userId":"04078799224499823775"}},"outputId":"5b190ab6-fa2c-4b8b-ae29-596ed3274e98"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["**This section contains the code for loading the json file after pdf extraction and the model initialization**"],"metadata":{"id":"UM30ip_zePoT"}},{"cell_type":"code","source":["#Load the json file after pdf extraction\n","dataset_path = \"/content/drive/My Drive/Colab Notebooks/CS5242 Project/\"\n","\n","def load_data(file_path):\n","    with open(file_path, 'r') as file:\n","        data = json.load(file)\n","    return data\n","\n","# Join the paths\n","data_file_path = os.path.join(dataset_path, 'testimage16.json')\n","\n","# Load  data\n","pdf_data = load_data(data_file_path)\n","\n","# Device configuration\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","#Define the Model\n","class SummarizationModel:\n","    def __init__(self, model_name, device):\n","        self.model_name = model_name\n","        self.tokenizer = LEDTokenizer.from_pretrained(model_name)\n","        self.model = LEDForConditionalGeneration.from_pretrained(model_name).to(DEVICE)\n","        self.config=LEDForConditionalGeneration.from_pretrained(model_name).config\n","\n","#Instantiate the model\n","\n","model_name = \"allenai/led-large-16384-arxiv\"\n","model_summarizer = SummarizationModel(model_name, device=DEVICE)\n","model = model_summarizer.model\n","tokenizer=model_summarizer.tokenizer\n","#print(modelsummarizer.config)\n","\n","#"],"metadata":{"id":"aSo5fz1lnMtO","executionInfo":{"status":"ok","timestamp":1712817456929,"user_tz":-480,"elapsed":3519,"user":{"displayName":"Sudha Ravi","userId":"04078799224499823775"}}},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":["**Model Inference**"],"metadata":{"id":"WJxfFkFnwZSl"}},{"cell_type":"code","source":["#Generate Summary for the content using the loaded model\n","def generate_summary(content):\n","        max_length=300\n","        num_beams=4\n","        inputs = tokenizer(content, return_tensors=\"pt\", max_length=1024, truncation=True)\n","        summary_ids = model.generate(inputs.input_ids.to(DEVICE), max_length=max_length, num_beams=num_beams, early_stopping=True)\n","        summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n","        return summary_text\n","\n","#Pase each sections and subsection to generate summaries from the model\n","def process_section(section,results):\n","\n","    # Process the content of the each section\n","    section_summary_results = {}\n","    content = section[\"Text\"]\n","    section_name=section[\"Section\"]\n","    summary_text = generate_summary(content)\n","    section_summary_results[\"Section Name\"] = section_name\n","    section_summary_results[\"Generated Summary\"] = summary_text\n","    results.append(section_summary_results)\n","    print(\"Section Name: \", section_name)\n","    wrapped_output = textwrap.fill(str(summary_text), width=80)\n","    print(\"Generated Summary: \", wrapped_output)\n","        # Process the subsections if they exist\n","    if \"Subsections\" in section:\n","        for subsection in section[\"Subsections\"]:\n","            process_section(subsection,results)\n","\n","\n","# Summarize the section contents and subsection contents\n","def summarize_pdf(pdf_data, output_file):\n","    all_results = []\n","    for section in pdf_data:\n","        process_section(section,all_results)\n","    with open(output_file, \"w\") as json_file:\n","        json.dump(all_results, json_file, indent=4)\n","\n","#Write the final summary to the summary jsonfile\n","output_file = \"summary_results_allenai.json\"\n","summarize_pdf(pdf_data, output_file)\n"],"metadata":{"id":"0sKu1Vxp8pSO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712817571230,"user_tz":-480,"elapsed":114315,"user":{"displayName":"Sudha Ravi","userId":"04078799224499823775"}},"outputId":"84196d20-12c5-4057-fd3e-558d37a225d7"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Section Name:  1 Introduction\n","Generated Summary:   self attention based architectures, in particular transformers, have become the\n","model of choice in natural language processing (nlp ). inspired by the\n","transformer scaling successes in nlp, we experiment with applying a standard\n","transformer directly to images, with the fewest possible modications. to do so,\n","we split an image into patches and provide the sequence of linear embeddings of\n","these patches as an input to a trans- former. image patches are treated the same\n","way as tokens (words ) in an nlp application.   we train the model on image\n","classication in supervised fashion. when trained on mid sized datasets such as\n","imagenet without strong regularization, these mod- els yield modest accuracies\n","of a few percentage points below resnets of comparable size.   however, the\n","picture changes if the models are trained on larger datasets ( m m images ).\n","we nd that large scale training trumps inductive bias.\n","Section Name:  2 Related Work\n","Generated Summary:   transformers were proposed by vaswani et al. for machine translation, and have\n","since be- come the state of the art method in many nlp tasks.   large\n","transformer based models are often pre trained on large corpora and then ne\n","tuned for the task at hand : bert (devlin et al., ) uses a denoising self\n","supervised pre training task, while the gpt line of work uses language mod-\n","eling as its pre training task.   naive application of self attention to images\n","would require that each pixel attends to every other pixel. with quadratic cost\n","in the number of pixels, this does not scale to realistic input sizes. thus, to\n","apply transformers in the context of image processing, several approximations\n","have been tried in the past. most related to ours is the model of cordonnier et\n","al., which extracts patches of size from the input image and applies full self\n","attention on top. this model is very similar to vit, but our work goes further\n","to demonstrate that large scale pre training makes vanilla transformers\n","competitive with (or even better than) state of the art cnns.\n","Section Name:  3 Method\n","Generated Summary:   we report on the results of a detailed analysis of the @xmath0@xmath1@xmath2@xm\n","ath3@xmath4@xmath5@xmath6@xmath7@xmath8@xmath9@xmath10@xmath11@xmath12@xmath13@x\n","math14@xmath15@xmath16@xmath17@xmath18@xmath19@xmath20@xmath21@xmath22@xmath23@x\n","math24@xmath25@xmath26@xmath27@xmath28@xmath29@xmath30@xmath40@xmath11@xmath11@x\n","math12@xmath13@xmath14@xmath11@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13@x\n","math14@xmath15@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13@xmath14@xmath15@x\n","math12@xmath13@xmath14@xmath15@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13@x\n","math14@xmath15@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13\n","Section Name:  NA\n","Generated Summary:   vision transformer ( vit ) is a vision transformer ( vit ) that transforms a d\n","sequence of token embeddings into a d image representation.   vision transformer\n","( vit ) is a scalable nlp transformer architecture and their efcient\n","implementations can be used almost out of the box.   vision transformer ( vit )\n","is a scalable nlp transformer architecture and their efcient implementations can\n","be used almost out of the box.   vision transformer ( vit ) is a scalable nlp\n","transformer architecture and their efcient implementations can be used almost\n","out of the box.   vision transformer ( vit ) is a scalable nlp transformer\n","architecture and their efcient implementations can be used almost out of the\n","box.   vision transformer ( vit ) is a scalable nlp transformer architecture and\n","their efcient implementations can be used almost out of the box.   vision\n","transformer ( vit ) is a scalable nlp transformer architecture and their efcient\n","implementations can be used almost out of the box.   vision transformer ( vit )\n","is a scalable nlp transformer architecture and their efcient implementations can\n","be used almost out of the box.   vision transformer ( vit ) is a scalable nlp\n","transformer\n","Section Name:  3.2 Fine-tuning and Higher Resolution\n","Generated Summary:   in this paper, we present a novel vision transformer for deep learning.   the\n","vision transformer can handle arbitrary sequence lengths ( up to memory\n","constraints ), however, the pre trained position embeddings may no longer be\n","meaningful.   we therefore perform d interpolation of the pre trained position\n","embeddings, according to their location in the original image, and perform d\n","patch extraction of the pre trained position embeddings, according to their\n","location in the original image.   the vision transformer can handle arbitrary\n","sequence lengths ( up to memory constraints ), however, the pre trained position\n","embeddings may no longer be meaningful.   we therefore perform d interpolation\n","of the pre trained position embeddings, according to their location in the\n","original image, and perform d patch extraction of the pre trained position\n","embeddings, according to their location in the original image.   the vision\n","transformer can handle arbitrary sequence lengths ( up to memory constraints ),\n","however, the pre trained position embeddings may no longer be meaningful.   we\n","therefore perform d interpolation of the pre trained position embeddings,\n","according to their location in the original image, and perform d patch\n","extraction of the pre trained position embeddings, according to their location\n","in the\n","Section Name:  4 Experiments\n","Generated Summary:   we report on the results of a detailed analysis of the @xmath0@xmath1@xmath2@xm\n","ath3@xmath4@xmath5@xmath6@xmath7@xmath8@xmath9@xmath10@xmath11@xmath12@xmath13@x\n","math14@xmath15@xmath16@xmath17@xmath18@xmath19@xmath20@xmath21@xmath22@xmath23@x\n","math24@xmath25@xmath26@xmath27@xmath28@xmath29@xmath30@xmath40@xmath11@xmath11@x\n","math12@xmath13@xmath14@xmath11@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13@x\n","math14@xmath15@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13@xmath14@xmath15@x\n","math12@xmath13@xmath14@xmath15@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13@x\n","math14@xmath15@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13\n","Section Name:  NA\n","Generated Summary:   we evaluate the representation learning capabilities of resnet, vision\n","transformer ( vit ), and the hybrid. to understand the data requirements of each\n","model, we pre train on datasets of varying size and evaluate many benchmark\n","tasks. when considering the computational cost of pre training the model, vit\n","performs very favourably, attaining state of the art on most recognition\n","benchmarks at a lower pre training cost. lastly, we perform a small experiment\n","using self supervision, and show that self supervised vit holds promise for the\n","future.\n","Section Name:  4.1 Setup\n","Generated Summary:   we present a new vision transformer ( vit ) class of vision transformers ( cnns\n",").   vit transforms intermediate fea- ture maps into vit with patch size of one\n","pixel.   vit transformers sequence length is inversely proportional to the\n","square of the patch size, thus models with smaller patch size are\n","computationally more expensive.   we base vit congurations on those used for\n","bert ( devlin et al., ) and we add the larger huge model.   we base vit\n","congurations on those used for bert ( devlin et al., ) and we add the larger\n","huge model.   we base vit congurations on those used for bert ( devlin et al., )\n","and we add the larger huge model. to explore model scalability, we use the\n","ilsvrc- imagenet dataset with k classes and m images ( we refer to it as\n","imagenet in what follows ), its superset imagenet k with k classes and m images\n","( deng et al., ), and jft (sun et al., ) with k classes and m high resolution\n","images. to explore model scalability, we use the ilsvrc- imagenet dataset with k\n","classes and m images ( we refer to it as imagenet in what follows ), its\n","superset imagenet k with k classes and m images ( deng et al., ), and jft (sun\n","et al\n","Section Name:  4.2 Comparison to State of the Art\n","Generated Summary:   we report the results of pre training vision transformer models pre trained on\n","tpuv hardware on popular image classication benchmarks.   our vision transformer\n","models pre trained on the jft m dataset outperform resnet based baselines on all\n","datasets, while taking substantially less computational resources to pre train\n","than prior state of the art baselines.   we further improve the performance on\n","the more challenging datasets imagenet, cifar-, and the vtab suite, especially\n","on the more challenging datasets imagenet, cifar-, and the vtab suite.\n","interestingly, this published as a conference paper at iclr.\n","Section Name:  4.3 Pre-training Data Requirements\n","Generated Summary:   vision transformers outperform resnets with the same compu- tational budget.\n","with fewer inductive biases for vision than resnets, how crucial is the dataset\n","size?   we perform two series of experiments. first, we pre train our models on\n","random subsets of m, m, and m as well as the full jft- m dataset.   second, we\n","train our models on random subsets of m, m, and m as well as the full jft- m\n","dataset.   we do not perform additional regularization on the smaller subsets\n","and use the same hyper parameters for all settings.   we do not perform\n","additional regularization on the smaller subsets and use the same hyper\n","parameters for all settings.   we do, however, use early stopping, and report\n","the best validation accuracy achieved during training.   we do, however, use\n","early stopping, and report the best validation accuracy achieved during\n","training.   we do, however, use early stopping, and report the best validation\n","accuracy achieved during training.   we do, however, use early stopping, and\n","report the best validation accuracy achieved during training.   we do, however,\n","use early stopping, and report the best validation accuracy achieved during\n","training.   we do, however, use early stopping, and report the best validation\n","accuracy achieved during training.   we do, however, use early stopping, and\n","report the best validation accuracy achieved during training.\n","Section Name:  4.4 Scaling Study\n","Generated Summary:   convolutional local feature processing ( vit ) has emerged as a powerful tool\n","for the analysis of large datasets.   vit has been shown to outperform\n","convolutional local feature processing ( llp ) on the performance/compute trade\n","off, and vit has been shown to outperform convolutional local feature processing\n","( llp ) on the performance/compute trade off. here   we present a controlled\n","scaling study of different models by evaluating transfer performance from jft m.\n","in this setting data size does not bottleneck the models performances, and we\n","assess performance versus pre training cost of each model. the model set\n","includes: resnets, rx, rx rx, rx, rx, rx, pre trained for epochs, plus rx and rx\n","pre trained for epochs ; vision transformers, vit b/, b/, l/, l/, pre trained\n","for epochs, plus l/ and h/ pre trained for epochs ; and hybrids, r+vit b/, b/,\n","l/, l/ pre- trained for epochs, plus r+vit l/ pre trained for epochs ( for\n","hybrids, the number at the end of the model name stands not for the patch size,\n","but for the total dowsampling ratio in the resnet backbone ).\n","Section Name:  4.5 Inspecting Vision Transformer\n","Generated Summary:   self attention allows vit to integrate information across the entire image even\n","in the lowest layers.   we investigate to what degree the network makes use of\n","this capability, and show that the ability to integrate information globally is\n","indeed used by the model. to begin to understand how the vision transformer\n","processes im- age data, we analyze its internal representations.   self\n","attention allows vit to integrate information across the entire image even in\n","the lowest layers.   we investigate to what degree the network makes use of this\n","capability.   we nd that some heads attend to most of the image already in the\n","lowest layers, showing that the ability to integrate information globally is\n","indeed used by the model. other attention heads have consistently small\n","attention distances in the low layers, suggesting that it may serve a similar\n","function as early convolutional layers in cnns. globally, we nd that the model\n","attends to image regions that are semantically relevant for classication.\n","specically, we compute the average distance in image space across which\n","information is integrated, based on the attention weights.   this attention\n","distance is analogous to receptive eld size in cnns. finally, we show that the\n","model learns to encode distance within the image in the similarity of position\n","em- beddings, i.e. closer patches tend to have more similar position em-\n","beddings, i.e. closer patches tend to have more similar position em- beddings.\n","self attention\n","Section Name:  4.6 Self-supervision\n","Generated Summary:   transformers show impressive performance on nlp tasks. however, much of their\n","success stems not only from their excellent scalability but also from large\n","scale self supervised pre training.   we perform a preliminary exploration on\n","masked patch prediction for self supervision, mimicking the masked language\n","modeling task used in bert. with self supervised pre training, our smaller vit\n","b/ model achieves % accuracy on imagenet, a signicant improvement of % to\n","training from scratch, but still % behind supervised pre training.\n","Section Name:  5 Conclusion\n","Generated Summary:   we explore the direct application of transformers to image recognition. unlike\n","prior works using self attention in computer vision, we do not introduce image\n","specic inductive biases into the architecture apart from the initial patch\n","extraction step. instead, we interpret an image as a sequence of patches and\n","process it by a standard transformer encoder as used in nlp. this simple, yet\n","scalable, strategy works surprisingly well when coupled with pre training on\n","large datasets. thus, vision transformer matches or exceeds the state of the art\n","on many image classication datasets, whilst being relatively cheap to pre train.\n","Section Name:  A Multihead Self-attention\n","Generated Summary:   multihead self attention ( msa ) is an extension of qkv self attention ( sa )\n","in which we run k self attention operations, called heads, in parallel and\n","project their concatenated outputs.   multihead self attention ( msa ) is an\n","extension of sa in which we run k self attention operations, called heads, in\n","parallel and project their concatenated outputs.   msa is an extension of sa in\n","which we run k self attention operations, called heads, in parallel and project\n","their concatenated outputs.   msa is an extension of sa in which we run k self\n","attention operations, called heads, in parallel and project their concatenated\n","outputs.   msa is an extension of sa in which we run k self attention\n","operations, called heads, in parallel and project their concatenated outputs.\n","msa is an extension of sa in which we run k self attention operations, called\n","heads, in parallel and project their concatenated outputs.   msa is an extension\n","of sa in which we run k self attention operations, called heads, in parallel and\n","project their concatenated outputs.   msa is a popular building block for neural\n","archi- tectures.\n","Section Name:  B Experiment details\n","Generated Summary:   we report on the results of a detailed analysis of the @xmath0@xmath1@xmath2@xm\n","ath3@xmath4@xmath5@xmath6@xmath7@xmath8@xmath9@xmath10@xmath11@xmath12@xmath13@x\n","math14@xmath15@xmath16@xmath17@xmath18@xmath19@xmath20@xmath21@xmath22@xmath23@x\n","math24@xmath25@xmath26@xmath27@xmath28@xmath29@xmath30@xmath40@xmath11@xmath11@x\n","math12@xmath13@xmath14@xmath11@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13@x\n","math14@xmath15@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13@xmath14@xmath15@x\n","math12@xmath13@xmath14@xmath15@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13@x\n","math14@xmath15@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13\n","Section Name:  C Additional Results\n","Generated Summary:   we report detailed results corresponding to the gures presented in the paper.\n","we show transfer accuracy on several datasets, as well as the pre training\n","compute ( in ex- aflops ) of different vision transformer ( vit ) models pre\n","trained on im- agenet, imagenet k or jftm. we report detailed results\n","corresponding to the gures presented in the paper. we show transfer accuracy on\n","several datasets, as well as the pre training compute ( in ex- aflops ) of\n","different vit models pre trained on im- agenet, imagenet k or jftm. table\n","corresponds to figure from the paper and shows transfer performance of different\n","vit models pre trained on datasets of increasing size: imagenet, imagenet k, and\n","jft m. table corresponds to published as a conference paper at iclr vit b/ vit\n","b/ vit l/ vit l/ vit h/ imagenet cifar- - cifar- - imagenet - imagenet real -\n","oxford flowers- - oxford iiit pets - imagenet k cifar- cifar- imagenet imagenet\n","real oxford flowers- oxford iiit pets jft m cifar- c\n","Section Name:  D Additional Analyses\n","Generated Summary:   we report on the results of a detailed analysis of the @xmath0@xmath1@xmath2@xm\n","ath3@xmath4@xmath5@xmath6@xmath7@xmath8@xmath9@xmath10@xmath11@xmath12@xmath13@x\n","math14@xmath15@xmath16@xmath17@xmath18@xmath19@xmath20@xmath21@xmath22@xmath23@x\n","math24@xmath25@xmath26@xmath27@xmath28@xmath29@xmath30@xmath40@xmath11@xmath11@x\n","math12@xmath13@xmath14@xmath11@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13@x\n","math14@xmath15@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13@xmath14@xmath15@x\n","math12@xmath13@xmath14@xmath15@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13@x\n","math14@xmath15@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13\n","Section Name:  D.1 SGD vs. Adam for ResNets\n","Generated Summary:   in this paper we show the experiments that motivated the choice of adam as the\n","optimizer used to pre train resnets on jft. here we show the experiments that\n","motivated this choice, namely, we compare the ne tuning published as a\n","conference paper at iclr resnet resnetx dataset with the tuning published as a\n","conference paper at iclr resnet resnetx dataset adam sgd adam sgd imagenet cifar\n","cifar oxford iiit pets oxford flowers- average table : fine tuning resnet models\n","pre trained with adam and sgd.\n","Section Name:  D.2 Transformer shape\n","Generated Summary:   in this paper, we present a transformer architecture for very large models.\n","we ran ablations on scaling different dimensions of the transformer architecture\n","to nd out which are best suited for scaling to very large models.   overall, we\n","nd that scaling all dimensions proportionally results in robust improvements\n","without introducing parameters.   these ndings suggest that compute might be a\n","better predictor of performance than the number of parameters, and that scaling\n","should emphasize depth over width if any.\n","Section Name:  D.3 Head Type and class token\n","Generated Summary:   we present an ablation study on positional embeddings with vit b/ model\n","evaluated on imagenet -shot linear. instead of using only image patch\n","embeddings, globally average pooling ( gap ) them, followed by a linear\n","classierjust like resnets nal feature mapperformed very poorly. however, we\n","found that this is neither due to the extra token, nor to the gap operation.\n","instead, published as a conference paper at iclr epochs of training imagenet\n","linear -shot accuracy cls token, lr=e- gap, lr=e- gap, lr=e- figure : comparison\n","of class token and global average pooling classiers both work similarly well,\n","but require different learning rates. the difference in performance is fully\n","explained by the requirement for a different learning rate, see figure  the\n","difference in performance is fully explained by the requirement for a different\n","learning rate, see figure  the ablation study on positional embeddings with vit\n","b/ model evaluated on imagenet -shot linear. instead, published as a conference\n","paper at iclr epochs of training imagenet linear -shot accuracy cls token, lr=e-\n","gap, lr=e- figure : comparison of class\n","Section Name:  D.4 Positional Embedding\n","Generated Summary:   we ran ablations on different ways of encoding spatial information using\n","positional embedding. we tried the following cases : providing no positional\n","information: considering the inputs as a bag of patches. -dimensional positional\n","embedding: considering the inputs as a sequence of patches in the raster order.\n","-dimensional positional embedding: considering the inputs as a grid of patches\n","in two dimensions. relative positional embeddings: considering the relative\n","distance between patches to en- code the spatial information as instead of their\n","absolute position. in addition to different ways of encoding spatial\n","information, we also tried different ways of in- corporating this information in\n","our model. to do so, we use - dimensional relative attention, in which we dene\n","the relative distance all possible pairs of patches. thus, for every given pair\n","(one as query, and the other as key/value in the at- tention mechanism ), we\n","have an offset pq pk, where each offset is associated with an embedding. then,\n","we simply run extra attention, where we use the original query ( the content of\n","query ), but use relative positional embeddings as keys. we then use the log-\n","its from the relative attention as a bias term and add it to the logits of the\n","main attention ( content based attention ) before applying the softmax. for the\n","-dimensional and -dimensional positional embeddings, we tried three different\n","cases : add positional embeddings to the inputs right after published as a\n","conference paper at\n","Section Name:  D.5 Empirical Computational Costs\n","Generated Summary:   we are building a new class of high performance image processing ( vit )\n","architectures on a tpuv accelerator.   vit is a new class of high performance\n","image processing architectures ( flops ) that are based on the same architecture\n","as our hardware.   vit is a new class of flops based on the same architecture as\n","our hardware.   vit is a new class of flops based on the same architecture as\n","our hardware.   vit is a new class of flops based on the same architecture as\n","our hardware.   vit is a new class of flops based on the same architecture as\n","our hardware.   vit is a new class of flops based on the same architecture as\n","our hardware.   vit is a new class of flops based on the same architecture as\n","our hardware.   vit is a new class of flops based on the same architecture as\n","our hardware.   vit is a new class of flops based on the same architecture as\n","our hardware.   vit is a new class of flops based on the same architecture as\n","our hardware.   vit is a new class of flops based on the same architecture as\n","our hardware.   vit is a new class of flops based on the same architecture as\n","our hardware.   vit is a new class of flops based on the same architecture as\n","our hardware.   vit is a new class of flops based on the same architecture as\n","our hardware.\n","Section Name:  D.6 Axial Attention\n","Generated Summary:   axial attention is a simple, yet effective technique to run self- attention on\n","large inputs that are organized as multidimensional tensors. in axial attention,\n","each attention mixes information along a particular axis, while keeping\n","information along the other axes independent. along this line, wang et al. ( b )\n","proposed the axialresnet model in which all the convolutions with kernel size in\n","a resnet are replaced by axial self attention, i.e. a row and column attention,\n","augmented by relative positional encoding.   we have implemented axialresnet as\n","a baseline model. moreover, we have modied vit to process inputs in the\n","-dimensional shape, instead of a - dimensional sequence of patches, and\n","incorporate axial transformer blocks, in which instead of a self attention\n","followed by an mlp, we have a a row self attention plus an mlp followed by a\n","column self attention plus an mlp. in our experiments, we reproduced the scores\n","reported in (wang et al., b) in terms of accuracy, however, our implementation,\n","similar to the open source implementation, is very slow on tpus. therefore, we\n","were not able to use it for extensive large scale experiments.   these may be\n","unlocked by a carefully optimized implementation.\n","Section Name:  D.7 Attention Distance\n","Generated Summary:   to understand how vit uses self attention to integrate information across the\n","image, we analyzed the average distance spanned by attention weights at\n","different layers.   this attention distance is analogous to receptive eld size\n","in cnns. average attention distance is highly variable across heads in lower\n","layers, with some heads attending to much of the image, while others attend to\n","small regions at or near the query location. as depth increases, attention\n","distance increases for all heads. in the second half of the network, most heads\n","attend widely across tokens.\n","Section Name:  D.8 Attention Maps\n","Generated Summary:   we study the effect of a single token on the attention of vit- l/.   we show\n","that the attention of vit- l/ can be used to control the attention of vit- l/.\n","we show that vit- l/ can be used to control the attention of vit- l/, and that\n","vit- l/ can be used to control the attention of vit- l/.   we also show that\n","vit- l/ can be used to control the attention of vit- l/, and that vit- l/ can be\n","used to control the attention of vit- l/.      the attention of vit- l/ can be\n","used to control the attention of vit- l/.   vit- l/ can be used to control the\n","attention of vit- l/, and vit- l/ can be used to control the attention of vit-\n","l/.   vit- l/ can be used to control the attention of vit- l/, and vit- l/ can\n","be used to control the attention of vit- l/  @xcite.   vit- l/ can be used to\n","control the attention of vit- l/, and vit- l/ can be used to control the\n","attention of vit- l/  @xcite.   vit- l/ can also be used to control the\n","attention of vit- l/  @xcite.   vit- l/\n","Section Name:  D.9 ObjectNet Results\n","Generated Summary:   agship vit h/@xmath0/@xmath1/@xmath2/@xmath3/@xmath4/@xmath5/@xmath6/@xmath7/@x\n","math8/@xmath9/@xmath10/@xmath11/@xmath12/@xmath13/@xmath14/@xmath15/@xmath16/@xm\n","ath17/@xmath18/@xmath19/@xmath20/@xmath21/@xmath22/@xmath23/@xmath24/@xmath25/@x\n","math26/@xmath27/@xmath28/@xmath29/@xmath30/@xmath31/@xmath40/@xmath11/@xmath11/@\n","xmath12/@xmath13/@xmath14/@xmath20/@xmath11/@xmath12/@xmath13/@xmath14/@xmath20/\n","@xmath30/@xmath11/@xmath12/@xmath13/@xmath14/@xmath20/@xmath30/@xmath11/@xmath12\n","/@xmath13/@xmath14/@xmath20/@xmath30/@xmath11/@xmath12\n","Section Name:  D.10 VTAB Breakdown\n","Generated Summary:   retinopathy is a degenerative disease of the retina.   retinopathy is\n","characterized by a progressive loss of photoreceptivity and photodissociation (\n","pda ).   retinopathy can be caused by the loss of photoreceptivity and\n","photodissociation ( pda ).   retinopathy can be caused by the loss of\n","photoreceptivity and photodissociation ( pda ).   retinopathy can be caused by\n","the loss of photoreceptivity and photodissociation ( pda ).   retinopathy can be\n","caused by the loss of photoreceptivity and photodissociation ( pda ).\n","retinopathy can be caused by the loss of photoreceptivity and photodissociation\n","( pda ).   retinopathy can be caused by the loss of photoreceptivity and\n","photodissociation ( pda ).   retinopathy can be caused by the loss of\n","photoreceptivity and photodissociation ( pda ).   retinopathy can be caused by\n","the loss of photoreceptivity and photodissociation ( pda ).   retinopathy can be\n","caused by the loss of photoreceptivity and photodissociation ( pda ).\n","retinopathy can be caused by the loss of photoreceptivity and photodissociation\n","( pda ).\n","Section Name:  NA\n","Generated Summary:   transformers for image recognition at scale alexey dosovitskiy, lucas beyer,\n","alexander kolesnikov, dirk weissenborn, xiaohua zhai, thomas unterthiner,\n","mostafa dehghani, matthias minderer, georg heigold, sylvain gelly, jakob\n","uszkoreit, neil houlsby, equal technical contribution, equal advising google\n","research, brain team {adosovitskiy, neilhoulsby}@google.com abstract while the\n","transformer architecture has become the de facto standard for natural language\n","processing tasks, its applications to computer vision remain limited. in vision,\n","attention is either applied in conjunction with convolutional networks, or used\n","to replace certain components of convolutional networks while keeping their\n","overall structure in place.   we show that this reliance on cnns is not\n","necessary and a pure transformer applied directly to sequences of image patches\n","can perform very well on image classication tasks. when pre trained on large\n","amounts of data and transferred to multiple mid sized or small image recognition\n","benchmarks (imagenet, cifar-, vtab, etc ), vision transformer attains excellent\n","results compared to state\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"b_jZLpmLWuQ2","executionInfo":{"status":"ok","timestamp":1712817571231,"user_tz":-480,"elapsed":26,"user":{"displayName":"Sudha Ravi","userId":"04078799224499823775"}}},"execution_count":36,"outputs":[]}]}