{"cells":[{"cell_type":"markdown","metadata":{"id":"DMsrySAKzEq6"},"source":["\\# This notebook has steps for model initialisation , Training and Testing**\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# ensure that the imported .py file will get auto imported and updated whenever there is a change\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33213,"status":"ok","timestamp":1712817450684,"user":{"displayName":"Sudha Ravi","userId":"04078799224499823775"},"user_tz":-480},"id":"DjYjTBehuyBg","outputId":"2458fdef-2af1-4fac-c567-04f5077ee5f8"},"outputs":[],"source":["!pip install transformers\n","!pip install tokenizer\n","!pip install datasets\n","!pip install rouge_score\n","!pip install sentencepiece\n","!pip install rouge\n","!pip install transformers[torch]"]},{"cell_type":"markdown","metadata":{},"source":["# Run Model Trainer"]},{"cell_type":"markdown","metadata":{},"source":["Load the datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2738,"status":"ok","timestamp":1712817453416,"user":{"displayName":"Sudha Ravi","userId":"04078799224499823775"},"user_tz":-480},"id":"jOMAm4fQ3pRj","outputId":"5b190ab6-fa2c-4b8b-ae29-596ed3274e98"},"outputs":[],"source":["import os\n","import json\n","import torch\n","from torch.utils.data import DataLoader\n","import matplotlib.pyplot as plt\n","from ModelSummarizer import SummarizationModel\n","from ModelSummarizer import load_data\n","\n","# Define the path for the datasets\n","\n","train_file_path = os.path.join('../dataset/', 'dataset_ground_truth.json')  # 100 pdfs\n","test_file_path =  os.path.join('../dataset/', 'dataset_test_ground_truth.json')   #20 pdfs\n","val_file_path =  os.path.join('../dataset/', 'dataset_eval_ground_truth.json')  #20 pdfs\n","\n","model_name = \"allenai/led-large-16384-arxiv\"\n","summarizer = SummarizationModel(model_name)\n","model = summarizer.model\n","\n","# Load training data\n","train_data = load_data(train_file_path)\n","\n","# Load testing data\n","test_data = load_data(test_file_path)\n","\n","#Load val data\n","val_data=load_data(val_file_path)\n","\n","#Define Sequence length of model\n","seq_length=1024\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Model Training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","#Initialize the list for storing the losses\n","train_losses = []\n","val_losses = []\n","rouge_scores = []\n","\n","#Declare variable for storing the checkpoint\n","checkpoint_filename = \"model_checkpoint.pt\"\n","\n","# Initialize variables for training\n","best_val_loss = float('inf')\n","epochs_no_improve = 0\n","num_epochs=2\n","patience = 3\n","train_loader = DataLoader(train_data, batch_size=1, shuffle=True)\n","val_loader = DataLoader(val_data, batch_size=1, shuffle=True)\n","\n","for param in summarizer.model.parameters():\n","    param.requires_grad = False\n","\n","for name, param in summarizer.model.named_parameters():\n","  if 'lm_head' in name:  # Unfreeze parameters in the lm_head module\n","        param.requires_grad = True\n","  elif name.startswith('led.decoder.layers.11'):  # Unfreeze the last layer of the decoder\n","        param.requires_grad = True\n","  else:\n","        param.requires_grad = False\n","\n","for epoch in range(num_epochs):\n","  train_loss = summarizer.train_model(train_loader)\n","  avg_train_loss = train_loss / len(train_loader)\n","  train_losses.append(avg_train_loss)\n","\n","  # Validate the model\n","  val_loss,total_rouge1_f1,total_rouge2_f1,total_rougeL_f1,num_samples  = summarizer.validate_model(val_loader)\n","  avg_rouge1_f1 = total_rouge1_f1 / num_samples\n","  avg_rouge2_f1 = total_rouge2_f1 / num_samples\n","  avg_rougeL_f1 = total_rougeL_f1 / num_samples\n","  rouge_scores.append((avg_rouge1_f1, avg_rouge2_f1, avg_rougeL_f1))\n","\n","  avg_val_loss = val_loss / len(val_loader)\n","  val_losses.append(avg_val_loss)\n","  \n","  # CosineAnnealing LR to adjust the learning rate\n","  summarizer.scheduler.step()\n","\n","  # Log metrics to file\n","  summarizer.log_metrics(epoch, avg_train_loss, avg_val_loss, (avg_rouge1_f1, avg_rouge2_f1, avg_rougeL_f1))\n","\n","  print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n","\n","  # Save checkpoint when the val loss improves\n","  if avg_val_loss < best_val_loss:\n","    best_val_loss = avg_val_loss\n","    epochs_no_improve = 0\n","    checkpoint_path = os.path.join('Checkpoints/', checkpoint_filename)\n","    torch.save(model.state_dict(), checkpoint_path)\n","  else:\n","    epochs_no_improve += 1\n","    if epochs_no_improve == patience:\n","      print(\"Early stopping triggered\")\n","      break\n","\n","# Plotting losses\n","\n","plt.plot(train_losses, label='Training Loss')\n","plt.plot(val_losses, label='Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["Model Testing using Test dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","#Load the file from Checkpoint\n","\n","checkpoint_file_path = os.path.join('Checkpoints', 'model_checkpoint.pt')   \n","model.load_state_dict(torch.load(checkpoint_file_path))\n","all_results = []\n","test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n","\n","for data in test_loader:\n","    section_result = summarizer.test_model(data,model)\n","    all_results.append(section_result)\n","\n","# Calculate average ROUGE scores across all sections\n","total_sections = len(all_results)\n","print(\"total sections\",total_sections)\n","total_rouge1 = sum(result.get(\"ROUGE-1 F1\", 0) for result in all_results)\n","total_rouge2 = sum(result.get(\"ROUGE-2 F1\", 0) for result in all_results)\n","total_rougeL = sum(result.get(\"ROUGE-L F1\", 0) for result in all_results)\n","\n","average_rouge1 = total_rouge1 / total_sections\n","average_rouge2 = total_rouge2 / total_sections\n","average_rougeL = total_rougeL / total_sections\n","\n","print(\"Average ROUGE-1 F1:\", average_rouge1)\n","print(\"Average ROUGE-2 F1:\", average_rouge2)\n","print(\"Average ROUGE-L F1:\", average_rougeL)\n","\n","# Save results to JSON file\n","with open(\"summary_results_with_rouge.json\", \"w\") as json_file:\n","    json.dump(all_results, json_file, indent=4)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3.9.13 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"2e392266d6a5626032387dc1b7e3fce21e875c52abef25ac7b7f00de119355c6"}}},"nbformat":4,"nbformat_minor":0}
