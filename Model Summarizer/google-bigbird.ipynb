{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\\# This notebook has the code for inference**\n","\n"],"metadata":{"id":"DMsrySAKzEq6"}},{"cell_type":"code","source":["!pip install transformers\n","!pip install tokenizer\n","!pip install datasets\n","!pip install rouge_score\n","!pip install sentencepiece\n","!pip install rouge"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DjYjTBehuyBg","executionInfo":{"status":"ok","timestamp":1712824401398,"user_tz":-480,"elapsed":70221,"user":{"displayName":"Sudha Ravi","userId":"04078799224499823775"}},"outputId":"c33cb8f2-2c3c-4ef1-f77d-6e1e4fc2df3c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.3)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n","Collecting tokenizer\n","  Downloading tokenizer-3.4.3-py2.py3-none-any.whl (112 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.3/112.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tokenizer\n","Successfully installed tokenizer-3.4.3\n","Collecting datasets\n","  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n","Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Installing collected packages: xxhash, dill, multiprocess, datasets\n","Successfully installed datasets-2.18.0 dill-0.3.8 multiprocess-0.70.16 xxhash-3.4.1\n","Collecting rouge_score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.25.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.2)\n","Building wheels for collected packages: rouge_score\n","  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=7b30176933eb26ab747a38bb47151b4f2b2ab72368c0221c4a462d1e75391a00\n","  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n","Successfully built rouge_score\n","Installing collected packages: rouge_score\n","Successfully installed rouge_score-0.1.2\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n","Collecting rouge\n","  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n","Installing collected packages: rouge\n","Successfully installed rouge-1.0.1\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","import pandas as pd\n","import textwrap\n","import json\n","import os\n","import torch.optim as optim\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","import logging\n","transformers_logger = logging.getLogger(\"transformers\")\n","logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n","from transformers import BigBirdPegasusForConditionalGeneration, AutoTokenizer\n","from datasets import load_dataset, load_metric\n","import torch\n","from rouge import Rouge\n","\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jOMAm4fQ3pRj","executionInfo":{"status":"ok","timestamp":1712824441258,"user_tz":-480,"elapsed":33746,"user":{"displayName":"Sudha Ravi","userId":"04078799224499823775"}},"outputId":"fc1a654a-02f0-4cb9-9d54-330c216d842f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["**This section contains the code for loading the json file after pdf extraction and the model initialization**"],"metadata":{"id":"UM30ip_zePoT"}},{"cell_type":"code","source":["#Load the json file after pdf extraction\n","dataset_path = \"/content/drive/My Drive/Colab Notebooks/CS5242 Project/\"\n","\n","def load_data(file_path):\n","    with open(file_path, 'r') as file:\n","        data = json.load(file)\n","    return data\n","\n","# Join the paths\n","data_file_path = os.path.join(dataset_path, 'testimage16.json')\n","\n","# Load  data\n","pdf_data = load_data(data_file_path)\n","\n","# Device configuration\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","#Define the Model\n","class SummarizationModel:\n","    def __init__(self, model_name, device):\n","        self.model_name = model_name\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n","        self.model = BigBirdPegasusForConditionalGeneration.from_pretrained(model_name).to(DEVICE)\n","        self.config=BigBirdPegasusForConditionalGeneration.from_pretrained(model_name).config\n","\n","#Instantiate the model\n","\n","model_name = \"google/bigbird-pegasus-large-arxiv\"\n","model_summarizer = SummarizationModel(model_name, device=DEVICE)\n","model = model_summarizer.model\n","tokenizer=model_summarizer.tokenizer\n","#print(modelsummarizer.config)\n","\n","#"],"metadata":{"id":"aSo5fz1lnMtO","executionInfo":{"status":"ok","timestamp":1712824819309,"user_tz":-480,"elapsed":12061,"user":{"displayName":"Sudha Ravi","userId":"04078799224499823775"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["**Model Inference**"],"metadata":{"id":"WJxfFkFnwZSl"}},{"cell_type":"code","source":["#Generate Summary for the content using the loaded model\n","def generate_summary(content):\n","        max_length=300\n","        num_beams=4\n","        inputs = tokenizer(content, return_tensors=\"pt\", max_length=1024, truncation=True)\n","        summary_ids = model.generate(inputs.input_ids.to(DEVICE), max_length=max_length, num_beams=num_beams, early_stopping=True)\n","        summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n","        return summary_text\n","\n","#Pase each sections and subsection to generate summaries from the model\n","def process_section(section,results):\n","\n","    # Process the content of the each section\n","    section_summary_results = {}\n","    content = section[\"Text\"]\n","    section_name=section[\"Section\"]\n","    summary_text = generate_summary(content)\n","    section_summary_results[\"Section Name\"] = section_name\n","    section_summary_results[\"Generated Summary\"] = summary_text\n","    results.append(section_summary_results)\n","    print(\"Section Name: \", section_name)\n","    wrapped_output = textwrap.fill(str(summary_text), width=80)\n","    print(\"Generated Summary: \", wrapped_output)\n","        # Process the subsections if they exist\n","    if \"Subsections\" in section:\n","        for subsection in section[\"Subsections\"]:\n","            process_section(subsection,results)\n","\n","\n","# Summarize the section contents and subsection contents\n","def summarize_pdf(pdf_data, output_file):\n","    all_results = []\n","    for section in pdf_data:\n","        process_section(section,all_results)\n","    with open(output_file, \"w\") as json_file:\n","        json.dump(all_results, json_file, indent=4)\n","\n","#Write the final summary to the summary jsonfile\n","output_file = \"summary_results_googlebigbird.json\"\n","summarize_pdf(pdf_data, output_file)\n"],"metadata":{"id":"0sKu1Vxp8pSO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712827633751,"user_tz":-480,"elapsed":2790160,"user":{"displayName":"Sudha Ravi","userId":"04078799224499823775"}},"outputId":"4ef91ea0-d160-4f7a-bbf3-5913691b513c"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Section Name:  1 Introduction\n","Generated Summary:  we present a vision transformer ( vit ) for large scale image recognition.<n> we\n","split an image into patches and provide the sequence of linear embeddings of\n","these patches as an input to a trans- former.<n> image patches are treated the\n","same way as tokens ( words ) in an nlp application.<n> we train the model on\n","image classication in supervised fashion.<n> when trained on mid sized datasets\n","such as imagenet without strong regularization, these mod- els yield modest\n","accuracies of a few percentage points below resnets of comparable size.<n> this\n","seemingly discouraging outcome may be expected : transformers lack some of the\n","inductive biases fine tuning code and pre trained models are available at\n","https://github.com/ google research/vision_transformer arxiv:v jun published as\n","a conference paper at iclr inherent to cnns, such as translation equivariance\n","and locality, and therefore do not generalize well when trained on insufcient\n","amounts of data. however, the picture changes if the models are trained on\n","larger scale ( m m m images ) images. when pre trained on the public imagenet k\n","dataset or the in house jft m dataset, vit approaches or beats state of the art\n","on multiple image recognition benchmarks. in particular, the best model reaches\n","the accuracy of % on imagenet k or the in house jft m m m and cifar\n","Section Name:  2 Related Work\n","Generated Summary:  vanilla transformers were proposed for machine translation, and have since been\n","the state of the art method in many nlp tasks. to apply transformers in the\n","context of image processing, several approximations have been tried in the past,\n","but have failed to scale to realistic input sizes. in a different line of work,\n","sparse transformers (child et al., ) employ scalable approximations to global\n","self attention in order to be applicable to images. in a different line of work,\n","we split an image into xed size patches, linearly embed each of them, add\n","position embeddings, and feed the resulting sequence of vectors to a standard\n","transformer encoder. in order to perform classication, we use the standard\n","approach of adding an extra learnable classication token to the sequence.\n","published as a conference paper at iclr transformer encoder mlp head vision\n","transformer (vit ) * linear projection of flattened patches * extra learnable\n","embedding patch + position embedding class bird ball car... embedded patches\n","multi head attention norm + l x + transformer encoder figure : model overview.\n","we split an image into xed size patches, linearly embed each of them, add\n","position embeddings, and feed the resulting sequence of vectors to a standard\n","transformer encoder. in order to perform classication, we use the standard\n","approach of adding an extra learnable classication token to the sequence.\n","published as a conference paper at\n","Section Name:  3 Method\n","Generated Summary:  we study the dynamics of a self - propelled bose - einstein condensate ( bec )\n","in an optical trap.<n> we show that the bec can be made to behave in a way\n","similar to a classical bose - einstein condensate ( bec ) in a trap with\n","periodic boundary conditions.<n> in particular, we show that the bec can be made\n","to behave in a manner similar to a classical bec, with the boundary conditions\n","depending on the strength of the condensate field.<n> the properties of the bec\n","can be compared to those of the corresponding classical bec. <n> bose - einstein\n","condensates ( becs ) are one of the most studied topics in condensed matter\n","physics @xcite. in this context,<n> the study of the dynamics of a bec in an\n","optical trap is of particular interest.<n> the main advantage of a bec is that\n","it can be made to behave in a way that is similar to a classical bec @xcite.<n>\n","the main difference between a classical bec and a quantum bec is that the bec\n","can be made to behave in a way that is similar to a classical bec @xcite.<n> the\n","main difference between a classical bec and a quantum bec is that the quantum\n","bec can be made to behave in a way similar to a classical bec @xcite @xcite. in\n","the classical be\n","Section Name:  NA\n","Generated Summary:  in this paper we present a new image classification model, called vision\n","transformer ( vit ), which is based on a scalable nlp transformer\n","architecture.<n> we compare it to state of the art cnns ( lecun et al., ), and\n","show that vision transformer outperforms cnns both in terms of accuracy and\n","computational cost.\n","Section Name:  3.2 Fine-tuning and Higher Resolution\n","Generated Summary:  in this short note, we discuss some of the issues that arise when trying to\n","apply a vision transformer to a pre trained position embedding.<n> in\n","particular, we show how to tune the ne to the d structure of the images, and how\n","to perform resolution adjustment and patch extraction. <n> [ [ section ] ] in\n","this note, we discuss some of the issues that arise when trying to apply a\n","vision transformer to a pre trained position embedding.<n> in particular, we\n","show how to tune the ne to the d structure of the images, and how to perform\n","resolution adjustment and patch extraction.\n","Section Name:  4 Experiments\n","Generated Summary:  we study the dynamics of a self - propelled bose - einstein condensate ( bec )\n","in an optical trap.<n> we show that the bec can be made to behave in a way\n","similar to a classical bose - einstein condensate ( bec ) in a trap with\n","periodic boundary conditions.<n> in particular, we show that the bec can be made\n","to behave in a manner similar to a classical bec, with the boundary conditions\n","depending on the strength of the condensate field.<n> the properties of the bec\n","can be compared to those of the corresponding classical bec. <n> bose - einstein\n","condensates ( becs ) are one of the most studied topics in condensed matter\n","physics @xcite. in this context,<n> the study of the dynamics of a bec in an\n","optical trap is of particular interest.<n> the main advantage of a bec is that\n","it can be made to behave in a way that is similar to a classical bec @xcite.<n>\n","the main difference between a classical bec and a quantum bec is that the bec\n","can be made to behave in a way that is similar to a classical bec @xcite.<n> the\n","main difference between a classical bec and a quantum bec is that the quantum\n","bec can be made to behave in a way similar to a classical bec @xcite @xcite. in\n","the classical be\n","Section Name:  NA\n","Generated Summary:  in this paper, we present an extensive evaluation of the state of the art in\n","supervised learning of low - dimensional representations.<n> we consider three\n","supervised learning approaches : resnet, vision transformer ( vit ), and the\n","hybrid.<n> we show that vit achieves state of the art results on many\n","recognition benchmarks.<n> finally, we show that self supervised vit holds\n","promise for the future of unsupervised learning. <n> unsupervised learning of\n","low - dimensional representations is a key challenge in machine learning.<n>\n","while supervised learning approaches such as machine learning ( ml ) @xcite,\n","machine learning ( mle ) @xcite, and supervised learning ( sle ) @xcite have\n","achieved state of the art results in many problems in machine learning,\n","unsupervised learning ( el ) has not yet achieved its full potential. in this\n","paper, we present an extensive evaluation of the state of the art in supervised\n","learning of low - dimensional representations.<n> we consider three supervised\n","learning approaches : resnet, vision transformer ( vit ), and the hybrid.<n> we\n","show that vit achieves state of the art results on many recognition\n","benchmarks.<n> finally, we show that self supervised vit holds promise for the\n","future of unsupervised learning. <n> @xcite unsupervised learning of low -\n","dimensional representations is a key challenge for machine learning in many\n","machine learning,\n","Section Name:  4.1 Setup\n","Generated Summary:  in this conference paper, we report on a two - step supervised learning approach\n","that scales linearly with the number of layers.<n> first, we develop a scalable\n","model transfer pipeline that transfers all training layers from a pre trained\n","baseline model to several downstream tasks.<n> second, we develop a new tuning\n","scheme that scales linearly with the number of layers after training on the\n","respective tasks.<n> experiments show that our approach scales linearly with the\n","number of layers after training on a variety of downstream tasks.\n","Section Name:  4.2 Comparison to State of the Art\n","Generated Summary:  in this paper, we report our results at iclr ours jft ours ik bit l noisy\n","student ( vit h/ ) ( vit l/) ( vit l/) ( resnetx ) imagenet / imagenet real\n","cifar- oxford iiit pets oxford flowers- vtab ( tasks ) tpuv core days k k k k\n","table : comparison with state of the art on popular image classication\n","benchmarks. we rst compare our largest models vit h/ and vit l/ to state of the\n","art cnns from the literature. the smaller vit l/ outperforms bit l ( which is\n","pre trained on the same dataset, while requiring substantially less computa-\n","tional resources to train ) on all tasks, while requiring substantially less\n","compute to pre train. the larger model, vit h/, further improves the\n","performance, especially on the more challenging datasets imagenet, cifar-, and\n","the vtab suite. interestingly, this published as a conference paper at iclr ours\n","jft ours ik bit l noisy student ( vit h/) ( vit l/) ( resnetx ) imagenet /\n","imagenet real cifar- oxford iiit pets oxford flowers- vtab ( tasks ) tpuv core\n","days k k k k k table : comparison with state of\n","Section Name:  4.3 Pre-training Data Requirements\n","Generated Summary:  in this conference paper, we present pre training experiments on imagenet,\n","imagenet k, jft m, and vtab to assess the relative merits of vision\n","transformers, resnets, and hybrid imagenet transformers ( vits ).<n> first, we\n","pre train vit models on increasing size datasets : imagenet, imagenet k, and\n","jft- m. to boost the performance on the smaller datasets, we optimize three\n","basic regularization parameters : weight decay, dropout, and label smoothing.<n>\n","second, we train our models on random subsets of m, m, and m as well as the full\n","jft- m dataset. to save compute, we report few shot linear accuracy instead of\n","full ne- tuning accuracy. published as a conference paper at iclr imagenet\n","imagenet k jft m pre training dataset imagenet top accuracy bit vit b/ vit b/\n","vit l/ vit l/ vit h/ published as a conference paper at iclr imagenet imagenet k\n","jft m pre training dataset imagenet top accuracy bit vit b/ vit b/ vit l/ vit l/\n","vit h/ published as a conference paper at iclr imagenet imagenet k jft m pre\n","training dataset imagenet top accuracy bit vit b/ vit b/ vit b/\n","Section Name:  4.4 Scaling Study\n","Generated Summary:  we report on a study of several convolutional neural network ( cnn ) models on a\n","set of synthetic and real datasets.<n> we compare the performance of different\n","transfer schemes ( resnets, vision transformers, vit ) and pre trained neural\n","networks ( hybrid models ).<n> we find that vision transformers perform better\n","than vit at a fixed pre training cost, and that they do not saturate in size.\n","<n> convolutional neural networks ( cnns ) are a powerful tool in many fields of\n","science, such as neural networks, signal processing, computer vision, and\n","bioinformatics @xcite. in this paper<n> we report on a study of several cnn\n","models on a set of synthetic and real datasets.<n> we compare the performance of\n","different transfer schemes ( resnets, vision transformers, vit ) and pre trained\n","neural networks ( hybrid models ).<n> we find that vision transformers perform\n","better than vit at a fixed pre training cost, and that they do not saturate in\n","size. <n> convolutional neural networks ( cnns ) are a powerful tool in many\n","fields of science, such as neural networks, signal processing, computer vision,\n","and bioinformatics @xcite. in this paper<n> we report on a study of several cnn\n","models on a set of synthetic and real datasets @xcite. in this paper,\n","Section Name:  4.5 Inspecting Vision Transformer\n","Generated Summary:  in this paper, we present a preliminary investigation of the im- age vision\n","transformer.<n> we show that the model learns to encode distance within the\n","image in the similarity of position em- beddings, i.e. closer patches tend to\n","have more similar position em- beddings. finally, a sinusoidal structure is\n","sometimes apparent for larger grids.<n> we analyze the internal representations\n","of the model, and show that self attention allows vit to integrate information\n","across the entire image even in the lowest layers.<n> we compute the average\n","distance in image space across which information is integrated, based on the\n","attention weights.<n> we nd that some heads attend to most of the image already\n","in the lowest layers, showing that the ability to integrate information globally\n","is used by the model.\n","Section Name:  4.6 Self-supervision\n","Generated Summary:  we present a preliminary exploration of supervised and contrastive pre training\n","on the supervised version of the patch prediction task in bert.<n> the\n","supervised pre training is based on the rgb embedding filters of radford et al.\n","<n>, a conference paper published as a conference paper at iclr 2014.<n> we show\n","that the supervised pre training significantly improves the accuracy of the\n","model.\n","Section Name:  5 Conclusion\n","Generated Summary:  we present a new strategy for pre training on large classication datasets using\n","transformers. unlike prior works using self attention in computer vision, we do\n","not introduce image specic inductive biases into the architecture apart from the\n","initial patch extraction step. instead, we interpret an image as a sequence of\n","patches and process it by a standard transformer encoder as used in nlp. thus,\n","vision transformer matches or exceeds the state of the art on many image\n","classication datasets, whilst being relatively cheap to pre train. while these\n","initial results are encouraging, many challenges remain.\n","Section Name:  A Multihead Self-attention\n","Generated Summary:  in this paper, we propose a multihead self attention ( msa ) algorithm for\n","unsupervised @xmath0-body sequences based on pairwise similarity ( kj )\n","representations.<n> the proposed algorithm is an extension of the standard qkv\n","self attention ( sa, vaswani et al., _ proc.<n> natl.<n> acad.<n> sci.<n> usa\n","_<n> * 104 *, 6271 - 6287, 2010 ).<n> the proposed algorithm is a generalization\n","of the msa algorithm of vaswani et al., _<n> proc.<n> natl.<n> acad.<n> sci.<n>\n","usa _<n> * 104 *, 6271 - 6287, 2010.<n> the proposed algorithm is a\n","generalization of the msa algorithm of vaswani et al., _<n> proc.<n> natl.<n>\n","acad.<n> sci.<n> usa _<n> * 104 *, 6271 - 6287, 2010.<n> the basic idea is to\n","compute a weighted sum over all values v in the input sequence. for each element\n","in an input sequence,<n> we compute a weighted sum over all values v in the\n","sequence.<n> the weights are based on the pairwise similarity between two\n","elements of the input elements in the same query\n","Section Name:  B Experiment details\n","Generated Summary:  we study the dynamics of a self - propelled bose - einstein condensate ( bec )\n","in an optical trap.<n> we show that the bec can be made to behave in a way\n","similar to a classical bose - einstein condensate ( bec ) in a trap with\n","periodic boundary conditions.<n> in particular, we show that the bec can be made\n","to behave in a manner similar to a classical bec, with the boundary conditions\n","depending on the strength of the condensate field.<n> the properties of the bec\n","can be compared to those of the corresponding classical bec. <n> bose - einstein\n","condensates ( becs ) are one of the most studied topics in condensed matter\n","physics @xcite. in this context,<n> the study of the dynamics of a bec in an\n","optical trap is of particular interest.<n> the main advantage of a bec is that\n","it can be made to behave in a way that is similar to a classical bec @xcite.<n>\n","the main difference between a classical bec and a quantum bec is that the bec\n","can be made to behave in a way that is similar to a classical bec @xcite.<n> the\n","main difference between a classical bec and a quantum bec is that the quantum\n","bec can be made to behave in a way similar to a classical bec @xcite @xcite. in\n","the classical be\n","Section Name:  C Additional Results\n","Generated Summary:  we present the first scaling experiments of vision transformer models on im-\n","agenet, imagenet k, jftm, and resnet.<n> we compare the performance of different\n","vision transformer models ( vit, b/ vit b/ vit l/ vit l/ vit h/ imagenet resnetx\n","resnet resnetx resnet rx+vit b/ rx+vit b/ rx+vit l ) pre trained on im- agenet,\n","imagenet k, and jftm datasets.<n> we show that vit and b/ vit models pre trained\n","on im- agenet and resnet achieve similar transfer performance, while rx+vit\n","models pre trained on jftm and resnet achieve higher transfer performance than\n","rx+vit models. <n> vision is one of the most important aspects of machine\n","learning, with applications ranging from dimensionality reduction in\n","dimensionality space to pattern recognition in pattern recognition @xcite. in\n","this paper, we present the first scaling experiments of vision transformer\n","models on im- agenet, imagenet k, and jftm datasets.<n> we compare the\n","performance of different vision transformer models ( vit, b/ vit b/ vit l/ vit\n","l/ vit l/ vit h/ imagenet ) pre trained on im- agenet, imagenet k, image\n","Section Name:  D Additional Analyses\n","Generated Summary:  we study the dynamics of a self - propelled bose - einstein condensate ( bec )\n","in an optical trap.<n> we show that the bec can be made to behave in a way\n","similar to a classical bose - einstein condensate ( bec ) in a trap with\n","periodic boundary conditions.<n> in particular, we show that the bec can be made\n","to behave in a manner similar to a classical bec, with the boundary conditions\n","depending on the strength of the condensate field.<n> the properties of the bec\n","can be compared to those of the corresponding classical bec. <n> bose - einstein\n","condensates ( becs ) are one of the most studied topics in condensed matter\n","physics @xcite. in this context,<n> the study of the dynamics of a bec in an\n","optical trap is of particular interest.<n> the main advantage of a bec is that\n","it can be made to behave in a way that is similar to a classical bec @xcite.<n>\n","the main difference between a classical bec and a quantum bec is that the bec\n","can be made to behave in a way that is similar to a classical bec @xcite.<n> the\n","main difference between a classical bec and a quantum bec is that the quantum\n","bec can be made to behave in a way similar to a classical bec @xcite @xcite. in\n","the classical be\n","Section Name:  D.1 SGD vs. Adam for ResNets\n","Generated Summary:  in this conference paper, we compare the performance of adam and sgd as\n","optimizers for pre training on jft.<n> we show that adam performs better than\n","sgd on most datasets and on average.<n> this justies the choice of adam as the\n","optimizer for pre training resnets on jft.\n","Section Name:  D.2 Transformer shape\n","Generated Summary:  in this paper, we study the scaling of image processing by nd algorithms.<n> we\n","show that scaling the dimensions of the model, the patch size, and the effective\n","sequence length all play a role in improving performance. <n> [ [ section ] ]\n","image processing by nd is a well - known and well - studied algorithm.<n> it has\n","been shown to outperform all other algorithms @xcite.<n> nd is based on a vit\n","model with layers, d =, dmlp = and a patch size,. <n> [ [ section ] ] in this\n","paper, we study the scaling of image processing by nd algorithms.<n> we show\n","that scaling the dimensions of the model, the patch size, and the effective\n","sequence length all play a role in improving performance. <n> [ [ section ] ]\n","image processing by nd is a well - known and well - studied algorithm.<n> it has\n","been shown to outperform all other algorithms @xcite. <n> nd is based on a vit\n","model with layers, d =, dmlp = and a patch size,. <n> [ [ section ] ] in this\n","paper, we study the scaling of image processing by nd algorithms.<n> we run\n","ablations on scaling different dimensions of the transformer architecture to nd\n","to nd to nd\n","Section Name:  D.3 Head Type and class token\n","Generated Summary:  in this paper, we present a new approach to the problem of hidden object\n","classification, based on a multi - layer perceptron ( mlp ) with tanh as non\n","linearity in the single hidden layer.<n> this is done in such a way that the non\n","linearity in the single hidden layer is inherited from the transformer model for\n","text, and we use it throughout the main paper. in order to stay as close as\n","possible to the original transformer model, we made use of an additional token,\n","which is taken as image representation.<n> the output of this token is then\n","trans- formed into a class prediction via a small multi layer perceptron ( mlp )\n","with tanh as non linearity in the single hidden layer.<n> this design is\n","inherited from the transformer model for text, and we use it throughout the main\n","paper. <n> we present a new approach to the problem of hidden object\n","classification, based on a multi - layer perceptron ( mlp ) with tanh as non\n","linearity in the single hidden layer.<n> this is done in such a way that the non\n","linearity in the single hidden layer is inherited from the transformer model for\n","text, and we use it throughout the main paper. <n> we present a new approach to\n","the problem of hidden object classification, based on a multi - layer perceptron\n","( vit b ) model, which\n","Section Name:  D.4 Positional Embedding\n","Generated Summary:  we ran ablations on different ways of encoding spatial information using\n","positional embedding. we tried the following cases : providing no positional\n","information : considering the inputs as a bag of patches. <n> -dimensional\n","positional embedding : considering the inputs as a sequence of patches in the\n","raster order (default across all other experiments in this paper ). <n>\n","-dimensional positional embedding : considering the inputs as a grid of patches\n","in two dimensions. in this case, two sets of embeddings are learned, each for\n","one of the axes, x embedding, and y -embedding, each with size d/ then, based on\n","the coordinate on the path in the input, we concatenate the x and y embedding to\n","get the nal positional embedding for that patch. relative positional embeddings\n",": considering the relative distance between patches to en- code the spatial\n","information as instead of their absolute position.\n","Section Name:  D.5 Empirical Computational Costs\n","Generated Summary:  in this paper, we present results on inference speed for a set of state of the\n","art particle - in - cell ( pic ) models, on a mimd fpga, running on acerx.<n> we\n","compare performance of these models with that of resnet models, on the same\n","batch size, and across various architectures.<n> we find that large vit models\n","have a clear advantage in terms of memory efciency over resnet models.\n","Section Name:  D.6 Axial Attention\n","Generated Summary:  axial attention is a simple yet effective technique to run self- attention on\n","large inputs that are organized as multidimensional tensors. in axial attention,\n","each attention mixes information along a particular axis, while keeping\n","information along the other axes independent. along this line, wang et al.<n>\n","proposed the axialresnet model in which all the convolutions with kernel size in\n","a resnet are replaced by axial self attention, i.e. a row and column attention,\n","augmented by relative positional encoding. in this conference paper at iclr\n","total compute imagenet -shot linear top- accuracy axialvit b/ axialvit b/ vit b/\n","resnet axialresnet peak inference speed imagenet - shot linear top- accuracy\n","axial vit b/ axial vit b/\n","Section Name:  D.7 Attention Distance\n","Generated Summary:  vit is a convolutional neural network ( cnns ) that, unlike other cnns, is not\n","limited to small regions in the image. in this paper<n>, we explore how cnns can\n","use self - attention to integrate information across the image.<n> we show that\n","self - attention is essential for vit s ability to integrate information across\n","the network.<n> we also show that self - attention is essential for vit s\n","ability to learn from small patterns in the image. <n> convolutional neural\n","networks ( cnns ) are among the first large - scale techniques to have been\n","developed to solve vision problems @xcite. in cnns,<n> the number of variables\n","used to perform a query grows exponentially with the number of processors\n","@xcite.<n> this exponential growth is due to the fact that cnns use a large\n","number of primitives to perform the same task. in this paper<n>, we explore how\n","convolutional neural networks ( cnns ) can use self - attention to integrate\n","information across the image.<n> we show that self - attention is essential for\n","vit s ability to learn from small patterns in the image. in this paper<n>, we\n","explore how convolutional neural networks ( cnns ) use self - attention to\n","integrate information across the image @xcite. in cnns, the number of cnn\n","Section Name:  D.8 Attention Maps\n","Generated Summary:  in this paper, we present a new method for extracting attentions from a noisy\n","input space.<n> the method is based on the _ attention map _, which maps the\n","attentions from the output token to the input space.<n> we show that the\n","attention map can be used to extract attentions from a noisy input space.<n> we\n","also show that the attention map can be used to build a bridge between the\n","attention map and the input space. <n> [ theorem]acknowledgement [\n","theorem]algorithm [ theorem]axiom [ theorem]claim [ theorem]conclusion [\n","theorem]condition [ theorem]conjecture [ theorem]corollary [ theorem]criterion [\n","theorem]definition [ theorem]example [ theorem]exercise [ theorem]lemma [\n","theorem]notation [ theorem]problem [ theorem]proposition [ theorem]remark [\n","theorem]solution [ theorem]summary in this paper, we present a new method for\n","extracting attentions from a noisy input space.<n> the method is based on the _\n","attention map _, which maps the attentions from the output token to the input\n","space.<n> we show that the attention map can be used to build a bridge between\n","the attention map and the input space. <n> [ theorem]acknowledgement [\n","theorem]algorithm [ theorem]claim [ theorem]conjecture [ theorem]condition [\n","theorem]\n","Section Name:  D.9 ObjectNet Results\n","Generated Summary:  we present a method for the efficient generation of non - classical states of\n","light based on the application of a non - linear optical system.<n> the method\n","is based on the use of a non - linear optical system vit h in combination with a\n","non - linear schrdinger equation ( nlse ) for the generation of non - classical\n","states of light.<n> the non - linearity of the vit h is achieved by the use of a\n","non - linear schrdinger equation ( nlse ) for the generation of non - classical\n","states of light.<n> the non - linearity of the vit h is achieved by the use of a\n","non - linear schrdinger equation ( nlse ) for the generation of non - classical\n","states of light.<n> the non - linearity of the vit h is achieved by the use of a\n","non - linear schrdinger equation ( nlse ) for the generation of non - classical\n","states of light.<n> the non - linearity of the vit h is achieved by the use of a\n","non - linear schrdinger equation ( nlse ) for the generation of non - classical\n","states of light.<n> the non - linearity of the vit h is achieved by the use of a\n","non - linear schrdinger equation ( nlse ) for the generation of non - classical\n","states of light. <n> the non - linearity of the vit h is the result\n","Section Name:  D.10 VTAB Breakdown\n","Generated Summary:  in this paper, we study the performance of a supervised learning algorithm based\n","on the parallel tree algorithm ( tda ) vtab k in terms of number of iterations,\n","number of tests, and number of tests per task.<n> we show that the number of\n","tests per task can be reduced by a factor of @xmath0, and the number of tests\n","per task can be further reduced by a factor of @xmath1, when vtab k is used\n","instead of tda.<n> we also show that the number of tests per task can be further\n","reduced by a factor of @xmath2, and the number of tests per task can be further\n","reduced by a factor of @xmath3, when vtab k is used instead of tda. <n> the\n","parallel tree algorithm ( tda ) @xcite is one of the most widely used learning\n","methods in computer science and machine learning.<n> the tda has been applied to\n","a variety of problems in machine learning and artificial intelligence. in this\n","paper<n>, we study the performance of a supervised learning algorithm based on\n","the parallel tree algorithm ( tda ) vtab k in terms of number of iterations,\n","number of tests, number of tests per task, number of tests per task, and number\n","of tasks per task.<n> we show that the number of iterations can be reduced by a\n","factor of\n","Section Name:  NA\n","Generated Summary:  we present vision transformer ( vit ), a new approach to training convolutional\n","networks ( cnns ) for computer vision tasks.<n> unlike previous attempts to\n","apply attention to cnns, vit applies directly to sequences of image patches and\n","achieves state of the art results on classic image recognition tasks ( imagenet,\n","cifar-, vtab, etc ).\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"b_jZLpmLWuQ2"},"execution_count":null,"outputs":[]}]}