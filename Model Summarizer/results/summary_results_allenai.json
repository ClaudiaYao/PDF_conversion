[
    {
        "Section Name": "1 Introduction",
        "Generated Summary": " self attention based architectures, in particular transformers, have become the model of choice in natural language processing (nlp ). inspired by the transformer scaling successes in nlp, we experiment with applying a standard transformer directly to images, with the fewest possible modications. to do so, we split an image into patches and provide the sequence of linear embeddings of these patches as an input to a trans- former. image patches are treated the same way as tokens (words ) in an nlp application. \n we train the model on image classication in supervised fashion. when trained on mid sized datasets such as imagenet without strong regularization, these mod- els yield modest accuracies of a few percentage points below resnets of comparable size. \n however, the picture changes if the models are trained on larger datasets ( m m images ). \n we nd that large scale training trumps inductive bias. "
    },
    {
        "Section Name": "2 Related Work",
        "Generated Summary": " transformers were proposed by vaswani et al. for machine translation, and have since be- come the state of the art method in many nlp tasks. \n large transformer based models are often pre trained on large corpora and then ne tuned for the task at hand : bert (devlin et al., ) uses a denoising self supervised pre training task, while the gpt line of work uses language mod- eling as its pre training task. \n naive application of self attention to images would require that each pixel attends to every other pixel. with quadratic cost in the number of pixels, this does not scale to realistic input sizes. thus, to apply transformers in the context of image processing, several approximations have been tried in the past. most related to ours is the model of cordonnier et al., which extracts patches of size from the input image and applies full self attention on top. this model is very similar to vit, but our work goes further to demonstrate that large scale pre training makes vanilla transformers competitive with (or even better than) state of the art cnns. "
    },
    {
        "Section Name": "3 Method",
        "Generated Summary": " we report on the results of a detailed analysis of the @xmath0@xmath1@xmath2@xmath3@xmath4@xmath5@xmath6@xmath7@xmath8@xmath9@xmath10@xmath11@xmath12@xmath13@xmath14@xmath15@xmath16@xmath17@xmath18@xmath19@xmath20@xmath21@xmath22@xmath23@xmath24@xmath25@xmath26@xmath27@xmath28@xmath29@xmath30@xmath40@xmath11@xmath11@xmath12@xmath13@xmath14@xmath11@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13"
    },
    {
        "Section Name": "NA",
        "Generated Summary": " vision transformer ( vit ) is a vision transformer ( vit ) that transforms a d sequence of token embeddings into a d image representation. \n vision transformer ( vit ) is a scalable nlp transformer architecture and their efcient implementations can be used almost out of the box. \n vision transformer ( vit ) is a scalable nlp transformer architecture and their efcient implementations can be used almost out of the box. \n vision transformer ( vit ) is a scalable nlp transformer architecture and their efcient implementations can be used almost out of the box. \n vision transformer ( vit ) is a scalable nlp transformer architecture and their efcient implementations can be used almost out of the box. \n vision transformer ( vit ) is a scalable nlp transformer architecture and their efcient implementations can be used almost out of the box. \n vision transformer ( vit ) is a scalable nlp transformer architecture and their efcient implementations can be used almost out of the box. \n vision transformer ( vit ) is a scalable nlp transformer architecture and their efcient implementations can be used almost out of the box. \n vision transformer ( vit ) is a scalable nlp transformer"
    },
    {
        "Section Name": "3.2 Fine-tuning and Higher Resolution",
        "Generated Summary": " in this paper, we present a novel vision transformer for deep learning. \n the vision transformer can handle arbitrary sequence lengths ( up to memory constraints ), however, the pre trained position embeddings may no longer be meaningful. \n we therefore perform d interpolation of the pre trained position embeddings, according to their location in the original image, and perform d patch extraction of the pre trained position embeddings, according to their location in the original image. \n the vision transformer can handle arbitrary sequence lengths ( up to memory constraints ), however, the pre trained position embeddings may no longer be meaningful. \n we therefore perform d interpolation of the pre trained position embeddings, according to their location in the original image, and perform d patch extraction of the pre trained position embeddings, according to their location in the original image. \n the vision transformer can handle arbitrary sequence lengths ( up to memory constraints ), however, the pre trained position embeddings may no longer be meaningful. \n we therefore perform d interpolation of the pre trained position embeddings, according to their location in the original image, and perform d patch extraction of the pre trained position embeddings, according to their location in the"
    },
    {
        "Section Name": "4 Experiments",
        "Generated Summary": " we report on the results of a detailed analysis of the @xmath0@xmath1@xmath2@xmath3@xmath4@xmath5@xmath6@xmath7@xmath8@xmath9@xmath10@xmath11@xmath12@xmath13@xmath14@xmath15@xmath16@xmath17@xmath18@xmath19@xmath20@xmath21@xmath22@xmath23@xmath24@xmath25@xmath26@xmath27@xmath28@xmath29@xmath30@xmath40@xmath11@xmath11@xmath12@xmath13@xmath14@xmath11@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13"
    },
    {
        "Section Name": "NA",
        "Generated Summary": " we evaluate the representation learning capabilities of resnet, vision transformer ( vit ), and the hybrid. to understand the data requirements of each model, we pre train on datasets of varying size and evaluate many benchmark tasks. when considering the computational cost of pre training the model, vit performs very favourably, attaining state of the art on most recognition benchmarks at a lower pre training cost. lastly, we perform a small experiment using self supervision, and show that self supervised vit holds promise for the future. "
    },
    {
        "Section Name": "4.1 Setup",
        "Generated Summary": " we present a new vision transformer ( vit ) class of vision transformers ( cnns ). \n vit transforms intermediate fea- ture maps into vit with patch size of one pixel. \n vit transformers sequence length is inversely proportional to the square of the patch size, thus models with smaller patch size are computationally more expensive. \n we base vit congurations on those used for bert ( devlin et al., ) and we add the larger huge model. \n we base vit congurations on those used for bert ( devlin et al., ) and we add the larger huge model. \n we base vit congurations on those used for bert ( devlin et al., ) and we add the larger huge model. to explore model scalability, we use the ilsvrc- imagenet dataset with k classes and m images ( we refer to it as imagenet in what follows ), its superset imagenet k with k classes and m images ( deng et al., ), and jft (sun et al., ) with k classes and m high resolution images. to explore model scalability, we use the ilsvrc- imagenet dataset with k classes and m images ( we refer to it as imagenet in what follows ), its superset imagenet k with k classes and m images ( deng et al., ), and jft (sun et al"
    },
    {
        "Section Name": "4.2 Comparison to State of the Art",
        "Generated Summary": " we report the results of pre training vision transformer models pre trained on tpuv hardware on popular image classication benchmarks. \n our vision transformer models pre trained on the jft m dataset outperform resnet based baselines on all datasets, while taking substantially less computational resources to pre train than prior state of the art baselines. \n we further improve the performance on the more challenging datasets imagenet, cifar-, and the vtab suite, especially on the more challenging datasets imagenet, cifar-, and the vtab suite. \n interestingly, this published as a conference paper at iclr. "
    },
    {
        "Section Name": "4.3 Pre-training Data Requirements",
        "Generated Summary": " vision transformers outperform resnets with the same compu- tational budget. with fewer inductive biases for vision than resnets, how crucial is the dataset size? \n we perform two series of experiments. first, we pre train our models on random subsets of m, m, and m as well as the full jft- m dataset. \n second, we train our models on random subsets of m, m, and m as well as the full jft- m dataset. \n we do not perform additional regularization on the smaller subsets and use the same hyper parameters for all settings. \n we do not perform additional regularization on the smaller subsets and use the same hyper parameters for all settings. \n we do, however, use early stopping, and report the best validation accuracy achieved during training. \n we do, however, use early stopping, and report the best validation accuracy achieved during training. \n we do, however, use early stopping, and report the best validation accuracy achieved during training. \n we do, however, use early stopping, and report the best validation accuracy achieved during training. \n we do, however, use early stopping, and report the best validation accuracy achieved during training. \n we do, however, use early stopping, and report the best validation accuracy achieved during training. \n we do, however, use early stopping, and report the best validation accuracy achieved during training. "
    },
    {
        "Section Name": "4.4 Scaling Study",
        "Generated Summary": " convolutional local feature processing ( vit ) has emerged as a powerful tool for the analysis of large datasets. \n vit has been shown to outperform convolutional local feature processing ( llp ) on the performance/compute trade off, and vit has been shown to outperform convolutional local feature processing ( llp ) on the performance/compute trade off. here \n we present a controlled scaling study of different models by evaluating transfer performance from jft m. in this setting data size does not bottleneck the models performances, and we assess performance versus pre training cost of each model. the model set includes: resnets, rx, rx rx, rx, rx, rx, pre trained for epochs, plus rx and rx pre trained for epochs ; vision transformers, vit b/, b/, l/, l/, pre trained for epochs, plus l/ and h/ pre trained for epochs ; and hybrids, r+vit b/, b/, l/, l/ pre- trained for epochs, plus r+vit l/ pre trained for epochs ( for hybrids, the number at the end of the model name stands not for the patch size, but for the total dowsampling ratio in the resnet backbone ). "
    },
    {
        "Section Name": "4.5 Inspecting Vision Transformer",
        "Generated Summary": " self attention allows vit to integrate information across the entire image even in the lowest layers. \n we investigate to what degree the network makes use of this capability, and show that the ability to integrate information globally is indeed used by the model. to begin to understand how the vision transformer processes im- age data, we analyze its internal representations. \n self attention allows vit to integrate information across the entire image even in the lowest layers. \n we investigate to what degree the network makes use of this capability. \n we nd that some heads attend to most of the image already in the lowest layers, showing that the ability to integrate information globally is indeed used by the model. other attention heads have consistently small attention distances in the low layers, suggesting that it may serve a similar function as early convolutional layers in cnns. globally, we nd that the model attends to image regions that are semantically relevant for classication. \n specically, we compute the average distance in image space across which information is integrated, based on the attention weights. \n this attention distance is analogous to receptive eld size in cnns. finally, we show that the model learns to encode distance within the image in the similarity of position em- beddings, i.e. closer patches tend to have more similar position em- beddings, i.e. closer patches tend to have more similar position em- beddings.    \n self attention"
    },
    {
        "Section Name": "4.6 Self-supervision",
        "Generated Summary": " transformers show impressive performance on nlp tasks. however, much of their success stems not only from their excellent scalability but also from large scale self supervised pre training. \n we perform a preliminary exploration on masked patch prediction for self supervision, mimicking the masked language modeling task used in bert. with self supervised pre training, our smaller vit b/ model achieves % accuracy on imagenet, a signicant improvement of % to training from scratch, but still % behind supervised pre training. "
    },
    {
        "Section Name": "5 Conclusion",
        "Generated Summary": " we explore the direct application of transformers to image recognition. unlike prior works using self attention in computer vision, we do not introduce image specic inductive biases into the architecture apart from the initial patch extraction step. instead, we interpret an image as a sequence of patches and process it by a standard transformer encoder as used in nlp. this simple, yet scalable, strategy works surprisingly well when coupled with pre training on large datasets. thus, vision transformer matches or exceeds the state of the art on many image classication datasets, whilst being relatively cheap to pre train. "
    },
    {
        "Section Name": "A Multihead Self-attention",
        "Generated Summary": " multihead self attention ( msa ) is an extension of qkv self attention ( sa ) in which we run k self attention operations, called heads, in parallel and project their concatenated outputs. \n multihead self attention ( msa ) is an extension of sa in which we run k self attention operations, called heads, in parallel and project their concatenated outputs. \n msa is an extension of sa in which we run k self attention operations, called heads, in parallel and project their concatenated outputs. \n msa is an extension of sa in which we run k self attention operations, called heads, in parallel and project their concatenated outputs. \n msa is an extension of sa in which we run k self attention operations, called heads, in parallel and project their concatenated outputs. \n msa is an extension of sa in which we run k self attention operations, called heads, in parallel and project their concatenated outputs. \n msa is an extension of sa in which we run k self attention operations, called heads, in parallel and project their concatenated outputs. \n msa is a popular building block for neural archi- tectures. \n"
    },
    {
        "Section Name": "B Experiment details",
        "Generated Summary": " we report on the results of a detailed analysis of the @xmath0@xmath1@xmath2@xmath3@xmath4@xmath5@xmath6@xmath7@xmath8@xmath9@xmath10@xmath11@xmath12@xmath13@xmath14@xmath15@xmath16@xmath17@xmath18@xmath19@xmath20@xmath21@xmath22@xmath23@xmath24@xmath25@xmath26@xmath27@xmath28@xmath29@xmath30@xmath40@xmath11@xmath11@xmath12@xmath13@xmath14@xmath11@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13"
    },
    {
        "Section Name": "C Additional Results",
        "Generated Summary": " we report detailed results corresponding to the gures presented in the paper. we show transfer accuracy on several datasets, as well as the pre training compute ( in ex- aflops ) of different vision transformer ( vit ) models pre trained on im- agenet, imagenet k or jftm. we report detailed results corresponding to the gures presented in the paper. we show transfer accuracy on several datasets, as well as the pre training compute ( in ex- aflops ) of different vit models pre trained on im- agenet, imagenet k or jftm. table corresponds to figure from the paper and shows transfer performance of different vit models pre trained on datasets of increasing size: imagenet, imagenet k, and jft m. table corresponds to published as a conference paper at iclr vit b/ vit b/ vit l/ vit l/ vit h/ imagenet cifar- - cifar- - imagenet - imagenet real - oxford flowers- - oxford iiit pets - imagenet k cifar- cifar- imagenet imagenet real oxford flowers- oxford iiit pets jft m cifar- c"
    },
    {
        "Section Name": "D Additional Analyses",
        "Generated Summary": " we report on the results of a detailed analysis of the @xmath0@xmath1@xmath2@xmath3@xmath4@xmath5@xmath6@xmath7@xmath8@xmath9@xmath10@xmath11@xmath12@xmath13@xmath14@xmath15@xmath16@xmath17@xmath18@xmath19@xmath20@xmath21@xmath22@xmath23@xmath24@xmath25@xmath26@xmath27@xmath28@xmath29@xmath30@xmath40@xmath11@xmath11@xmath12@xmath13@xmath14@xmath11@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13@xmath14@xmath15@xmath12@xmath13"
    },
    {
        "Section Name": "D.1 SGD vs. Adam for ResNets",
        "Generated Summary": " in this paper we show the experiments that motivated the choice of adam as the optimizer used to pre train resnets on jft. here we show the experiments that motivated this choice, namely, we compare the ne tuning published as a conference paper at iclr resnet resnetx dataset with the tuning published as a conference paper at iclr resnet resnetx dataset adam sgd adam sgd imagenet cifar cifar oxford iiit pets oxford flowers- average table : fine tuning resnet models pre trained with adam and sgd. "
    },
    {
        "Section Name": "D.2 Transformer shape",
        "Generated Summary": " in this paper, we present a transformer architecture for very large models. \n we ran ablations on scaling different dimensions of the transformer architecture to nd out which are best suited for scaling to very large models. \n overall, we nd that scaling all dimensions proportionally results in robust improvements without introducing parameters. \n these ndings suggest that compute might be a better predictor of performance than the number of parameters, and that scaling should emphasize depth over width if any. "
    },
    {
        "Section Name": "D.3 Head Type and class token",
        "Generated Summary": " we present an ablation study on positional embeddings with vit b/ model evaluated on imagenet -shot linear. instead of using only image patch embeddings, globally average pooling ( gap ) them, followed by a linear classierjust like resnets nal feature mapperformed very poorly. however, we found that this is neither due to the extra token, nor to the gap operation. instead, published as a conference paper at iclr epochs of training imagenet linear -shot accuracy cls token, lr=e- gap, lr=e- gap, lr=e- figure : comparison of class token and global average pooling classiers both work similarly well, but require different learning rates. the difference in performance is fully explained by the requirement for a different learning rate, see figure  the difference in performance is fully explained by the requirement for a different learning rate, see figure  the ablation study on positional embeddings with vit b/ model evaluated on imagenet -shot linear. instead, published as a conference paper at iclr epochs of training imagenet linear -shot accuracy cls token, lr=e- gap, lr=e- figure : comparison of class"
    },
    {
        "Section Name": "D.4 Positional Embedding",
        "Generated Summary": " we ran ablations on different ways of encoding spatial information using positional embedding. we tried the following cases : providing no positional information: considering the inputs as a bag of patches. -dimensional positional embedding: considering the inputs as a sequence of patches in the raster order. -dimensional positional embedding: considering the inputs as a grid of patches in two dimensions. relative positional embeddings: considering the relative distance between patches to en- code the spatial information as instead of their absolute position. in addition to different ways of encoding spatial information, we also tried different ways of in- corporating this information in our model. to do so, we use - dimensional relative attention, in which we dene the relative distance all possible pairs of patches. thus, for every given pair (one as query, and the other as key/value in the at- tention mechanism ), we have an offset pq pk, where each offset is associated with an embedding. then, we simply run extra attention, where we use the original query ( the content of query ), but use relative positional embeddings as keys. we then use the log- its from the relative attention as a bias term and add it to the logits of the main attention ( content based attention ) before applying the softmax. for the -dimensional and -dimensional positional embeddings, we tried three different cases : add positional embeddings to the inputs right after published as a conference paper at"
    },
    {
        "Section Name": "D.5 Empirical Computational Costs",
        "Generated Summary": " we are building a new class of high performance image processing ( vit ) architectures on a tpuv accelerator. \n vit is a new class of high performance image processing architectures ( flops ) that are based on the same architecture as our hardware. \n vit is a new class of flops based on the same architecture as our hardware. \n vit is a new class of flops based on the same architecture as our hardware. \n vit is a new class of flops based on the same architecture as our hardware. \n vit is a new class of flops based on the same architecture as our hardware. \n vit is a new class of flops based on the same architecture as our hardware. \n vit is a new class of flops based on the same architecture as our hardware. \n vit is a new class of flops based on the same architecture as our hardware. \n vit is a new class of flops based on the same architecture as our hardware. \n vit is a new class of flops based on the same architecture as our hardware. \n vit is a new class of flops based on the same architecture as our hardware. \n vit is a new class of flops based on the same architecture as our hardware. \n vit is a new class of flops based on the same architecture as our hardware. \n vit is a new class of flops based on the same architecture as our hardware. \n"
    },
    {
        "Section Name": "D.6 Axial Attention",
        "Generated Summary": " axial attention is a simple, yet effective technique to run self- attention on large inputs that are organized as multidimensional tensors. in axial attention, each attention mixes information along a particular axis, while keeping information along the other axes independent. along this line, wang et al. ( b ) proposed the axialresnet model in which all the convolutions with kernel size in a resnet are replaced by axial self attention, i.e. a row and column attention, augmented by relative positional encoding. \n we have implemented axialresnet as a baseline model. moreover, we have modied vit to process inputs in the -dimensional shape, instead of a - dimensional sequence of patches, and incorporate axial transformer blocks, in which instead of a self attention followed by an mlp, we have a a row self attention plus an mlp followed by a column self attention plus an mlp. in our experiments, we reproduced the scores reported in (wang et al., b) in terms of accuracy, however, our implementation, similar to the open source implementation, is very slow on tpus. therefore, we were not able to use it for extensive large scale experiments. \n these may be unlocked by a carefully optimized implementation. "
    },
    {
        "Section Name": "D.7 Attention Distance",
        "Generated Summary": " to understand how vit uses self attention to integrate information across the image, we analyzed the average distance spanned by attention weights at different layers. \n this attention distance is analogous to receptive eld size in cnns. average attention distance is highly variable across heads in lower layers, with some heads attending to much of the image, while others attend to small regions at or near the query location. as depth increases, attention distance increases for all heads. in the second half of the network, most heads attend widely across tokens. "
    },
    {
        "Section Name": "D.8 Attention Maps",
        "Generated Summary": " we study the effect of a single token on the attention of vit- l/. \n we show that the attention of vit- l/ can be used to control the attention of vit- l/. \n we show that vit- l/ can be used to control the attention of vit- l/, and that vit- l/ can be used to control the attention of vit- l/. \n we also show that vit- l/ can be used to control the attention of vit- l/, and that vit- l/ can be used to control the attention of vit- l/.    \n the attention of vit- l/ can be used to control the attention of vit- l/. \n vit- l/ can be used to control the attention of vit- l/, and vit- l/ can be used to control the attention of vit- l/. \n vit- l/ can be used to control the attention of vit- l/, and vit- l/ can be used to control the attention of vit- l/  @xcite. \n vit- l/ can be used to control the attention of vit- l/, and vit- l/ can be used to control the attention of vit- l/  @xcite. \n vit- l/ can also be used to control the attention of vit- l/  @xcite. \n vit- l/"
    },
    {
        "Section Name": "D.9 ObjectNet Results",
        "Generated Summary": " agship vit h/@xmath0/@xmath1/@xmath2/@xmath3/@xmath4/@xmath5/@xmath6/@xmath7/@xmath8/@xmath9/@xmath10/@xmath11/@xmath12/@xmath13/@xmath14/@xmath15/@xmath16/@xmath17/@xmath18/@xmath19/@xmath20/@xmath21/@xmath22/@xmath23/@xmath24/@xmath25/@xmath26/@xmath27/@xmath28/@xmath29/@xmath30/@xmath31/@xmath40/@xmath11/@xmath11/@xmath12/@xmath13/@xmath14/@xmath20/@xmath11/@xmath12/@xmath13/@xmath14/@xmath20/@xmath30/@xmath11/@xmath12/@xmath13/@xmath14/@xmath20/@xmath30/@xmath11/@xmath12/@xmath13/@xmath14/@xmath20/@xmath30/@xmath11/@xmath12"
    },
    {
        "Section Name": "D.10 VTAB Breakdown",
        "Generated Summary": " retinopathy is a degenerative disease of the retina. \n retinopathy is characterized by a progressive loss of photoreceptivity and photodissociation ( pda ). \n retinopathy can be caused by the loss of photoreceptivity and photodissociation ( pda ). \n retinopathy can be caused by the loss of photoreceptivity and photodissociation ( pda ). \n retinopathy can be caused by the loss of photoreceptivity and photodissociation ( pda ). \n retinopathy can be caused by the loss of photoreceptivity and photodissociation ( pda ). \n retinopathy can be caused by the loss of photoreceptivity and photodissociation ( pda ). \n retinopathy can be caused by the loss of photoreceptivity and photodissociation ( pda ). \n retinopathy can be caused by the loss of photoreceptivity and photodissociation ( pda ). \n retinopathy can be caused by the loss of photoreceptivity and photodissociation ( pda ). \n retinopathy can be caused by the loss of photoreceptivity and photodissociation ( pda ). \n retinopathy can be caused by the loss of photoreceptivity and photodissociation ( pda ). "
    },
    {
        "Section Name": "NA",
        "Generated Summary": " transformers for image recognition at scale alexey dosovitskiy, lucas beyer, alexander kolesnikov, dirk weissenborn, xiaohua zhai, thomas unterthiner, mostafa dehghani, matthias minderer, georg heigold, sylvain gelly, jakob uszkoreit, neil houlsby, equal technical contribution, equal advising google research, brain team {adosovitskiy, neilhoulsby}@google.com abstract while the transformer architecture has become the de facto standard for natural language processing tasks, its applications to computer vision remain limited. in vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. \n we show that this reliance on cnns is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classication tasks. when pre trained on large amounts of data and transferred to multiple mid sized or small image recognition benchmarks (imagenet, cifar-, vtab, etc ), vision transformer attains excellent results compared to state"
    }
]