[
    {
        "Section Name": [
            "Abstract"
        ],
        "Generated Summary": [
            "The mackey-higson bijection is a natural one-to-one correspondence between all irreducible tempered representations of a group and its cartan motion group. The text discusses the topology of the tempered dual e g and that of the unitary dual c g. It also discusses the continuity of the mackey and Higginbotham bijections. The section discusses the relation between the tempered and unitary representations of the group and the Mackey-Higginbothaman bijection."
        ],
        "ROUGE-1 F1": 68.65671641791045,
        "ROUGE-2 F1": 42.42424242424242,
        "ROUGE-L F1": 55.223880597014926
    },
    {
        "Section Name": [
            "1. Introduction"
        ],
        "Generated Summary": [
            "The Mackey-higson bijection between the tempered dual of a group and the unitary dual of the group is known to be a non-trivial one-to-one correspondence. The bijection induces a homeomorphism between these duals, but it is not continuous. The text discusses the properties of the bijection, focusing on the topology of the component connected to each tempered dual. It concludes with a discussion of the relationship between the mackey bijection and the Baum-connes-kasparov isomorphism for the K-theory of the reduced C-algebra of a reductive algebraic group."
        ],
        "ROUGE-1 F1": 62.61682242990655,
        "ROUGE-2 F1": 32.075471698113205,
        "ROUGE-L F1": 34.57943925233645
    },
    {
        "Section Name": [
            "2. Topology of the tempered dual"
        ],
        "Generated Summary": [
            "The text presents a presentation of the text in the context of Harish-chandras plancherel formula for the category of tempered representations. The presentation is modeled on the P-adic case, more precisely, on schneider and zinks tempered version of the Bernstein decomposition of the Berstein category of representations. It involves a direct product decomposition and a direct sum decomposition. The text also mentions the parallel with the non-archimedean case."
        ],
        "ROUGE-1 F1": 44.15584415584416,
        "ROUGE-2 F1": 14.473684210526317,
        "ROUGE-L F1": 25.974025974025977
    },
    {
        "Section Name": [
            "3. Topology of the motion group dual and remarks on the Mackey-Higson bijection"
        ],
        "Generated Summary": [
            "The text discusses the mackey-higson bijection m : c g and e g. It discusses the notion of a mackey parameter for a unitary irreducible representation of the motion group g and its relation to the Mackey parameter of the reductive group g. The bijection is shown to be a discontinuity point in the construction of the correspondence in the text. It is shown that the bijection can not be a homeomorphism, and that it is never continuous, unless g and g are isomor- phic. The topology of cG is discussed in detail."
        ],
        "ROUGE-1 F1": 60.773480662983424,
        "ROUGE-2 F1": 32.402234636871505,
        "ROUGE-L F1": 29.83425414364641
    },
    {
        "Section Name": [
            "4. Continuity of the Mackey-Higson bijection"
        ],
        "Generated Summary": [
            "The text discusses the construction of tempered representations of the reductive group lseq. The text also discusses the computation of the mackey parameters for each of the tempered representations. It concludes with a discussion of the relationship between tempered representations and the lower kseq-type of the Kirchhoff-Langevin ( VL)representation of lseq, highlighting the importance of the irreducible representation of VLseq in the context of Vlseq and its relation to VL."
        ],
        "ROUGE-1 F1": 40.0,
        "ROUGE-2 F1": 13.114754098360654,
        "ROUGE-L F1": 25.94594594594594
    },
    {},
    {
        "Section Name": [
            "Abstract"
        ],
        "Generated Summary": [
            "The text discusses various notions of poincar e duality in the context of the abstract. The text establishes a number of foundational results related to poinar e spaces and their applications. One application settles an old conjecture of c.T.c.wall in the armative. Another result shows that for any natural number n, there exists a nite Cw pair with the property that the pair fails to satisfy poinearity. Additionally, a relative version of a result of gottlieb is established."
        ],
        "ROUGE-1 F1": 56.96202531645569,
        "ROUGE-2 F1": 19.23076923076923,
        "ROUGE-L F1": 37.974683544303794
    },
    {},
    {},
    {
        "Section Name": [
            "3. The Thom Isomorphism"
        ],
        "Generated Summary": [
            "Poincar e duale duale e duality e dualit.poincar E duale. Poincar, e duali e duala. Poinar, italiana, istituita de duality, e. e. duality. Italia, italia,05.45.+a,04.50.+v,05.-a,03.65.+f,05,05-a,05.,"
        ],
        "ROUGE-1 F1": 7.547169811320753,
        "ROUGE-2 F1": 0.0,
        "ROUGE-L F1": 7.547169811320753
    },
    {},
    {},
    {},
    {},
    {
        "Section Name": [
            "8. Historical Remarks"
        ],
        "Generated Summary": [
            "The text discusses the definition of a relative version of the bration theorem for poincar e pairs. It states that, roughly, in the presence of suitable niteness assumptions, the total space of a bration satises Poincar E duality is unique if and only if its base and ber do. The text also discusses the relationship between these spaces and their bound- aries. Key words and phrases. This section introduces the concept of a space pair. The section discusses the construction of the text, the proof, and the proof of a theorem related to the text. Additionally, the section delves into the definition and proof of skew-commutativity of cup products."
        ],
        "ROUGE-1 F1": 40.16064257028112,
        "ROUGE-2 F1": 4.8582995951417,
        "ROUGE-L F1": 23.293172690763054
    },
    {},
    {},
    {},
    {},
    {
        "Section Name": [
            "Abstract"
        ],
        "Generated Summary": [
            "Cherenkov light imaging in astroparticle physics is exploited by a variety of experimental techniques to detect and measure extraterrestrial particles impinging on earth. A selection of detection principles is discussed and corresponding experiments are presented together with breakthrough-results they achieved. Future develop- ments are highlighted. Key words and phrases include cherenkov detectors, neutrino telescopes, gamma-ray telescopes, cosmic-ray detectors, AMR telescopes, AMRO telescopes,AMRO detectors,AMR telescopes."
        ],
        "ROUGE-1 F1": 56.57894736842106,
        "ROUGE-2 F1": 26.66666666666666,
        "ROUGE-L F1": 44.736842105263165
    },
    {
        "Section Name": [
            "1 Introduction"
        ],
        "Generated Summary": [
            "The section discusses the detection principles of cherenkov radiation in astroparticle physics. The technical details and selected technical results are presented. The section also delves into the technical details of the sensors used in the experiments and summarizes the technical requirements for different types of detectors. The sections also highlight the impact of Cherenkov detection techniques on the physics of neutrino oscillations, gamma-ray astronomy, and high- energy cosmic neutrinos on the development of ground- based gamma- ray astronomy."
        ],
        "ROUGE-1 F1": 42.45283018867924,
        "ROUGE-2 F1": 20.0,
        "ROUGE-L F1": 19.81132075471698
    },
    {
        "Section Name": [
            "2 Ground-based gamma-ray detectors"
        ],
        "Generated Summary": [
            "The study of gamma-ray astronomy requires high- altitude imaging air cherenkov telescopes. They are pointing instruments with a field of view of a few degrees in diameter. they cover a signicant fraction of the sky, albeit with a higher energy threshold than iacts and inferior sensitivity at energies below about tev. gamma-rays below gev are only accessible to satellite experiments. They cover a significant fraction of sky, but require clear, preferentially moon-less nights and sites with negligible light pollution and an elevation of typically km. They can be used to directly measure the gamma- induced particle cascade."
        ],
        "ROUGE-1 F1": 50.77720207253886,
        "ROUGE-2 F1": 17.801047120418847,
        "ROUGE-L F1": 24.87046632124352
    },
    {
        "Section Name": [
            "3 Neutrino telescopes"
        ],
        "Generated Summary": [
            "The section introduces the concept of neutrino telescopes. The basic princi- ple is to observe cherenkov light from charged secondary particles emerging from neutrinos and passing a detector volume lled with a transparent dielec- tric medium and observed by an arrangement of pmts ( due to their comparatively high noise rates, sipms are not suited for neutrINO telescopes). for the low-energy regime ( typically mevmulti-gev), detectors are installed in deep-underground caverns and the pmts cover a large percentage of the detector volume outer surface. for high energies, naturally abundant volumes of water or ice are instrumented with three-dimensional arrays of pmT arrays."
        ],
        "ROUGE-1 F1": 65.38461538461539,
        "ROUGE-2 F1": 31.06796116504854,
        "ROUGE-L F1": 45.19230769230769
    },
    {
        "Section Name": [
            "4 Cosmic-ray and hybrid detectors"
        ],
        "Generated Summary": [
            "The section discusses the use of cherenkov detectors in hybrid detectors for cosmic-ray studies, focusing on the detection of charged particles in air showers. The section discusses hybrid detectors in the context of different detector systems and their application to cosmic rays and gamma-ray astronomy. It also delves into the physics of hybrid detectors and their impact on detector performance, emphasizing the importance of their contribution to the control of systemat- ics and thus to the resulting experimental precision. It concludes with a brief overview of the status of different hybrid detector systems."
        ],
        "ROUGE-1 F1": 35.684647302904565,
        "ROUGE-2 F1": 10.0418410041841,
        "ROUGE-L F1": 19.08713692946058
    },
    {
        "Section Name": [
            "5 Conclusion and Outlook"
        ],
        "Generated Summary": [
            "Cherenkov detectors play a crucial role in gamma-ray, neu- trino and cosmic-ray astroparticle physics. Many of the re- cent breakthrough-results would not have been possible without them, and they are essential for the future experiments being constructed or planned. The section discusses the status and perspectives of cherenkov telescopes and their applications in high-energy neutrino astrophysics, emphasizing the importance of small- and medium-size telescopes for high-performance detectors."
        ],
        "ROUGE-1 F1": 44.73684210526316,
        "ROUGE-2 F1": 17.333333333333336,
        "ROUGE-L F1": 31.57894736842105
    },
    {
        "Section Name": [
            "Abstract"
        ],
        "Generated Summary": [
            "The study of ranking scores is important for quantifying the strength of skills, relevancies or preferences, and prediction of ranking outcomes when ranking scores are estimates of parameters of a statistical model. One of the most popular statistical models for ranking outcomes is the bradley-terry model for paired comparisons, and its extensions to choice and full ranking outcomes. In this paper, we study a popular method for inference of model parameters, namely the maximum likelihood algorithm, for maximum likelihood estimation and maximum a posteriori probability estimation. The convergence rate is shown to be linear with the rate crucially determined by the algebraic connectivity of the matrix of item pair co-occurrences in observed comparison data. The key component of the accelerated mm algorithm is a parameter rescaling performed at each iteration step that is carefully chosen based on theoretical analysis and characterisation of the convergence rate. experimental results, performed on both synthetic and real-world data, demonstrate the identied slow convergence issue of the classic mm algorithm, and show that signicant efciency gains can be obtained by our new proposed method."
        ],
        "ROUGE-1 F1": 56.08108108108109,
        "ROUGE-2 F1": 31.97278911564626,
        "ROUGE-L F1": 37.16216216216216
    },
    {
        "Section Name": [
            "1 Introduction"
        ],
        "Generated Summary": [
            "The text discusses iterative optimization methods for inference of ranking scores. The focus is on mm algorithms, where ranking scores correspond to param- eter estimates of popular bradley-terry fam- ily of models, and proposes an acceler- ated mm algorithm that resolves a slow con- vergence issue found to hold for a classic mm algorithm. Additionally, the text delves into the connection between mm algorithms and ML estimators, highlighting conditions for convergence of mm algorithms for ranking scores inference."
        ],
        "ROUGE-1 F1": 40.74074074074075,
        "ROUGE-2 F1": 13.08411214953271,
        "ROUGE-L F1": 22.22222222222222
    },
    {
        "Section Name": [
            "2 Problem formulation"
        ],
        "Generated Summary": [
            "The text discusses convergence results for maximum likelihood ( ml) and map estimation algorithms in the context of the bradley-terry model for paired comparisons with win-lose outcomes. The paper also delves into the literature on categorical data analysis using this parametrization. The study focuses on different models such as ties, choice, and full ranking outcomes. Maximum likelihood estimation is achieved by minimizing a function and minimiz- ing a surrogate function that majorizes function f. map estimation is obtained by using a bayesian infer- ence framework, which amounts to nding a maximum a posteriori estimate of the param- eter vector under a given prior distribution. The bayesian method introduced by caron and doucet assumes the prior distribution to be of product-form with marginal distributions such that i has a gamma distribution where is the shape parameter and > is the rate parameter."
        ],
        "ROUGE-1 F1": 44.052863436123346,
        "ROUGE-2 F1": 14.222222222222221,
        "ROUGE-L F1": 22.026431718061673
    },
    {
        "Section Name": [
            "3 Convergence rates"
        ],
        "Generated Summary": [
            "The section presents results on the rate of convergence for gradient descent and mm algorithms for ml and map estimation for the bradley-terry model of paired compar- isons. The results provide characteriza- tions of the convergence rates that are equiv- alent to those for the Braddale-Gillespie model up to constant factors. These results are then used to derive convergence rate bounds for the BRADLEY-TERRY model. Additionally, the results presented in this section can be extended to other instances of generalized Bradsales, including the rao-kupper model, the Luce choice model, and the plackett-luce ranking model."
        ],
        "ROUGE-1 F1": 61.05263157894737,
        "ROUGE-2 F1": 39.36170212765957,
        "ROUGE-L F1": 48.421052631578945
    },
    {
        "Section Name": [
            "4 Accelerated MAP inference"
        ],
        "Generated Summary": [
            "The section introduces transformed versions of gradient descent and map estimation algorithms. The key ele- ment is a transformation of the parameter vec- tor estimate in each iteration of an iterative optimization algorithm. The transformation ensures mono- tonic improvement of the optimization objec- tive along the sequence of parameter vector estimates and ensures certain second-order properties of the objective function hold along this sequence. Additionally, the transformed algorithms have to satisfy certain conditions in order to provide a convergence rate guarantee, which we discuss in the following section."
        ],
        "ROUGE-1 F1": 53.503184713375795,
        "ROUGE-2 F1": 25.806451612903224,
        "ROUGE-L F1": 35.6687898089172
    },
    {
        "Section Name": [
            "5 Numerical results"
        ],
        "Generated Summary": [
            "In this section, the convergence times of mm algorithms for different generalized bradley-terry models are evaluated for a collection of real-world datasets. Overall, the numerical results validate that the convergence of the mm algorithm for map estimation can be much slower than for ml estimation, mm algorithm has convergence time that increases as parameter of the prior distribution decreases, and a signicant reduction of the convergence time can be achieved by the accelerated mm algorithm dened in section the code and datasets for reproducing our experiments are available online at: https://github.com/gdmmbt/ acceleratedbradley"
        ],
        "ROUGE-1 F1": 73.84615384615387,
        "ROUGE-2 F1": 42.487046632124354,
        "ROUGE-L F1": 59.48717948717949
    },
    {
        "Section Name": [
            "6 Further discussion"
        ],
        "Generated Summary": [
            "The rate of convergence of gradient descent and mm algo- rithms for ranking scores inference is studied for generalized bradley- terry models. The convergence time is shown to be linear with the convergence time bound o/a), where d is the maximum number of observed comparisons per item and a is the algebraic connectiv- ity of the matrix m of the observed counts of item-pair co-occurrences. The bound is tight. The results provide insights into how the observed comparison data affect the convergence rate. The study also identifies a slow convergence issue for gradient descent for the map estimation problem. The pro- posed acceleration method for map estimation resolves this issue, and yields a convergence time that is bounded by the best of what can be achieved for the ml and map estimation problems."
        ],
        "ROUGE-1 F1": 67.11409395973153,
        "ROUGE-2 F1": 41.89189189189189,
        "ROUGE-L F1": 43.624161073825505
    },
    {
        "Section Name": [
            "7 Proofs and Additional Results"
        ],
        "Generated Summary": [
            "The text discusses the algorithm for ranking scores inference from comparison data. The algorithm involves gradient descent iter- ation update for input with step size. Additionally, the algorithm involves iterating iteratively over a set of points. The text also mentions the algorithm in the context of the text. The section discusses the relation between the algorithm and the theorem. The theorem asserts that the algorithm can be used to improve the performance of the algorithm compared to the original algorithm. The proof involves the computation of a bound on the step size of the iterative update."
        ],
        "ROUGE-1 F1": 32.91139240506329,
        "ROUGE-2 F1": 6.41025641025641,
        "ROUGE-L F1": 20.253164556962027
    },
    {
        "Section Name": [
            "Abstract"
        ],
        "Generated Summary": [
            "The text discusses the main concepts of the sterile neutrino, a hypothetical particle, coined to resolve anomalies in data and retain consistency with observed widths of the w and z bosons. The text also delves into the consequences of the existence of such a particle, paying particular attention to a possible loss of coherence in a model of neutrinos oscillations with a state with a specific set of parameters. The current status of searches for such a state is also briey reviewed. This is a pedagogical introduction."
        ],
        "ROUGE-1 F1": 61.27167630057804,
        "ROUGE-2 F1": 32.748538011695906,
        "ROUGE-L F1": 49.71098265895954
    },
    {
        "Section Name": [
            "1 Introduction"
        ],
        "Generated Summary": [
            "The text delves into the physics of neutrino oscillation in the context of the standard model. The text presents the text discusses the implications of the model in the framework of a three-neutrino model. It emphasizes the importance of wave-packets. The model includes the effects of mixing of leptons from different generations in interactions with the w-boson and charged lepton from different lepton generations. The paper concludes that the model predicts the existence of a quantum eect ob- served at macroscopic scales oscillation of lepton avor neutrinos."
        ],
        "ROUGE-1 F1": 38.91891891891892,
        "ROUGE-2 F1": 14.207650273224044,
        "ROUGE-L F1": 20.54054054054054
    },
    {
        "Section Name": [
            "2 Anomalies in neutrino data"
        ],
        "Generated Summary": [
            "The text discusses the possibility of a sterile neutrino state in the context of a three-neutrino model. The motivation for proposing a sterile state is driven by the existence of an anoma- lies in neutrinos data which can not be described by a three neutrinino model with values of m and |m | given by eq. [ 1,2,3,4,5,6,7,8,9,10,11."
        ],
        "ROUGE-1 F1": 41.904761904761905,
        "ROUGE-2 F1": 23.300970873786405,
        "ROUGE-L F1": 28.57142857142857
    },
    {
        "Section Name": [
            "3 Concept of Sterile Neutrino"
        ],
        "Generated Summary": [
            "The text discusses the mass-generation mechanism in the standard model ( sm) by adding to the sm a neutrino field without adding the fourth left-handed field of leptons. The mass-term is not gauge invariant, leading to massless fermions in the sm. The discussion focuses on the discussion of neutrinos in the context of the sm, focusing on their mass-terms. The text also mentions anomalies in data related to the presence or absence of a specific mass in the data."
        ],
        "ROUGE-1 F1": 41.02564102564102,
        "ROUGE-2 F1": 18.65284974093264,
        "ROUGE-L F1": 24.615384615384617
    },
    {
        "Section Name": [
            "4 Summary"
        ],
        "Generated Summary": [
            "The text discusses main concepts of sterile neutrinos, a yet hypothetical particle, coined to resolve anomalies in neutrino data. The text introduces the concept of a superposition of sterile and inert numbers of sterile fields, focusing on the text's main concepts. The concept is discussed in the framework of an extension of the standard model. The section introduces the concepts of the concept and the main concepts related to the existence and properties of a hypothetical particle. Additionally, the text also delves into the physics of the interaction amplitude of the hypothetical particle and its properties."
        ],
        "ROUGE-1 F1": 39.344262295081975,
        "ROUGE-2 F1": 11.570247933884298,
        "ROUGE-L F1": 26.229508196721312
    },
    {
        "Section Name": [
            "5 Acknowledgments"
        ],
        "Generated Summary": [
            "The text discusses the section on the section describing the section of the section devoted to the section related to the study of the structure of the spin-1/2 system in the context of the theory of spin-3/4 symmetry breaking. The section discusses the relation between the section discussed in the section and the section presented in the technical presentation at the international conference on high-energy physics ( ichep),Geneva, switzerland, jul -,,, - y. Fukuda,"
        ],
        "ROUGE-1 F1": 16.883116883116884,
        "ROUGE-2 F1": 6.578947368421052,
        "ROUGE-L F1": 12.987012987012985
    },
    {
        "Section Name": [
            "Abstract"
        ],
        "Generated Summary": [
            "The text discusses the section on fusible rings in the context of semiprime left goldie rings. The text establishes a new characterization of left- and right-goldie rings by showing that a ring is left-Goldie if and only if it is regular left- or right-fusible and has nite left Goldie dimension. Additionally, the text establishes the existence of a new class of semisprime rings with left-,right- and leftGoldie dimensions."
        ],
        "ROUGE-1 F1": 56.41025641025641,
        "ROUGE-2 F1": 38.260869565217384,
        "ROUGE-L F1": 49.572649572649574
    },
    {
        "Section Name": [
            "1. Introduction"
        ],
        "Generated Summary": [
            "The text discusses the study of fusible rings and related rings. The text introduces the concept of left Fusible and regular left FUSible rings, a class of rings with left quotient rings. It is proved that a new description of semiprime left goldie rings is given in the context of the text. Additionally, it is shown that the property of left fusibility is lifted to matrix rings for rings having left quotients. As a result, the text also introduces a new class of regular FUSIBLE rings."
        ],
        "ROUGE-1 F1": 61.627906976744185,
        "ROUGE-2 F1": 36.470588235294116,
        "ROUGE-L F1": 36.04651162790697
    },
    {
        "Section Name": [
            "2. Results"
        ],
        "Generated Summary": [
            "The text discusses the existence of left fusible rings and their properties. The text also delves into the existence and properties of abelian left P.q.-baer rings. It concludes with a discussion of the existence, uniqueness, and uniqueness of left p.q.q-baer and left P- and P-p.p.rings, and their relationship to other ring types, such as regular P.Q- and p-p-p rings, regular P- or P-q-p Rings,"
        ],
        "ROUGE-1 F1": 49.717514124293785,
        "ROUGE-2 F1": 21.71428571428571,
        "ROUGE-L F1": 33.898305084745765
    },
    {},
    {},
    {},
    {},
    {},
    {
        "Section Name": [
            "Abstract"
        ],
        "Generated Summary": [
            "The text discusses the use of intel xeon phi knights landing in the context of the Intel xeon Phi Knights Landing method. The section introduces the concept of knights landing, focusing on the application of intel X-Phi knights Landing method, which is based on intel x-phi Knights landing. The concept involves the construction of a set of knights with a specific number of knights in a specific configuration. The knights are described in detail, with the focus on the geometry and dynamics of the knights landing."
        ],
        "ROUGE-1 F1": 41.37931034482758,
        "ROUGE-2 F1": 19.580419580419576,
        "ROUGE-L F1": 28.965517241379313
    },
    {
        "Section Name": [
            "1 \u0412\u0432\u0435\u0434\u0435\u043d\u0438\u0435."
        ],
        "Generated Summary": [
            "The text discusses the section discusses the technical aspects of the analysis of the section on the physics of the xeon phi knights landing in the framework of the concept of the knights landing. The text also delves into the study of the impact of the Xeon Phi knights on the structure of the ground state in the context of the Y2K model. The analysis is focused on the section devoted to the calculation of the number of knights in the Y1 and Y2k models."
        ],
        "ROUGE-1 F1": 21.333333333333332,
        "ROUGE-2 F1": 4.0540540540540535,
        "ROUGE-L F1": 20.000000000000004
    },
    {
        "Section Name": [
            "2 \u041f\u043e\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0430 \u0437\u0430\u0434\u0430\u0447\u0438."
        ],
        "Generated Summary": [
            "The text discusses the section discusses the behavior of the spin-1/2 and spin-3/4 spin-2/4 magnetic moments in a magnetic field in the presence of magnetic moments. The magnetic moments are determined by the magnetic moments of the magnetic moment of a magnetic moment in the magnetic field. The spin-4 magnetic moment can be represented as a function of magnetic moment and magnetic moment by a sum rule. It is shown analytically and numerically that magnetic moments can be determined by a specific magnetic moment."
        ],
        "ROUGE-1 F1": 26.845637583892618,
        "ROUGE-2 F1": 5.442176870748299,
        "ROUGE-L F1": 16.10738255033557
    },
    {
        "Section Name": [
            "3 \u041f\u0430\u0440\u0430\u043b\u043b\u0435\u043b\u044c\u043d\u044b\u0439 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u043f\u043e\u0438\u0441\u043a\u0430 \u0434\u0438\u0441\u0441\u043e\u043d\u0430\u043d\u0441\u043e\u0432 PhiDD"
        ],
        "Generated Summary": [
            "The section discusses the section on the section describing the section of the section where the section describes the section in the section about the section. The section also discusses the relation between the section and the section discussed in the previous section, including the section related to the section involving the section mentioned in the sections on the sections of the sections mentioned in section 4 and 5. Additionally, the section detailing the section is discussed in section 5, where the sections are related to each other through the section referred to as the section 4."
        ],
        "ROUGE-1 F1": 17.177914110429448,
        "ROUGE-2 F1": 0.0,
        "ROUGE-L F1": 12.269938650306747
    },
    {
        "Section Name": [
            "4 \u0412\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u044b."
        ],
        "Generated Summary": [
            "The section discusses the computation of the number of points in a 2-dimensional ( 2d)waveguide. The section also delves into the calculation of a 2d waveguide in a 1d ( 1d)plane, focusing on the relation between the size of the waveguide and the area of the 2d plane. Additionally, the section explores the computation in a 3d ( 3d,1d,2d,3d,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,"
        ],
        "ROUGE-1 F1": 15.384615384615383,
        "ROUGE-2 F1": 2.5974025974025974,
        "ROUGE-L F1": 14.1025641025641
    },
    {
        "Section Name": [
            "5 \u0417\u0430\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0435."
        ],
        "Generated Summary": [
            "The task of discords discovery is applied in a wide range of subject domains related to time series: medicine, economics, climate modeling, etc. in this paper, we propose a novel parallel algorithm for discords detection for intel xeon phi Knights landing many-core systems for the case when input data fit in main memory. the algorithm exploits the ability to independently calculate euclidean distances between the subsequences of the time series. computations are paralleled through openMP technology. experimental evaluation confirms the high scalability of the developed algorithm."
        ],
        "ROUGE-1 F1": 54.545454545454554,
        "ROUGE-2 F1": 22.988505747126435,
        "ROUGE-L F1": 29.545454545454547
    },
    {
        "Section Name": [
            "Abstract"
        ],
        "Generated Summary": [
            "The text discusses a new variant of cluster editing, called cluster editing with vertex splitting, whereby a vertex can be split into two or more vertices. This operation allows a vertex to be replaced by two vertices whose combined neighborhood is the neighborhood of the vertex. The text introduces a new graph mod- ication problem, called the cluster editing problem, where the task is to transform a graph into a cluster graph by performing up to k edge additions or deletions. It is np-complete and xed-parameter tractable when parameterized by the total number of allowed vertex- splitting and edge-editing operations. In particular, an o-time algorithm and a k-vertex kernel are obtained."
        ],
        "ROUGE-1 F1": 57.268722466960355,
        "ROUGE-2 F1": 29.333333333333332,
        "ROUGE-L F1": 30.83700440528634
    },
    {
        "Section Name": [
            "Introduction"
        ],
        "Generated Summary": [
            "The text discusses cluster editing with vertex splitting, a variant of cluster editing in which each vertex can be split into multiple clusters. The text explores the problem of determining the order of operations in an optimal solution and explores the parameterized complexity of the problem. It is shown that the problem is np-complete, even on graphs with bounded maximum degree. The study focuses on vertex splitting in an attempt to allow for overlapping clusters. A polynomial kernel is obtained using the notion of critical cliques."
        ],
        "ROUGE-1 F1": 49.03846153846153,
        "ROUGE-2 F1": 22.33009708737864,
        "ROUGE-L F1": 27.884615384615387
    },
    {
        "Section Name": [
            "Preliminaries"
        ],
        "Generated Summary": [
            "The text discusses the problem of determining if a graph is a cluster graph by applying an edit sequence to it. The text introduces the concept of cluster graphs and explores their properties. It also introduces a graph-theoretic approach to computing cluster graphs. It concludes with a proof that cluster editing with vertex splitting is np-complete under exponential-time hypothesis. Additionally, the text introduces a reduction from -sat. The proof involves two gadgets, a variable gadget and a clause gadget. The variable gadget is a wheel graph with two center vertices. The clause gadget is an undirected crown graph."
        ],
        "ROUGE-1 F1": 52.79187817258884,
        "ROUGE-2 F1": 23.58974358974359,
        "ROUGE-L F1": 31.472081218274113
    },
    {
        "Section Name": [
            "NP-hardness"
        ],
        "Generated Summary": [
            "The text editing algorithm for cluster editing with vertex splitting is known to be np-hard on graphs with maximum maximum degree. This section explores the impact of vertex splitting on the hardness of the algorithm. The text editing problem is equivalent to the text editing task in the context of graph theory, with the goal of reducing the complexity of the problem to a graph with maximum degree reduction. The reduction involves reducing the maximum degree of the produced instances to the maximum number of times a variable appears in a clause."
        ],
        "ROUGE-1 F1": 51.61290322580645,
        "ROUGE-2 F1": 23.91304347826087,
        "ROUGE-L F1": 26.881720430107524
    },
    {
        "Section Name": [
            "The Edit-Sequence Approach"
        ],
        "Generated Summary": [
            "The text discusses graph-modication problems with vertex splitting. The text also discusses the relationship between vertex splittings and edge additions. It concludes by discussing the relation between vertex splitting and edge addition. The section concludes with the definition of vertex splitting in terms of edge additions and deletions, showing that vertex splitting can be moved to the front or to the back of the edit sequence, and showing that edge additions can be removed from the front of an edit sequence. The theorem concludes with a proof that the statement holds for any graphmodication problem that adds edges, removes edges, and splits vertices."
        ],
        "ROUGE-1 F1": 58.119658119658126,
        "ROUGE-2 F1": 25.0,
        "ROUGE-L F1": 33.33333333333333
    },
    {
        "Section Name": [
            "Critical Cliques"
        ],
        "Generated Summary": [
            "The text discusses a new problem related to cluster editing with vertex splitting. The text discusses optimal solutions for cluster graphs with vertex splittings and discusses the existence of induced p induced by a certain lemma. The lemma is adapted from lemma by guo with a careful restatement in the context of the new problem. The paper also mentions a weaker version of the lemma in a previous version of this paper, where a slightly stronger lemma was claimed to be true. It was observed by firbas et al."
        ],
        "ROUGE-1 F1": 37.63440860215054,
        "ROUGE-2 F1": 10.869565217391305,
        "ROUGE-L F1": 20.43010752688172
    },
    {
        "Section Name": [
            "A 6k-vertex kernel "
        ],
        "Generated Summary": [
            "The text discusses cluster editing with vertex splitting with vertex splittings. The text presents a kernel for cluster editing by guo. It involves splitting a vertex into two copies, one containing a copy of each vertex corresponding to a node in a connected component in a and other copies of a vertex in an isolated clique. Additionally, the text introduces a kernel based on the critical clique lemma and a similar kernel by Guo. The kernel is shown to be optimal for cluster graphs with at most k vertices. The proof involves three reduction rules, proving that they are safe, that they can be performed exhaustively in linear time, and that their application results in an equivalent instance."
        ],
        "ROUGE-1 F1": 51.50214592274678,
        "ROUGE-2 F1": 19.913419913419915,
        "ROUGE-L F1": 25.75107296137339
    },
    {
        "Section Name": [
            "An FPT algorithm"
        ],
        "Generated Summary": [
            "The text discusses cluster editing with vertex splitting. It involves splitting vertices into multiple cliques in a solution. The text explores the problem of finding a solution where all vertices in the same critical clique belong to the same clique in the solution. Key theorem shows that cluster editing can be solved in linear time in o(k log k + n + m) time. The proof involves computing the kernel for the kernel and computing the time-exponential reduction of the kernel."
        ],
        "ROUGE-1 F1": 44.9438202247191,
        "ROUGE-2 F1": 15.909090909090912,
        "ROUGE-L F1": 26.96629213483146
    },
    {
        "Section Name": [
            "Conclusion"
        ],
        "Generated Summary": [
            "The text discusses the complexity of cluster editing with vertex splitting, which involves splitting a vertex into two vertices. The text explores the notion of vertex splitting in the context of vertex-splitting and explores the applicability of the vertex splitting operation to clustering problems where a data element may have roles in more than one cluster. On the one hand, it is shown that this new problem is np-complete and, assuming the Eth, there are no o-time or poly-time algorithms for it. However, on the other hand, the text also provides a kernel and an algorithm for computing a kernel in which the number of vertices plus edges is in o. The paper concludes with a brief overview of the parameterized complexity of the problem."
        ],
        "ROUGE-1 F1": 50.63291139240506,
        "ROUGE-2 F1": 19.574468085106382,
        "ROUGE-L F1": 29.535864978902953
    },
    {
        "Section Name": [
            "Abstract"
        ],
        "Generated Summary": [
            "The prospects for discovering a top quark decaying into one light higgs boson along with a charm quark in top-quark pair production at hadron colliders are investigated. A general two higgs doublet model is adopted to study the signature of avor changing neutral higgs interactions with t ch, followed by h ww ++ e /t, where h is the cp-even higgs and e/t stands for missing transverse energy from neutrinos. The discovery potential for this fcnh signal and physics background from dominant processes with realistic acceptance cuts as well as tagging and mistagging eciencies."
        ],
        "ROUGE-1 F1": 66.66666666666666,
        "ROUGE-2 F1": 35.23316062176166,
        "ROUGE-L F1": 52.307692307692314
    },
    {
        "Section Name": [
            "I Introduction"
        ],
        "Generated Summary": [
            "The standard model has been successful in explaining almost all experimental data to date, culminating in the discovery of the standard higgs boson at the cern large hadron collider. The lhc has become a top quark factory. The production cross section of top-quark pair is approximately pb in pp collisions with a tev center-of-mass energy energy, and it becomes tt pb at s = tev. The number of (t t) pairs generated would increase to about"
        ],
        "ROUGE-1 F1": 40.42553191489361,
        "ROUGE-2 F1": 12.903225806451612,
        "ROUGE-L F1": 23.404255319148938
    },
    {
        "Section Name": [
            "II The Higgs Signal and Physics Background"
        ],
        "Generated Summary": [
            "The section discusses the production of top quark pair production in pp collisions from gluon fusion and quark-antiquark fusion, followed by one top- and charm-quark decays into a higgs boson and a charm quark, while the other top- quark decay into a charm and a bottom quark. The section also delves into the physics background processes and their impact on the production cross section for the fcnh higgs signal, focusing on pp collisions."
        ],
        "ROUGE-1 F1": 65.27777777777777,
        "ROUGE-2 F1": 45.07042253521127,
        "ROUGE-L F1": 47.22222222222222
    },
    {
        "Section Name": [
            "III Realistic Acceptance Cuts"
        ],
        "Generated Summary": [
            "The section discusses the discovery potential of ppT t tch bjjc++ e /t + x at the center of mass energy of tev. The section delves into the study of this charming fcnh signal from top decays at the lhc. The study includes the selection of background events, cuts on invariant mass of jets, tagging and mistagging eciencies, and higher order qcd corrections. The analysis shows that the minimum cross section needed for signicance at l = fb and ab is less than 10%."
        ],
        "ROUGE-1 F1": 49.35064935064935,
        "ROUGE-2 F1": 24.454148471615717,
        "ROUGE-L F1": 27.705627705627705
    },
    {
        "Section Name": [
            "IV Discovery Potential at the LHC"
        ],
        "Generated Summary": [
            "The discovery potential of the higgs signal at the large hadron collider ( lhc) is analyzed using realistic cuts. The discovery contours at the lhC are presented in the parameter plane of the cross section of the fcnh signal. The analysis suggests an improvement in the reach of atlas at a luminosity of fb, which gets better at higher energies, i.e s = and tev. the discovery potential at the future high energy Lhc with high luminosity is quite promising as it covers the entire parameter space."
        ],
        "ROUGE-1 F1": 50.81967213114754,
        "ROUGE-2 F1": 23.140495867768593,
        "ROUGE-L F1": 31.147540983606557
    },
    {
        "Section Name": [
            "V Conclusions"
        ],
        "Generated Summary": [
            "The text discusses the prospects for discovering the rare decay of the light higgs boson to tree-level fcnh interactions, focusing on the channel where top quarks are pair produced and decay, one haronically and the other through the Fcnh mode. The analysis suggests that the discovery contours at future pp colliders in the plane of [cos,tc] for s = tev, with l = fb, can probe to as low as b,Tch = tc cos."
        ],
        "ROUGE-1 F1": 47.8468899521531,
        "ROUGE-2 F1": 20.28985507246377,
        "ROUGE-L F1": 20.095693779904305
    },
    {
        "Section Name": [
            " Acknowledgments"
        ],
        "Generated Summary": [
            "The section of the text discusses the section on the section describing the section in the text titled, \"The section on section 4.5.4.2.3.4, \" in the context of the section presented in the proceedings of the international conference on high-energy physics, \" held at the university of science and technology of Taiwan, Taipei, Taiwan, on may 4-5,2016. The section discusses the sections on the sections in the section discussed in the technical presentation."
        ],
        "ROUGE-1 F1": 26.573426573426577,
        "ROUGE-2 F1": 4.25531914893617,
        "ROUGE-L F1": 22.377622377622377
    },
    {},
    {},
    {},
    {
        "Section Name": [
            "Abstract"
        ],
        "Generated Summary": [
            "The text discusses three puzzles of the cosmic ray anisotropy in the pev and sub-pev energy region. They are the so called inverse anisotropic behavior, the irregularity in the energy dependence of the amplitude and phase of the RST harmonic and the contribution of the single source. The text also delves into the physics of the rst harmonic in the context of the section on the structure of the universe in the light of cosmological models. The section concludes with a discussion on the contribution from the single sources."
        ],
        "ROUGE-1 F1": 52.554744525547434,
        "ROUGE-2 F1": 32.59259259259259,
        "ROUGE-L F1": 48.17518248175182
    },
    {
        "Section Name": [
            "1 Introduction"
        ],
        "Generated Summary": [
            "The text discusses the study of cosmic ray anisotropy in the sub-pev and pev energy region. The study of the origin of cosmic rays is discussed in the context of the cosmic ray energy spectrum. The text also delves into the section on cosmic ray sources and properties of interstellar medium and their influence on cosmic rays anisotropies. The section discusses the section devoted to the analysis of the energy spectra of sub- andpev cosmic rays and their origin."
        ],
        "ROUGE-1 F1": 43.27485380116959,
        "ROUGE-2 F1": 23.668639053254438,
        "ROUGE-L F1": 26.900584795321635
    },
    {
        "Section Name": [
            "2 Puzzle 1: an inverse anisotropy"
        ],
        "Generated Summary": [
            "The section discusses the effect of an observed inverse anisotropy in the energy spectrum of supernova remnants and pulsars at sub-tev and sub-pev energies. It is shown that the observed phase of maximum supernova fluxes from the inner galaxy occupies the longitude-region between and, i.e. it occupies the region delimited by two dashed lines. This phenomenon is caused by an inverse gradient of the maximum fluxes with respect to the phase expected from the outer galaxy. The origin of this phenomenon is discussed."
        ],
        "ROUGE-1 F1": 42.307692307692314,
        "ROUGE-2 F1": 17.475728155339805,
        "ROUGE-L F1": 22.115384615384613
    },
    {
        "Section Name": [
            "3 Puzzle 2: peculiarity of the amplitude and phase"
        ],
        "Generated Summary": [
            "The amplitude and equatorial phase of the RST harmonic of the cr anisotropy are calculated in the framework of a model with three basic ingredients: the galactic disk, the halo and a single source. The model successfully reproduces the data at energies up to a few pev. It predicts the existence of a phase opposite to that of the background in the energy interval where the amplitude has a minimum and the subsequent rise of the amplitude above this minimum. However, the model does not reproduce the dip in the amplitude observed in the experimental data at energy of loge = and its subsequent rise above this dip."
        ],
        "ROUGE-1 F1": 51.982378854625544,
        "ROUGE-2 F1": 20.444444444444446,
        "ROUGE-L F1": 33.480176211453745
    },
    {
        "Section Name": [
            "4 Puzzle 3: the nature of the Single Source"
        ],
        "Generated Summary": [
            "The existence of a single source has been proposed by us to explain the puzzling sharpness of the knee in the size spectrum of extensive air showers ( see and later publications). The physical basis of this proposal is the evident non-uniformity of the spatial and temporal distributions of sn explosions and subsequent snr. as a result one sn could explode not very long ago and close to the solar system. its contribution to the cr intensity is rather high and it gives rise to a small peak above the background from other snr - it is our single source."
        ],
        "ROUGE-1 F1": 48.372093023255815,
        "ROUGE-2 F1": 23.474178403755865,
        "ROUGE-L F1": 39.06976744186046
    },
    {
        "Section Name": [
            "5 Conclusion"
        ],
        "Generated Summary": [
            "The text discusses the study of the anisotropy of galactic cosmic rays in the range of the pev and sub-pev energy region. The section discusses the origin of cosmic ray anisotropies in the context of stochastic supernova explosions. The text also delves into the origin and evolution of the dipole and dipole dipole anisotropic behavior of cosmic rays. It concludes with a brief overview of the section on the search for cosmic rays with the SK and SK collators."
        ],
        "ROUGE-1 F1": 50.0,
        "ROUGE-2 F1": 29.333333333333332,
        "ROUGE-L F1": 36.84210526315789
    },
    {
        "Section Name": [
            "Abstract"
        ],
        "Generated Summary": [
            "The text discusses the lowest two-sided cell of weighted coxeter groups of rank and the left cells in it. The text also mentions conjectures for the based ring of the cell and the cell of the lower-est cell. In this section, the text mentions the conjectures p p and e p hold for the cell in the text and does some calculation for the lowest- est cell in weighted Coxeter group of rank. Additionally, the construction of the based rings for the cells in the cell are discussed."
        ],
        "ROUGE-1 F1": 47.05882352941176,
        "ROUGE-2 F1": 28.35820895522388,
        "ROUGE-L F1": 35.29411764705882
    },
    {
        "Section Name": [
            "0. Introduction"
        ],
        "Generated Summary": [
            "The text discusses the text discusses a conjecture of G.lusztig on boundness of weighted coxeter groups. The text also mentions the existence of a lowest two-sided cell in the weighted Coxeter group. It is proved that the maximal weight value of the longest elements of the nite parabolic subgroups of a weighted coxoneter group is a bound for the longest element of the Nite subgroup of the weighted coxton group. This conjecture was conjectured by G. LusztIG."
        ],
        "ROUGE-1 F1": 58.42696629213483,
        "ROUGE-2 F1": 30.681818181818176,
        "ROUGE-L F1": 33.70786516853933
    },
    {
        "Section Name": [
            "1. Preliminaries"
        ],
        "Generated Summary": [
            "The text discusses weighted coxeter groups, hecke algebra, two-sided cell, left cell, based ring. Key words and phrases. The text also mentions the conjecture by g.lusztig in the section on weighted coxeseter groups and the conjectured conjecture by jianwei gao in the same section. Additionally, the text mentions the theorem by c. Yang and the conjecture of j. C. Shih in the context of weighted coxoneter groups."
        ],
        "ROUGE-1 F1": 46.753246753246756,
        "ROUGE-2 F1": 11.842105263157896,
        "ROUGE-L F1": 29.87012987012987
    },
    {},
    {
        "Section Name": [
            "3. The lowest two-side cell c0"
        ],
        "Generated Summary": [
            "The text discusses the text presented in the section on weighted coxeter groups. The text presents the text in the form of a table. It also presents the discussion on the text and its relation to the text. Additionally, the text also mentions the text under the title of the section, \"The text discussed in this section is a part of the text discussed by the section mentioned in the text titled, \" The text discussed under the name of section.\" The text also includes the discussion about the text underneath the title."
        ],
        "ROUGE-1 F1": 36.781609195402304,
        "ROUGE-2 F1": 18.6046511627907,
        "ROUGE-L F1": 22.988505747126435
    },
    {
        "Section Name": [
            "4. The based ring of c0"
        ],
        "Generated Summary": [
            "The text discusses the study of the based ring of the lowest two-sided cell of weighted coxeter groups of rank. The text also discusses the computation of the txty function. The section discusses the existence of indecomposable elements and their existence for finite coxeseter groups. The lemmas and proofs are provided to simplify the computation. Additionally, the text also mentions the existence and properties of the Txty functions for finite and infinite coxeseteter graphs."
        ],
        "ROUGE-1 F1": 37.24696356275303,
        "ROUGE-2 F1": 13.061224489795919,
        "ROUGE-L F1": 21.052631578947366
    },
    {},
    {
        "Section Name": [
            "Abstract"
        ],
        "Generated Summary": [
            "The length of an optimal scalar linear index code of a groupcast index coding problem is equal to the minrank of its side information graph. The computation of minrank is an np-hard problem. The number of computations required to compute minrank depends on the number of edges present in the side-information graph. There exists a low rank matrix completion method and clique cover method to nd suboptimal solutions to index coding problems represented by a side-info graph. However, both methods are np-difficult. In this paper, by introducing the notion of Minrank-critical edges, and deriving some properties of minRank, we present a method for reduction of the given minrank computation problem into a smaller problem. also, using these properties, a method is presented for constructing index codes ( not necessarily optimal length) for groupcast indices coding problems. The construction technique is independent of field size and hence can be used to construct index codes over binary fields. The constructed index codes are compared with the best known in the literature."
        ],
        "ROUGE-1 F1": 57.46268656716418,
        "ROUGE-2 F1": 27.06766917293233,
        "ROUGE-L F1": 31.343283582089548
    },
    {
        "Section Name": [
            "I Introduction"
        ],
        "Generated Summary": [
            "An index coding problem comprises of a transmitter with a set of messages and a transmitter that broadcasts coded messages, called the index code, over a noiseless channel. The transmitter can take cognizance of the side-information of the receivers and broadcast coded messages to each receiver using its side- information and the coded messages. The objective is to minimize the number of coded transmissions. The length of an optimal scalar linear index code is dened as the minrankq of its side infor- mation hypergraph. The study of index coding problems is challenging due to the difficulty of computing the rank of a binary matrix. Researchers have proposed various methods to solve the problem. The researchers have proposed heuristic methods to cover the least-difference difference between different cliques in the graph."
        ],
        "ROUGE-1 F1": 52.46913580246913,
        "ROUGE-2 F1": 24.22360248447205,
        "ROUGE-L F1": 31.48148148148148
    },
    {},
    {},
    {
        "Section Name": [
            "IV conclusion and discussions"
        ],
        "Generated Summary": [
            "The text discusses a method to address groupcast index coding problems in the context of the side-information graph. The section discusses the non-critical edges in the graph and the clique cover by using binary operations on adjacency matrix. The text also mentions the method to compute the minrank of the graph in a more efficient way. It also mentions a method for the computation of the noncritical edges. The paper also discusses the properties of minrank and presents a simple heuristic method to nd the Clique cover."
        ],
        "ROUGE-1 F1": 58.76288659793815,
        "ROUGE-2 F1": 33.333333333333336,
        "ROUGE-L F1": 36.082474226804116
    },
    {},
    {},
    {
        "Section Name": [
            "Abstract"
        ],
        "Generated Summary": [
            "The text discusses the existence of solutions to a kirchhoproblem in the presence of a strongly-singular term perturbed by a discontinuous nonlinearity of the heaviside type in the setting of orlicz- sobolev space. The text establishes an optimal condition for existence of w solutions to the problem, which allows to constrain the energy functional to a subset of the space, to apply techniques of convex analysis and generalized gradient in clarke sense. Key words: non-local KirchHoproblems, discontinuous perturbation, -laplacian operator."
        ],
        "ROUGE-1 F1": 64.77272727272727,
        "ROUGE-2 F1": 41.37931034482759,
        "ROUGE-L F1": 59.09090909090909
    },
    {
        "Section Name": [
            "1 Introduction"
        ],
        "Generated Summary": [
            "The text presents equivalent conditions for the existence of three solutions for the quasilinear problem m  Z dx  Z = bu + f in, U = bu in, u > in,U = on with boundary condition still in the sense of the trace. These conditions are linked to an optimal compatibility condition for existence of solution to the strongly-singular problem. The text establishes an optimal condition to the relationship between existence of solutions to the singular problem, independent of the dimension n, in the presence of both a discontinuous nonlinearity of the heaviside type and a non-local term. The study is based on variational methods, using techniques of the generalized gradient in clarke sense."
        ],
        "ROUGE-1 F1": 50.70422535211267,
        "ROUGE-2 F1": 20.85308056872038,
        "ROUGE-L F1": 30.985915492957744
    },
    {
        "Section Name": [
            "2 Non-smooth analysis for locally Lipschitz functional"
        ],
        "Generated Summary": [
            "The section discusses the properties of the functional. The main contribution of the section is establishing appropriated assumptions under the n- function, the non-local term m and the discontinuous function f that make possible to approach (ii = iii), in theorem, via ricceris theorem. Additionally, the section delves into the critical point theory for the functional, focusing on the palais-smale condition for short) if: { un} w, is such that i c and + nv un, v w,,, where n +, then {un} possesses a convergent subsequence."
        ],
        "ROUGE-1 F1": 50.0,
        "ROUGE-2 F1": 25.263157894736842,
        "ROUGE-L F1": 34.375
    },
    {
        "Section Name": [
            "3 Proof of Theorem ??"
        ],
        "Generated Summary": [
            "The section presents a proof of the theorem stating that the generalized gradient of a convex functional at a critical point is a supersolution for the problem. The proof relies on the existence of a unique solution for a specific critical point of the equation. It also involves the proof of a theorem related to the criticality of the convex function at the subdierential of the locally lipschiz functional at the critical point. Additionally, the proof involves proving the equality holds true."
        ],
        "ROUGE-1 F1": 54.117647058823536,
        "ROUGE-2 F1": 27.380952380952383,
        "ROUGE-L F1": 32.94117647058823
    },
    {
        "Section Name": [
            "4 Proof of Theorem ??"
        ],
        "Generated Summary": [
            "The text discusses the equivalence of the concepts of compactness results for minimizing sequences on nehari sets type and the discontinuity of the nonlinearity f in the context of non-local and non-homogeneous operators. The main difficulty in proving these results is to establish the equivalences between the sets a and n. The proof involves borrowing ideas from Yijing, who proved the results for homogeneous operators, and Yijing for the homogeneous case. By using similar ideas as done yijing, the main obstacles to prove these results are overcome."
        ],
        "ROUGE-1 F1": 54.216867469879524,
        "ROUGE-2 F1": 24.390243902439025,
        "ROUGE-L F1": 26.506024096385538
    },
    {
        "Section Name": [
            "Abstract"
        ],
        "Generated Summary": [
            "The text recognition tasks like handwritten character recognition using convolutional neural networks (CNN) is a challenging task for computer vision tasks. CNN architectures such as lenet and alexnet have become very prominent over the last two decades, however the spatial invariance of different kernels has been a prominent issue till now. With the introduction of capsule networks, kernels can work together in consensus with one another with the help of dynamic routing, that combines individual opinions of multiple groups of kernels called capsules to employ equivariance among kernels. The text recognition task is a typical example of such task, highlighting the importance of capsule network for handling different kernels. In the current work, capsule network is implemented on handwritten indic digits and character datasets to show its superiority over networks like lenet."
        ],
        "ROUGE-1 F1": 49.09090909090909,
        "ROUGE-2 F1": 20.18348623853211,
        "ROUGE-L F1": 30.909090909090907
    },
    {
        "Section Name": [
            "I Introduction"
        ],
        "Generated Summary": [
            "The capsule network was proposed to address the challenge of object recognition. The capsule network consists of a group of kernels that work together and pass information to next layers through a mutual agreement that is achieved by dynamic routing of information during the forward pass. The performance of capsule networks is compared with other convolutional neural networks, such as alexnet and lenet. The experiments show that capsule networks outperform other networks in certain indic digit and character datasets. The study also highlights the importance of capsule network for object recognition tasks."
        ],
        "ROUGE-1 F1": 53.27102803738318,
        "ROUGE-2 F1": 26.41509433962264,
        "ROUGE-L F1": 34.57943925233645
    },
    {
        "Section Name": [
            "II CNN Refresher"
        ],
        "Generated Summary": [
            "The text discusses the architecture of convolutional neural networks and their applications in image processing. It describes a schematic diagram with convolution and pooling operations followed by a fully connected network for classication. The network consists of a series of convolutions, activations, and non-linear activations in the middle followed by fully connected layers for classization. A schematic diagram is shown with a typical convolution operation followed by the network's output layer. The final layer is shown to be the output layer, with the number of neurons in this layer being proportional to the total number of channels in the network."
        ],
        "ROUGE-1 F1": 39.166666666666664,
        "ROUGE-2 F1": 16.80672268907563,
        "ROUGE-L F1": 25.0
    },
    {
        "Section Name": [
            "III The Capsule Network"
        ],
        "Generated Summary": [
            "The text presents a schematic diagram of a capsule network, which consists of a primary capsule layer and a digit capsule layer. The network consists of two different capsule layers. The primary capsule layers consist of primary capsules and digit capsules. The digit capsule layers are obtained by calculating digit capsules through dynamic routing. A schematic diagram is provided to highlight the major differences between capsule network and typical convolution networks. The main difference between capsule networks and convolutions is that primary capsules are replaced by digit capsules, whereas convolutions are replaced with a digit capsules layer."
        ],
        "ROUGE-1 F1": 46.93877551020408,
        "ROUGE-2 F1": 18.55670103092784,
        "ROUGE-L F1": 31.63265306122449
    },
    {
        "Section Name": [
            "IV Experimentations and Results"
        ],
        "Generated Summary": [
            "The experiments focus on the implementation of capsule networks for handwritten indic digits and character databases. The experiments have been carried out on different datasets. The results show that capsule networks outperform other famous networks like lenet and alexnet. Results show that combining capsule networks with other networks outperforms other networks in case of certain datasets. Results also show that the combination of all three networks outperformed other networks, with the exception of Telugu digits, where combination of the three networks proved to be the best."
        ],
        "ROUGE-1 F1": 42.458100558659225,
        "ROUGE-2 F1": 19.2090395480226,
        "ROUGE-L F1": 29.050279329608937
    },
    {
        "Section Name": [
            "V Conclusion"
        ],
        "Generated Summary": [
            "The capsule networks have been implemented on handwritten indic digits and character databases. It has been shown that capsule networks are much superior and robust compared to the lenet architecture. The capsule networks can act as a booster when combined with other networks like lenet and alexnet. The best per- formance was achieved by combining Alexnet with capsule networks for most of datasets. Only in case of telugu dataset, combination of all three networks worked the best. From the results it can be concluded that even with times more parameters, the capsule networks failed to capture some information."
        ],
        "ROUGE-1 F1": 55.55555555555555,
        "ROUGE-2 F1": 33.6734693877551,
        "ROUGE-L F1": 43.43434343434343
    },
    {},
    {
        "Section Name": [
            "Abstract"
        ],
        "Generated Summary": [
            "The Peebles-vilenkin quintessential ination model is revisited. It is shown that, at present time, the dark energy density of the universe is approximately about 20 % of the total energy budget. The value of mass m is calculated using the observational value of the power spectrum of the scalar pertur- bations, and the value is calculated by solving the corresponding dynamical system whose initial conditions are taken at the matter-radiation equality and are obtained from three observational data: the red shift at the redshift, the ratio of the matter energy density to the critical one at the present time and the current value of hubble parameter."
        ],
        "ROUGE-1 F1": 47.44186046511628,
        "ROUGE-2 F1": 20.657276995305168,
        "ROUGE-L F1": 36.279069767441854
    },
    {},
    {
        "Section Name": [
            "1 Introduction"
        ],
        "Generated Summary": [
            "The text discusses the section discusses the text describing the section of the section describing the electronic structure of the spin-1/2-dimensional spin-3/4 system in the context of a spin-2/3-dimensional system. The section delves into the structure of spin2/4 systems in the framework of the 3-dimensional ( 3d)symmetric 4-dimensional(4d)dimensional ( 4d)waveguide model, focusing on the 4d spin-4 system."
        ],
        "ROUGE-1 F1": 12.612612612612612,
        "ROUGE-2 F1": 1.834862385321101,
        "ROUGE-L F1": 12.612612612612612
    },
    {},
    {},
    {},
    {},
    {},
    {},
    {},
    {},
    {
        "Section Name": [
            "Abstract"
        ],
        "Generated Summary": [
            "An upper bound for the sum of fractional parts of certain smooth functions is es- tablished. The main feature is an improvement of the main term due to the use of weyls bound for exponential sums and a device used by popov. The text is available at http://www.arxiv.org/abs/10.1103/arxivo-10.1303.0201.html. Additionally, the text can be downloaded from the arxiv website."
        ],
        "ROUGE-1 F1": 44.827586206896555,
        "ROUGE-2 F1": 28.070175438596483,
        "ROUGE-L F1": 41.37931034482759
    },
    {
        "Section Name": [
            "1. Introduction and main result"
        ],
        "Generated Summary": [
            "The text discusses the study of exponential sums of the shape x n<nn) where n is a large number and f is a smooth function. The main purpose of this note is to improve the main term in the cases k for and k {,,,} for. To do this, the study is extended to smooth functions. Key words and phrases. Differential differencing method, Popov's device, exponential sums, F k. The text also mentions a device due to popov, also used in to estimate the sums. The paper concludes with the discussion of the relation between the main and secondary terms."
        ],
        "ROUGE-1 F1": 46.63212435233161,
        "ROUGE-2 F1": 14.659685863874348,
        "ROUGE-L F1": 29.01554404145078
    },
    {
        "Section Name": [
            "2. Technical lemmas"
        ],
        "Generated Summary": [
            "The section discusses sums of certain fractional parts of certain fractionsal parts. The section discusses the behavior of the sum of certain parts of a given fractional part. It also discusses the relationship between the sum and the product of certain sums. It discusses the relation between the product and the sum. It concludes with a lemma relating the sum to the product. The lemma involves a lemmas for the sum, and the lemma for the product, involving the sum in a certain form. It is shown that, under certain conditions, the sum can be expressed in terms of a specific fractional sum."
        ],
        "ROUGE-1 F1": 37.267080745341616,
        "ROUGE-2 F1": 12.578616352201259,
        "ROUGE-L F1": 22.36024844720497
    },
    {
        "Section Name": [
            "3. Proof of Theorem ??"
        ],
        "Generated Summary": [
            "The text presents a proof of the proposition that, for certain sums of certain fractional parts, the sum of certain fractionsal parts can be expressed as a sum of sums of specific fractional sums. The asserted result involves the summation of certain parts of certain sums. It asserts that, by partial summation, the sums can be represented as sums of the parts of a given fractional part. The proof involves a lemma and lemmas involving the choice of certain terms in the lemma."
        ],
        "ROUGE-1 F1": 32.857142857142854,
        "ROUGE-2 F1": 13.043478260869565,
        "ROUGE-L F1": 20.0
    },
    {
        "Section Name": [
            "4. Extension to integer points close to smooth curves"
        ],
        "Generated Summary": [
            "The section introduces the concept of dene r and dene dk. It establishes the existence of a bound for the r of a function under the hypothesis that, under certain conditions, it is known that, for any integer l +, there exists a function r such that, in the case of any integer k, k,k,sk,ck,ck+ such that for any x and any j {k,k +} j f cj j with k = sknk+ dene Dk := k+k. The proof involves the assumption that, if k = or k = and k =, then, under a certain hypothesis f, the function can be represented as a function."
        ],
        "ROUGE-1 F1": 39.325842696629216,
        "ROUGE-2 F1": 6.818181818181819,
        "ROUGE-L F1": 19.101123595505616
    },
    {},
    {
        "Section Name": [
            "Abstract"
        ],
        "Generated Summary": [
            "Cross-platform recommendation aims to improve recommendation accuracy through associating in- formation from different platforms. existing cross- platform recommendation approaches assume all cross-platform information to be consistent with each other and can be aligned. However, there exist two main challenges: i) there exist in- consistencies in cross-Platform association due to platform-specic disparity, and ii) data from dis- tinct platforms may have different semantic granu- larities. In this paper, we propose a model for deep, disparity-preserved deep cross- Platform association, taking platform- specic disparity and granularity difference into consideration. The proposed model em- ploys a partially-connected multi-modal autoen-Coder, which is capable of explicitly capturing platform-speic information, as well as utiliz- ing nonlinear mapping functions to handle gran- ularity differences. extensive experiments for the proposed model on real-world dataset demonstrate that the model outperforms ex- isting methods in terms of various evaluation metrics."
        ],
        "ROUGE-1 F1": 57.49999999999999,
        "ROUGE-2 F1": 31.092436974789923,
        "ROUGE-L F1": 40.0
    },
    {
        "Section Name": [
            "1 Introduction"
        ],
        "Generated Summary": [
            "With the emergence of various online services, peo- ple are now getting used to engaging on different platforms simultaneously in order to meet their increasing diverse in- formation needs. The complementary information from various platforms jointly reects user inter- ests and preferences, providing us with a great opportunity to tackle the data sparsity problem and improve the recom- mendation accuracy through associating information across corresponding author platforms. However, existing works on cross-platform video recommendation ignore inconsistencies in cross- platform association and differences in semantic granulari- ties, two challenging phenomena discovered in this paper. The first challenge is caused by platform-specic disparity, in addition to a users inher- ent personal preferences, its interests shown on different plat- forms also contain platform- specic factors due to different focuses of various platforms. The second challenge is that different platforms may have different semantic granularityities. The paper proposes a novel disparity- preserved deep cross-Platform association model, which employs a partially- connected multi-modal autoencoder to address these challenges. The proposed model is able to handle the inconsistency phenomenon, which is caused due to platform-speic disparities, and solve the granularity difference problem through nonlinear functions. The study demonstrates that"
        ],
        "ROUGE-1 F1": 47.09677419354839,
        "ROUGE-2 F1": 16.883116883116884,
        "ROUGE-L F1": 27.741935483870968
    },
    {
        "Section Name": [
            "2 Related Work"
        ],
        "Generated Summary": [
            "The section discusses the data inconsistency phenomenon in cross-platform data association, highlighting the importance of aligning information across platforms, and proposes a framework for cross-domain data analysis. The proposed model aims to address this issue by categorizing users activities across platforms in a user-centric way. The model utilizes a topic-based approach, where users from different platforms are grouped into groups based on their activities, and then associate these groups in a specific topic space. The study aims to bridge different platforms through exploiting the in- formation from a small number of overlapped crowds. An important aspect of the proposed model is to address the issue of data inconsistency, and propose a novel criterion to assesses the degree of consistency, and embed it into a boosting framework to selectively transfer knowledge."
        ],
        "ROUGE-1 F1": 44.505494505494504,
        "ROUGE-2 F1": 14.917127071823202,
        "ROUGE-L F1": 19.78021978021978
    },
    {
        "Section Name": [
            "3 Measurement and Observation"
        ],
        "Generated Summary": [
            "The text discusses the problem of user-centric cross-platform association, where users interests on different platforms may be diverse and inconsis- tent. This section introduces the concept of user interest disparity. It explores the existence of inconsistency in user interest disparities, highlighting the importance of understanding the relationship between users interests across platforms. The text introduces a model to address this issue, and proposes a method to address the inconsistency through the use of multi-modal autoencoder to conduct disparity-preserved deep cross-Platform association. The paper introduces a new approach to the study of users interests in different platforms using latent Dirichlet allocation, and explores the impact of the inconsistency on user interests. The study demonstrates that users interests may have different granularities across platforms, indicating that two platforms may have distinct semantic granularity. Additionally, through a measurement study on casia-crossosn, a dataset linking user accounts between youtube and twitter, it is observed that users having similar interests on one platform tend to behave quite similarly to randomly sampled users on the other they no longer share similar interests."
        ],
        "ROUGE-1 F1": 45.270270270270274,
        "ROUGE-2 F1": 19.727891156462587,
        "ROUGE-L F1": 22.972972972972972
    },
    {},
    {
        "Section Name": [
            "5 Cross-platform Video Recommendation"
        ],
        "Generated Summary": [
            "The section discusses the section presents the cross-platform video recom- mendation approach based on the proposed dca model. The section also delves into the implementation of the Dca model in the context of the video Recom- Mendation approach. The proposed DCA model is applied to the study of recommedation in video games. It is shown that the DCA approach can be applied to video games with a specific model for video recommendation. Additionally, the section delves further into the section on the video recOMM-Mendation approach in the framework of the dCA model."
        ],
        "ROUGE-1 F1": 18.69158878504673,
        "ROUGE-2 F1": 9.523809523809524,
        "ROUGE-L F1": 16.82242990654206
    },
    {
        "Section Name": [
            "6 Experiments"
        ],
        "Generated Summary": [
            "The section discusses the application of the proposed method to a real-world cross-platform dataset. It compares the proposed approach with several state-of-the-art algorithms to demonstrate the advantages of the approach. The section concludes with the comparison of the presented method with the existing algorithms and the comparison with the experimental results to demonstrate its advantages. The proposed method is applicable to a wide range of datasets, including real-time data mining, image processing, and image processing. Additionally, it can be applied to arbitrary datasets."
        ],
        "ROUGE-1 F1": 37.28813559322034,
        "ROUGE-2 F1": 22.413793103448278,
        "ROUGE-L F1": 33.89830508474576
    },
    {
        "Section Name": [
            "7 Conclusion"
        ],
        "Generated Summary": [
            "The text discusses the existence of inconsistency in cross-platform recommendation. The text also mentions the granularity difference problem, highlighting the need for cross-domain recommendation. However, the text does not address the inconsistency issue. In this paper, by proposing a transfer probabilistic collective factoriza- tion model ( DCA)model, which tackles the inconsistency problem, and proposing a DCA-based cross- platform video recommendation method based on the DCA model, it is demonstrated that DCA can address both the inconsistency and granularity differences problem. The study further demonstrates the superiority of DCA method over state-of-the- art methods."
        ],
        "ROUGE-1 F1": 44.30379746835443,
        "ROUGE-2 F1": 21.794871794871796,
        "ROUGE-L F1": 30.379746835443044
    }
]